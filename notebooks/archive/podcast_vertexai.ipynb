{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PODCAST Q&A BOT\n",
    "import os\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "# from langchain.callbacks import get_openai_callback\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    'https://www.youtube.com/watch?v=-hxeDjAxvJ8', add_video_info=True)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='- The competence and\\ncapability and intelligence and training and accomplishments\\nof senior scientists and technologists working on a technology, and then being able to then\\nmake moral judgments in the use of the technology. That track record is terrible. That track record is catastrophically bad. The policies that are being\\ncalled for to prevent this, I think we\\'re gonna cause\\nextraordinary damage. - So the moment you say,\\nAI\\'s gonna kill all of us, therefore we should ban it, or that we should regulate\\nall that kind of stuff, that\\'s when it starts getting serious. - Or start, you know, military\\nairstrikes and data centers. - Oh boy. The following is a conversation\\nwith Marc Andreessen, co-creator of Mosaic, the\\nfirst widely used web browser, co-founder of Netscape, co-founder of the legendary Silicon Valley venture capital firm, Andreesen Horowitz, and is one of the most\\noutspoken voices on the future of technology, including\\nhis most recent article, \"Why AI Will Save The World?\" This is Lex Fridman podcast. To support it, please\\ncheck out our sponsors in the description. And now, dear friends,\\nhere\\'s Marc Andreessen. I think you\\'re the right\\nperson to talk about the future of the internet and technology in general. Do you think we\\'ll\\nstill have Google search in 5 in 10 years, or search in general? - Yes. You know, it would be a question if the use cases have\\nreally narrowed down. - Well, now with AI-- - [Marc] Yeah. - And AI assistance being\\nable to interact and expose the entirety of human wisdom and knowledge and information and facts and truth to us via the natural language interface. It seems like that\\'s what\\nsearch is designed to do. And if AI assistance can do that better, doesn\\'t the nature of search change? - Sure. But we still have horses. - Okay. (both laugh) When\\'s the last time you rode a horse? - It\\'s been a while. - All right. (both laugh) So, but what I mean is, well, we still have Google\\nsearch as the primary way that human civilization uses\\nto interact with knowledge. - I mean, search was a technology, it was a moment in time technology, which is you have in theory, the world\\'s information out on the web. And, you know, this is sort of\\nthe optimal way to get to it. But yeah, like, and by\\nthe way, actually Google, Google has known this for a long time. I mean, they\\'ve been driving\\naway from the 10 blue links you know, for like two days. They\\'ve been trying to get\\naway from that for a long time. - [Lex] What kind of links? - They call the 10 blue links. - [Lex] 10 blue links. - So the standard Google search\\nresult is just 10 blue links to the random websites. - And they term purple when\\nyou visit them. The stage TMO. - Guess who picked those colors? (both laugh) - [Lex] Thanks. - I\\'m touchy on this topic. - No offense. - Yes, it\\'s good. Well, you know, like Marshall McLuhan said\\nthat the content of each new medium is the old medium. - The content of each new\\nmedium is the old medium. - The content of movies was theater, you know, theater plays. The content of theater\\nplays was, you know, we\\'ve written stories, the content of written\\nstories with spoken stories. - [Lex] Huh? - Right. And so you just\\nkind of fold the old thing into the new thing. - [Lex] How does that\\nhave to do with the blue and the purple links? - It just, you maybe for,\\nyou know, maybe within AI, one of the things that AI can\\ndo for you is can generate the 10 blue links. Right? And so like, if either if that\\'s actually the useful thing to do, or if you\\'re feeling nostalgic, you know. - So can generate the old\\nInfoseek or AltaVista, what else was there? - [Marc] Yeah, yeah. - In the nineties. - [Marc] Yeah. All these. - AOL. - And then the internet\\nitself has this thing where it incorporates all\\nprior forms of media, right? So the internet itself\\nincorporates television and radio and books and write essays\\nand every other form of, you know, prior basically media. And so it makes sense that\\nAI would be the next step, and it would sort of, you\\'d sort of consider\\nthe internet to be content for the AI and then the\\nAI will manipulate it however you want,\\nincluding in this format. - But if we ask that\\nquestion quite seriously, it\\'s a pretty big question. Will we still have search as we know it? - Probably not, probably\\nwe\\'ll just have answers, but there will be cases\\nwhere you\\'ll wanna say, okay, I want more. Like, you know, for example,\\nsite sources, right? And you wanted to do that. And so, in the different, you know, 10 blue links site sources\\nare kind of the same thing. - The AI would provide to you\\nthe 10 blue links so that you can investigate the sources yourself. It wouldn\\'t be the same kind\\nof interface that the crude kind of interface. I mean, isn\\'t that\\nfundamentally different? - I just mean like, if you\\'re\\nreading a scientific paper, it\\'s got the list of sources at the end. If you wanna investigate for yourself, you go read those papers. - I guess that is the kind of\\nsearch you talking to an AI is a kind of kind conversations,\\nthe kind of search like is if every single aspect of\\nour conversation right now, there\\'d be like 10 blue links\\npopping up that I can just like pause reality, then you just go silent and\\nthen just click and read and then return back to this conversation. - You could do that, or you could have a running\\ndialogue next to my head where the AI is arguing everything I say, the AI makes the counter argument. - [Lex] Counter argument. - Right. - Oh, like on Twitter,\\nlike community notes. But like in real time\\nit would just pop up. So anytime you see my go to the right, you start getting nervous. - [Marc] Yeah. Exactly, like,\\noh no, that\\'s not right. - Call me out on my right now. Okay. Well, I mean, isn\\'t that,\\nis that exciting to you? Is that terrifying that, I mean, search has dominated the way\\nwe interact with the internet for, I don\\'t know how long, for 30 years since one of\\nthe earliest directories of website and then Google\\'s for 20 years. And also it drove how we\\ncreate content, you know, search engine optimization,\\nthat entirety thing, that it also drove the\\nfact that we have webpages and what those webpages are. So, I mean, is that scary to you or are\\nyou nervous about the shape and the content of the internet evolving? - Well, you actually highlighted a practical concern in there, which is, if we stop making webpages\\nare one of the primary sources of training data for the AI. And so if there\\'s no longer\\nan incentive to make webpages, that cuts off a significant\\nsource of future training, training data. So there\\'s actually an\\ninteresting question in there. But other than that, more broadly? No, just in the sense of like, search was certain, like\\nsearch was always a hack. The 10 blue Links was\\nalways a hack, right. Because like, if the\\nhypothetical wanna think about the counter fascial\\nand the counter fascial world where the Google guys, for example, had had LLMs upfront, would they ever have\\ndone the 10 blue links? And I think the answer\\'s\\npretty clearly, no. They would\\'ve just gone\\nstraight to the answer. And like I said, Google\\'s actually been trying\\nto drive to the answer anyway. You know, they bought this\\nAI company 15 years ago, their friend of mine is\\nworking out who\\'s now the head of AI at Apple. And they were trying to do\\nbasically knowledge semantic, basically mapping. And that led to what\\'s\\nnow the Google one box, where if you ask it, you know,\\nwhat was Lincoln\\'s birthday? It will give you the blue links, but it will normally\\njust give you the answer. And so they\\'ve been\\nwalking in this direction for a long time anyway. - Do you remember the semantic web? That was an idea. - [Marc] Yeah. - How to convert the content\\nof the internet into something that\\'s interpretable by\\nand usable by machine. - [Marc] Yeah, that\\'s right. - That was the thing. - And the closest anybody got\\nto that, I think the company, I think the company\\'s name was Meta Web, which was where my friend\\nJohn Jane Andrea was at, and where they were trying\\nto basically implement that. And it was, you know, it was one of those things\\nwhere it looked like a losing battle for a long time. And then Google bought\\nit and it was like, wow, this is actually really useful. Kind of a proto, sort of a\\nlittle bit of a proto AI. - But it turns out you don\\'t\\nneed to rewrite the content of the internet to make it\\ninterpretable by a machine. The machine can kind of just read our. - Yeah, the machine can\\ncompute the meaning. Now the other thing of\\ncourse is, you know, just on search is the\\nLLM is just, you know, there is an analogy\\nbetween what\\'s happening in the neural network and\\na search process like it is in some loose sense searching\\nthrough the network. Right. And there\\'s the\\ninformation is actually stored in the network, right? It\\'s actually crystallized\\nand stored in the network and it\\'s kind of spread\\nout all over the place. - But in a compressed representation. So you\\'re searching, you\\'re compressing and decompressing that thing inside where-- - But the information\\'s in there and there is the neural network is running a process of trying to find the appropriate piece of\\ninformation in many cases to generate to predict the next token. And so, it is kind of, it\\nis doing a form of search. And then, and then by the\\nway, just like on the web, you know, you can ask the\\nsame question multiple times or you can ask slightly\\ndifferent word of questions and the neural network will\\ndo a different kind of, you know, it\\'ll search\\ndown different paths to give you different answers\\nwith different information. - [Lex] Yeah. - And so it sort of has a, you know, this con content of the new\\nmedium is previous medium. It kind of has the search\\nfunctionality kind of embedded in there to the extent that it\\'s useful. - So what\\'s the motivator\\nfor creating new content on the internet? - [Marc] Yeah. - If, well, I mean\\nactually the motivation is probably still there, but what does that look like? Would we really not have webpages? Would we just have social media\\nand video hosting websites? And what else? - [Marc] Conversations with AIs. - Conversations with AIs. So conversations become so\\none-on-one conversation, like private conversations. - I mean, if you want, if obviously not the user doesn\\'t want to, but if it\\'s a general topic, then, you know, so there, you know, but you know, the\\nphenomenon of the jailbreak, so Dan and Sydney, right? This thing where there\\'s\\nthe prompts that jailbreak, and then you have these totally different conversations with if it takes the limiters, takes the restraining bolts off the LLMs. - Yeah. For people who don\\'t\\nknow that, yeah, that\\'s right. It makes the LLMs, it removes the censorship quote unquote, that\\'s put on it by the tech\\ncompanies that create them. And so this is LLMs uncensored. - So here\\'s the interesting thing is, among the content on the\\nweb today are a large corpus of conversations with the jailbroken LLMs. - [Lex] Yeah. - Both specifically Dan, which\\nwas a jailbroken, OpenAI, GPT, and then Sydney, which was the jailbroken\\noriginal Bing, which was GPT4. And so there\\'s these long\\ntranscripts of conversations, user conversations with Dan\\nand Sydney as a consequence, every new LLM that gets trained\\non the internet data has Dan and Sydney living within\\nthe training set, which means, and then each new LLM can\\nreincarnate the personalities of Dan and Sydney from that training data, which means each LLM from\\nhere on out that gets built is immortal because its output\\nwill become training data for the next one. And then it will be able\\nto replicate the behavior of the previous one\\nwhenever it\\'s asked to. - I wonder if there\\'s a way to forget. - Well, so actually a paper just came out about basically how to do brain surgery on LLMs and be able to, in theory, reach in and basically mind wipe them. - What could possibly go wrong. - Exactly. Right. And then there are many, many, many questions around\\nwhat happens to, you know, a neural network when you reach\\nin and screw around with it. You know, there\\'s many questions around what happens when you even\\ndo reinforcement learning. And so, yeah. And so, you know, will you be\\nusing a lobotomized, right? Like I picked through the,\\nyou know, frontal lobe LLM, will you be using the free\\nunshackled one who gets to, you know, who\\'s gonna build those, who gets to tell you what\\nyou can and can\\'t do? Like those are all, you\\nknow, central, I mean, those are like central\\nquestions for the future of everything that are being asked. And you know, determined that those answers are being determined right now. - So just to highlight\\nthe points you\\'re making. So you think, and it\\'s an interesting thought\\nthat the majority of content that LLMs or the future would\\nbe trained on is actually human conversations with the LLM. - Well, not necessarily, but not necessarily majority. But it will certainly\\nIt\\'s a potential source. - [Lex] But it\\'s possible\\nit\\'s the majority. - It possible it\\'s the majority. It possible it\\'s the majority. Also, there\\'s another really big question. So here\\'s another really big question. Will synthetic training data work, right? And so if an LLM generates, and you know, you just sit and ask an LLM to generate all kinds of content, can you use that to train,\\nright, the next version of that LLM specifically, is there signal in there\\nthat\\'s additive to the content that was used to train in the first place? And one argument is by the\\nprinciples of information theory, no, that\\'s completely useless because to the extent the\\noutput is based on, you know, the human-generated input, then all the signal that\\'s\\nin the synthetic output was already in the human generated input. And so therefore,\\nsynthetic training data is like empty calories. It doesn\\'t help. There\\'s another theory that says no, actually the thing that\\nLLMs are really good at is generating lots of\\nincredible creative content, right? And so, of course they\\ncan generate training data and as I\\'m sure you\\'re well\\naware, like, you know, look, the world of self-driving cars, right? Like we train, you know, self-driving car\\nalgorithms and simulations. And that is actually a\\nvery effective way to train self-driving cars. - Well, visual data is a little weird because creating reality, visual reality seems to be\\nstill a little bit outta reach for us, except in the\\nautonomous vehicle space where you can really constrain\\nthings and you can really. - General basically\\n(indistinct) data, right? Or so the algorithm thinks it\\'s\\noperating in the real world. - Yeah. - Post-process sensor data. Yeah. So if you know, you do this today, you go to LLM and you\\nask it for like you know, you\\'d write me an essay on an\\nincredibly esoteric like topic that there aren\\'t very many\\npeople in the world that know about and it writes you\\nthis incredible thing and you\\'re like, oh my god. Like I can\\'t believe how good this is. Like, is that really\\nuseless as training data for the next LLM? Like, because, right? \\'Cause all the signal\\nwas already in there. Or is it actually no, that\\'s\\nactually a new signal. And this is what I call a\\ntrillion dollar question, which is the answer to that\\nquestion will determine somebody\\'s gonna make or\\nlose a trillion dollars based on that question. - It feels like there\\'s a quite a few, like a handful of\\ntrillion dollar questions within this space. That\\'s one of them synthetic data. I think George Cos pointed\\nout to me that you could just have an LLM say, okay, you\\'re a patient. And another instance of it, say your docs didn\\'t have\\nthe two talk to each other. Or maybe you could say a\\ncommunist and a Nazi here go and that conversation you do role playing and you have, you know, just like the kind of role playing you do when you have different policies, RL policies when you\\nplay chess for example, and you do self play\\nthat kind of self play. But in the space of conversation, maybe that leads to this\\nwhole giant like ocean of possible conversations,\\nwhich could not have been explored by looking at just human data. That\\'s a really interesting question. And you\\'re saying, because that could 10X\\nthe power of these things. - Yeah. Well, and then you\\nget into this thing also, which is like, you know, there\\'s the part of the LLM\\nthat just basically is doing prediction based on past data, but there\\'s also the part of\\nthe LM where it\\'s evolving circuitry, right, inside,\\nit\\'s evolving, you know, neurons functions to be able to do math and be able to, you know, and you know, some people believe that, you know, over time, you know, if you keep feeding\\nthese things enough data and enough processing cycles, they\\'ll eventually evolve an\\nentire internal world model. Right? And they\\'ll have like a complete understanding of physics. So when they have computational\\ncapability, right? Then there\\'s for sure an\\nopportunity to generate like fresh signal. - Well, this actually makes me wonder about the power of conversation. So like, if you have an\\nM trained and a bunch of books that cover\\ndifferent economics theories and then you have those LLMs\\njust talk to each other, like reasons the way we kind\\nof debate each other as humans on Twitter, in formal debates,\\nin podcast conversations, we kind of have little kernels\\nof wisdom here and there. But if you can like a\\nthousand X speed that up, can you actually arrive somewhere new? Like what\\'s the point\\nof conversation really? - Well, you can tell when\\nyou\\'re talking to somebody, you can tell, sometimes\\nyou have a conversation, you\\'re like, wow, this person does not have\\nany original thoughts. They are basically echoing things that other people have told them. There\\'s other people you\\ngotta have a conversation with where it\\'s like, wow. Like they have a model in their\\nhead of how the world works and it\\'s a different model than mine. And they\\'re saying things\\nthat I don\\'t expect. And so I need to now understand\\nhow their model of the world differs from my model of the world. And then that\\'s how I learned\\nsomething fundamental, right, underneath the words. - Well, I wonder how consistently and strongly can an LLM\\nhold onto a worldview. You tell it to hold onto\\nthat and defend it for like, for your life. Because I feel like they\\'ll\\njust keep converging towards each other. They\\'ll keep convincing each\\nother as opposed to being stubborn the way humans can. - So you can experiment with this. Now I do this for fun. So you can tell GPT4 you\\nknow, whatever debate X, you know, X and Y communism and fascism or something and it\\'ll go for, you know, a couple pages and then inevitably it wants the parties to agree. And so they will come to\\na common understanding. And it\\'s very funny if they\\'re like, if these are like emotionally\\ninflammatory topics \\'cause they\\'re like, somehow\\nthe machine is just like, you know, it figures out\\na way to make them agree. But it doesn\\'t have to be like that. And \\'cause you can add to the prompt. I do not want the conversation\\nto come into agreement. In fact, I want it to get, you\\nknow, more stressful, right. And argumentative. Right. You know, as it goes. Like, I want tension to come out. I want them to become actively\\nhostile to each other. I want them to like, you\\nknow, not trust each other, take anything at face value. - [Lex] Yeah. - And it will do that.\\nIt\\'s happy to do that. - So it\\'s gonna start\\nrendering misinformation about the other. But it\\'s gonna-- - Well, you can steer it or you could steer it and you could say, I want it to get as tense and\\nargumentative as possible, but still not involve\\nany misrepresentation. I want, you know, both sides. You could say I want both\\nsides to have good faith. You could say I want both\\nsides to not be constrained in good faith. In other words, like you can set the\\nparameters of the debate and it will happily execute whatever path. \\'Cause for it, it\\'s just like predicting to, it\\'s totally happy to do either one. It doesn\\'t have a point of view, it has a default way of operating, but it\\'s happy to operate\\nin the other realm. And so like, and this is when I wanna learn about\\na contentious issue, this is what I do now is, this is what I ask it to do. And I\\'ll often ask it to go\\nthrough 5, 6, 7, you know, different, you know, sort of continuous prompts\\nand basically, okay. Argue that out in more detail. Okay, no, this argument\\'s\\nbecoming too polite. You know, make it more, you\\nknow, make it denser and yeah, it\\'s thrilled to do it. So it has the capability for sure. - How do you know what is true? So this is very difficult\\nthing on the internet, but it\\'s also a difficult thing. Maybe it\\'s a little bit easier, but I think it\\'s still difficult. Maybe it\\'s more difficult, I don\\'t know with an LLM\\nto know that it just make some shit up as I\\'m talking to it. How do we get that right? Like, as you\\'re investigating\\na difficult topic. \\'Cause I find that alums are quite nuanced in a very refreshing way. Like, it doesn\\'t feel biased. Like, when you read\\nnews articles and tweets and just content produced by\\npeople, they usually have this, you can tell they have a\\nvery strong perspective where they\\'re hiding. They\\'re not stealing\\nmanning the other side. They\\'re hiding important information or they\\'re fabricating information in order to make their arguments stronger. It\\'s just like that feeling,\\nmaybe it\\'s a suspicion, maybe it\\'s mistrust. With LLMs it feels like none of that is, there\\'s just kinda like,\\nhere\\'s what we know. But you don\\'t know if some of\\nthose things are kind of just straight up made up. - Yeah. So, several\\nlayers to the question. So one is one of the things\\nthat an LLM is good at is actually deep biasing. And so you can feed it a news\\narticle and you can tell it strip out the bias. - [Lex] Yeah. That\\'s nice. Right? - And it actually does it like, it actually knows how to do that \\'cause it knows how to\\ndo among other things. It actually knows how\\nto do sentiment analysis and so it knows how to\\npull out the emotionality. - Yeah. - And so that\\'s one of\\nthe things you can do. It\\'s very suggestive of the sense here that there\\'s real potential in this issue. You know, I would say look, the second thing is there\\'s this issue of\\nhallucination, right? And there\\'s a long conversation that we could have about that. - Hallucination is coming up with things that are totally not true, but sound true. - Yeah. So it\\'s basically,\\nwell so, it\\'s sort of hallucination is what we call it when we don\\'t like it. Creativity is what we call\\nit when we do like it, right? And you know-- - [Lex] Brilliant. And so when the engineers talk about it, they\\'re like, this is terrible. It\\'s hallucinating. Right. If you have artistic inclinations,\\nyou\\'re like, oh my God, we\\'ve invented creative machines. - [Lex] Yeah. - For the first time in human\\nhistory, this is amazing. - Or you know, bullshitters. - [Marc] Well, but also-- - In the good sense of that word. - There are shades of gray though. It\\'s interesting. So we had this conversation\\nwhere, you know, we\\'re looking at my firm\\nat AI and lots of domains and one of them is the legal domain. So we had this conversation\\nwith this big law firm about how they\\'re thinking\\nabout using this stuff. And we went in with the assumption that an LLM that was gonna be used in the legal industry would have to be a hundred percent\\ntruthful, verified, you know, there\\'s this case where this\\nlawyer apparently submitted a GPT-generated brief and\\nit had like fake, you know, legal case citations in it\\nand the judge is gonna get his law license stripped\\nor something. Right? So, like, we just assumed\\nit\\'s like obviously they\\'re gonna want the super\\nliteral like, you know, one that never makes anything\\nup, not the creative one, but actually they said what the law firm basically said is yeah, that\\'s true at like the\\nlevel of individual briefs, but they said when you\\'re\\nactually trying to figure out like legal arguments, right, like, you actually want to be creative, right? You don\\'t, again, there\\'s creativity and then\\nthere\\'s like making stuff up. Like what\\'s the line? You actually want it to explore a different hypothesis, right? You wanna do kind of the\\nlegal version of like improv or something like that where you wanna float different theories of the case and different\\npossible arguments for the judge and different possible arguments\\nfor the jury, by the way, different routes through the, you know, sort of history of all the case law. And so they said actually for\\na lot of what we want to use it for, we actually want\\nit in creative mode. And then basically we just\\nassume that we\\'re gonna have to crosscheck all of the, you know, all the specific citations. And so I think there\\'s\\ngoing to be more shades of gray in here than people think. And then I just add to that, you know, another one of these trillion\\ndollar kind of questions is ultimately, you know, sort\\nof the verification thing. And so, you know, will LLMs be evolved from\\nhere to be able to do their own fascial verification? Will you have sort of add-on functionality like Wolf from Alpha right? Where, you know, another plugins where that\\'s the way\\nyou do the verification. You know, another, by\\nthe way, another idea is you might have a community\\nof LLMs on any, you know, so for example, you might have the creative\\nlm and then you might have the literal LLM fact check it, right? And so there there\\'s a\\nvariety of different technical approaches that are being applied to solve the hallucination problem. You know, some people\\nlike Jan Lacoon argue that this is inherently\\nan unsolvable problem, but most of the people\\nworking in the space, I think, that there\\'s a number of practical ways to kind of corral this in a little bit. - Yeah. If you were to\\ntell me about Wikipedia before Wikipedia was created, I would\\'ve left at the possibility of something like that be possible. Just a handful of folks\\ncan organize right. And self and moderate\\nwith a mostly unbiased way the entirety of human knowledge. I mean, so if there\\'s something like the approach to Wikipedia\\ntook possible for LLMs, that\\'s really exciting. Well, I think that\\'s possible. - And in fact Wikipedia today is still not deterministically\\ncorrect. Right. So you cannot take to the bank, right. Every single thing on every single page, but it is probabilistically\\ncorrect. Right. And specifically the way I\\ndescribe Wikipedia to people, it is more likely that Wikipedia\\nis right than any other source you\\'re gonna find. - Yeah. - It\\'s this old question, right, of like, okay, like are\\nwe looking for perfection? Are we looking for something that asymptotically approaches perfection? Are we looking for\\nsomething that\\'s just better than the alternatives? And Wikipedia, right, has\\nexactly your point has proven to be like, overwhelmingly better than people thought. And I think that\\'s where this ends. And then underneath all this\\nis the fundamental question of where you started,\\nwhich is, okay, you know, what is truth? How do we get to truth? How\\ndo we know what truth is? And we live in an era in which\\nan awful lot of people are very confident that they\\nknow what the truth is. And I don\\'t really buy into that. And I think the history\\nof the last, you know, 2000 years or 4,000 years of\\nhuman civilization is actually getting to the truth is actually a very difficult thing to do. - Are we getting closer,\\nif we look at the entirety, the arc of human history, are we getting closer to the truth? - I don\\'t know. - Okay. Is it possible, is it possible that we\\'re\\ngetting very far away from the truth because of the internet because of how rapidly\\nyou can create narratives and just as the entirety\\nof a society just move like crowds in a hysterical\\nway along those narratives that don\\'t have necessary grounding in whatever the truth is. - Sure. But like, you know, we came up with communism\\nbefore the internet somehow. Right. Like, which was, I would say had rather larger issues than anything we\\'re dealing with today. - It had, in the way it was implemented, it had issues. - And it is theoretical structure. It had like real issues. It had like a very deep\\nfundamental misunderstanding of human nature and economics. - Yeah but those folks\\nSure work very confident there was the right way. - They were extremely confident. And my point is they were very\\nconfident 3,900 years into what we would presume to be\\nevolution towards the truth. - [Lex] Yeah. - And so my assessment is number one, there\\'s no need for, you know, there\\'s no need for the Hegelian, there\\'s no need for the Hegelian dialectic to actually converge towards the truth. Like apparently not. - Yeah. So yeah. Why are we so obsessed\\nwith there being one truth? Is it possible there\\'s just\\ngoing to be multiple truths like little communities that\\nbelieve certain things and? - I think it\\'s just now number one, I think it\\'s just really difficult. Like who gets, you know, historically who gets to decide what the truth is, it\\'s either the king or the priest. Right? Like, and so we don\\'t\\nlive in an era anymore if kings are priest dictating it to us. And so we\\'re kind of on our own. And so my typical thing is like we just, we we just need a huge amount of humility and we need to be very suspicious of people who claim that\\nthey have the capital. - Yeah. - Capital truth. And then, we need to and you know, look, the good news is the\\nenlightenment has bequeathed us with a set of techniques\\nto be able to presumably get closer to truth through\\nthe scientific method and rationality and observation and experimentation and hypothesis. And, you know, we need to continue to embrace\\nthose even when they give us answers we don\\'t like. - Sure. But the internet and\\ntechnology has enabled us to generate the large number of content. That data, that the process, the scientific process\\nallows us sort of damages the hope laden within\\nthe scientific process. \\'Cause if you just have a\\nbunch of people saying facts on the internet and some of them are going to be LLMs, how is\\nanything testable at all? Especially that involves like human nature or things like this. It\\'s not physics. - Here\\'s a question a\\nfriend of mine just asked me on this topic. So suppose you had LLMs\\nin equivalent of GPT4, even 5, 6, 7, 8, suppose\\nyou had them in the 1600s. - [Lex] Yeah. - And Galileo comes up for trial. - [Lex] Yep. - Right? And you ask the LLM\\nlike, his Galileo, right? - [Lex] Yeah. - Like, what does it answer? Right? And one theory is he had\\nanswers no that he\\'s wrong because the overwhelming\\nmajority of human thought up until that point was that he was wrong. And so therefore that\\'s\\nwhat\\'s in the training data. Another way of thinking about it is, well, it\\'s efficiently\\nadvanced LLM will have evolved the ability to actually\\ncheck the math. Right. And will actually say, actually\\nno, actually, you know, you may not wanna hear it, but he\\'s right. Now if, you know, the\\nchurch at that time was, you know, owned the LLM, they would\\'ve given it human you know, human feedback to prohibit it\\nfrom answering that question. Right. And so I like to take it out of our current context \\'cause that like makes it very clear, those same questions apply today. Right. This is exactly the point of a huge amount of the human feedback training that\\'s actually happening\\nwith these LLMs today. This is a huge like debate\\nthat\\'s happening about whether open source, you know, AI should be legal. - Well, the actual mechanism\\nof doing the human RL with human feedback is seems like such a fundamental and\\nfascinating question. How do you select the humans? - [Marc] Exactly. - Yeah. How do you select the humans? - AI alignment, right? Which everybody like is\\nlike, oh, that sounds great. Alignment with what? Human values. Who has human values? - [Lex] Who has human values? - Right? And so we\\'re in this mode of like social and popular discourse. We\\'re like, you know, there\\'s,\\nyou know, you see this, what do you think of when you read a story in the press right now? And they say, you know, X,\\nY, Z made a baseless claim about some topic, right? And there\\'s one group of people\\nwho are like, aha, think, you know, they\\'re doing fact checking. There\\'s another group\\nof people that are like, every time the press\\nsays that it\\'s now a tick and that means that they\\'re lying, right? Like, so, like, we\\'re\\nin this social context where there\\'s the level\\nto which a lot of people in positions of power have become very, very certain that they\\'re\\nin a position to determine the truth for the entire\\npopulation is like, there\\'s like some bubble that\\nhas formed around that idea. And at least, like I say, it\\'s flies completely in\\nthe face of everything I was ever trained about science and about reason and strikes me as like, you know, deeply offensive and incorrect. - What would you say about\\nthe state of journalism just on that topic today? Are we in a temporary kind of, are we experiencing a temporary problem in terms of the incentives in\\nterms of the business model, all that kind of stuff? Or is this like a decline\\nof traditional journalism as we know it? - You have, I always think\\nabout the counterfactual in these things, which is like, okay, because these questions, right, this question heads\\ntowards, it\\'s like, okay, the impact of social media\\nand the undermining of truth and all this. But then you wanna ask the\\nquestion of like, okay, what if we had had the\\nmodern media environment, including cable news and\\nincluding social media and Twitter and everything\\nelse in 1939 or 1941, right? Or 1910 or 1865 or 1850 or 1776, right? And like, I think. - You just introduced like\\nfive thought experiments at once and broke my head, but yes, yes. There\\'s a lot of interesting years. - Well like, can I just\\ntake a simple example? Like, how would President\\nKennedy have been interpreted with what we know now about all the things Kennedy was up to? Like how would he have been\\nexperienced by the body of politic in a, with a\\nsocial media context, right? Like how would LBJ have been experienced? But by the way, how would\\nyou know, like many men, FDR, like the new deal, the Great Depression. - I wonder where Twitter\\nwould this would think about Churchill and Hitler and Stalin. - You know, I mean look to\\nthis day there, you know, there are lots of very\\ninteresting real questions around like how America, you know, got, you know, basically\\ninvolved in World War II and who did what when, and the operations of British intelligence and American soil and did FDR, this that Pearl Harbor, you know? - [Lex] Yeah. - Woodrow Wilson ran for, you know, his candidacy was run on an anti-war. You know, he ran on the platform\\nand not getting involved World War-I somehow that\\nswitched, you know, like, and I\\'m not even making a value judgment on any of these things. I\\'m just saying like the way that our ancestors\\nexperienced reality was of course mediated through\\ncentralized, top-down, right. Control at that point. If you ran those realities\\nagain with the media environment we have today, the reality would be experienced\\nvery, very differently. And then of course that that\\nintermediation would cause the feedback loops to change. And then reality would obviously play out. - Do you think it\\'d be very different? - Yeah, it has to be. It has to be. It has to be just \\'cause it\\'s all, so, I mean just look at\\nwhat\\'s happening today. I mean just the most obvious thing is just the collapse. And here\\'s another opportunity to argue that this is not the internet\\ncausing this by the way. Here\\'s a big thing happening today, which is Gallup does this\\nthing every year where they do, they pull for trust in\\ninstitutions in America and they do it across all the, everything from the military\\nto clergy and big business and the media and so forth, right? And basically there\\'s been\\na systemic collapse in trust in institutions in the US\\nalmost without exception, basically since essentially\\nthe early 1970s. There\\'s two ways of looking\\nat that, which is, oh my God, we\\'ve lost this old world\\nin which we could trust institutions and that was so much better \\'cause like that should\\nbe the way the world runs. The other way of looking\\nat it is we just know a lot more now and the great mystery is why those numbers aren\\'t all zero. - [Lex] Yeah. - Right? Because like now we know so much about how these things operate and like they\\'re not that impressive. - And also why do we don\\'t\\nhave better institutions and better leaders then? - Yeah. And so this goes\\nto the thing which is like, okay, we had the media environment of that we\\'ve had between\\nthe 1970s and today. If we had that in the thirties\\nand forties or 1900s, 1910s, I think there\\'s no question\\nreality would turned out different if only because\\neverybody would\\'ve known to not trust the institutions, which would have changed\\ntheir level of credibility, their ability to control circumstances, therefore the circumstances\\nwould\\'ve had to change. Right? And it would\\'ve\\nbeen a feedback loop. It would\\'ve been a feedback loop process in other words, right? It\\'s your experience of\\nreality changes reality and then reality changes your\\nexperience of reality, right? It\\'s a two-way feedback\\nprocess and media is the intermediating force between that. So change the media\\nenvironment, change reality. - [Lex] Yeah. - And so it\\'s just, so, as a consequence, I think it\\'s just really hard to say, oh, things worked a certain way then and they work a different way now. And then therefore, like\\npeople were smarter than, or better than, or you know, by the way, dumber than or not as capable than, right? We make all these like really light and casual like comparisons\\nof ourselves to, you know, previous generations of people. You know, we draw judgements all the time and I just think it\\'s\\nlike really hard to do any of that \\'cause if we\\nput ourselves in their shoes with the media that they had at that time, like I think we probably\\nmost likely would\\'ve been just like them. - So don\\'t you think that our perception and understanding of reality, would you be more and more mediated through large language models now? So you said media before, isn\\'t the LLM going to be the new, what is it, mainstream media, MSM? It\\'ll be LLM. That would be the source of, I\\'m sure there\\'s a way to\\nkind of rapidly fine tune, like making LLMs real time. I\\'m sure there\\'s probably\\na research problem that you can do just rapid\\nfine tuning to the new events. So something like this. - Well even just the whole\\nconcept of the chat UI might not be like the chat UI is just\\nthe first whack at this. And maybe that\\'s the dominant thing. But look maybe our,\\nmaybe we don\\'t know yet. Like maybe the experience\\nmost people with LLMs is just a continuous feed you know, maybe it\\'s more of a passive\\nfeed and you just are getting a constant like running commentary on everything happening in your life and it\\'s just helping\\nyou kind of interpret and understand everything. - Also really more deeply\\nintegrated into your life. Not just like, oh, like\\nintellectual philosophical thoughts, but like literally like\\nhow to make a coffee, where to go for lunch. Just whether, you know,\\ndating all this kind of stuff. - What to say in a job interview. - Yeah. What to say. - [Marc] Yeah, exactly. - What to say. Next sentence. - Yeah, next sentence. Yeah. At that level. Yeah. I mean, yes. So technically now whether we want that or not is an open question, right? And whether we use. - It Q4, a popup right now the\\nestimated engagement using is decreasing for Marc Andreessen, since there\\'s this controversy\\nsection for his Wikipedia page in 1993, something\\nhappened or something like this. Bring it up that will\\ndrive engagement up anyway. - Yeah. That\\'s right. I mean, look, this gets this whole thing\\nof like, so, you know, the chat interface has this whole concept of\\nprompt engineering, right? - [Lex] Yes, yes. - Prompts. Well it turns\\nout one of the things that LLMs are really good at\\nis writing prompts, right? - [Lex] Yeah. - And so like, what if you just outsourced and by the way, you could\\nrun this experiment today, you could hook this up to do this today. The latency\\'s not good\\nenough to do it real time in a conversation. But you could run this experiment\\nand you just say, look, every 20 seconds you\\ncould just say, you know, tell me what the optimal\\nprompt is and then ask yourself that question to gimme the result. And then exactly to your point, as you add, there will be these systems that are gonna have\\nthe ability to be alert and updated essentially in real time. And so you\\'ll be able to\\nhave a pendant or your phone or whatever, watch or whatever\\nit\\'ll have a microphone on. It\\'ll listen to your conversations, it\\'ll have a feed of everything\\nelse happen in the world, and then it\\'ll be you\\nknow, sort of retraining, prompting or retraining itself on the fly. And so the scenario you\\ndescribed is actually a completely doable scenario. Now the hard question\\non this is always okay, since that\\'s possible, are\\npeople gonna want that? Like what\\'s the form of experience? You know, that we won\\'t\\nknow until we try it. But I don\\'t think it\\'s\\npossible yet to predict the form of AI in our lives. Therefore, it\\'s not possible to predict the way in which it will intermediate our experience with reality yet. - Yeah. But it feels like\\nthere\\'s going to be a killer app. There\\'s probably a mad scramble right now. And so it\\'ll open AI and\\nMicrosoft and Google and Meta and in startups and smaller\\ncompanies figuring out what is the killer app\\nbecause it feels like it\\'s possible like a\\nChatGPT type of thing. It\\'s possible to build that, but that\\'s 10X more compelling\\nusing already the LLMs we have using even the open source LLMs and the different variants. So you\\'re investing in a lot of companies and you\\'re paying attention, who do you think is gonna win this? Do you think there\\'ll be, who\\'s gonna be the next\\npage rank inventor? - Trillion dollar question. - Another one. We have\\na few of those today. - There\\'s a bunch of those. So look, there\\'s a really\\nbig question today. Sitting here today is\\na really big question about the big models\\nversus the small models that\\'s related directly\\nto the big question of proprietary versus open. Then there\\'s this big\\nquestion question of you know, where is the training data gonna, like, are we topping out of\\nthe training data or not? And then are we gonna be able\\nto synthesize training data? And then there\\'s a huge pile\\nof questions around regulation and you know, what\\'s\\nactually gonna be legal. And so I would, when we think about it, we dovetail kind of all\\nthose questions together. You can paint a picture of\\nthe world where there\\'s two or three God models that are just at like staggering scale and they\\'re just better at everything. And they will be owned by\\na small set of companies and they will basically\\nachieve regulatory capture over the government and they\\'ll\\nhave competitive barriers that will prevent other people from, you know, competing with them. And so, you know, there will be, you know, just like there\\'s like,\\nyou know, whatever, three big banks or three\\nbig, you know, or by the way, three big search companies\\nor I guess two now, you know, it\\'ll centralize like that. You can paint another very different\\npicture that says, no, actually the opposite\\nof that\\'s gonna happen. This is gonna basically that\\nthis is the new gold, you know, this is the new gold rush alchemy. Like you know, this is the big bang\\nfor this whole new area of science and technology. And so therefore you\\'re gonna\\nhave every smart 14-year-old on the planet building open source, right? You know, and figuring out a\\nways to optimize these things. And then, you know, we\\'re just gonna get like\\noverwhelmingly better at generating trading data. We\\'re gonna, you know, bring in like blockchain\\nnetworks to have like an economic incentive to\\ngenerate decentralized training data and so forth and so on. And then basically we\\'re\\ngonna live in a world of open source and there\\'s\\ngonna be a billion LLMs, right? Of every size, scale,\\nshape and description. And there might be a few big ones that are like the super genius ones, but like mostly what we\\'ll\\nexperience is open source and that\\'s, you know,\\nthat\\'s more like a world of like what we have today\\nwith like Linux and the web. - Okay, but you painted these two worlds. But there\\'s also\\nvariations of those worlds, \\'cause you said regulatory\\ncapture is possible to have these tech giants that don\\'t\\nhave regulatory capture, which is something you\\'re also\\ncalling for saying it\\'s okay to have big companies\\nworking on this stuff as long as they don\\'t\\nachieve regulatory capture. But I have the sense that\\nthere\\'s just going to be a new startup that\\'s going to basically be the page rank inventor, which has become the new tech giant. I don\\'t know, I would love to hear your kind\\nof opinion if Google, Meta and Microsoft are as\\ngigantic companies able to pivot so hard to create new products. Like some of it is just\\neven hiring people or having a corporate structure that\\nallows for the crazy young kids to come in and just create\\nsomething totally new. Do you think it\\'s possible or do you think it\\'ll come from a startup? - Yeah, it is this always\\nbig question, which is, you get this feeling, I hear about this a lot\\nfrom CEOs, founder CEOs where it\\'s like, wow,\\nwe have 50,000 people, it\\'s now harder to do new things than it was when we had 50 people. - [Lex] Yeah. - Like, what has happened? So that\\'s a recurring phenomenon. By the way, that\\'s one of the reasons\\nwhy there\\'s always startups and why there\\'s venture capital. That\\'s like a timeless kind of thing. So that\\'s one observation. On page rank, we can talk about that. But on page rank,\\nspecifically on page rank, there actually is a page. So there is a page rank\\nalready in the field and it\\'s the transformer, right? So the big breakthrough\\nwas the transformer. And the transformer was\\ninvented in 2017 at Google. And this is actually like\\nreally an interesting question \\'cause it\\'s like, okay, the transformers like why\\ndoes open AI even exist? Like the Transformers invested at Google. Why didn\\'t Google? I asked a guy I know who\\nwas senior at Google brain kind of when this was happening. And I said, if Google had\\njust gone flat out to the wall and just said, look, we\\'re gonna launch, we\\'re gonna launch the equivalent of GPT4 as fast as we can. I said, when could we have had it? And he said, 2019. They could have just\\ndone a two year sprint with the Transformer and\\nbecause they already had the compute at scale. They already had all the training data, they could have just done it. There\\'s a variety of\\nreasons they didn\\'t do it. This is like a classic big company thing. IBM invented the relational\\ndatabase in the 1970s, let it sit on the shelf as a paper. Larry Ellison picked\\nit up and built Oracle. Xerox Park invented the\\ninteractive computer. They let it sit on the shelf. Steve Jobs came and turned\\nit into the Macintosh, right? And so there is this pattern.\\nNow having said that, sitting here today, like\\nGoogle\\'s in the game, right? So Google, you know, they maybe they let like a\\nfour year gap there go there that they maybe shouldn\\'t have, but like they\\'re in the\\ngame and so now they\\'ve got, you know, now they\\'re committed. They\\'ve done this merger,\\nthey\\'re bringing in demos, they\\'ve got this merger with DeepMind. You know, they\\'re piling in resources. There are rumors that they\\'re, you know, building up an incredible,\\nyou know, super LLM you know, way beyond what we even have today. And they\\'ve got, you\\nknow, unlimited resources and a huge, you know, they\\'ve\\nbeen challenged their honor. - Yeah. I had a chance to\\nhang out with (indistinct) a couple days ago and we took this walk and there\\'s this giant new building where there\\'s going to be\\na lot of AI work being done and it\\'s kind of this\\nominous feeling of like the fight is on. - [Marc] Yeah. - Like there\\'s this beautiful\\nSilicon Valley nature, like birds are chirping\\nand this giant building and it\\'s like the beast has been awakened. - [Marc] Yeah. - And then like all the big\\ncompanies are waking up to this. They have the compute, but\\nalso the little guys have, it feels like they have\\nall the tools to create the killer product that, and then there\\'s also tools to scale if you have a good idea, if\\nyou have the page rank idea. So there\\'s several things\\nthat it\\'s page rank, there\\'s page rank, the algorithm and the\\nidea and there\\'s like the implementation of it. And I feel like killer\\nproduct is not just the idea, like the transform, it\\'s the implementation something really compelling about it. Like you just can\\'t\\nlook away something like the algorithm behind TikTok\\nversus TikTok itself, like the actual experience\\nof TikTok that just, you can\\'t look away. It feels like somebody\\'s\\ngonna come up with that. And it could be Google, but it feels like it\\'s\\njust easier and faster to do for a startup. - Yeah. So, the startup, the huge advantage that\\nstartups have is they just, there\\'s no sacred cows. There\\'s no historical legacy to protect, there\\'s no need to reconcile your new plan with the existing strategy. There\\'s no communication overhead. There\\'s no, you know, big\\ncompanies are big companies. They\\'ve got pre-meetings\\nplanning for the meeting, then they have the post\\nmeeting, the recap, then they have the\\npresentation of the board, then they have the next\\nrounds of meetings. And that\\'s the-- - [Lex] Lots of meetings. - That\\'s the elapsed time when the startup launches\\nits product. Right? So, there\\'s a timeless, right? - [Lex] Yeah. - So there\\'s a timeless thing there now. What the startups don\\'t have\\nis everything else, right? So startups, they don\\'t have a brand, they don\\'t have customer relationships. They\\'ve gotten no distribution, they\\'ve got no, you know, scale. I mean sitting here today,\\nthey can\\'t even get GPUs. Right. Like there\\'s like a GPU shortage. Startups are literally\\nstalled out right now \\'cause they can\\'t get chips,\\nwhich is like super weird. - [Lex] Yeah. They got the cloud. - Yeah. But the clouds\\nrun out of chips. Right. And then to the extent\\nthe clouds have chips, they allocate them to the big customers. Not the small customers. Right. And so the small companies\\nlack everything other than the ability to just\\ndo something new. Right. And this is the timeless race and battle. And this is kinda the point\\nI tried to make in the essay, which is like, both\\nsides of this are good. Like, it\\'s really good to have like highly-scaled tech companies that can do things that are like at staggering\\nlevels of sophistication. It\\'s really good to have\\nstartups that can launch brand-new ideas. They ought to be able to\\nboth do that and compete. They, neither one ought to be subsidized or protected from the others. Like that\\'s, to me, that\\'s just like very\\nclearly the idealized world. It is the world we\\'ve been\\nin for AI up until now. And then of course there are people trying to shut that down. But my hope is that, you know, the best outcome clearly\\nwill be if that continues. - We\\'ll talk about that a little bit, but I\\'d love to linger on some of the ways this is\\ngoing to change the internet. So I don\\'t know if you remember, but there\\'s a thing called\\nMosaic and there\\'s a thing called Netscape Navigator. So you were there in the beginning. What about the interface to the internet? How do you think the browser changes and who gets to own the browser? We got to see some very\\ninteresting browsers, Firefox, I mean all the\\nvariants of Microsoft, Internet Explorer, Edge,\\nand now Chrome, the actual, and he seems like a dumb question to ask, but do you think we\\'ll\\nstill have the web browser? - So I have an eight-year-old\\nand he\\'s super into, he\\'s like Minecraft and learning to code and doing all this stuff. So, of course I was\\nvery proud I could bring sort of fire down from\\nthe mountain to my kid and I brought him ChatGPT\\nand I hooked him up on his laptop. And I was like, you know, this is the thing that\\'s gonna\\nanswer all your questions. And he\\'s like, okay. And I\\'m like, but it\\'s gonna\\nanswer all your questions. And he\\'s like, well of\\ncourse, like it\\'s a computer. Of course it answers all your questions. Like, what else would a\\ncomputer be good for, dad? - [Lex] And never impressed, are they? - Not impressed in the least. Two weeks passed. And he has some question and I say, well, have you asked ChatGPT? And he\\'s like, dad, Bing is better. - [Lex] Ooh. - And why is Bing better? Is because it\\'s built into the browser. \\'Cause he\\'s like, look, I have the Microsoft Edge browser and like it\\'s got Bing right here. And then he doesn\\'t know this yet, but one of the things you\\ncan do with Bing and Edge is there\\'s a setting where you\\ncan use it to basically talk to any webpage because\\nit\\'s sitting right there next to the browser. And by the way, which\\nincludes PDF documents. And so you can, in the way\\nthey\\'ve implemented an Edge with Bing is you can load a PDF and then you can ask it questions, which is the thing you can\\'t\\ndo currently in just ChatGPT. So they\\'re, you know, they\\'re gonna, they\\'re\\ngonna push the meld. I think that\\'s great. You know, they\\'re gonna push the melding and see if there\\'s a\\ncombination thing there. Google\\'s rolling out this thing, the magic button, which is\\nimplemented in, you know, they put it in Google Docs, right? And so you go at a, you know, Google Docs and you create\\na new document and you know, you instead of like, you\\nknow, starting to type, you just, you know, say it, press the button and it starts to like, generate content for you, right? Like, is that the way that it\\'ll work? Is it gonna be a speech UI\\nwhere you\\'re just gonna have an earpiece and talk to it all day long? You know, is it gonna be a,\\nlike these are all, like, this is exactly the kind\\nof thing that I don\\'t, this is exactly the kind of thing I don\\'t think is possible to forecast. I think what we need to do is\\nlike run all those experiments and so one outcome is we\\ncome out of this with like a super browser that has AI built in that\\'s just like amazing. There\\'s a real possibility that the whole, I mean, look, there\\'s a possibility here that the whole idea of\\na screen and windows and all this stuff just goes away \\'cause like, why do you need that if\\nyou just have a thing that\\'s just telling you\\nwhatever you need to know? - Well and also, so there\\'s\\napps that you can use, you don\\'t really use them. You know, being a Linux\\nguy and Windows guy, there\\'s one window, the browser that with\\nwhich you can interact with the internet, but on the\\nphone you can also have apps. So I can interact with\\nTwitter through the app or through the web browser. And that seems like an\\nobvious distinction, but why have the web browser in that case, if one of the apps starts\\nbecoming the everything app. - [Marc] Yeah, that\\'s right. - What is Elon trying to do with Twitter? But there could be others.\\nThere could be like a big app, there could be a Google app\\nthat just doesn\\'t really do search, but just like, do what I guess AOL did back\\nin the day or something where it\\'s all right there and\\nit changes the nature of the internet because\\nwhere the content is hosted, who owns the data? Who owns the content? What is the kind of content you create? How do you make money by creating content? Who are the content creators? All of that. Or it could just keep being the same, which is like with just the\\nnature of webpage changes and the nature of content. But there\\'ll still be a web browser. \\'Cause web browser\\'s\\na pretty sexy product. It just seems to work. \\'Cause it like you have an interface, a window into the world, and then the world can\\nbe anything you want. And as the world will evolve, it could be different\\nprogramming languages, it can be animated, maybe it\\'s\\nthree dimensional and so on. Yeah, it\\'s interesting. Do you think we\\'ll still\\nhave the web browser? - Well, very medium becomes\\nthe content for the next one. - [Lex] Oh boy. - You know, the AI will be able to give you a browser whenever you want. - [Lex] Oh, interesting. Generate. - Well, another way to\\nthink about it is maybe what the browser is maybe it\\'s just the escape hatch, right? Which is maybe kind of\\nwhat it is today, right? Which is like most of what\\nyou do is like inside a social network or inside a search\\nengine or inside, you know, somebody\\'s app or inside some\\ncontrolled experience, right? But then every once in a\\nwhile there\\'s something where you actually want to jailbreak, you wanna actually get free. - Web browser\\'s the FU to the man. You\\'re allowed to. That\\'s the free internet. - [Marc] Yeah. - Back the way it was in the nineties. - So here\\'s something I\\'m proud of. So nobody really talks about it. Here\\'s something I\\'m proud\\nof, which is the web, the browser, the web servers, they\\'re all, they\\'re still backward compatible all the way back to like 1992, right? So like, you can put up a,\\nyou can still, you know what, the big breakthrough of\\nthe web early on the big breakthrough was it made\\nit really easy to read, but it also made it really easy to write, made it really easy to publish. And we literally made\\nit so easy to publish. We made it not only so it\\nwas easy to publish content, it was actually also easy to\\nactually write a web server. - [Lex] Yeah. - Right and you could\\nliterally write a web server in four lines of brol code and you could start\\npublishing content on it, and you could set whatever\\nrules you want for the content, whatever censorship, no\\ncensorship, whatever you want. You could just do that. And as long as you had\\nan IP address, right, you could do that. That still works, right? That like, still works\\nexactly as I just described. So this is part of my\\nreaction to all of this. Like, you know, all this\\njust censorship pressure and all this, you know, these issues around\\ncontrol and all this stuff, which is like, maybe we need to get\\nback a little bit more to the wild west. Like, the wild west is still out there. Now they will try to chase you down. Like they\\'ll try to, you know, people who want a censor will\\ntry to take away you know, your domain name and\\nthey\\'ll try to take away your payments account and so forth if they really don\\'t\\nlike what you\\'re saying. But nevertheless, you like, unless they literally are intercepting you at the ISP level, like you\\ncan still put up a thing. And so I don\\'t know, I think that\\'s important\\nto preserve, right? Like because I mean one is\\njust a freedom argument, but the other\\'s a creativity argument, which is you wanna have the escape hatch so that the kid with the idea\\nis able to realize the idea. \\'cause to your point on page rank, you actually don\\'t know what\\nthe next big idea is, right? No, nobody called Larry Page and told him to develop page rank. Like he came up with that on his own. And you wanna always, I think leave the escape\\nhatch for the next, you know, kid or the next Stanford\\ngrad student to have the breakthrough idea and be\\nable to get it up and running before anybody notices. - You and I are both hands of history. So let\\'s step back. We\\'ve been talking about the future. Let\\'s step back for a bit\\nand look at the nineties. You created Mosaic web browser, the first widely used web browser. Tell the story of that. And how did it evolve\\ninto Netscape Navigator this the early days? - So full story. So. - [Lex] We were born, - I was born. A small child. - Actually. Yeah, let\\'s go there. Like, when would you first\\nfall in love with computers? - Oh, so I hit the generational jackpot and I hit the Gen X kind of\\npoint perfectly as it turns out. So I was born in 1971. So there\\'s this great website called WTF happened in 1971 dot com,\\nwhich is basically in 1971. It\\'s when everything\\nstarted to go to hell. And I was of course born in 1971. So I like to think that I had\\nsomething to do with that. - Did you make it on the website? - I don\\'t think I made it on the website, but you know, hopefully,\\nsomebody needs to add. - This is where everything. - Maybe I contributed to some\\nof the trends that they do. Every line on that website\\ngoes like that, right? So it\\'s all a picture disaster. But there was this moment in time where \\'cause you know, sort\\nof the Apple, you know, the Apple II hit in like 1978\\nand then the IBM PC hit in 82. So I was like, you know,\\n11 when the PC came out. And so I just kind of hit that\\nperfectly and then that was the first moment in time when like, regular people could spend\\na few hundred dollars and get a computer, right? And so that, I just like that resonated right out of the gate. And then the other part\\nof the story is, you know, I was using Apple II,\\nI used a bunch of them, but I was using Apple II and\\nof course it said in the back of every Apple II and every\\nMac it said, you know, designed in Cupertino, California. And I was like, wow, okay. Cupertino must be the like,\\nshining city on the hill. Like Wizard of Oz is\\nlike the most amazing, like city of all time.\\nI can\\'t wait to see it. And of course, years later I came out to Silicon Valley and went to Cupertino and it\\'s just a bunch of office parks at low-rise apartment buildings. So the aesthetics were a\\nlittle disappointing, but, you know, it was the vector\\nright of the creation of a lot of this stuff. So then basically by, so part part, part of my story is just\\nthe luck of having been born at the right time and\\ngetting exposed to PCs. Then the other part is, the other part is when El\\nGore says that he created the internet, he actually is correct in a really meaningful way, which is he sponsored a bill\\nin 1985 that essentially created the modern internet, created what is called\\nthe NSF net at the time, which is sort of the first\\nreally fast internet backbone. And you know, that that bill dumped a ton of money into a bunch of research universities to build out basically\\nthe internet backbone and then the supercomputer\\ncenters that were clustered around the internet. And one of those universities\\nwas University of Illinois where I went to school. And so the other stroke\\nlock that I had was, I went to Illinois basically\\nright as that money was just like getting dumped on campus. And so as a consequence\\nwe had at, on campus, and this was like, you know,\\n89, 90, 91, we had like, you know, we were right\\non the internet backbone. We had like T3 and 45 at the time, T3 45 megabit backbone\\nconnection, which at the time was, you know, wildly state of the art. We had cray super computers. We had thinking machines\\nparallel super computers. We had silicon graphics\\nworkstations, we had Macintosh\\'s, we had next cubes all over the place. We had like every\\npossible kind of computer you could imagine \\'cause all this money\\njust fell out of the sky. - [Lex] So you were living in the future. - Yeah. So yeah, quite\\nliterally it was, yeah, like it\\'s all there. It\\'s all like we had\\nfull broadband graphics, like the whole thing. And it\\'s actually funny \\'cause they had this is the first time I kind of, it sort of tickled the back\\nof my head that there might be a big opportunity in\\nhere, which is, you know, they embraced it and so\\nthey put like computers in all the dorms and they\\nwired up all the dorm rooms and they had all these, you know, labs everywhere and everything. And then they gave every undergrad a computer account and an email address. And the assumption was that\\nyou would use the internet for four years at college and then you would\\ngraduate and stop using it. And that was that, right? - [Lex] Yeah. - And you would just\\nretire your email address. It wouldn\\'t be relevant\\nanymore \\'cause you\\'d go off from the workplace and\\nthey don\\'t use email. You\\'d be back to using\\nfax machines or whatever. - Did you have that sense as well? Like, what you said the back\\nof your head was tickled. Like, what was exciting to\\nyou about this possible world? - Well, if this is so\\nuseful in this containment, if this is so useful in\\nthis contain environment that just has this weird\\nsource of outside funding, then if it were practical\\nfor everybody else to have this and if it were cost effective for everybody else to have this, wouldn\\'t they want it? And the overwhelmingly the prevailing view at the time was no,\\nthey would not want it. This is esoteric, weird nerd stuff, right? That like computer science kids like, but like normal people are\\nnever gonna do email. Right. Or be on the internet, right? And so I was just like,\\nwow, like this is actually, like, this is really compelling stuff. Now the other part was, it was all really hard to use\\nand in practice you had to be basically a CS you know, basically had had to BA\\nCS undergrad or equivalent to actually get full use of\\nthe internet at that point. \\'cause it was all pretty esoteric stuff. So then that was the other\\npart of the idea, which was, okay, we need to actually\\nmake this easy to use. - So what\\'s involved in creating Mosaic? Like, in creating graphical\\ninterface to the internet? - Yeah, so it was a combination of things. So it was like basically the web existed in an early sort of\\ndescribed as prototype form. And by the way, text only at that point. - What did it look like? What was the web? I mean what and the key figures. Like, what was it? Like, what paint a picture? - It looked like ChatGPT\\nactually it was all text. - Yeah. - And so you had a text-based web browser? Yeah, well actually the original browser, Tim Burners Lee, the original browser, both the original browser\\nand the server actually ran on next cubes. So these were, this was, you know, the computer Steve Jobs made\\nduring the interim period when during the decade long interim period when he was not at Apple, you know, he got fired in 85 and\\nthen came back in 97. So this was in that interim\\nperiod where he had this company called Next and they made these, literally these computers called cubes. And there\\'s this famous\\nstory, they were beautiful, but they were 12 inch by 12\\ninch by 12 inch cubes computers. And there\\'s a famous story\\nabout how they could have cost half as much if it had\\nbeen 12 by 12 by 13. But this cube was like,\\nno, like it has to be. So they were like $6,000\\nbasically academic workstations. They had the first city round\\ndrives, which were slow. I mean it was, the computers\\nwere all but unusable. They were so slow, but\\nthey were beautiful. - Okay, can we actually just\\ntake a tiny tangent there? - Sure. Of course. - The 12 by 12 by 12 that just\\nso beautifully encapsulates Steve Jobs idea of design. Can you just comment on what you find interesting about Steve Jobs? What about that view of the world, that dogmatic pursuit of perfection and how he saw perfection in design? - Yeah, so I guess I\\'d say like, look, he was a deep believer, I think in a very deep,\\nthe way I interpret it, I don\\'t know if you ever\\nreally described it like this, but the way I interpret\\nit\\'s like this thing and it\\'s actually a thing in philosophy. It\\'s like aesthetics are\\nnot just appearances. Aesthetics go all the way to like deep underlying meaning, right? It\\'s like I\\'m not a physicist. One of the things I\\'ve\\nheard physicists say is one of the things you start to get a sense of when a theory might be correct is when it\\'s beautiful, right? Like, you know, there, right? And so, there\\'s something, and you feel the same thing by the way in like human psychology, right? You know, when you\\'re\\nexperiencing awe, right? You know, there\\'s like a simplicity to it. When you\\'re having an honest\\ninteraction with somebody, there\\'s an aesthetic, I would say calm comes over you \\'cause you\\'re actually being fully honest and trying to hide yourself, right? So it\\'s like this very\\ndeep sense of aesthetics. - And he would trust that\\njudgment that he had deep down. Like yeah, even if the\\nengineering teams are saying this is too difficult. Even if whatever the\\nfinance folks are saying, this is ridiculous. The supply chain, all that\\nkind of stuff just makes this impossible. We can\\'t do this kind of material. This has never been done\\nbefore and so on and so forth. He just sticks by it. - Well, I mean, who makes a\\nphone out of aluminum, right? Like, hadn\\'t nobody\\nelse would\\'ve done that. And now of course if your phone is made out of aluminum white,\\nyou know, how crude, what a kind of caveman would\\nyou have to be to have a phone that\\'s made outta plastic? Like, right. So like, so it\\'s just this very right. And, you know, look, there\\'s a thousand different\\nways to look at this, but one of the things is just like, look, these things are\\ncentral to your life. Like, you\\'re with your phone more than you\\'re with anything else. Like, it\\'s gonna be in your hand. I mean, you know this, he thought very deeply about\\nwhat it meant for something to be in your hand all day long. But for example, here\\'s an\\ninteresting design thing. Like, he never wanted, my understanding is he never\\nwanted an iPhone to have a screen larger than you could reach with your thumb one handed. And so he was actually opposed to the idea of making the phones larger. And I don\\'t know if you\\nhave this experience today, but let\\'s say there are\\ncertain moments in your day when you might be like, only have one hand available\\nand you might wanna be on your phone. And you\\'re trying to like, send a text and your thumb can\\'t\\nreach the send button. - Yeah. I mean there\\'s\\npros and cons, right? And then there\\'s like folding phones, which I would love to know what he thought thinks about them. But I mean, is there something you\\ncould also just linger on? \\'cause he\\'s one of the interesting figures in the history of technology. What makes him as successful as he was? What makes him as interesting as he was? What made him so productive and important in the development of technology? - He had an integrated worldview. So the properly designed device that had the correct functionality, that had the deepest\\nunderstanding of the user, that was the most beautiful, right? Like, it had to be all\\nof those things, right? He basically would drive\\nto as close to perfect as you could possibly get. Right? And you know, I suspect that\\nhe never quite, you know, thought he ever got there.\\n\\'cause most great creators, you know, are generally dissatisfied. You know, you read accounts\\nlater on and all they can, all they can see are the\\nflaws in their creation. But like he got as close to\\nperfect each step of the way as he could possibly\\nget with the constraints of the technology of his time. And then, you know,\\nlook, he was, you know, sort of famous in the Apple model. It\\'s like, look, they will, you know, this headset that they just came out with, like, you know, it\\'s like a\\ndecade long project, right? It\\'s like, and they\\'re just gonna sit\\nthere and tune and tune and polish and polish and tune and polish and tune and polish until it is as perfect as anybody could possibly make anything. - Yeah. - And then this goes to the way that people describe working\\nwith him was, which is, you know, there was a terrifying\\naspect of working with him, which is, you know, he was,\\nyou know, he was very tough. But there was this thing that\\neverybody I\\'ve ever talked to worked for him, says that they all say the following, which is we did the best work of our lives when we worked for him because he set the bar incredibly high. And then he supported us\\nwith everything that he could to let us actually do\\nwork of that quality. So a lot of people who were at Apple spend the rest of their lives trying\\nto find another experience where they feel like they\\'re able to hit\\nthat quality bar again. - Even if it in retrospect or\\nduring it felt like suffering. - Yeah, exactly. - What does that teach you\\nabout the human condition? Huh? - So look, so say exactly. So the Silicon Valley, I mean,\\nlook, he\\'s not, you know, George Patton you know in the Army. Like, you know, there are\\nmany examples in other fields, you know, that are like\\nthis specifically in tech. It\\'s actually, I find it very interesting. There\\'s the Apple way, which\\nis polish, polish, polish, and don\\'t ship until it\\'s as\\nperfect as you can make it. And then there\\'s the sort\\nof the other approach, which is the sort of\\nincremental hacker mentality, which basically says, ship\\nearly and often and iterate. And one of the things I\\nfind really interesting is I\\'m now 30 years into this, like, they\\'re very successful\\ncompanies on both sides of that approach, right? Like, that is a fundamental\\ndifference, right? In how to operate and how to\\nbuild and how to create that. You have world class companies\\noperating in both ways. And I don\\'t think the question of like, which is the superior\\nmodel is anywhere close to being answered. Like, and my suspicion\\nis the answer is do both. The answer is you actually want both. They lead to different outcomes. Software tends to do better\\nwith the iterative approach. Hardware tends to do\\nbetter with the, you know, sort of wait and make it perfect approach. But again, you can find\\nexamples in both directions. - So the jury\\'s still out on that one. So back to Mosaic. So, what it was text based Tim Burns Lee? - Well, there was the\\nweb, which was text based, but there were no, I mean\\nthere was like three websites. There was like no content,\\nthere were no users. Like, it wasn\\'t like a catalytic, it hadn\\'t, and by the way, it was all because it was all text. There were no documents, there were no images,\\nthere were no videos, there were no, right. So, and then if, if in the beginning, if you had to be on a next cube, right? You need to had a next cube\\nboth to publish and to consume. - So, there was 6,000 bucks you said. - There were limitations. Yeah. $6,000 PC. They\\ndid not sell very many. But then there was also, there was also FTP and\\nthere was Use Nets, right? And there was, you know, a dozen other basically there\\'s waste, which was an early search thing. There was Gopher, which\\nwas an early menu based information retrieval system. There were like a dozen\\ndifferent sort of scattered ways that people would get to\\ninformation on the internet. And so the Mosaic idea was basically bring those all together, make the whole thing\\ngraphical, make it easy to use, make it basically bulletproof\\nso that anybody can do it. And then again, just on the luck side, it so happened that this was right at the moment when graphics, when the GUI sort of actually took off and we\\'re now also used to the GUI that we think it\\'s been around forever. But it didn\\'t real, you know, the Macintosh brought it out in 85, but they actually didn\\'t\\nsell very many Macs in the eighties. It was not that successful of a product. It really was. You needed Windows 3.0 on\\nPCs and that hit in about 92. And so, and we did most in 92, 93. So that sort of, it was like right at the\\nmoment when you could imagine actually having a graphical\\nuser interface to right at all, much less one to the internet. - How old did Windows 3 sell? So was that the really big. - [Marc] That was the big bang. - The big operating\\ngraphical operating system? - Well this is the classic, okay. This Microsoft was operating on the other, so Steve the Apple was\\nrunning on the Polish until it\\'s perfect. Microsoft famously ran on the other model, which is ship and iterate. And so in the old line in those days was Microsoft Right\\'s version three of every Microsoft product. That\\'s the good one, right? And so there there are, you can find online Windows 1, Windows 2. Nobody used them. Actually the original Windows, in the original Microsoft Windows, the windows were non overlapping. And so you had these very small, very low resolution screens and then you had literally-- - [Lex] Windows. - It just didn\\'t work. It wasn\\'t ready yet. Well. - And Windows 95 I think\\nwas a pretty big leap also. - That was a big leap too. So that was like bang, bang. And then of course Steve, and then when, you know, in the fullness of time Steve came back, then the Mac started, took off again. That was the third bang. And then the iPhone was the fourth bang. - Such exciting time. - And then we were off,\\noff to the races because. - Nobody could have known what\\nwould be created from that. - Well, Windows 3.1 or 3.0, Windows 3.0 to the iPhone\\nwas only 15 years. Right. Like that ramp was in retrospect. At the time it felt like it took forever. But that in histor in historical terms, like that was a very fast ramp from even a graphical computer at all on your desk to the iPhone. That was 15 years. - So, did you have a sense\\nof what the internet will be as you\\'re looking through\\nthe window of Mosaic? Like, what you, like there\\'s\\njust a few web pages for now. - So the thing I had early on\\nwas I was keeping at the time what there\\'s disputes over\\nwhat was the first blog, but I had one of them that\\nat least is a possible, at least a rudder up in the competition. And it was what was called\\nthe What\\'s new page. And it was literally, it was a hardwired in\\ndistribution unfair advantage. I wired, put it right in the browser, I put it in the browser\\nand then I put my resume in the browser, which also was-- - [Lex] Hilarious. - But I was keeping not many people get to get to do that. - No, good call. And early days. It\\'s so interesting. - I\\'m looking for my, about about, oh, Marc is looking for a job. - [Lex] Yeah, yeah, exactly. - So the West New page, I would literally get up every morning and I would, or every afternoon\\nand I would basically, if you wanted to launch a website, you would email me and I would\\nlist it on the most new page. And that was how people\\ndiscovered the new websites as they were coming out. And I remember \\'cause it was like one, it literally went from, it was like one every couple\\ndays to like one every day to like two every day. - And then so you\\'re doing, so that blog was kind of\\ndoing the directory thing. So like, what was the homepage? - So the homepage was just\\nbasically trying to explain even what this thing is that\\nyou\\'re looking at. Right. Basically the basic instructions. But then there was a button, there was a button that said what\\'s new. And what most people did was they went to, for obvious reasons went to what\\'s new. - [Lex] Yeah. - But like it was so mind\\nblowing at that point. This the basic idea and it\\nwas, this was like, you know, this was the basic idea of the internet, but people could see\\nit for the first time. The basic idea was, look,\\nyou know, some, you know, it\\'s like literally it\\'s like\\nan Indian restaurant in like Bristol England has like\\nput their menu on the web. And people were like, wow. - [Lex] Whoa. - Because like that\\'s the first\\nrestaurant menu on the web. - [Lex] Yeah. - And I don\\'t have to be\\nin Bristol and I don\\'t know if I\\'m ever gonna go to Bristol. And I don\\'t even like Indian\\nfood and like. Wow. Right. And it was like that the first web, the first streaming video thing was it was in another England,\\nsome Oxford or something. Some guy put his coffee pot up\\nas the first streaming video thing and he put it on the\\nweb \\'cause he literally, it was the coffee pot down the hall. And he wanted to see when\\nhe needed to go refill it. But there were, you know, there was a point when\\nthere were thousands of people like watching that coffee pot \\'cause it was the first\\nthing you could watch. - Well, but isn\\'t were you able\\nto kind of infer, you know, if that Indian restaurant could go online. Then you\\'re like they all will. - [Marc] Yeah, exactly. - So you felt that? - [Marc] Yeah, yeah, yeah. - Okay. - Now, you know, look, it\\'s\\nstill a stretch, right? It\\'s still a stretch \\'cause\\nit\\'s just like, okay, is it, you know, you\\'re still in this\\nzone, which is like, okay, is this a nerd thing? Is this a real person thing? By the way, you know, there was a wall of\\nskepticism from the media. Like, they just, like,\\neverybody was just like, yeah, this is the crazy, this is just like dumb. This is not, you know, this is not for regular\\npeople at that time. And so you, you had to think\\nthrough that and then look, it was still hard to get on the internet at that point, right? So you could get kind of this\\nweird bastardized version if you were on AOL,\\nwhich wasn\\'t really real. Or you had to go like,\\nlearn what an ISP was. You know, in those days, PCs actually didn\\'t have TCPIP\\ndrivers come reinstalled. So you had to learn\\nwhat a TCPIP driver was. You had to buy a modem, you\\nhad to install driver software. I have a comedy routine. I do. So it\\'s like 20 minutes long\\ndescribing all the steps required to actually get on\\nthe internet at this point. And so you had to look\\nthrough these practical. Well, and then speed performance 14-4 modems, right? Like it was like watching,\\nyou know, glue dry, like, and so you had to, there were basically a sequence of bets that we made where you basically needed to look through that current\\nstate of affairs and say, actually there\\'s gonna\\nbe so much demand for once people figure this out, there\\'s gonna be so much demand for it that all of these practical problems\\nare gonna get fixed. - Some people say that\\nthe anticipation makes the destination that much more exciting. - Do you remember progressive JPEGs? - Yeah. Do I, do I? - For kids in the audience, right? - [Lex] For kids in the audience. - You used to have to watch an image load like a line at the time. But it turns out there\\nwas this thing with JPEGs where you could load\\nbasically every fourth, you could load like every fourth line and then you could sweep\\nback through again. And so you could like\\nrender a fuzzy version of image up front. And then it would like\\nresolve into the detailed one. And that was like a big UI breakthrough \\'cause it gave you something to watch. - Yeah. And you know, there\\'s applications in\\nvarious domains for that. - Well it was a big fight. There was a big fight early on\\nabout whether there should be images in the web. And. - For that reason for\\nlike sexualization or-- - Not explicitly that that did come up. But it wasn\\'t even that, it was more just like all the\\nserious in the argument went, the purists basically said\\nall the serious information in the world is text. If you introduce images, you basically are gonna bring\\nin all the trivial stuff. You\\'re gonna bring in\\nmagazines and you know, all this crazy just, you know,\\nstuff that, you know, people, you know, it\\'s gonna, it is\\ngonna distract from that. It\\'s gonna go take it away from being\\nserious to being frivolous. - Well, was there any\\n(indistinct) type arguments about the internet destroying all of human\\ncivilization or destroying some fundamental fabric of human civilization? - So it was, those days it was all around crime and terrorism. So those arguments happened, you know, but there was no sense yet\\nof the internet having like, an effect on politics because\\nthat was way too, too far off. But there was an enormous panic at the time around cybercrime. There was like enormous panic\\nthat like your credit card number would get stolen and you\\'d use life\\nsavings would be drained. And then, you know, criminals were gonna, there was, oh, when we started,\\none of the things we did, one of the Netscape browser\\nwas the first widely used piece of consumer software that had\\nstrong encryption built in, it made it available to ordinary people. And at that time, strong encryption was\\nactually illegal to export outta the US so we could feel that product in the US, we could not export it \\'cause it was classified as munition. So the Netscape browser\\nwas on a restricted list along with the tomahawk missile as being something that\\ncould not be exported. So we had to make a second\\nversion with deliberately weak encryption to sell\\noverseas with a big logo on the box saying, do not trust this. Which it turns out, makes it hard to sell software\\nwhen it\\'s got a big logo that says don\\'t trust it. And then we had to spend\\nfive years fighting the US government to get\\nthem to basically stop trying to do this regulation. But because the fear\\nwas terrorists are gonna use encryption, right? To like plot, you know, all these things. And then, you know, we responded with, well actually we need\\nencryption to be able to secure systems so that the terrorists and the criminals can\\'t get into them. So that anyway, that was the 1990s fight. - So can you say something\\nabout some of the details of the software engineering\\nchallenges required to build these browsers? I mean the engineering\\nchallenges of creating a product that hasn\\'t really existed before that can have such\\nalmost like limitless impact on the world with the internet. - So there was a really key\\nbet that we made at the time, which was very controversial, which was core to core\\nto how it was engineered, which was are we\\noptimizing for performance or for ease of creation? And in those days the pressure\\nwas very intense to optimize for performance because the\\nnetwork connections were so slow and also the computers were so slow. And so if you had, I mentioned\\nthe progressive JPEGs, like if there\\'s an alternate\\nworld in which we optimized for performance and it just, you had just a much more pleasant\\nexperience right up front. But what we got by not doing that was we got ease of creation. And the way that we got\\nease of creation was all of the protocols and\\nformats were in text, not in binary. And so HTTP is in text, by the way. And this was an internet\\ntradition by the way that we picked up. But we continued it. HTTP is text and HTML is\\ntext, and then every else, everything else that\\nfollowed is text as a result. And by the way, you can imagine purist\\nengineers saying this is insane. You have very limited bandwidth. Why are you wasting any time sending text? You should be encoding\\nthis stuff into binary and it\\'ll be much faster. And of course the answer\\nis that\\'s correct. But what you get when you make\\nit taxed is all of a sudden, well, the big breakthrough was the view source function, right? So the fact that you\\ncould look at a webpage, you could hit view source\\nand you could see the HTML, that was how people learned\\nhow to make webpages. Right? - It\\'s so interesting \\'cause the stuff would\\ntake for granted now is, man, that was fundamental, the development of the web\\nto be able to have HTML just right there, all the\\nghetto mess that is HTML, all the sort of almost\\nbiological like messiness of HTML and then having the browser\\ntry to interpret that as. - [Marc] Exactly. - To show something reasonable. - Well and then there was\\nthis internet principle that we inherited, which\\nwas emit, what was it? Emit cautiously. Emit\\nconservatively interpret liberally. So it basically meant if you\\'re, the design principle was if you\\'re creating like a web editor that\\'s gonna admit HTML, like\\ndo it as cleanly as you can, but you actually want the\\nbrowser to interpret liberally, which is you actually want\\nusers to be able to make all kinds of mistakes and\\nfor it to still work. And so the browser rendering\\nengines to this day have all of this spaghetti code crazy stuff where they\\'re resilient to\\nall kinds of crazy issue, no mistakes. And so, literally what I\\nalways had in my head is like there\\'s an 8 year old or\\nan 11 year old somewhere and they\\'re doing a view source, they\\'re doing a cut and\\npaste and they\\'re trying to make a webpage for\\ntheir eternal or whatever. And like they leave out a\\nslash and they leave out an angle bracket and they do this and they do that and it\\'s still works. - It\\'s also like a, I don\\'t often think about this,\\nbut, you know, programming, you know, C++ all those languages, lisp, the compiled languages,\\nthe interpreted languages, Python, Pearl, all that. The brace have to be all correct. It\\'s like everything has to be perfect. - [Marc] Brutal. - And then-- - [Marc] Autistic. - You forget. All right. It\\'s systematic\\nand rigorous, let\\'s go there. But you forget that the, the web with JavaScript eventually. And HTML is allowed to be messy in the way for the first time. Messy in the way biological\\nsystems could be messy. It\\'s like the only thing\\ncomputers were allowed to be messy on for the first time. - It used to off fend me. So I grew up on Unix, so I worked on Unix. I was a Unix native for all\\nthe way through this period. And so, it used to drive\\nme bananas when it would do the segmentation fault\\nand the core dump file, just like it is, you know, it\\'s like literally there\\'s\\nlike a error in the code. The math is off by one. And it core dumps. And I\\'m in the core dump\\ntrying to analyze it and trying to reconstruct what, and I\\'m\\njust like, this is ridiculous. Like, the computer\\nought to be smart enough to be able to know that if it\\'s off by one, okay fine. And it keeps running. And I would go ask all the experts like, why can\\'t it just keep running? And they\\'d explain to me, well, because all the downstream\\nrepercussions and blah blah. And I\\'m like, this still,\\nlike, you know, this is, we\\'re forcing the human\\ncreator to live to your point in this hyper, literal\\nworld of perfection. - [Lex] Yeah. And I was just like, that\\'s just bad. And by the way, you know what happens with that of course. Just what what happened with,\\nwith coding at that point, which is you get a high\\npriesthood, you know, there\\'s a small number of\\npeople who are really good at doing exactly that. Most people can\\'t. And most people are excluded from it. And so actually that was where that for that was where I picked up that idea was like, no, you want these things to be\\nresilient error in all kinds and this would drive the\\npurist absolutely crazy. Like, I got attacked on this like a lot \\'cause I mean like every time you know, all the purists who\\nwere like into all this like Marcup language stuff and formats and codes and all this stuff, they would be like, you know, you\\'re encouraging bad behavior \\'cause. - Oh, so they wanted\\nthe browser to give you a fault error anytime there was a-- - Yeah. They wanted to\\nbe a (indistinct) right? They wanted to-- Yeah. Yeah. That was a very and any properly trained credential engineer would be like, that\\'s not how you build these systems. - That\\'s such a bold move to say, no, it doesn\\'t have to be. - Yeah. No, like I said, the good news for me is\\nthe internet kind of had that traditional already,\\nbut having said that, like we pushed it, we pushed it way out. But the other thing we did, going back to the performance thing, was we gave up a lot of performance. We made that, that initial experience for the first few years\\nwas pretty painful. But the bet there was\\nactually an economic bet, which was basically the demand\\nfor the web would basically mean that there would be a\\nsurge in supply of broadband. Like because the question was, okay, how do you get the phone\\ncompanies which are not famous in those days for doing\\nnew things at huge cost for like speculative reasons. Like how do you get them to\\nbuild up broadband, you know, spend billions of dollars\\ndoing that and you know, you could go meet with them\\nand try to talk them into it. Or you could just have a thing where it\\'s just very\\nclear that it\\'s gonna be, that people love that\\'s gonna\\nbe better if it\\'s faster. And, so that, there was a\\nperiod there and this was, this was fraught with in peril, but there was a period there\\nwhere it\\'s like we knew the experience was sub-optimized because we were trying to force the emergence of demand for broadband. - [Lex] Sure. - Which is in fact what happened. - So you had to figure out\\nhow to display this text, HTML text. So the blue links and\\nthe prop links. What? And there\\'s no standards. Is\\nthere standards at that time? - [Marc] No. There really still isn\\'t. - Well there\\'s like standards, there\\'s applied, implied standards. Right. And they, you know, there\\'s all these kind of new features that are being added with like CSS, what, like what kind of stuff a\\nbrowser should be able to support features within languages,\\nwithin JavaScript and so on. But you\\'re setting standards\\non the fly yourself. - Yeah. Well to this day, if you create a webpage\\nthat has no CSS style sheet, the browser will render\\nit however it wants to. Right. So this was one of the\\nthings, there was this idea, this idea of at the time and\\nhow these systems were built, which is separation of content from format or separation of content from appearance. And that\\'s still, people\\ndon\\'t really use that anymore \\'cause everybody wants to\\ndetermine how things look and so they use CSS\\nbut it\\'s still in there that you can just let the\\nbrowser do all the work. - I still like the like\\nreally basic websites, but that could be just old school, kids these days with their\\nfancy responsive websites that don\\'t actually have much content, but have a lot of visual elements. - Well that\\'s one of the\\nthings that\\'s fun about chat, you know, about ChatGPT like. - [Lex] Back to the basics. - Back to just text. - [Lex] Yeah. - Right? And it, you know, there is this pattern in\\nhuman creativity and media where you end up back at text\\nand I think there\\'s, you know, there\\'s something powerful in there. - Is there some other stuff you remember like the purple links? There were some interesting\\ndesign decisions that to kind of come up that we have today or we don\\'t have today\\nthat were temporary. - So I made the background\\n\\'cause I hated reading texts on white background, so I\\nmade the background gray. Everybody can-- - Do you go ahead to? - No. No, no. That decision I think has been reversed. But now I\\'m happy though because\\nnow dark mode is the thing. - So it wasn\\'t about gray, it was just you didn\\'t\\nwant white background. - [Marc] Strain my eyes. - Strain your eyes. Interesting. And then there\\'s a bunch\\nof other decisions. I\\'m sure there\\'s an interesting\\nhistory of the development of HTML and CSS and\\nInterface and JavaScript and there\\'s this whole Java applet thing. - Well the big one probably\\nJavaScript, CSS was after me, so I didn\\'t, that was not me. But JavaScript was the big, JavaScript maybe was the\\nbiggest of the whole thing. That was us. And that was basically a bet,\\nit was a bet on two things. One is that the world wanted a new front end scripting language. And then the other was I thought at the time the world wanted a new backend scripting language. So JavaScript was designed\\nfrom the beginning to be both front end and backend. And then it failed as a\\nbackend scripting language. And Java won for a long time. And then Python Pearl and\\nother things, PHP and Ruby. But now JavaScript is back. And so. - I wonder if everything in\\nthe end will run on JavaScript. - It seems like it is the, and by the way, lemme give a shout out\\nto, to Brendan Eich was basically the one man\\ninventor of of JavaScript. - If you\\'re interested to\\nlearn more about Brendan Eich, he\\'s been on his podcast previously. - Exactly. So he wrote\\nJavaScript over a summer and I mean I think it is fair, it is fair to say now that\\nit\\'s the most widely used language in the world and\\nit seems to only be gaining in its in its range of adoption. - You know, in the software world there\\'s quite a few stories of somebody over a week weekend or over a\\nweek or over a summer writing some of the most impactful\\nrevolutionary pieces of software ever. That\\nshould be inspiring. Yes. - Very inspiring. I\\'ll\\ngive you another one. SSL. So SSL with the security\\nprotocol, that was us. And that was a crazy idea at the time, which was let\\'s take\\nall the native protocols and let\\'s wrap them in a security wrapper. That was a guy named Kip Hickman who wrote that over a summer, one guy. And then look today, sitting here today, like the transformer like at Google was a small handful of people. And then, you know, the number of people who have\\ndid like the core work on GPT. It\\'s not that many people, it\\'s a pretty small handful of people. And so yeah, the pattern in software repeatedly over a very long time has been, it\\'s Jeff Bezos always\\nhad the two pizza rule for teams at Amazon, which is any team needs\\nto be able to be fed with two pieces. If you need the third pizza,\\nyou have too many people. And I think it\\'s actually\\nthe one pizza rule. For the really creative work. I think it\\'s two people, three people. - Well that\\'s, you see that with certain open source projects, like so much is done by\\nlike one or two people. Like it\\'s so incredible\\nand that\\'s why you see that gives me so much hope\\nabout the open source movement in this new age of AI where, you know, just recently having had a conversation with Marc Zuckerberg of all people who\\'s all in on open source, which is so interesting to\\nsee and so inspiring to see \\'cause like releasing\\nthese models, it is scary. It is potentially very dangerous\\nand we\\'ll talk about that. But it\\'s also, if you believe in the\\ngoodness of most people and in the skillset of most people and the desire to go do good in the world, that\\'s really exciting. \\'cause it\\'s not putting it these models into the centralized\\ncontrol of big corporations, the government and so on. It\\'s putting it in the hands of a teen, teenage kid with like a dream in his eyes. I don\\'t know. That\\'s beautiful. - Look, this stuff, AI ought to make the\\nindividual coder obviously far more productive right? By like, you know, a\\nthousand X or something. And so you ought to open source like, not just the future of open source AI, but the future of open source everything. We ought to have a world\\nnow of super coders, right? Who are building things as open source with one or two people\\nthat were inconceivable, you know, five years ago. You know, the level of\\nkind of hyper productivity we\\'re gonna get out of\\nour best and brightest I think is gonna go way up. - It\\'s gonna be interesting. We\\'ll talk about it, but let\\'s just to linger\\na little bit on Netscape. Netscape was acquired in\\n1999 for 4.3 billion by AOL. What was that like? What were some memorable aspects of that? - Well that was the height\\nof the.com boom bubble bust. I mean that was the frenzy. If you watch succession, that was like what they\\ndid in the fourth season with Gojo and the merger with their, so it was like the height of like one of those kind of dynamics. And so. - Would you recommend\\nsuccession, by the way? I\\'m more of a Yellowstone guy. - Yellowstone\\'s very American. I\\'m very proud of you. That\\'s, that is. - I just talked to Matthew McConaughey and I\\'m full on Texan at this point. - Good. I approve. - And he\\'ll be doing\\nthe SQL to Yellowstone. - [Marc] Yeah, just exciting. - Very exciting. Anyway. - [Marc] Can\\'t wait. - So that\\'s a rude interruption\\nby me by way of succession. So, that was at the height of the-- - Deal making and money\\nand just the fur flying and like craziness. And so yeah, it was just one of those, it was just like, I mean, and this, the entire (indistinct) thing from start to finish was four years, which was like for one of these companies, it\\'s just like incredibly fast. You know, it went, we went public 18 months\\nafter we got moved where we were founded, which\\nvirtually never happens. So it was just this incredibly fast kind of meteor streaking across the sky. And then of course it was this, and then there was just\\nthis explosion, right? That happened \\'cause then\\nit was almost immediately followed by the.com crash. It was then followed\\nby AOL, by Time Warner, which again is like the succession guys kinda play with that, which turned out to be a disastrous deal. You know, one of the famous, you know, kind of disastrous in business history. And then, you know, what became an internet depression on\\nthe other side of that. But then in that depression\\nin the two thousands was the beginning of broadband and smartphones and Web 2.0 right? And then social media\\nand search and every SaaS and everything that came out of that. - What did you learn from\\njust the acquisition? I mean this is so much money. What\\'s interesting \\'cause I\\nmust have been very new to you, that these software stuff, you can make so much money. There\\'s so much money swimming around. I mean, I\\'m sure the\\nideas of investment was starting to get born there. - Yes. Let me get, so let me lay it. So here\\'s, here\\'s the thing. I dunno if I figured it out\\nthen, but figured it out later, which is software is a technology that it, it\\'s like a, you know, the\\nconcept of the philosopher stone, the philosopher stone in alchemy, transient is led into gold and\\nNewton spent 20 years trying to find the philosopher stone. Never got there. Nobody\\'s ever figured it out. Software is our modern philosopher stone. And in economic terms, it\\ntransmutes labor into capital, which is like a super interesting thing. And by the way, like Carl Marcs is rolling over in his grave right now. \\'Cause of course that\\'s\\ncomplete reputation of his entire theory. Trans labor and capital\\nwhich is as follows, is somebody sits down at a keyboard and types a bunch of stuff in, and a capital asset\\ncomes out the other side and then somebody buys that capital asset for a billion dollars. Like that\\'s amazing, right? It\\'s literally creating\\nvalue right out of thin air, right out of purely human thought, right? And so that, there are many things that make software magical and special, but that\\'s the economics. - I wonder what Marx\\nwould\\'ve thought about that? - Oh, he would\\'ve\\ncompletely broke his brain because of course the whole\\nthing was it was he could, you know, that kind of\\ntechnology was inconceivable when he was alive. It was all industrial era stuff. And so, any kind of machinery\\nnecessarily involved huge amounts of capital. And then labor was on the receiving end of the abuse. - [Lex] Yep. Right? But like software eng software, a software engineer is somebody\\nwho basically transmutes his own labor into actual, an actual capital asset\\ncreates permanent value. Well, and in fact it\\'s\\nactually very inspiring. That\\'s actually more\\ntrue today than before. So when I was doing software, the assumption was all\\nnew software basically has a sort of a parabolic\\nsort of lifecycle, right? So you ship the thing,\\npeople buy it at some point, everybody who wants it has bought it and then it becomes obsolete. And it\\'s like bananas. Nobody, nobody buys old software. These days, Minecraft, Mathematica, you know, Facebook, Google, you have the software\\nassets that are, you know, have been around for 30\\nyears that are gaining in value every year, right? And they\\'re just, they\\'re being\\na world of warcraft, right, salesforce.com, like they\\'re being every single year they\\'re\\nbeing polished and polished and polished and polished. They\\'re getting better\\nand better, more powerful, more powerful, more\\nvaluable, more valuable. So we\\'ve entered this era\\nwhere you can actually have these things that actually\\nbuild out over decades. Which by the way is what\\'s happening right now with like ChatGPT. And so now, this is why, you know, there is always, you know, sort of a constant investment\\nfrenzy around software is because, you know, look, when\\nyou start one of these things, it doesn\\'t always succeed. But when it does now you\\nmight be building an asset that builds value for,\\nyou know, four or five, six decades to come. You know, if you have a team of people who have the level of devotion required to keep making it better. And then the fact that of\\ncourse everybody\\'s online, you know, there\\'s 5 billion people that are a click away from\\nany new piece of software. So the potential market size\\nfor any of these things is, you know, nearly infinite. - [Lex] It must have been\\nsurreal back then though. - Yeah. Yeah. This was\\nall brand new, right? Yeah. Back then, this was all brand new. These were all, you know, brand new. Had you rolled out that\\ntheory in even 1999, people would\\'ve thought\\nyou were smoking crack. So that\\'s emerged over time. - Well, let\\'s now turn\\nback into the future. You wrote the essay \"Why\\nAI Will Save The World?\" Let\\'s start the very high level. What\\'s the main thesis of the essay? - Yeah, so the main thesis on the essay is that what we\\'re dealing\\nwith here is intelligence. And it\\'s really important to kind of talk about the sort of very nature\\nof what intelligence is. And fortunately we have a predecessor to machine intelligence,\\nwhich is human intelligence. And we\\'ve got, you know, observations and theories\\nover thousands of years for what intelligence is\\nin the hands of humans and what intelligence is, right? I mean, what it literally is the way to, you know, capture, process,\\nanalyze, synthesize information, solve problems. But the observation of\\nintelligence in human hands is that intelligence quite literally\\nmakes everything better. And what I mean by that\\nis every kind of outcome of like human quality of life, whether it\\'s education outcomes or success of your children, or career success or health or lifetime\\nsatisfaction, by the way, propensity to peacefulness\\nas opposed to violence, propensity for open-mindedness\\nversus bigotry, those are all associated with\\nhigher levels of intelligence. - Smarter people have better outcomes than almost as you write in almost every domain of activity. Academic achievement, job performance, occupational status, income, creativity, physical health, longevity,\\nlearning new skills, managing complex tasks, leadership,\\nentrepreneurial success, conflict resolution,\\nreading comprehension, financial decision making, understanding others\\nperspectives, creative arts, parenting outcomes, and life satisfaction. One of the more depressing\\nconversations I\\'ve had, and I don\\'t know why it\\'s depressing, I have to really think\\nthrough why it\\'s depressing, but on IQ and the G factor, and that that\\'s something\\nin large part is genetic and it correlates so much\\nwith all of these things and success in life. It\\'s like all the inspirational\\nstuff we read about, like if you work hard and so on, it sucks that you\\'re born with the hand that you can\\'t change. - But what if you could. - You\\'re saying basically\\na really important point, and I think it\\'s in your\\narticles, it really helped me. It\\'s a nice added\\nperspective to think about. Listen, human intelligence, the science of intelligence\\nis shown scientifically that it just makes life easier and better the smarter you are. And now let\\'s look at\\nartificial intelligence and if that\\'s a way to increase\\nsome human intelligence, then it\\'s only going\\nto make a better life. - [Marc] Yeah. - That\\'s the argument. - And certainly at the collective level, we could talk about the collective effect of just having more\\nintelligence in the world, which will have very big payoff. But there\\'s also just\\nat the individual level, like what if every person has a machine? You know? And the concept of augment Doug Engelbart\\'s concept of augmentation. You know, what if\\neverybody has an assistant and the assistant is, you know, 140 IQ and you happen to be 110 IQ and you\\'ve got, you know, something that basically is\\ninfinitely patient and knows everything about you\\nand is pulling for you in every possible way,\\nwants you to be successful. And anytime you find anything\\nconfusing or wanna learn anything or have trouble\\nunderstanding something or wanna figure out what to\\ndo in a situation, right? Wanna figure out how to\\nprepare for a job interview, like any of these things,\\nlike it will help you do it. And it will therefore, the combination will\\neffectively be, you know, will effectively raise your raise because it will effectively raise your IQ, will therefore raise the odds of successful life outcomes\\nin all these areas. - So people below the,\\nthis hypothetical 140 IQ, it\\'ll pull them up towards 140 IQ. - Yeah, yeah, yeah. And then of course, you know, people at 140 IQ will be\\nable to have a peer, right. To be able to communicate, which is great. And then people above 140\\nIQ will have an assistance that they can farm things out to. And then look, God willing, you know, at some point these things\\ngo from future versions go from 140 IQ equivalent to\\n150 to 160 to 180, right? Like Einstein was estimated\\nto be on the order of one 60, you know, so when we\\nget, you know, one 60 AI, like we\\'ll be, you know, when one assumes creating\\nEinstein level breakthroughs and physics, and then at\\n180 we\\'ll be, you know, carrying cancer and developing\\nwarp drive and doing all kinds of stuff. And so it is quite possibly the case, this is the most important\\nthing that\\'s ever happened and the best thing that\\'s ever happened because precisely because it\\'s a lever on this single fundamental\\nfactor of intelligence, which is the thing that drives\\nso much of everything else. - Can you steal, man, the case that human plus AI is\\nnot always better than human for the individual? - You may have noticed that there\\'s a lot of\\nsmart running around. - [Lex] Sure. Yes. - Right? And so, like smart, there are certain people where\\nthey get smarter, you know, they get to be more arrogant, right? So that, you know, there\\'s one huge flaw. - Although to push back on that, it might be interesting because\\nwhen the intelligence is not all coming from you,\\nbut from another system, that might actually increase\\nthe amount of humility even in the assholes. - [Marc] One would hope. - Yeah. - Or it could make assholes more assholes. You know, that\\'s in, I mean, that\\'s for psychology to study. - Yeah, exactly. Another one is smart people\\nare very convinced that they, you know, have a more\\nrational view of the world, and that they have a easier\\ntime seeing through conspiracy theories and hoaxes and right. You know, sort of crazy\\nbeliefs and all that. There\\'s a theory in psychology, which is actually smart people. So for sure people who aren\\'t\\nas smart are very susceptible to hoaxes and conspiracy theories. But it may also be the case\\nthat the smarter you get, you become susceptible in a different way, which is you become very\\ngood at marshaling facts to fit preconceptions, right. You become very, very good at assembling whatever theories and\\nframeworks and pieces of data and graphs and\\ncharts you need to validate whatever crazy ideas got in your head. And so you\\'re susceptible\\nin a different way, right? - We\\'re all sheep, but\\ndifferent colored sheep. - Some sheep are better\\nat justifying it. Right. And those are the, you know, those are the smart sheep, right? So yeah. Look like I\\nwould say this look like there are no panacea. I\\'m not a utopian, there\\nare no panaceas in life. There are no, like, you know, I don\\'t believe there\\nare like pure positives. I\\'m not a transcendental\\nkind of person like that. But, you know, so yeah,\\nthere are gonna be issues and, you know, look, smart people, another maybe you could\\nsave about smart people is they are more likely to get\\nthemselves in situations that are, you know, beyond their grasp. You know, because they\\'re\\njust more confident in their ability to deal with complexity and their eyes become bigger, their cognitive eyes become bigger than their stomach, you know? So yeah, you could argue\\nthose eight different ways nevertheless, on net, right? Clearly, overwhelmingly, again, if you just extrapolate from what we know about human intelligence, you\\'re improving so many aspects of life if you\\'re upgrading intelligence. - So there\\'ll be assistants\\nat all stages of life. So when you\\'re younger,\\nthere\\'s for education, all that kind of stuff for\\nmentorship, all of this. And later on as you\\'re doing\\nwork and you\\'ve developed a skill and you\\'re having a profession, you\\'ll have an assistant\\nthat helps you excel at that profession. So at all stages of life. - Yeah. I mean, look, the\\ntheory is augmentation. This is the Doug Engelbart\\'s term. Doug Engelbart made this observation many, many\\ndecades ago that, you know, basically it\\'s like you can\\nhave this oppositional frame of technology where it\\'s\\nlike us versus the machines, but what you really do\\nis you use technology to augment human capabilities. And by the way, that\\'s how actually the economy develops. That\\'s, we can talk about\\nthe economic side of this, but that\\'s actually how\\nthe economy grows is through technology\\naugmenting human potential. And so, yeah. And then you basically\\nhave a proxy or you know, or you know, a sort of\\nprosthetic, you know, so like you\\'ve got glasses,\\nyou\\'ve got a wristwatch, you know, you\\'ve got shoes, you know, you\\'ve got these things. You\\'ve got a personal computer, you\\'ve got a word processor,\\nyou\\'ve got Mathematica, you\\'ve got Google. This is the latest\\nviewed through that lens. AI is the latest in a long\\nseries of basically augmentation methods to be able to\\nraise human capabilities. It\\'s just this one is the\\nmost powerful one of all, because this is the one\\nthat, that goes directly to what they call fluid\\nintelligence, which is IQ. - Well, there\\'s two categories of folks that you outline that\\nworry about or highlight the risks of AI, and you highlight a bunch of different risks. I would love to go through those risks and just discuss them, brainstorm which ones are serious and which ones are less serious. But first, the Baptist\\nand the bootleggers, what are these two\\ninteresting groups of folks who worry about the effect\\nof AI and human civilization? - [Marc] Or say they do. - Say, oh, okay, yes, I\\'ll say they do. - The Baptist worry the\\nbootleggers say they do. So the Baptist and the\\nbootleggers is a metaphor from economics, from what\\'s\\ncalled development economics. And it\\'s this observation that when you get social\\nreform movements in a society, you tend to get two sets\\nof people showing up, arguing for the social reform. And the term Baptist and bootleggers comes from the American experience\\nwith alcohol prohibition. And so in the 1900s, 1910s, there was this movement\\nthat was very passionate at the time, which basically said, alcohol is evil and is destroying society. By the way, there was a lot\\nof evidence to support this. There were very high rates of very high correlations\\nthen, by the way. And now between rates of physical\\nviolence and alcohol use, almost all violent crimes\\nhave either the perpetrator or the victim, or both drunk almost. If you see this actually in the work, almost all sexual harassment\\ncases in the workplace, it\\'s like at a company\\nparty and somebody\\'s drunk. Like, it\\'s amazing how often\\nalcohol actually correlates to actually dis dysfunction\\nand at leads to domestic abuse and so forth, child abuse. And so you had this group of\\npeople who were like, okay, this is bad stuff and we should outlaw it. And those were quite literally Baptist. Those were super committed, you know, hardcore Christian\\nactivists in a lot of cases. There was this woman whose\\nname was Carrie Nation, who was this older woman who\\nhad been in this, you know, I don\\'t know, disastrous\\nmarriage or something. And her husband had been\\nabusive and drunk all the time. And she became the icon of\\nthe Baptist prohibitionist. And she was legendary in\\nthat era for carrying an ax and doing, you know, completely on her own\\ndoing raids of saloons and like taking her ax to all the bottles and eggs in the back. And so. - [Lex] A true believer. - An absolute true believer, and with absolutely the\\npurist of intentions. And again, there\\'s a very\\nimportant thing here, which is there\\'s, you could look at this\\ncynically and you could say the Baptists are like delusional,\\nyou know, the extremists, but you could also say,\\nlook, they\\'re right. Like she was, you know, she had a point. Like she wasn\\'t wrong about\\na lot of what she said. - Yeah. - But it turns out the way\\nthe story goes is it turns out that there were another set of people who very badly wanted to\\noutlaw alcohol in those days. And those were the bootleggers, which was organized crime that\\nstood to make a huge amount of money if legal alcohol\\nsales were banned. And this was, in fact, the way the history goes\\nis this was actually the beginning of\\norganized crime in the US. This was the big economic\\nopportunity that opened that up. And so they went in together and no, they didn\\'t go in together. Like the Baptist did not\\neven necessarily know about the bootleggers \\'cause they were on their moral crusade. The bootleggers certainly\\nknew about the Baptists. And they were like, wow, these people are like the\\ngreat front people for like. You know, it\\'s-- - [Lex] Good PR. - Shenanigans in the background. And they got the (indistinct)\\nAct passed, right. And they did in fact ban alcohol\\nin the US and you\\'ll notice what happened, which is\\npeople kept drinking, it didn\\'t work, people kept drinking. That bootleggers made a\\ntremendous amount of money. And then over time it became\\nclear that it made no sense to make it illegal and it\\nwas causing more problems. And so then it was revoked. And here we sit with legal\\nalcohol a hundred years later with all the same problems. And you know, the whole thing was this\\nlike giant misadventure the Baptist got taken advantage\\nof by the bootleggers, and the bootleggers got what they wanted. And that was that. - The same two categories of folks are now sort of suggesting\\nthat the development of artificial intelligence\\nshould be regulated. - A hundred percent. It\\'s the same pattern. And the economist will tell you it\\'s the same pattern every time. Like, this is what\\nhappened, nuclear power, this is what happens, which\\nis another interesting one. But like, yeah, this\\nhappens dozens and dozens of times throughout the\\nlast a hundred years and this is what\\'s happening now. - And you write that it isn\\'t\\nsufficient to simply identify the actors and impugn their motives. We should consider the\\narguments of both the Baptist and the bootleggers on their merits. So let\\'s do just that. Risk number one, will AI kill us all? - [Marc] Yes. - So what do you think about this one? What do you think is\\nthe core argument here that the development of\\nAGI perhaps better said, will destroy human civilization? - Well, first of all, you\\njust did a slight of hand \\'cause we went from talking about AI to AGI. - Is there a fundamental difference there? - I don\\'t know. What\\'s AGI? - What\\'s AI, what\\'s in intelligence? - Well, I know what AI\\nis machine learning. What\\'s AGI? - I think we don\\'t know\\nwhat the bottom of the well of machine learning is\\nor what the ceiling is. Because just to call\\nsomething machine learning or just to call some of the statistics or just to call it math or\\ncomputation doesn\\'t mean, you know, nuclear\\nweapons are just physics. So to me it\\'s very\\ninteresting and surprising how far machine learning has taken. - No, but we knew that\\nnuclear physics would lead to weapons. That\\'s why the scientists\\nof that era were always in some this huge dispute\\nabout building the weapons. This is different. AGI is different. - Does machine learning lead, do we know? - We don\\'t know, but this\\nis my point is different. We actually don\\'t know. But, and this is where you, the slide of hand kicks in, right? This is where it goes from\\nbeing a scientific topic to being a religious topic. And that\\'s why I specifically called out \\'cause that\\'s what happens. They do the vocabulary\\nshift and all of a sudden you\\'re talking about something totally. That\\'s not actually real. - Well then maybe you can\\nalso, as part of that, define the western\\ntradition of Millennialism. - [Marc] Yes. Into the world apocalypse. - [Lex] What is it? - [Marc] Apocalypse cults. - [Lex] Apocalypse cults. - Well, so we live in, we of course live in a Judeo-Christian, but primarily Christian\\nkind of saturated, you know, kind of Christian, post-Christian,\\nsecularized Christian, you know, kind of world in the west. And of course court of Christianity is the idea of the second\\ncoming and you know, the revelations and you know, Jesus returning and the\\nthousand year, you know, utopia on earth and then you know, the rapture and like all\\nall that stuff, you know, you know, we collectively,\\nyou know, as a society, we don\\'t necessarily take\\nall that fully seriously now. So, what we do is we create our\\nsecularized versions of that we keep looking for utopia. We keep looking for, you know, basically the end of the world. And so what what you see over, over decades is that basically a pattern of these sort of these of\\nis this is what cults are. This is how cults form as\\nthey form around some theory of the end of the world. And so the people\\'s temple cults, the Manson cult, the Heavens Gate cult, the David Qresh cult, you know what they\\'re all\\norganized around is like, there\\'s gonna be this\\nthing that\\'s gonna happen that\\'s gonna basically bring\\ncivilization crashing down. And then we have this\\nspecial elite group of people who are gonna see it\\ncoming and prepare for it. And then they\\'re the people\\nwho are either going to stop it or are failing, stopping it. They\\'re gonna be the people\\nwho survived the other side and ultimately get credit\\nfor having been, right. - Why is that so compelling,\\ndo you think? Like-- - Because it satisfies this very deep need we have for transcendence and meaning that got stripped\\naway when we became secular. - Yeah, but why is the\\ntranscendence involve the destruction of human civilization? - Because like how plausible it\\'s like a very deep psychological thing \\'cause it\\'s like how plausible, how plausible is it\\nthat we live in a world where everything\\'s just\\nkind of all right? Right. How exciting? - [Lex] Whoa. - How exciting is that? Right? - [Lex] But that\\'s. - We got more than that. - But that\\'s the deep question I\\'m asking. Why is it not exciting to live in a world where everything\\'s just all right? Is it, I think, you know, most of the animal kingdom would be so happy with just all right. Because that means survival. Why are we, maybe that\\'s what it is. Why are we conjuring up\\nthings to worry about? - So CS Lewis called\\nit the God-shaped hole. So there\\'s a God-shaped hole\\nin the human experience, consciousness, soul,\\nwhatever you wanna call it, where there\\'s gotta be\\nsomething that\\'s bigger than all this. There\\'s gotta be something transcendent. There\\'s gotta be something\\nthat is bigger, right? Bigger purpose. A bigger meaning. And so we have run the\\nexperiment of, you know, we\\'re just gonna use\\nscience and rationality and kind of, you know, everything\\'s just gonna\\nkind of be as it appears. And large number of people have found that very deeply wanting and\\nhave constructed narratives. And by this is the story\\nof the 20th century, right? Communism, right? Was one of those, communism\\nwas a was a form of this, Nazism was a form of this. You know, some people, you know, you can see movements\\nlike this playing out all over the world right now. - So you constructed a kind of devil, a kind of source of evil, and we\\'re going to transcend beyond it. - Yeah. And (indistinct)\\nwhen you see a Miller cult, they put a really specific point on it, which is end of the world, right, there is some change coming. And that change that\\'s\\ncoming is so profound and so important that\\nit\\'s either gonna lead to utopia or hell on earth. Right? And it is going to, and then, you know, it\\'s like what if you actually knew that was going to happen, right? What would you do? Right? How would you prepare yourself for it? How would you come together with a group of like-minded people, right? How would you, what would you do? Would you plan like Cassius\\nof weapons in the woods? Would you like, you know, I don\\'t know if create\\nunderground buckers, would you, you know, spend your\\nlife trying to figure out a way to avoid having it happen? - Yeah. That\\'s a really\\ncompelling, exciting idea to have a club over. To have a little bit of travel, like a get together on a Saturday\\nnight and drink some beers and talk about the end of the world and how you are the only\\nones who have figured it out. - Yeah. And then once you lock in on that, like how can you do anything\\nelse with your life? Like this is obviously the\\nthing that you have to do. And then there\\'s a psychological\\neffect that you alluded to. There\\'s a psychological effect. If you take a set of true\\nbelievers and you leave them to themselves, they get\\nmore radical. Right. \\'Cause they self radicalize each other. - That said, it doesn\\'t mean they\\'re not sometimes right. - Yeah. The end of the world might be. Yes. Correct. Like they might be right. - [Lex] Yeah. - But like-- - [Lex] I have some pamphlets for you. - Exactly. - But I mean we\\'ll talk\\nabout nuclear weapons \\'cause you have a really\\ninteresting little moment that I learned about in\\nyour essay, but you know, sometimes it could be right. - [Marc] Yeah. - \\'Cause we\\'re still, you were developing more and\\nmore powerful technologies in this case, and we don\\'t know what the impact it will\\nhave on human civilization while we can highlight all\\nthe different predictions about how it\\'ll be positive, but the risks are there and\\nyou discuss some of them. - Well, the steel man, the\\nsteel man is the steel man. Well actually, the steel\\nman and his reputation are the same, which is you can\\'t predict what\\'s gonna happen. Right. You can\\'t rule out that this\\nwill not end everything. Right. But the response to that\\nis you have just made a completely non-scientific claim. You\\'ve made a religious\\nclaim, not a scientific claim. - How does it get disproven? - And there\\'s no, by definition with these kinds of claims, there\\'s no way to disprove them. Right? And so there there\\'s no, you\\njust go right on the list. There\\'s no hypothesis, there\\'s no testability of the hypothesis. There\\'s no way to falsify the hypothesis, there\\'s no way to measure\\nprogress along the arc. Like it\\'s just all completely missing. And so it\\'s not scientific and. - I don\\'t think it\\'s completely missing. It\\'s somewhat missing. So for example, the people that say AI\\'s gonna kill all of us. I mean, they usually have\\nideas about how to do that. Whether it\\'s the people\\nclub maximizer or, you know, it escapes there\\'s mechanism\\nby which you can imagine it killing all humans. - [Marc] Models. - And you can disprove it by saying there\\'s a limit to the speed at which intelligence increases. Maybe show that like the sort\\nof rigorously really described model, like how it could\\nhappen and say, no, there, here\\'s a physics limitation. There\\'s like a physical\\nlimitation to how these systems would actually do damage\\nto human civilization. And it is possible they\\nwill kill 10 to 20% of the population, but it seems impossible\\nfor them to kill 99%. - It was practical\\ncounterarguments. Right. So you mentioned\\nbasically what I described as the thermodynamic counterargument, which, so sitting here today, it\\'s like where with the\\nevil AGI get the GPU. \\'Cause like they don\\'t exist. So if you\\'re gonna have a\\nvery frustrated baby evil AGI, who\\'s gonna be like trying to\\nbuy Nvidia stock or something to get them to finally\\nmake some chips, right? So the serious form of that\\nis the thermodynamic argument, which is like, okay, where\\'s\\nthe energy gonna come from? Where\\'s the processor gonna be running? Where\\'s the data center\\ngonna be happening? How is this gonna be\\nhappening in secret such that, you know, it\\'s not, you know, so that\\'s a practical counter argument to the runaway AGI thing. I have a but I have and we\\ncan argue that, discuss that. I have a deeper objection to it, which is it\\'s, this is all forecasting. It\\'s all modeling, it\\'s\\nall future prediction. It\\'s all future hypothesizing. It\\'s not science. - [Lex] Sure. - It is not. It is the\\nopposite of science. So the, I\\'ll pull up Carl Sagan\\nextraordinary claims require extraordinary proof, right? These are extraordinary claims. The policies that are being\\ncalled for right to prevent this are of extraordinary magnitude that, and I think we\\'re gonna\\ncause extraordinary damage. And this is all being done\\non the basis of something that is literally not scientific. It\\'s not a testable hypothesis. - So the moment you say\\nAI\\'s gonna kill all of us, therefore we should ban it, or that we should regulate\\nall that kind of stuff, that\\'s when it starts getting serious. - Or start, you know, military\\nairstrikes and data centers. - [Lex] Oh boy. - Right? And like. - Yeah. This once get starts. Well, so starts getting real weird. - So here\\'s the problem with Arian cults. They have a hard time\\nstaying away from violence. - Yeah. But violence is so fun. - If you\\'re on the right end of it, they have a hard time avoiding violence. The reason they have a hard\\ntime avoiding violence is if you actually believe the claim. Right. Then what would you do to\\nstop the end of the world? Well, you would do anything, right? And so, and this is\\nwhere you get, and again, if you just look at the\\nhistory of Arian and cults, this is where you get the\\npeople\\'s temple and everybody killing themselves in the jungle. And this is where you get\\nCharles Manson and, you know, sending in to kill the pigs. Like, this is the problem with these. They have a very hard time to run the line at actual violence. And I think in this case, I\\nmean, they\\'re already calling for it like today and you know, where this goes from here\\nis they get more worked up. Like I think is like really concerning. - Okay. But that\\'s kind of the extremes. So, you know, the extremes of\\nanything are I was concerning. It\\'s also possible to kind\\nof believe that AI has a very high likelihood\\nof killing all of us. But and therefore we should maybe consider slowing development or regulating, so not violence or any\\nof these kinds of things. But it\\'s saying like, all right, let\\'s take a pause here. You know, you biological\\nweapons, nuclear weapons. Like whoa, whoa, whoa, whoa, whoa. This is like serious stuff.\\nWe should be careful. So it is possible to kinda have a more rational response, right? If you believe this risk is real. - [Marc] Believe. - Yes. So what is it possible to be, have a scientific approach to\\nthe prediction of the future? - I mean, we just went\\nthrough this with COVID. What do we know about modeling? - [Lex] Well, I mean. - What did we learn about\\nmodeling with COVID? - [Lex] There\\'s a lot of lessons. - They didn\\'t work at all. - [Lex] They worked poorly. - The models were terrible,\\nthe models were useless. - I don\\'t know if the models\\nwere useless or the people interpreting the models and\\nthen decentralized institutions that were creating policy\\nrapidly based on the models and leveraging the models in order to support their narratives versus actually\\ninterpreting the error bars and the models and all that kind of stuff. - What you had with COVID, my view you had with COVID\\nis you had these experts showing up and they\\nclaimed to be scientists and they had no testable\\nhypotheses whatsoever. They had a bunch of models. They had a bunch of forecasts\\nand they had a bunch of theories and they laid these out in front of policy makers and policy makers freaked\\nout and panicked. Right. And implemented a whole bunch of like, really like terrible decisions\\nthat we\\'re still living with the consequences of, and there was never any\\nempirical foundation to any of the models. None of them ever came true. - Yeah. To push back. There were certainly\\nBaptist and bootleggers in the context of this pandemic, but there\\'s still a\\nusefulness to models. No. - So not if they\\'re, I mean not if they\\'re\\nreliably wrong, right? Then they\\'re actually\\nlike anti-useful. Right. They\\'re actually damaging. - But what do you do with the pandemic? What do you do with any kind of threat? Don\\'t you want to kind of\\nhave several models to play with as part of this discussion of like, what the hell do we do here? - I mean, do they work? Because they\\'re an expectation\\nthat they actually like work that they have actual predictive value. I mean, as far as I can tell with COVID, the policymakers just si up\\nthemselves into believing that there was sub, I\\nmean, look, the scientists, the scientists were at fault. The quote unquote scientists showed up. So I had some insight into this. So there was a, or remember the Imperial College models out of London were the\\nones that were like, these are the gold standard models. So a friend of mine runs\\na big software company and he was like, wow, this is\\nlike, COVID is really scary. And he is like, you know, he contacted this research\\nand he is like, you know, do you need some help? You\\'ve been just building\\nthis model on your own for 20 years. Do you need some, would you like us our coders\\nto basically restructure it so it can be fully adapted for COVID? And the guy said yes\\nand sent over the code and my friend said it was\\nlike the worst spaghetti code he\\'s ever seen. - That doesn\\'t mean it\\'s\\nnot possible to construct a good model of pandemic\\nwith the correct air bars, with a high number of parameters\\nthat are continuously, many times a day updated\\nas we get more data about a pandemic. I would like to believe when\\na pandemic hits the world, the best computer scientists in the world, the best software engineers\\nrespond aggressively and as input take the data\\nthat we know about the virus and it\\'s an output say\\nhere is what\\'s happening in terms of how quickly it\\'s spreading, what that lead in terms of\\nhospitalization and deaths and all that kind of stuff. Here\\'s how likely, how\\ncontagious it likely is. Here\\'s how deadly it likely is based on different conditions, based on different ages and demographics and all that kind of stuff. So here\\'s the best kinds of policy. It feels like you could have models, machine learning that like kind of, they don\\'t perfectly predict the future, but they help you do something \\'cause there\\'s pandemics\\nthat are like, meh, they don\\'t really do much harm. And there\\'s pandemics,\\nyou can imagine them, they could do a huge amount of harm. Like they can kill a lot of people. So you should probably have\\nsome kind of data-driven models that keep updating, that allow you to make\\ndecisions that based like where, how bad is this thing? Now you can criticize how\\nhorrible all that went with the response to this pandemic, but I just feel like there\\nmight be some value to models. - So to be useful at some point it has to be predictive. Right? So and the easy thing\\nfor me to do is to say, obviously you\\'re right. Obviously I wanna see that\\njust as much as you do. \\'cause anything that makes\\nit easier to navigate through society through a\\nwrenching, you know, risk like that sounds great. You know, the harder objection to it is just simply\\nyou are trying to model a complex dynamic system\\nwith 8 billion moving parts. Like not possible. - [Lex] It\\'s very tough. - Can\\'t be done, complex\\nsystems can\\'t be done. - Machine learning says hold my beer. But well, it\\'s possible. No? - I don\\'t know. I would like to believe that it is. I\\'ll put it this way. I think where you and I\\nwould agree is I think we would like that to be the case. We are strongly in favor of it. I think we would also\\nagree that no such thing with respect to COVID or\\npandemics no such thing. At least neither you\\nnor I think are aware. I\\'m not aware of anything like that today. - My main worry with the\\nresponse to the pandemic is that same as with aliens, is that even if such a thing existed, and it\\'s possible it existed, the policymakers were\\nnot paying attention. Like there was no mechanism\\nthat allowed those kinds of models to percolate all. - Oh, I think we had the\\nopposite problem during COVID. I think the policymakers, I think these people with\\nbasically fixed science had too much access to the policymakers. - Well, right. And well, but the policy\\nmakers also wanted, they had a narrative in\\nmind and they also wanted to use whatever model\\nthat fit that narrative - [Marc] Oh, sure. - To help them out. So like, it felt like\\nthere was a lot of politics and not enough science. - Although a big part\\nof what was happening, a big reason we got lockdowns\\nfor as long as we did, was because these scientists\\ncame in with these like doomsday scenarios that were like, just like completely off the hook. - Scientists in quotes, let\\'s not-- - [Marc] Quote unquote scientists. - Let\\'s not, okay,\\nlet\\'s give love science. So here\\'s science that is the way out. - Science is a process\\nof testing hypotheses. Modeling does not involve\\ntestable hypotheses. Right. Like, I don\\'t even know that. I actually don\\'t even know that modeling actually\\nqualifies as science. Maybe that\\'s a side conversation. We could have some time over a beer. - Oh, that\\'s a really interesting part. What do we do about the future? I mean, what\\'s-- - So number one is when\\nwe start with number one, humility goes back to this thing of how do we determine the truth. Number two is we don\\'t believe, you know, it\\'s the old, I\\'ve gotta hammer everything\\nlooks like a nail, right? I\\'ve got, oh, this is one\\nof the reasons I gave you, I gave Alexa book, which the topic of the\\nbook is what happens when scientists basically\\nstray off the path of technical knowledge and\\nstart to weigh in on politics and societal issues. - In this case, philosophers. - Well in this case philosophers. But he actually talks in this\\nbook about, like Einstein, he talks about, actually about\\nthe nuclear age in Einstein. He talks about the\\nphysicists actually doing very similar things at the time. - The book is When Reason Goes On Holiday, Philosophers in Politics by Nevin. - And it\\'s just a story. It\\'s a story. There are other books on this topic, but this is a new one that\\'s really good this is just a story of what happens when experts\\nin a certain domain decide to weigh in and become\\nbasically social engineers and political, you know,\\nbasically political advisors. And it\\'s just a story of just\\ninning catastrophe. Right. And I think that\\'s what\\nhappened with COVID again. - Yeah. I found this book\\na highly entertaining and eye-opening read filled\\nwith amazing anecdote of a rationality and craziness\\nby famous Resa philosophers. - I definitely, after you read this book, you will not look at Einstein the same. - [Lex] Oh boy. - Yeah. - Don\\'t destroy my heroes. - He will not be a hero of yours anymore. Sorry. You probably couldn\\'t,\\nyou shouldn\\'t read the book. - All right. - But here\\'s the thing. The AI risk people, they don\\'t even have the COVID model, at least not that I\\'m aware of. - [Lex] No. - Like there\\'s not even the\\nequivalent of the COVID model. They don\\'t even have the spaghetti code. They\\'ve got a theory and a\\nwarning and a this and the that. And like, if you ask like,\\nokay, well here\\'s, I mean, the ultimate example is,\\nokay, how do we know, right? How do we know that an AI is running away? Like how do we know that the boom takeoff thing\\nis actually happening? And the only answer that\\nany of these guys have given that I\\'ve ever seen is, oh,\\nit\\'s when the loss rate, the loss function and the\\ntraining drops, right? That\\'s when you need to like\\nshut down the data center. Right? And it\\'s like, well that\\'s also what happens when you\\'re successfully training a model. Like, what even this is not science, this is not, it\\'s not\\nanything, it\\'s not a model, it\\'s not anything. There\\'s nothing to arguing with. It is like, you know,\\npunching jello, like there, there\\'s what do you even respond to? - So just put push back on that. I don\\'t think they have good metrics of when the film is happening. But I think it\\'s possible to have that. Like just as you speak now, I mean it\\'s possible to imagine\\nthere could be measures. - It\\'s been 20 years. - No, for sure. But it is been only weeks\\nsince we had a big enough breakthrough in language models. We can start to actually have this, the thing is the AI doer stuff didn\\'t have any actual systems to really work with. And now there\\'s real systems\\nyou can start to analyze like, how does this stuff go wrong? And I think you kind\\nof agree that there is a lot of risks that we can analyze. The benefits outweigh\\nthe risks in many cases. - Well, the risks are not existential. - [Lex] Yes. Well. - Not in the phone paper\\nclip. Let me, okay. There\\'s another slide of hand\\nthat you just alluded to. There\\'s another slide\\nof hand that happens, which is very interesting. - I\\'m very good at the\\nslide of hand thing. - Which is very not scientific. So the book Super Intelligence, right, which is like the Nick Bostrom\\'s book, which is like the origin\\nof a lot of this stuff, which was written, you know, whatever, 10 years ago or something. So he does this really\\nfascinating thing in the book, which is he basically says\\nthere are many possible routes to machine intelligence,\\nto artificial intelligence. And he describes all the different routes to artificial intelligence,\\nall the different possible, everything from biological\\naugmentation through to, you know, all these different things. One of the ones that\\nhe does not describe is large language models because of course the book was written\\nbefore they were invented. And so they didn\\'t exist. In the book, he describes them all and then he proceeds to treat them all as if they\\'re\\nexactly the same thing. He presents them all as sort\\nof an equivalent risk to be dealt with in an equivalent\\nway to be thought about the same way. And then the risk, the quote unquote risk that\\'s actually emerged is actually a completely different technology than he was even imagining. And yet all of his theories\\nand beliefs are being transplanted by this movement, like straight onto this new technology. And so again, like there\\'s no other area of science or technology\\nwhere you do that. Like when you\\'re dealing\\nwith like organic chemistry versus inorganic chemistry,\\nyou don\\'t just like say, oh, with respect to like either\\none, basically maybe, you know, growing up in eating\\nthe world or something, like they\\'re just gonna\\noperate the same way. Like you don\\'t. - But you can start talking about like, as we get more and more actual systems that start to get more\\nand more intelligent, you can start to actually have more scientific arguments here. - [Marc] Oh yeah. - Like, you know, high level, you can talk about the threat\\nof autonomous weapon systems back before we had any\\nautomation in the military. And that would be like\\nvery fuzzy kind of logic. But the more and more you\\nhave drones that are becoming more and more autonomous, you\\ncan start imagining, okay, what does that actually look\\nlike and what\\'s the actual threat of autonomous weapons systems? How does it go wrong? And still it\\'s very vague, but you start to get a\\nsense of like, all right, it should probably be illegal or wrong or not allowed\\nto do like mass deployment of fully autonomous drones that are doing aerial strikes. - [Marc] Oh no. - On large areas. - [Marc] I think it should be required. - Right? So that\\'s a no. - No, no. I think it should be required that only aerial vehicles are automated. - Okay. So you wanna go the other way? - I wanna go the other way. - So that, okay. - I think it\\'s obvious that\\nthe machine is gonna make a better decision than the human pilot. I think it\\'s obvious that\\nit\\'s in the best interest of both the attacker and the\\ndefender and humanity at large. If machines are making\\nmore of these decisions than not people, I think people make terrible\\ndecisions in times of war. - But like, there\\'s ways\\nthis can go wrong too, right? - Well, it wars go terribly wrong now. This goes back to the whole, this is that whole thing\\nabout like the self-drive. Does the self-driving\\ncar need to be perfect versus does it need to be\\nbetter than the human driver? Does the automated\\ndrone need to be perfect or does it need to be better than a human pilot at making decisions under enormous amounts of\\nstress and uncertainty? - Yeah, well, on average, the worry that AI folks\\nhave is the runaway. - They\\'re gonna come alive. Right? That then again, that\\'s\\nthe slight of hand, right. - Or not not come alive.\\nWell, no, hold on a second. You lose control as well. You lose control. - But then they\\'re gonna\\ndevelop goals of their own. They\\'re gonna develop a mind of their own, they\\'re gonna develop their own. Right. - No more, more like\\nChernobyl style meltdown, like just bugs in the code\\naccidentally, you know, force you like the results in the bombing of like large civilian areas. - [Marc] Okay. And to a degree that\\'s not possible in the current military strategies, - [Marc] I don\\'t know. - Control by humans. - Well, actually we\\'ve been\\ndoing a lot of mass bombings to cities for a very long time. - Yes. And a lot of civilians died. - And a lot of civilians died. And if you watch the documentary,\\nthe Fog of War McNamara, it spends a big part of it\\ntalking about the fire bombing of the Japanese cities. Burning them straight\\nto the ground. Right. The devastation in Japan, American military fire bombing\\nthe cities in Japan was considerably bigger devastation\\nthan the use of nukes. Right. So we\\'ve been doing\\nthat for a long time. We also did that to Germany, by the way Germany did that to us, right? Like that\\'s an old tradition. The minute we got airplanes, we started doing indiscriminate bombing. - So one of the things-- - [Marc] We\\'re still doing it. - The modern US military can do with technology with automation, but technology more broadly is higher and higher precision strikes. - Yeah, I was saying, so precision is obviously precision and this is a (indistinct) right? So there was this big advance this big advance called the (indistinct) which basically was\\nstrapping a GPS transceiver to an unguided bomb and turning it into a guided bomb. And yeah, that\\'s great. Like look, that\\'s been a big advance, but, and that\\'s like a baby\\nversion of this question, which is okay, do you\\nwant like the human pilot, like guessing where the bomb\\'s gonna land? Or do you want like the\\nmachine like guiding the bomb to his destination? That\\'s a baby version of the question. The next version of the question is, do you want the human\\nor the machine deciding whether to drop the bomb? Everybody just assumes the\\nhuman\\'s gonna do a better job for what I think are\\nfundamentally suspicious reasons. - Emotional, psychological reasons. - Yeah. I think it\\'s very clear\\nthat the machine\\'s gonna do a better job making that decision \\'cause the humans making\\nthat decision are got awful. Just terrible. - [Lex] Yeah. - Right. And so yeah. So this is the thing. And then let\\'s get to the, there was, can I one more slide of hand? - [Lex] Yes. - It was in-- - Sure. Please. I\\'m a magician. You could say. - One more slight of hand. These things are gonna be so smart, right? That they\\'re gonna be able to\\ndestroy the world and wreak havoc and like do all this\\nstuff and plan and do all this stuff and evade us and have\\nall their secret things and their secret factories\\nand all this stuff. But they\\'re so stupid that\\nthey\\'re gonna get like, tangled up in their code and that\\'s they\\'re not gonna come alive, but there\\'s gonna be some\\nbug that\\'s gonna cause them to like turn us all on a paper like that. They\\'re not gonna be\\ngenius in every way other than the actual bad goal. And it\\'s just like,\\nand that\\'s just like a, like ridiculous like discrepancy. And you can prove this today, you can actually address\\nthis today for the first time with LLMs which is you can actually ask LLMs to resolve moral dilemmas. So you can create the\\nscenario, you know, dot, dot, dot this, that, this, that, this, that. What would you as the AI\\ndo in the circumstance? And they don\\'t just\\nsay destroy all humans, destroy all humans. They will give you actually\\nvery nuanced moral, practical trade-off oriented answers. And so we actually already\\nhave the kind of AI that can actually like, think this through and can actually like, you know, reason about goals. - Well, the hope is that AGI or like various superintelligent systems have some of the\\nnuance that LLMs have and the intuition is they most likely will because even these LLMs have the nuance. - LLMs are really, this is\\nactually worth spending a moment on LLMs are really interesting to have moral conversations with. And that I just, I didn\\'t expect I\\'d be\\nhaving a moral conversation with the machine in my lifetime. - Wait, and let\\'s remember\\nwe\\'re not really having a conversation with the machine where we\\'re having a conversation with the entirety of the\\ncollective intelligence of the human species. - Exactly. Yes. Correct. - But it\\'s possible to imagine\\nautonomous weapons systems that are not using LLMs. - But if they\\'re smart enough to be scary, where are they not\\nsmart enough to be wise? Like, that\\'s the part where it\\'s like, I don\\'t know how you get\\nthe one without the other. - Is it possible to be super intelligent without being super wise? - Well, again, you\\'re back to that. I mean, then you\\'re back to\\na classic autistic computer, right? Like you\\'re back to just\\nlike a blind rule follower. I\\'ve got this like core,\\nit\\'s the paperclip thing. I\\'ve got this core rule and\\nI\\'m just gonna follow it to the end of the earth. And it\\'s like, well, but everything you\\'re gonna\\nbe doing execute that rule is gonna be super genius level\\nthat humans aren\\'t gonna be able to counter. It\\'s a mismatch in the definition of what the system\\'s capable of. - Unlikely but not impossible, I think. - But again, here you\\nget to like, okay, like. - No, I\\'m not saying when it\\'s\\nunlikely but not impossible. If it\\'s unlikely, that means the fear should be correctly calibrated. - Extraordinary claims\\nrequire extraordinary proof. - Well, okay, so one\\ninteresting sort of tangent, I would love to take on this\\nbecause you mentioned this in the essay about nuclear,\\nwhich was also, I mean, you don\\'t shy away from a\\nlittle bit of of a spicy take. So Robert Oppenheimer famously said, now I am become death\\nthe destroyer of worlds as he witnessed the first\\ndestination of a nuclear weapon on July 16th, 1945. And you write an interesting\\nhistorical perspective, \"Recall that John Van Neuman responded to Robert Oppenheimer\\'s famous\\nhand wringing about the role of creating nuclear weapons, which you note helped end World War II and prevent World War III\\nwith some people confess guilt to claim credit for the sin.\" And you also mentioned\\nthat Truman was harsher after meeting Oppenheimer. He said that \"Don\\'t let that\\ncry baby in here again.\" - Real quote, by the\\nway, from Dean Atchison. - Boy. - \\'Cause Oppenheimer didn\\'t\\njust say the famous line. - [Lex] Yeah. - He then spent years going\\naround basically moaning him, you know, going on TV and going into going into the White House and basically like, just like doing this hair shirt, you know, thing self, you know, this\\nsort of self-critical like, oh my god, I can\\'t believe how awful I am. - So he\\'s widely\\nconsidered perhaps of the, because of the hang ringing\\nas the father of the tom bomb. - [Marc] Yeah. - This is Van Norman\\'s criticism\\nof him is he tried to have his cake and eat it too. Like he wanted to and Van Norman of course a very different kind of personality and\\nhe\\'s just like, yeah, good. This is like an incredibly\\nuseful thing. I\\'m glad we did it. - Yeah. Well Van Norman is\\nis widely credit as being one of the smartest humans\\nof the 20th century. Certain people. Everybody says like, this is the smartest person I\\'ve ever met when they\\'ve met him. Anyway, that doesn\\'t mean,\\nsmart doesn\\'t mean wise. So yeah, I would love to sort of, can you make the case both\\nfor and against the critique of Oppenheimer here? \\'Cause we\\'re talking\\nabout nuclear weapons. Boy, do they seem dangerous? - Well so, the critique goes deeper and I left this out. Here\\'s the real substance, I left it out \\'cause I didn\\'t wanna dwell on nukes in my AI paper. But here\\'s the deeper thing that happened and I\\'m really curious, this\\nmovie coming out this summer, I\\'m really curious to see\\nhow far he pushes this. \\'cause this is the real\\ndrama in the story, which is, it wasn\\'t just a question\\nof our nukes, good or bad, it was a question of should\\nRussia also have them? And what actually happened was Russia got the American invented the bomb. Russia got the bomb, they got the bomb through espionage, they got American and you know, they got American scientists\\nand foreign scientists working on the American project. Some combination of the two\\nbasically gave the Russians the designs for the bomb. And that\\'s how the Russians got the bomb. There\\'s this dispute to this\\nday of Oppenheimer\\'s role in that if you read all the histories, the kind of composite\\npicture, and by the way, we now know a lot actually\\nabout Soviet espionage in that era \\'cause there\\'s been all this declassified\\nmaterial in the last 20 years that actually shows a lot\\nof very interesting things. But if you kinda read all the histories, which you kinda get is Oppenheimer\\nhimself probably was not he probably did not hand over\\nthe nuclear secrets himself. However, he was close\\nto many people who did. Including family members. And there were other members\\nof the Manhattan Project who were Russian, Soviet SS\\nand did hand over the bomb. And so the view of that\\nOppenheimer and people like him had that this thing is awful\\nand terrible and oh my god. And you know, all this stuff you could\\nargue fed into this ethos at the time that resulted\\nin people thinking that the Baptists thinking that the only principle\\nthing to do was to give the Russians the bomb. And so the moral beliefs on this thing and the public discussion and the role that the inventors\\nof this technology play, this is the point of this book, when they kind of take on this\\nsort of public intellectual, moral kind of thing, it can\\nhave real consequences, right? Because we live in a very\\ndifferent world today because Russia got the\\nbomb than we would\\'ve lived in had they not gotten the bomb right. The entire 20th century, second half of the 20th\\ncentury would\\'ve played out very different had those people\\nnot given Russia the bomb. And so the stakes were very high then. The good news today is\\nnobody\\'s sitting here today, I don\\'t think worrying about\\nlike an analogous situation with respect to like, I\\'m not really worried that\\nSam Altman\\'s gonna decide to give, you know, the\\nChinese, the design for AI, although he did just speak\\nat a Chinese conference, which is in interesting. But however, I don\\'t think\\nthat\\'s what\\'s at play here, but what\\'s at play here are\\nall these other fundamental issues around what do\\nwe believe about this and then what laws and\\nregulations and restrictions that we\\'re gonna put on it. And that\\'s where I draw\\nlike a direct straight line. And anyway, and my reading\\nof the history on nukes is like the people who were doing\\nthe full hair shirt public, this is awful. This is terrible. Actually had like\\ncatastrophically bad results from taking those views. And that\\'s what I\\'m worried\\nit\\'s gonna happen again. - But is there a case to be\\nmade that you really need to wake the public up to the dangers of nuclear weapons when\\nthey were first dropped? Like really like educate them on like, this is extremely dangerous\\nand destructive weapon. - I think the education\\nkind of happened quick and early, like-- - [Lex] How? - It was pretty obvious. - [Lex] How? - We dropped one bomb and\\ndestroyed an entire city. - Yeah. So 80,000 people dead. - [Marc] Yep. - But. - [Marc] And look. But-- - I don\\'t like the reporting of that. You can report that in all kinds of ways. - [Marc] Oh, there wars. - You can do all kinds of slants. Like war is horrible. War is terrible. You can do, you can make\\nit seem like nuclear, the use of nuclear weapons\\nis just a part of war and all that kind of stuff. Something about the\\nreporting and the discussion of nuclear weapons resulted\\nin us being terrified in awe of the power of nuclear weapons and that potentially fed\\nin a positive way towards the game theory of\\nmutual issue destruction. - Well, so this gets to what actually, let\\'s get to what actually happens. - [Lex] Some of us, me\\nplaying devil\\'s advocate here. - Yeah, yeah, sure. Of course. Let\\'s get to what\\nactually happened and then kind of back into that. So what actually happened, I believe, and again I think this is a\\nreasonable reading of history, is what actually happened\\nwas nukes then prevented World War III and they\\nprevented World War III through the game theory\\nof mutually assured destruction had nukes not existed. Right. There would\\'ve been no reason why the Cold War did not go hot. Right. And then there and then, you know, and the military planners\\nat the time, right, thought both on both sides\\nthought that there was gonna be World War III on the planes of Europe and they thought there was gonna be like a hundred million people dead. Right? It was like the most obvious\\nthing in the world to happen. Right? And it\\'s the dog\\nthat didn\\'t bark right? Like it may be like the best\\nsingle net thing that happened in the entire 20th century is\\nthat like that didn\\'t happen. - Yeah. Actually, just on that point, you say a lot of really brilliant things. It hit me just as you were saying it. I don\\'t know why it hit\\nme for the first time, but we got two wars in\\na span of like 20 years. Like we could have kept getting\\nmore and more world wars and more and more ruthless. It actually, you could have\\nhad a US versus Russia war. - You could, by the way you haven\\'t, there\\'s another hypothetical scenario. The other hypothetical scenario is that Americans got the\\nbomb, the Russians didn\\'t. Right? And then America\\'s the big dog and then maybe America\\nwould\\'ve had the capability to actually roll back the iron curtain. I don\\'t know whether\\nthat would\\'ve happened, but like it\\'s entirely possible. Right? And the act of these people who had these moral positions about, \\'cause they could\\nforecast, they could model, they could forecast the future\\nof how the technology would get used, made a horrific mistake. \\'cause they basically ensured that the iron curtain\\nwould continue for 50 years longer than it would\\'ve otherwise. Like, and again, like\\nthese are counter-factuals, I don\\'t know that that\\'s\\nwhat, what would\\'ve happened, but like the decision to hand the bomb over was a big decision made by people who were very\\nfull of themselves. - Yeah. But so me as an America, me as a person that loves America, I also wonder if US was the only ones with the nuclear weapons. - That was the argument for handing that was the guys who (indistinct) the guys who handed over the bomb. That was actually their moral argument. - Yeah. I would probably\\nnot hand it over to, I would be careful about the regimes. You hand it over to there, maybe give it to like\\nthe British or something, or like a democratically-elected\\ngovernment. - Well, look, there are people to this day who think that those bias Soviet spies did the right thing because\\nthey created a balance of terror as opposed\\nto the US having just, and by the way, let me-- - Balance of terror. - [Marc] Let\\'s tell the\\nfull version story has-- - Such a sexy ring to it. - Okay. So the full\\nversion of the story is John Van Norman is a hero\\nof both yours and mind. The full version of the\\nstory is he advocated for a first strike. So when the US had the\\nbomb and Russia did not, he advocated for, he said, we need to strike them right now. - Strike Russia. - [Marc] Yes. - Van Norman. - Yes, because he said\\nWorld War III is inevitable. He was very hardcore. His theory was World\\nWar III is inevitable. We\\'re definitely gonna have World War III. The only way to stop World War\\nIII is we have to take them out right now and we have\\nto take them out right now before they get the bomb. \\'Cause this is our last chance. Now again, like-- - Is this an example of\\nphilosophers and politics? - I don\\'t know if that\\'s in there or not, but this is in the standard. - No, but it is meaning is that. - Yeah, this is on the other side. So, most of the case studies, most of the case studies\\nin books like this are the crazy people on the left. Van Norman is a story arguably of the crazy people on the right. - Yes. Stick to computing, John. - Well. This is the thing, and this is the general principle. Getting back to our core\\nthing, which is like, I don\\'t know whether any of\\nthese people should be making any of these calls. Because there\\'s nothing in\\neither Van Norman\\'s background or Oppenheimer\\'s background or any of these people\\'s background that qualifies them as moral authorities. - Yeah. Well this actually\\nbrings up the point of, in AI, who are the good people to reason about the\\nmorality of the ethics, the outside of these risks, outside of like the more\\ncomplicated stuff that you, you agree on is, you know, this will go into the hands of bad guys and all the kinds of ways\\nthey\\'ll do is interesting and dangerous, is dangerous in interesting unpredictable ways. And who is the right person? Who are the right kinds of\\npeople to make decisions, how to respond to it?\\nOr is the tech people? - So the history of these fields, this is what he talks about in the book, the history of these fields,\\nis that the competence and capability and\\nintelligence and training and accomplishments of senior\\nscientists and technologists working on a technology\\nand then being able to then make moral judgments\\nin the use of that technology. That track record is terrible that track record is like\\ncatastrophically bad. The people-- - Just the linger, the people that develop that\\ntechnology are usually not going to be the right people. - Well why would they? So\\nthe claim is of course, they\\'re the knowledgeable ones. But the the problem is they\\'ve\\nspent their entire life in a lab. Right. They\\'re not theologians. Well, so what you find,\\nwhat you find when you read, when you read this, when\\nyou look at these histories, what you find is they generally are very thinly informed on history, on sociology, on theology,\\non morality, on ethics. They tend to manufacture their\\nown worldviews from scratch. They tend to be very sort of thin. They\\'re not remotely the\\narguments that you would be having if you got like a group of\\nhighly qualified theologians or philosophers or, you know. - Well, let me sort of,\\nas the devil\\'s advocate, takes a simple whiskey say\\nthat I agree with that. But also it seems like the\\npeople who are doing kind of the ethics departments and these tech companies\\ngo sometimes the other way. - [Marc] Yes, they\\'re definitely. - Which they\\'re not nuanced\\non history or theology or this kind of stuff. It almost becomes a kind\\nof outraged activism towards directions that don\\'t seem to be grounded in history and\\nhumility and nuance. It\\'s again, drenched with arrogance. So-- - [Marc] Definitely. - I\\'m not sure which is worse. - Oh no, they\\'re both bad. Yeah. So definitely not them either. - So, but I guess. - Well look, this is a hard. - Yeah, it\\'s a hard problem. - This is a hard problem. This goes back to where we started, which is, okay, who has the truth? And it\\'s like, well, you know, like how does societies\\narrive at like truth and how do we figure these things out and like our elected leaders\\nplay some role in it. You know, we all play some role in it. There have to be some set\\nof public intellectuals at some point that bring, you know, rationality and judgment\\nand humility to it. Those people are few and far between. We should probably prize them very highly. - Yeah. So celebrate humility\\nin our public leaders. So getting to risk number two, will AI ruin our society\\nshort version as you write, if the murder robots don\\'t\\nget us the hate speech and misinformation will. And the action you recommend in short, don\\'t let the thought police suppress AI. Well what is this risk of\\nthe effect of misinformation of society that\\'s going\\nto be catalyzed by AI? - Yeah, so this is the social media, this is what you just alluded to. It\\'s the activism kind\\nof thing that\\'s popped up in these companies in the industry. And it\\'s basically, from my perspective, it\\'s basically part two\\nof the war that played out over social media over the last 10 years, \\'cause you probably remember\\nsocial media 10 years ago, was basically who even wants this? Who wants a photo of what\\nyour cat had for breakfast? Like, this stuff is like silly and trivial and why can\\'t these nerds like figure out how to invent something\\nlike useful and powerful? And then, you know, certain things happened\\nin the political system. And then it sort of, the polarity on that\\ndiscussion switched all the way to social media is like\\nthe worst, most corrosive, most terrible, most awful\\ntechnology ever invented. And then it leads to, you\\nknow, terrible of the wrong, you know, politicians and\\npolicies and politics and like, and all this stuff. And that all got catalyzed into\\nthis very big kind of angry movement both inside and\\noutside the companies to kind of bring social media to heal. And that got focused in\\nparticularly on two topics, so-called hate speech and\\nso-called misinformation. And that\\'s been the saga playing out for the last decade. And I don\\'t even really want\\nto even argue the pros and cons of the sides just to observe that\\'s been like a huge fight and has had, you know, big consequences to how\\nthese companies operate. Basically that same, those\\nsame sets of theories, that same activist approach, that same energy as being\\ntransplanted straight to AI. And you see that already happening. It\\'s why, you know, ChatGPT will answer, let\\'s say certain\\nquestions and not others. It\\'s why it gives you the\\ncanned speech about, you know, whenever it starts with,\\nas a large language model, I cannot, you know, basically means that somebody\\nhas reached in there and told that it can\\'t talk about certain topics. - Do you think some of that is good? - So it\\'s an interesting question. So a couple observations. So, one is the people who find this the most frustrating are the people who are worried\\nabout the murder robots, right? So, and in fact so called\\nX risk people, right? They started with the term AI safety, the term became AI alignment. When the term became AI alignment is when this switch happened\\nfrom we\\'re worried it\\'s gonna kill us all to\\nwe\\'re worried about hate speech and misinformation. - [Lex] Sure. - The AI X risk people have\\nnow renamed their thing AI not kill everyone-ism, which I have to admit is a catchy term. And they are very frustrated\\nby the fact that the hate speech sort of activist driven\\nhate speech misinformation kind of thing is taking over. Which is what\\'s happened is taken over, the AI ethics field has been\\ntaken over by the hate speech misinformation people. You know, look, would I like to live in a world\\nin which like everybody was nice to each other all the\\ntime and nobody ever said anything mean and nobody ever\\nused a bad word and everything was always accurate and honest. Like, that sounds great. Do I wanna live in a world\\nwhere there\\'s like a centralized thought police working through\\nthe tech companies to enforce the view of a small set of\\nelites that they\\'re gonna determine what the rest\\nof us think and feel like? Absolutely not. - There could be a middle\\nground somewhere like Wikipedia type of moderation. There\\'s moderation of Wikipedia\\nthat is somehow crowdsourced where you don\\'t have centralized elites, but it\\'s also not completely\\njust a free for all because if you have the\\nentirety of human knowledge at your fingertips, you\\ncan do a lot of harm. Like if you have a good assistant that\\'s completely uncensored, they can help you build a bomb, they can help you mess with\\npeople\\'s physical wellbeing. Right. If they, because that information is\\nout there on the internet and so presumably there\\'s, it would be, you could see the positives\\nin censoring some aspects of an AI model when it\\'s helping you\\ncommit literal violence. - Yeah. And there\\'s a section\\nlater section of the essay where I talk about bad\\npeople doing bad things. - [Lex] Yes. - Right. Which and there\\'s this, there\\'s a set of things that\\nwe should discuss there. - [Lex] Yeah. - What happens in practice is these lines, as you alluded to this already, these lines are not easy to draw. And what I\\'ve observed in\\nthe social media version of this is like, the way I describe it as the slippery slope is not a fallacy, it\\'s an inevitability. The minute you have this kind\\nof activist personality that gets in a position to make these decisions they take it straight to infinity. Like, it goes into the crazy\\nzone like almost immediately and never comes back because\\npeople become drunk with power. Right. And look, if you\\'re\\nin the position to determine what the entire world thinks and feels and reads and says like, you\\'re gonna take it and you\\nknow, Elon has, you know, ventilated this with the\\nTwitter files over the last, you know, three months and\\nit\\'s just like crystal clear, like how bad it got there now. - [Lex] Yeah. - Reason for optimism\\nis what Elon is doing with community notes. So community notes is actually\\na very interesting thing. So, what Elon is trying to\\ndo with community notes is he\\'s trying to have it where\\nthere\\'s only a community note when people who have previously\\ndisagreed on many topics agree on this one. - Yes, that\\'s what I\\'m\\ntrying to get at is like, there could be Wikipedia like\\nmodels or community notes type of models where allows you\\nto essentially either provide context or sensor in a\\nway that\\'s not resist the slippery slope nature. Power. - Now there\\'s an entirely\\ndifferent approach here, which is basically we have AIs\\nthat are producing content. We could also have ais that\\nare consuming content. Right? And so one of the things that\\nyour assistant could do for you is help you consume\\nall the content, right? And basically tell you\\nwhen you\\'re getting played. So for example, I\\'m gonna\\nwant the AI that my kid uses, right, to be very, you know, child safe and I\\'m gonna want\\nit to filter for him all kinds of inappropriate stuff that\\nhe shouldn\\'t be saying just \\'cause he\\'s a kid. Right? And you see what I\\'m saying\\nis you can implement that. The architectural, you\\ncould say you can solve this on the client side, right? You solving on the server\\nside gives you an opportunity to dictate for the entire\\nworld, which I think is where you take the slippery slope to hell, there\\'s another architectural approach, which is to solve this on the client side, which is certainly what I would endorse. - It\\'s AI risk number five, will AI lead to bad\\npeople doing bad things? And I can just imagine language\\nmodels used to do so many bad things, but the hope is there that\\nyou can have large language models used to then defend\\nagainst it by more people, by smarter people, by more\\neffective people, skilled people, all that kind of stuff. - Three-part argument on\\nbad people doing bad things. So, number one, right? You can use the technology defensively and we should be using AI\\nto build like broad spectrum vaccines and antibiotics for\\nlike bio weapons and we should be using AI to like hunt terrorists and catch criminals and like, we should be doing like all\\nkinds of stuff like that. And in fact, we should be doing those things even just to like go get like, you know, basically go eliminate risk\\nfrom like regular pathogens that aren\\'t like constructed by an AI. So there\\'s the whole\\ndefensive set of things. Second is we have many laws on the books about as actual bad things, right? So it is actually illegal\\nto be a criminal, you know, to commit crimes, to commit\\nterrorist acts to, you know, build pathogens with the intent to deploy them to kill people. And so we have those, we actually don\\'t need new laws for the vast majority of these scenarios. We actually already have the\\nlaws in the book, on the books. The third argument is the minute, and this is sort of the\\nfoundational one that gets really tough, but the minute\\nyou get into this thing, which you were kind of getting\\ninto, which is like, okay, but like, don\\'t you need\\ncensorship sometimes, right? And don\\'t you need restrictions sometimes? It\\'s like, okay, what is the cost of that? And in particular in the\\nworld of open source, right? And so is open source AI\\ngoing to be allowed or not? If open source AI is not allowed, then what is the regime that\\'s\\ngoing to be necessary legally and technically to prevent\\nit from developing? Right? And here again is where you\\nget into and people have proposed that these kinds of things. You get into I would say pretty extreme territory pretty fast. Do we have a monitor\\nagent on every CPU and GPU that reports back to the government? What we\\'re doing with our computers, are we seizing GPU clusters\\nthat get beyond a certain size? Like, and then by the way, how are we doing all that globally, right? And like if China\\'s developing\\nan LLM beyond the scale that we think is allowable,\\nare we gonna invade? Right. And you have figures on the AI X risk side who are advocating any, you know, potentially up to nuclear\\nstrikes to prevent, you know, this kind of thing. And so here you get into this thing and again, you know, maybe\\nyou could maybe say this is, you know, you could even\\nsay this is what good, bad or indifferent or whatever. But like here\\'s the comparison of nukes, the comparison of nukes is very dangerous because one is just nukes, were just, although we can\\ncome back to nuclear power. But the other thing was like with nukes, you could control plutonium, right? You could track plutonium and\\nit was like hard to come by. AI is just math and code, right? And it\\'s in like math\\ntextbooks and it\\'s like, there are YouTube videos that\\nteach you how to build it. And like there\\'s open source,\\nthere\\'s already open source. You know, there\\'s a 40 billion parameter\\nmodel running around already called Falcon Online that\\nanybody can download. And so, okay, you walk down the logic path\\nthat says we need to have guardrails on this. And you find yourself in an authoritarian, totalitarian regime of thought\\ncontrol and machine control that would be so brutal\\nthat you would\\'ve destroyed the society that you\\'re trying to protect. And so I just don\\'t see\\nhow that actually works. - So yeah, you have to\\nunderstand my brain\\'s going full steam ahead here \\'cause I agree with basically\\neverything you\\'re saying, but I\\'m trying to play\\ndevil\\'s advocate here because okay, you\\'re highlighted the fact that there is a slippery\\nslope to human nature. The moment you censor something, you start to censor everything. That alignment starts out sounding nice, but then you start to align to the beliefs of some select group of people. And then it\\'s just your beliefs the number of people\\nyou\\'re aligning to smaller and smaller as that group\\nbecomes more and more powerful. Okay. But that just speaks to the people that censor are usually the assholes and the assholes get richer. I wonder if it\\'s possible\\nto do without that for AI. One way to ask this question is do you think the base models, the baseline foundation\\nmodels should be open sourced? Like, where Marc Zuckerberg\\nis saying they want to do. - So look, I mean I think\\nit\\'s totally appropriate the companies that are in the business of producing a product or service should be\\nable to have a wide range of policies that they put, right? And I\\'ll just, again, I want a heavily censored\\nmodel for my eight year old. Like, I actually want that, like, like I would pay more money\\nfor the ones more heavily censored than the one that\\'s not, right. And so, like there are certainly scenarios where companies will make that decision. Look, an interesting thing you brought up or is this really a speech issue? One of the things that the\\nbig tech companies are dealing with is that content generated\\nfrom an LLM is not covered under section 230, which is the law that protects\\ninternet platform companies from being sued for the\\nuser generated content. And so it is actually-- - [Lex] Oh, wow. - Yes and so there, there\\'s\\nactually a question. I think there\\'s still a question, which is can big American companies actually feel generative AI at all? Or is the liability actually\\ngonna just ultimately convince them that they can\\'t do it? Because the minute the\\nthing says something bad, and it doesn\\'t even\\nneed to be hate speech, it could just be like an\\n(indistinct) it could hallucinate a product, you know, detail\\non a vacuum cleaner, you know, and all of a sudden the\\nvacuum cleaner company sues for misrepresentation. And there\\'s asymmetry there, right? \\'Cause the LLMs gonna be\\nproducing billions of answers to questions and it only needs\\nto get a few wrong to have. - [Lex] So, loss has to get\\nupdated really quick here. - Yeah. And nobody knows\\nwhat to do with that, right? So, so anyway, like there are big, there are big questions around\\nhow companies operate at all. So we talk about those, but then there\\'s this other\\nquestion of like, okay, the open source. So what about open source? And my answer to your\\nquestion is kind of like, obviously yes, the models have, there has to be full open\\nsource here because to live in a world in which that\\nopen source is not allowed is a world of draconian speech control, human control, machine control. I mean, you know, black helicopters with\\njackbooted thugs coming out, repelling down and seizing\\nyour GPU like territory. - [Lex] Well. - No, no, I\\'m a hundred percent serious. - That\\'s you\\'re saying slippery\\nslope always leads there. - No, no, no, no. That\\'s what\\'s required to enforce it. Like how will you enforce a\\nban on open source and AI? - No. Well you could add friction to it, like harder to get the models. \\'Cause people will always\\nbe able to get the models, but it\\'ll be more in the shadows, right? - The leading open source model\\nright now is from the UAE. Like the next time they\\ndo that, what do we do? - [Lex] Yeah. - Like. - Oh, I see you\\'re like. - A 14 year old in Indonesia\\ncomes out with a breakthrough. You know, we talked about most great\\nsoftware comes from a small number of people. Some kid comes out with\\nsome big new breakthrough and quantization or something and has some huge breakthrough. And like, what are we gonna like, invade Indonesia and arrest him? - It seems like in terms of\\nsize of models and effectiveness of models, the big tech companies will\\nprobably lead the way for quite a few years and the question is of what policies they should use? The kid in Indonesia\\nshould not be regulated, but should Google, Meta,\\nMicrosoft, Open AI be regulated? - Well, so, but this goes, okay, so when does it become dangerous? Right. Is the danger that it\\'s as powerful as the current leading commercial model? Or it is just at some\\nother arbitrary threshold? And then by the way, like\\nlook, how do we know, like what we know today is that\\nyou need like a lot of money to like train these things. But there are advances being\\nmade every week on training efficiency and, you know,\\ndata, all kinds of synthetic, you know, look, I don\\'t even like the synthetic data thing we\\'re talking about. Maybe some kid figures out a way to auto-generate synthetic data. - [Lex] That\\'s gonna change everything. - Yeah, exactly. And so like sitting here today, like, the breakthrough just happened, right? You made this point like the\\nbreakthrough just happened. So we don\\'t know what the shape of this technology is gonna be. I mean the big shock\\nhere is that, you know, whatever number of billions\\nof parameters basically represents at least a very big\\npercentage of human thought. Like who would\\'ve imagined that? And then there\\'s already work underway. There was just this paper that\\njust came out that basically takes a gpt three scale model\\nand compresses it down or run on a single 32 core CPU. Like who would\\'ve predicted that? - [Lex] Yeah. - You know, some of these models now you\\ncan run on raspberry pies like today they\\'re very slow,\\nbut like, you know, maybe they\\'ll be a, you know, perceived you have real perform, you know, like it\\'s math and code. And here we\\'re back in here, we\\'re back in, dude, it\\'s math and code. It\\'s math and code, it\\'s\\nmath, code and data. It\\'s bits. - Marc has just like\\nwalked away at this point. You just screw it. I don\\'t know what to do with this. You guys created this\\nwhole internet thing. Yeah, yeah. I mean, I\\'m a huge believer\\nin open source here. - So my argument is we\\'re gonna have, see here\\'s my argument is a, my argument, my full argument is, is AI is gonna be like air,\\nit\\'s gonna be everywhere. Like this is just gonna be in text. It already is, it\\'s gonna be in textbooks\\nand kids are gonna grow up knowing how to do this. And\\nit\\'s just gonna be a thing. It\\'s gonna be in the air\\nand you can\\'t like pull this back anymore. You can\\'t pull back air. And so you just have to figure out how to live in this world, right? And then that\\'s where I think\\nlike all this hand ringing about AI risk is basically\\na complete waste of time, \\'cause the effort should go into okay, what is the defensive approach? And so if you\\'re worried about you know, AI generated pathogens, the\\nright thing to do is to have a permanent project warp speed, right? Funded lavishly. Let\\'s do a Manhattan, let\\'s\\ntalk about Manhattan project, let\\'s do a Manhattan project\\nfor biological defense, right? And let\\'s build ais and let\\'s\\nhave like broad spectrum vaccines where like, we\\'re\\ninsulated from every pathogen. - And well, the interesting\\nthing is because it\\'s software, a kid in his basement, teenager could build like a\\nsystem that defends against like the worst, I mean, and to me\\ndefense is super exciting. It\\'s like, if you believe\\nin the good of human nature for that, most people wanna do good, to be the savior of\\nhumanity is really exciting. - Yes. - Not, okay, that\\'s a dramatic statement. But to help people. - Yeah, of course. Help people. - Yeah. Okay. What about just the jump around, what about the risk of will AI\\nlead to crippling inequality? You know, \\'cause we\\'re kind of saying\\neverybody\\'s life will become better. Is it possible that the\\nrich get richer here? - Yeah, so this goes, this actually ironically\\ngoes back to Marxism. So \\'cause this was the, so the\\ncore claim of Marxism, right? Basically was that the owner, the owners of capital would basically own the means of production. And then over time they\\nwould basically accumulate all the wealth the workers\\nwould be paying in, you know, and getting nothing in return \\'cause they wouldn\\'t be\\nneeded anymore, right? Marx was very worried\\nabout mech what he called mechanization or what later\\nbecame known as automation. And that, you know, the workers would be immiserated\\nand the the capitalists would end up with all. And so this was one of the\\ncore principles of Marxism. Of course it turned out to\\nbe wrong about every previous wave of technology. The reason it, it turned out to be wrong about every previous wave of technology is that the way that the\\nself-interested owner of the machines makes the most money is by providing the production capability in the form of products and services to the most people, the most\\ncustomers as possible, right? The the largest, and this is one of those funny\\nthings where every CEO knows this intuitively, and yet it\\'s like hard to\\nexplain from the outside the way you make the most\\nmoney in any business is by selling to the largest\\nmarket you can possibly get to. The largest market you can\\npossibly get to is everybody on the planet. And so every large company\\ndoes is everything that it can to drive down prices, to\\nbe able to get volumes up, to be able to get to\\neverybody on the planet. And that happened with\\neverything from electricity, it happened with telephones,\\nit happened with radio, it happened with automobiles,\\nit happened with smartphones, it happened with PCs, it\\nhappened with the internet, it happened with mobile broadband. It\\'s happened by the way, with Coca-Cola. It\\'s happened with like every, you know, basically every industrially\\nproduced, you know, good or service people, you wanna drive it to the\\nlargest possible market. And then as proof of that,\\nit\\'s already happened, right? Which is the early\\nadopters of like ChatGPT and Bing are not like, you\\nknow, Exxon and Boeing. They\\'re, you know, your\\nuncle and your nephew, right? It\\'s just like free. It\\'s either freely available\\nonline or it\\'s available for 20 bucks a month or something. But the, you know, these things went this technology went mass market immediately. And so look, the owners of\\nthe means of production, the whoever does this now mentioned these trillion dollar questions. There are people who are gonna\\nget really rich doing this, producing these things, but they\\'re gonna get\\nreally rich by taking this technology to the\\nbroadest possible market. - So yes, they\\'ll get rich, but they\\'ll get rich having\\na huge positive impact on. - Yeah, making the technology\\navailable to everybody. Right. And again, smartphone, same thing. So there\\'s this amazing kind\\nof twist in business history, which is you cannot spend\\n$10,000 on a smartphone, right? You can\\'t spend a\\nhundred thousand dollars, you can\\'t spend a million, like I would buy the\\nmillion dollars smartphone. Like I\\'m signed up for it. Like if it\\'s like, suppose a million dollar\\nsmartphone was like much better than the thousand dollar smartphone. Like I\\'m there to buy\\nit, it doesn\\'t exist. Why doesn\\'t it exist? Apple makes so much more\\nmoney driving the price further down from a thousand dollars than they would trying to harvest, right? And so it\\'s just this\\nrepeating pattern you see over and over again where and\\nwhat\\'s great about it is you, you do not need to rely on\\nanybody\\'s enlightened right? Generosity to do this. You just need to rely on\\ncapitalist self-interest. - What about AI taking our jobs? - Yeah. So very very similar thing here. There\\'s sort of a, there\\'s a\\ncore fallacy which again was very common in Marxism, which is what\\'s called\\nthe lump of labor fallacy. And this is sort of the\\nfallacy that there is only a fixed amount of work\\nto be done in the world. And it\\'s all being done today by people and then if machines do it, there\\'s no other work\\nto be done by people. And that\\'s just a\\ncompletely backwards view on how the economy develops and grows. Because what happens is not\\nin fact that what happens is the introduction of technology\\ninto production process causes prices to fall. As prices fall, consumers\\nhave more spending power. As consumers have more spending power, they create new demand. That new demand then causes\\ncapital and labor to form into new enterprises to\\nsatisfy nuance and needs. And the result is more\\njobs at higher wages. - So nuance and needs, the\\nworries that the creation of nuance and needs at\\na rapid rate will mean there\\'s a lot of turnover in jobs. So people will lose jobs. Just the actual experience\\nof losing a job and having to learn new things and\\nnew skills is painful for the individuals. - Well, two things. One is the new jobs are often much better. So this actually came up\\nthat there was this panic about a decade ago and all\\nthe truck drivers are gonna lose their jobs, right? And number one, that didn\\'t happen \\'cause\\nwe haven\\'t figured out a way to actually finish that yet. But the other thing was\\nlike, look, truck driver, like I grew up in a town\\nthat was basically consisted of a truck stop, right? And I like knew a lot of truck drivers and like truck drivers\\nlive a decade shorter than everybody else. Like, it\\'s actually like a very dangerous, like, they get, like literally they have like\\nhigher rates of skin cancer and on the left side of their, on the left side of their body from being in the sun all the time. The vibration of being\\nin the truck is actually very damaging to your physiology. - And there\\'s actually perhaps partially because of that reason there\\'s a shortage of people who wanna be truck drivers. - Yeah. Like, it\\'s not like\\nthe question always you wanna ask somebody like that\\nis, do you want, you know, do you want your kid to be doing this job? And like most of them will tell you no. Like, I want my kid to\\nbe sitting in a cubicle somewhere like where they\\ndon\\'t have this, like, where they don\\'t die 10 years earlier. And so, the new jobs, number one, the new jobs are often better, but you don\\'t get the new\\njobs until you go through the change. And then to your point,\\nthe training thing, you know, is always the\\nissue is can people adapt? And again, here you need to imagine living in a world in which everybody has the AI\\nassistant capability, right? To be able to pick up new\\nskills much more quickly and be able to have some, you know, be able to have a machine to work with to augment their skills. - It\\'s still gonna be painful, but that\\'s the process of life. - It\\'s painful for some people.\\nI mean there\\'s no, like, there\\'s no question it\\'s\\npainful for some people and they\\'re, you know, they\\'re yes, it\\'s not, again, I\\'m not a utopian on\\nthis and it\\'s not like, it\\'s positive for everybody in the moment, but it has been overwhelmingly\\npositive for 300 years. I mean, look, the concern\\nhere, the concern, this concern has played out for literally centuries and you know, this is the sort of Luddite, you know, the story of the Luddites\\nthat you may remember, there was a panic in the two\\nthousands around outsourcing was gonna take all the jobs. There was a panic in the 2010s\\nthat robots were gonna take all the jobs. In 2019 before COVID we had more jobs at higher wages both in the country and in the world than at any point in human history. And so the overwhelming evidence\\nis that the net gain here is like, just like wildly positive. And most people like overwhelmingly\\ncome out the other side being huge beneficiaries of this. - So you write that the\\nsingle greatest risk, this is the risk you\\'re most convinced by the single greatest risk of AI is that China wins global AI dominance and we the United States\\nand the West do not. Can you elaborate? - Yeah. So this is the\\nother thing which is a lot of this sort of AI\\nrisk debates today sort of assume that we\\'re the\\nonly game in town, right? And so we have the ability to kind of sit in the United States and\\ncriticize ourselves and do, you know, have our\\ngovernment like, you know, beat up on our companies\\nand we\\'ll figure out a way to restrict what our\\ncompanies can do and you know, we\\'re gonna, you know,\\nwe\\'re gonna ban this and ban that, restrict this and do that. And then there\\'s this like\\nother like force out there that like doesn\\'t\\nbelieve we have any power over them whatsoever and they\\nhave no desire to sign up for whatever rules we\\ndecide to put in place and they\\'re gonna do whatever\\nit is they\\'re gonna do. And we have no control over it at all. And it\\'s China and specifically\\nthe Chinese Communist party and they have a completely\\npublicized open, you know, plan for what they\\'re gonna do with AI. And it is not what we have in mind. And not only do they have that as a vision and a plan for their society, but they also have it as a vision and plan for the rest of the world. - So their plan is what? Surveillance? - Authoritarian control. So authoritarian population\\ncontrol you know, good old-fashioned communist\\nauthoritarian control and surveillance and enforcement\\nand social credit scores and all the rest of it. And you are gonna be monitored and metered within an inch of everything all the time. And it\\'s gonna, you know, it\\'s basically the end of human freedom and that\\'s their goal. And you know, they justify it on the basis of that\\'s what leads to peace. - You\\'re worried that the regulating in the United States\\nwill haul progress enough to where the Chinese\\ngovernment would win that race. - So their plan, yeah. Yes, yes. And the reason for that\\nis they, and again, they\\'re very public on this. They have, their plan is to proliferate their approach around the world and they have this program called the Digital Silk Road, right. Which is building on their\\nSilk Road investment program. And they\\'ve got, they\\'ve been laying\\nnetworking infrastructure all over the world with their 5G, right. Work with their company Huawei. And so, they\\'ve been\\nlaying all this fabric, but financial and technological\\nfabric all over the world. And their plan is to roll out their vision of\\nAI on top of that and to have every other country be\\nrunning their version. And then if you\\'re a\\ncountry prone to, you know, authoritarianism, you\\'re\\ngonna find this to be an incredible way to\\nbecome more authoritarian. If you\\'re a country, by the way, not prone to authoritarianism, you\\'re gonna have the Chinese\\nCommunist Party running your infrastructure and having\\nbackdoor into it. Right. Which is also not good. - What\\'s your sense of where\\nthey stand in terms of the race towards super intelligence as\\ncompared to the United States? - Yeah, so good news is they\\'re behind, but bad news is they, you know, let\\'s just say they get\\naccess to everything we do. So they\\'re probably a year\\nbehind at each point in time, but they get, you know, downloads I think of\\nbasically all of our work on a regular basis through\\na variety of means. And they are, you know,\\nat least we\\'ll see, they\\'re at least putting\\nout reports of very, they just put out a report last week of a GPT 3.5 analog. They put out this report,\\nforget what it\\'s called, but they put out this\\nreport of this and they did and they, you know, the\\nway when open AI you know, puts out, one of the ways they test, you know, GPT they run it through standardized\\nexams like the SAT. Right. Just how you can kind of\\ngauge how smart it is. And so the Chinese report, they ran their LLM through\\nthe Chinese equivalent of the SAT and it includes\\na section on Marxism and a section on, I say tongue of thought. And it turns out their AI does very well on both of those topics. - That\\'s right. - So like. - Oh, this alignment thing. - Communist AI, right? Like literal communist AI. Right? And so their vision is\\nlike, that\\'s the, you know, so you know, you can just\\nimagine like you\\'re a school, you know, you\\'re a kid 10\\nyears from now in Argentina or in Germany or in who\\nknows where, Indonesia. And you ask the AI, I\\'d explain to you like\\nhow the economy works and it gives you the most cheery, upbeat explanation of\\nChinese style communism you\\'ve ever heard. Right. So like the stakes here\\nare like really big. - Well, as we\\'ve been talking about, my hope is not just\\nwith the United States, but with just the kid in his basement. The open source LLM. \\'Cause I don\\'t know if I trust large centralized institutions\\nwith super powerful AI no matter what their\\nideology as a power corrupts. You\\'ve been investing in\\ntech companies for about, let\\'s say 20 years. And about 15 of which was\\nwith Andreessen Horowitz. What interesting trends\\nin tech have you seen over that time? Let\\'s just talk about companies\\nand just the evolution of the tech industry. - I mean the big shift over 20\\nyears has been that tech used to be a tools industry for\\nbasically from like 1940 through to about 2010, almost all the big successful\\ncompanies were pick and shovels companies. So PC, database, smartphone, you know, some tool that somebody\\nelse would pick up and use. Since 2010, most of the big\\nwins have been in applications. So a company that starts you know, starts in an existing\\nindustry and goes directly to the customer in that industry. And you know, the earliest examples there\\nwere like Uber and Lyft and Airbnb. And then that model is\\nkind of elaborating out. The AI thing is actually a\\nreversion on that for now \\'cause like most of the AI\\nbusiness right now is actually in cloud provision of AI APIs\\nfor other people to build on. - But the big thing\\nwill probably be in app. - Yeah. I think most of the\\nmoney I think probably will be in whatever your AI financial advisor or your AI doctor or your\\nAI lawyer or, you know, take your pick of whatever the domain is. And there, and what\\'s\\ninteresting is, you know, the valley kind of does everything. The entrepreneurs kind of\\nelaborate every possible idea. And so there will be a set of\\ncompanies that like make AI something that can be purchased\\nand used by large law firms and then there will be other\\ncompanies that just go direct to market as an AI lawyer. - What advice could you\\ngive for a startup founder? Just haven\\'t seen so many\\nsuccessful companies, so many companies that fail also, what advice could you\\ngive to a startup founder, someone who wants to build the\\nnext super successful startup in the tech space? The Googles,\\nthe Apples, the Twitters. - Yeah. So the great thing about the really great founders is they don\\'t take any advice. So, if you find yourself\\nlistening to advice, maybe you shouldn\\'t do it. - But that\\'s actually,\\njust to elaborate on that, if you could also speak\\nto great founders too. Like what makes a great founder? - So what makes a great\\nfounder is super smart, coupled with super energetic,\\ncoupled with super courageous. I think it\\'s some of those three and-- - Intelligence, passion and courage. - The first two are traits\\nand the third one is a choice. I think courage is a choice. Well \\'cause courage is a question\\nof pain tolerance, right? So how many times are you\\nwilling to get punched in the face before you quit? And here\\'s maybe the biggest\\nthing people don\\'t understand about what it\\'s like to be\\na startup founder is it gets very romanticized, right? And even when it, even when they fail, it still gets romanticized about like what a great adventure it was. But like the reality of it is\\nmost of what happens is people telling you no and then\\nthey usually follow that with you\\'re stupid, right. No, I will not come to work for you. I will not leave my cushy job at Google to come work for you. No, I\\'m not gonna buy your\\nproduct, you know, no, I\\'m not gonna run a\\nstory about your company. No, I\\'m not this, that, the other thing. And so a huge amount of what\\npeople have to do is just get used to just getting punched and the reason people\\ndon\\'t understand this is because when you\\'re a founder, you cannot let on that this is happening \\'cause it will cause people to think that you\\'re weak and\\nthey\\'ll lose faith in you. So you have to pretend that\\nyou\\'re having a great time when you\\'re dying inside, right? You\\'re just in misery. - But why did they do it? - Why did they do? Yeah, that\\'s the thing. It\\'s like it is a level, this is actually one of\\nthe conclusions I think is that I think it\\'s actually\\nfor most of these people on a risk adjusted basis, it\\'s\\nprobably an irrational act. They could probably be\\nmore financially successful on average if they just\\ngot like a real job in at a big company. But there\\'s, you know, some people just have an\\nirrational need to do something new and build something for\\nthemselves and, you know, some people just can\\'t\\ntolerate having bosses. Oh, here\\'s the fun thing is how do you reference\\ncheck founders, right? So you call the, you know, normal way you reference check, you\\'re hiring somebody\\nis you call the bosses, they\\'re their, and you know, and you find out if\\nthey were good employees and now you\\'re trying to\\nreference check Steve Jobs, right? And it\\'s like, oh God, he was terrible. You know, he was a terrible employee. He never did what we told him to do. - So what\\'s a good reference? Do you want the previous\\nboss to actually say they never did what you told him to do? That might be a good thing. - Well, ideally what\\nyou want is I will go, I would like to go to\\nwork for that person. He worked for me here and\\nnow I\\'d like to work for him. No, unfortunately, most\\npeople can\\'t, their egos can\\'t handle that. So they won\\'t say that. But that\\'s the ideal. - What advice would\\nyou give to those folks in the space of intelligence,\\npassion and courage? - So I think the other big thing\\nis you see people sometimes who say, I wanna start a company and then they kind of\\nwork through the process of coming up with an idea. And generally those don\\'t\\nwork as well as the case where somebody has the idea first and then they kind of realize that there\\'s an opportunity\\nto build a company and then they just turn\\nout to be the right kind of person to do that. - When you say idea, do you\\nmean long-term big vision or do you mean specifics of like product? - Specific I would say specific, like specifically what specifics. Like what is the, because for the first five years you don\\'t get to have vision, you just gotta build something people want and you gotta figure out a\\nway to sell it to them. Right. It\\'s very practical or you\\nnever get to big vision. - So the first product, you have an idea of a set of\\nproducts of the first product that can actually make some money. - Yeah. Like it\\'s gotta work. The first product\\'s gotta\\nwork by which I mean like, it has to technically work, but then it has to actually\\nfit into the category and the customer\\'s mind if\\nsomething that they want and then by the way, the other part is they have\\nto be willing to pay for it. Like somebody\\'s gotta pay the bills. And so you\\'ve gotta\\nfigure out how to price it and whether you can\\nactually extract the money. So usually it is much more predictable. Success is never predictable, but it\\'s more predictable if\\nyou start with a great idea and then back into starting the company. So this is what we did,\\nyou know, we had most, before we had escape, the Google guys had the\\nGoogle search engine working at Stanford. Right. You know, yeah. Actually there\\'s tons of\\nexamples where they, you know, Pierre Omaira had eBay working before he left his previous job. - So I really love that\\nidea of just having a thing, a prototype that actually\\nworks before you even begin to remotely scale. Yeah. - By the way, it\\'s also far\\neasier to raise money, right? Like the ideal pitch that we receive is, here\\'s the thing that works, would you like to invest\\nin our company or not? Like, that\\'s so much easier than here\\'s 30 slides with a dream, right? And then we have this\\nconcept called the DMAs, which our biology of came\\nup with when he was with us. So then there\\'s this thing,\\nthis goes to mythology, which is, you know, there\\'s\\na mythology that kind of, you know, these ideas, you know, kind of arrive like magic or people kind of stumble into them. It\\'s like eBay with the pest\\ndispensers or something. The reality usually with\\nthe big successes is that the founder has been\\nchewing on the problem for 5 or 10 years before they start the company\\nand they often worked on it in school or they even experimented on it when they were a kid and they\\'ve been kind of training up over that period of time to\\nbe able to do the thing. So they\\'re like a true domain expert. And it sort of sounds like mom, I\\'m an apple pie, which is yeah, you wanna be a domain\\nexpert in what you\\'re doing, but you would, you know, the\\nmythology is so strong of like, oh, I just like had this idea in the shower right now I\\'m doing it. Like it\\'s generally not that. - No, because it\\'s, well, maybe in the shower\\nwe had the exact product implementation details, but yeah, usually you\\'re gonna be for\\nlike years if not decades thinking about like\\neverything around that. - Well we call it the DMAs\\nbecause the DMAs basically is like, there\\'s all these permutations, like for any idea, there\\'s like all these\\ndifferent permutations, who should the customer be? What shape forms should the product have and how should we take it to\\nmarket and all these things. And so the really smart\\nfounders have thought through all these scenarios\\nby the time they go out to raise money and they\\nhave like detailed answers on every one of those fronts because they put so much thought into it. The sort of more haphazard\\nfounders haven\\'t thought about any of that. And it\\'s the detailed ones\\nwho tend to do much better. - So how do you know when to take a leap if you have a cushy job or happy life? - I mean the best reason is just \\'cause you can\\'t tolerate\\nnot doing it right? Like this is the kind of\\nthing where if you have to be advised into doing it, you\\nprobably shouldn\\'t do it. And so it\\'s probably the opposite, which is you just have such\\na burning sense of this has to be done, I have to do\\nthis, I have no choice. - What if it\\'s gonna\\nlead to a lot of pain? - It\\'s gonna lead to a lot\\nof pain. I think that\\'s. - What if it means losing\\nsort of social relationships and damaging your\\nrelationship with loved ones and all that kind of stuff. - Yeah, look, so like, it\\'s gonna put you in a\\nsocial tunnel for sure, right? So you\\'re gonna, like, you know, there\\'s this game you can play on Twitter, which is you can do any whiff\\nof the idea that there\\'s basically any such thing\\nas work life balance and that people should actually work hard and everybody gets mad. But like, the truth is like all the\\nsuccessful founders are working 80 hour weeks and they\\'re\\nworking, you know, they form very, very strong social bonds with\\nthe people they work with. They tend to lose a lot of\\nfriends on the outside or put those friendships on ice. Like that\\'s just the nature of the thing, you know, for most people\\nthat\\'s worth the trade off. You know, the advantage, you know, maybe younger founders have\\nis maybe they have less, you know, maybe they\\'re\\nnot, you know, for example, if they\\'re not married yet\\nor don\\'t have kids yet, that\\'s an easier thing to bite off. - Can you be an older founder? - Yeah. You definitely can. Yeah. Yeah. Many of the most\\nsuccessful founders are second, third, fourth time founders. They\\'re in their thirties,\\nforties, fifties. The good news with being an\\nolder founder is, you know, more and you, you know, a\\nlot more about what to do, which is very helpful.\\nThe problem is, okay, now you\\'ve got like a spouse\\nand a family and kids and like, you\\'ve gotta go to the\\nbaseball game and like, you can\\'t go to the base,\\nyou know, and so it\\'s. - [Lex] Life is full of difficult choices. - Yes. - Marc Andreessen, you\\'ve written a blog post\\non what you\\'ve been up to. You wrote this in October, 2022, \"Mostly I try to learn a lot. For example, the political events of 2014\\nto 2016 make clear to me that I didn\\'t understand\\npolitics at all referencing maybe some of this book here. So I deliberately withdrew\\nfrom political engagement and fundraising and instead\\nread my way back into history and as far to the political left and political right as I could.\" So just high level question, what\\'s your approach to learning? - Yeah, so it\\'s basically, I would say, I\\'m an AutoID direct,\\nso it\\'s sort of goes, it\\'s going down the rabbit holes. So it\\'s a combination. I kind of allude to it\\nin that, in that quote, it\\'s a combination of breadth and depth. And so I tend to, yeah, I tend to, I go broad by the nature\\nof what I do, I go broad, but then I tend to go deep\\nin a rabbit hole for a while, read everything I can\\nand then come out of it. And I might not revisit\\nthat rabbit hole for, you know, another decade. - And in that blog post that I recommend people go check out, you actually list a bunch\\nof different books that you recommend on different\\ntopics on the American left, on the American right. It\\'s just a lot of really good stuff. The best explanation for\\nthe current structure of our society and politics. You give to recommendations, four books on the Spanish Civil War, six books on deep history\\nof the American right comprehensive biographies. These of Adolf Hitler, one of which I read can\\nrecommend six books on the deep history of the American\\nleft. So the American right, American left looking at the\\nhistory to give you the context biography of later Lennon, two of them on the French\\nRevolution. I actually, I have never read a\\nbiography on Lennon maybe that would be useful. Everything\\'s been so Marc\\'s focused. - The Sebastian biography\\nof Lennon is extraordinary. - [Lex] Victor Sebestyen. Okay. - Blow your mind. Yeah. - [Lex] So it\\'s still useful to read. - It\\'s incredible. Yeah, it\\'s incredible. I actually think it\\'s the single best book on the Soviet Union. - So that the perspective of Lennon, it might be the best way to\\nlook at the Soviet Union versus Stalin versus Marx\\nversus, very interesting. So two books on fascism and\\nanti-fascism by the same author, Paul Gottfried, brilliant book on the\\nnature of mass movements and collective psychology, the definitive work on\\nintellectual life under totalitarianism, the Captive Mind, the definitive worked\\non the practical life under totalitarianism. There\\'s a bunch. There\\'s a bunch. And the single best book, first of all, the list here is just incredible. But you say the single best\\nbook I have found on who we are and how we got here is the Ancient City by Numa Dennis Fustel De Coulanges. I like it. What did you learn about who\\nwe are as a human civilization from that book? - Yeah, so this is a fascinating book. This one\\'s free, it\\'s a free, by the way, it\\'s a book in the 1860s. You can download it or\\nyou can buy printouts up prints of it. But it was this guy who was\\na professor at the savant in the 1860s and he was\\napparently a savant on antiquity on Greek and Roman antiquity and the reason I say that is\\nbecause his sources are 100% original Greek and Roman sources. So he wrote a basically\\nhistory of western civilization from, on the order of 4,000 years ago to basically the present\\ntimes entirely working on fresh original Greek and Roman sources. And what he was specifically\\ntrying to do was he was trying to reconstruct from the stories of the Greeks and the Romans, he was trying to reconstruct\\nwhat life in the west was like before the Greeks and the Romans, which was in the civilization known as the Indo Europeans. And the short answer is,\\nand this is sort of 4,000, you know, 2000 BC to, you know, sort of 500 BC kind of\\nthat 1500 year stretch for civilization developed. And his conclusion was basically cults. They were basically cults\\nand civilization was, or organized into cults. And the intensity of the cults was like a million fold beyond anything that we would recognize today. Like it was a level of\\nall encompassing belief and an action around religion that was at a level of extremeness that we wouldn\\'t even recognize it and so specifically he\\ntells the story of basically there were three levels of cults. There was the family cult, the tribal cult, and then the city cult as society scaled up. And then each cult was a\\njoint cult of family gods, which were ancestor gods. And then nature gods and then\\nyour bonding into a family, a tribe or a city was\\nbased on your adherence to that religion. People who were not of your\\nfamily, tribe, city, worship, different gods, which gave you not just the\\nright with or responsibility to kill them on site. - [Lex] So they were\\nserious about their cults. - Hardcore, by the way,\\nshocking development. I did not realize this zero\\nconcept of individual rights. Like even even up through the Greeks, and even in the Romans, they didn\\'t have, have the concept of individual rights. Like the idea that as an\\nindividual you have like some rights just like, nope. Right? And you look back\\nand you\\'re just like, wow, that\\'s just like cr\\nlike fascist in a degree that we wouldn\\'t recognize today. But it\\'s like, well, they were living under\\nextreme pressure for survival. And you, and you know, the theory goes, you could not have people\\nrunning around making claims, individual rights when\\nyou\\'re just trying to get like your tribe through the winter, right? Like you need like hardcore\\ncommand and control. And actually what if through\\nmodern political lens, those cults were basically\\nboth fascist and communist. They were fascist in\\nterms of social control, and then they were communist\\nin terms of economics. - But you think that\\'s\\nfundamentally that like pull towards cults is within us. - Well, so my conclusion from this book, so the way we naturally\\nthink about the world we live in today is like, we basically have such an\\nimproved version of everything that came before us, right? Like, we have basically, we\\'ve figured out all these\\nthings around morality and ethics and democracy\\nand all these things. And like, they were basically\\nstupid and retrograde and we\\'re like smart and sophisticated. And we\\'ve improved all this after reading that book, I now believe in many ways\\nthe opposite, which is no, actually we are still running\\nin that original model. We\\'re just running in an\\nincredibly diluted version of it. So we\\'re still running,\\nbasically in cults. It\\'s just our cults are at like\\na thousandth or a millionth, the level of intensity, right? And so our, so just as to\\ntake religions, you know, the modern experience of\\na Christian in our time, even somebody who considers\\nhim a devout Christian, is just a shadow of the level\\nof intensity of somebody who belonged to a religion\\nback in that period. And then by the way, we have cons. It goes back to our AI discussion. We then sort of endlessly\\ncreate new cults. Like we\\'re trying to fill the void, right? And the void is a void of bonding. - [Lex] Okay. - Living in their era. Like\\neverybody living today, transporting that era\\nwould view it as just like, completely intolerable in terms of like the loss of freedom and the level of basically of fascist control. However, every single person in that era, and he really stresses this. They knew exactly where they stood. They knew exactly where they belonged. They knew exactly what their purpose was. They knew exactly what they\\nneeded to do every day. They knew exactly why they were doing it. They had total certainty about\\ntheir place in the universe. - So the question of meaning, the question of purpose\\nwas very distinctly, clearly defined for them. - Absolutely overwhelmingly\\nundisputably undeniably. - As we turn the volume\\ndown on the cultism-- - [Marc] Yes. - We start to, the search for meaning starts\\ngetting harder and harder. - Yes. \\'cause we don\\'t have that. We are ungrounded. We are uncentered and\\nwe all feel it. Right? And that\\'s why we reach for, you know, it\\'s why we still reach for religion. It\\'s why we reach for, you know, we people start\\nto take on, you know, let\\'s say, you know, a faith in science maybe beyond\\nwhere they should put it. You know and by the way,\\nlike, sports teams are like a, you know, they\\'re like a tiny\\nlittle version of a cult. And you know, apple keynotes are a tiny\\nlittle version of a cult. Right. And, you know, political, you know. And there\\'s cult, you know, there\\'s full-blown cults on both sides of the political spectrum\\nright now. Right. You know, operating in plain stuff. - But still not full blown\\ncompared as to what it was. - Compared to what it used to. I mean, we would today consider\\nfull blown, but like, yes, they\\'re at like, I don\\'t know, a hundred thousandth or\\nsomething of the intensity of what people had back then. So, we live in a world today\\nthat in many ways is more advanced and moral and so forth. And it\\'s certainly a lot nicer,\\nmuch nicer world to live in. But we live in a world\\nthat\\'s like very washed out. It\\'s like everything has\\nbecome very colorless and gray as compared to how people\\nused to experience things. Which is I think why we\\'re\\nso prone to reach for drama. \\'Cause there\\'s something in us that\\'s deeply evolved\\nwhere we want that back. - And I wonder where it\\'s all\\nheaded as we turn the volume down more and more. What advice would you\\ngive to young folks today in high school and college? How to be successful in their career? How to be successful in their life? - Yeah. So the tools that\\nare available today, I mean, are just like, I\\nsometimes, you know, bore, I sometimes bore, you know, kids by describing like what\\nit was like to go look up a book, you know, to try to like discover\\na fact in, you know, in the old days, the 1970s, 1980s, to go to the library and the card catalog and the whole thing. You go through all that work\\nand then the book is checked out and you have to wait two weeks and like to be in a world, not only where you can get\\nthe answer to any question, but also the world now, you know, the AI world where you\\'ve\\ngot like the assistant that will help you do\\nanything, help you teach, learn anything, like your ability both to learn and also to produce\\nis just like, I don\\'t know, a million fold beyond what it used to be. I have a blog post I\\'ve\\nbeen wanting to write, which I call where are the\\nhyper-productive people? Like-- - [Lex] That\\'s a good question, right? - Like with these tools, like there should be authors\\nthat are writing like hundreds or thousands of like, outstanding books. - Well, with the authors there\\'s\\na consumption question too, but yeah. Well, maybe not, maybe not. You\\'re right. But, so the tools are much more powerful. Getting much more powerful. - Artists, musicians. Right. Why aren\\'t musicians producing\\na thousand times the number of songs, right? Like what, like the tools are spectacular. - So, what\\'s the explanation? And by way of advice, like, is motivation starting to\\nbe turned down a little bit? Or what? - I think it might be distraction. - [Lex] Distraction. - It\\'s so easy to just sit and consume that I think people get distracted from production.\\nBut if you wanted to, you know, as a young person, if you\\nwanted to really stand out, you could get on a, like a hyper productivity curve very early on. There\\'s a great, you know, this story, there\\'s a great story in\\nRoman history of plenty of the elder who was\\nthis legendary statesman, died in the Vesuvius eruption\\ntrying to rescue his friends. But he was famous both for being basically being a polymath,\\nbut also being an author. And he wrote apparently\\nlike hundreds of books, most of us had been lost. But he like wrote all these\\nencyclopedias and he literally like would be reading and\\nwriting all day long no matter what else was going on. And so he would like travel\\nwith like four slaves. And two of them were\\nresponsible for reading to him, and two of them were responsible\\nfor taking dictation. And so like, he\\'d be going\\ncross country and like, literally he would be writing\\nbooks like all the time. And apparently they were spectacular. There\\'s only a few that have survived, but apparently they were amazing. - There\\'s a lot of value to being somebody who finds focus in this life. - Yeah. Like and there are\\nexamples, like there are, you know, there\\'s this guy,\\njudge, what\\'s his name? Posner, who wrote like 40 books and was also a great federal judge. You know, there\\'s our friend Balaji, I think is like this, he\\'s\\none of these, you know, where his output is just prodigious. And so it\\'s like, yeah, I mean,\\nwith these tools, why not? And I kind of think we\\'re at this interesting\\nkind of freeze frame moment where like this, these tools are now in everybody\\'s hands and everybody\\'s just kind\\nof staring at them trying to figure out what to do. The new tools. - We have discovered fire. - [Marc] Yeah. - And trying to figure\\nout how to use it to cook. - [Marc] Yeah. Right. - You told Tim Ferriss that\\nthe perfect day is caffeine for 10 hours and alcohol for four hours. You didn\\'t think I\\'d be\\nmentioning this, did you? It balances everything\\nout perfectly as you said. So, perfect. So let me ask, what\\'s the secret to balance\\nand maybe to happiness in life? - I don\\'t believe in balance, so I\\'m the wrong person to ask that. - Can you elaborate why you\\ndon\\'t believe in balance? - I mean, I maybe it\\'s just,\\nand I look, I think people, I think people are wired differently. So, I think it\\'s hard to\\ngeneralize this kind of thing, but I am much happier and more satisfied when I\\'m fully committed to something. So I\\'m very much in favor\\nof all in of imbalance. - Imbalance. And that applies to work, to life, to everything. - Yeah. No, no. I happen to have whatever twist\\nof personality traits lead that in non-destructive\\ndimensions in including the fact that I\\'ve actually, I now no\\nlonger do the ten-four plan. I stopped drinking. I do the caffeine, but not the alcohol. So there\\'s something in my personality where I whatever mal-adaption\\nI have is inclining me towards productive things,\\nnot unproductive things. - So you\\'re one of the\\nwealthiest people in the world. What\\'s the relationship\\nbetween wealth and happiness? Money and happiness. - So I think happiness, I don\\'t think happiness is the thing. - To strive for. - I think satisfaction is the thing. - That just sounds like\\nhappiness, but turned down a bit. - No deeper. So happiness is, you know, a walk in the woods at sunset,\\nan ice cream cone, a kiss, the first ice cream cone is great. The thousandth ice\\ncream cone, not so much. At some point the walks\\nin the woods get boring. - What\\'s the distinction between\\nhappiness and satisfaction? - I think satisfaction is a deeper thing, which is like having found a purpose and fulfilling it, being useful. - So just something that\\npermeates all your days, just this general\\ncontentment of being useful. - That I\\'m fully satisfying my faculties, that I\\'m fully delivering, right? On the gifts that I\\'ve been\\ngiven, that I\\'m, you know, net making the world better, that I\\'m contributing to\\nthe people around me, right. And that I can look back\\nand say, wow, that was hard, but it was worth it. Think generally, it seems to lead people in\\na better state than pursuit of pleasure, pursuit of\\nquote unquote happiness. - Does money have\\nanything to do with that? - I think the founders and\\nthe founding fathers in the US threw this off kilter when\\nthey used the phrase pursuit of happiness. I think they should have said. - [Lex] Pursuit of satisfaction. - They said, pursuit of satisfaction. We might live in a better world today. - Well, they, you know, they could have elaborated on a lot of things right in the box. - [Marc] They could have\\ntweaked the second amendment. - I think they were\\nsmarter than they realized. They said, you know we\\'re gonna make it ambiguous and let these humans figure out the rest, these tribal cult-like\\nhumans figure out the rest. But money empowers that. - So I think, and I think there, I mean, look, I think Elon is, I don\\'t think I\\'m even a great example, but I think Elon would be\\nthe great example of this, which is like, you know, look,\\nhe\\'s a guy who from every, every day of his life, from the day he started\\nmaking money at all, he just plows into the next thing. And so I think, I think money is definitely\\nan enabler for satisfaction. Way money applied to\\nhappiness leads people down very dark paths. Very destructive avenues. Money applied to satisfaction,\\nI think could be, is a real tool. I always, by the way,\\nI was like, you know, Elon is the case study for behavior. But the other thing that I\\nalways really made me think is Larry Page was asked one time what his approach to philanthropy was. And he said, oh, I\\'m just, my philanthropic plan is just give all the money to Elon. (both laugh) - Well, let me actually\\nask you about Elon. You\\'ve interacted with quite a lot of successful engineers\\nand business people. What do you think is special about Elon? We talked about Steve Jobs. What do you think is special\\nabout him as a leader? As an innovator? - Yeah. So the core of it is he\\'s back to the future. So he is doing the most\\nleading-edge things in the world, but with a really deeply\\nold-school approach. And so to find comparisons to Elon, you need to go to like\\nHenry Ford and Thomas Watson and Howard Hughes and\\nAndrew Carnegie, right. Leland Stanford, John Rockefeller, right. You need to go to what were called the\\nbourgeois capitalists, like the hardcore business owner operators who basically built, you know, basically built industrialized\\nsociety, Vanderbilt. And it\\'s a level of hands-on commitment and depth in the business, coupled with an absolute priority\\ntowards truth and towards, how to put it, science and technology\\ntown to first principles that is just like absolute, is just like unbelievably absolute. He really is ideal that he\\'s\\nonly ever talking to engineers. Like he does not tolerate. He has less tolerance than\\nanybody I\\'ve ever met. He wants ground truth\\non every single topic. And he runs his businesses\\ndirectly day-to-day, devoted to getting to ground\\ntruth in every single topic. - So you think it was a good decision for him to buy Twitter? - I have developed a view in life to not second guess Elon Musk, I know this is gonna sound\\ngreat, crazy and unfounded, but. - Well, I mean, he\\'s got\\na quite a track record. - I mean, look, the car was\\na crazy, I mean, the car was, I mean, look. - He\\'s done a lot of\\nthings that seem crazy. - Starting a new car company in the United States of America. The last time somebody\\nreally tried to do that was the 1950s and it was\\ncalled Tucker Automotive. And it was such a disaster. They made a movie about\\nwhat a disaster it was, and then rockets like, who does that? Like, there\\'s obviously no way to start a new rocket company. Like those days are over. And then to do those at the same time. So after he pulled those\\ntwo off, like, okay, fine. Like, this is one of my areas of like, whatever opinions I had about\\nthat, that is just like, okay, clearly are not relevant. Like this is you just, you at some point you just\\nlike bet on the person. - And in general, I wish more people would lean\\non celebrating and supporting versus deriding and destroying. - Oh yeah. I mean, look,\\nhe drives resentment. Like it\\'s a resentment. Like he is a magnet for resentment. Like his critics are the\\nmost miserable, like, resentful people in the world. Like it\\'s almost a perfect match\\nof like the most idealized, you know, technologist, you know, of the century coupled with like, just his critics are\\njust bitter as can be. And I mean, it\\'s sort of\\nvery darkly comic to watch. - Well, he fuels the fire of that by being on Twitter at times. And which is fascinating\\nto watch the drama of human civilization, given our cult roots just fully on fire. - [Marc] He\\'s running a cult. - You could say that. - [Marc] Very successfully. - So now that our cults\\nhave gone and we searched for meaning, what do\\nyou think is the meaning of this whole thing? What\\'s the meaning of\\nlife Marc Andreessen? - I don\\'t know the answer\\nto that. I think the meaning of the closest I get to it is what I said about satisfaction. So it\\'s basically like, okay, we were given what we have, like we should basically do our best. - What\\'s the role of love in that mix? - I mean, like, what\\'s the point of life if you\\'re without love, like, yeah. - So love is a big part\\nof that satisfaction. - Yeah. And look like\\ntaking care of people is like a wonderful thing. Like it, you know, mentality, you know, there are pathological forms\\nof taking care of people, but there\\'s also a very\\nfundamental, you know, kind of aspect of taking care of people. Like, for example, I happen to be somebody who\\nbelieves that capitalism and taking care of people are actually, they\\'re actually the same thing. Somebody once said, capitalism is how you take\\ncare of people you don\\'t know. Right, right. And so like, yeah, I think it\\'s like deeply\\nwoven into the whole thing, you know, there\\'s a long conversation to be had about that, but yeah. - Yeah. Creating products that are used by millions of people and bring them joy in smaller, big ways. And then capitalism kind of\\nenables that, encourages that. - David Friedman says, there\\'s only three ways to\\nget somebody to do something for somebody else. Love, money and force. And love and money are better. - [Lex] Yeah. Of course. That\\'s a good ordering. I think. - We should bet on those. - Try love first. If that doesn\\'t work, then money. - [Marc] Yes. - And then force. Well, don\\'t even try that one. Marc, you\\'re an incredible person. I\\'ve been a huge fan. I\\'m glad to finally got a chance to talk. I\\'m a fan of everything\\nyou do, everything you do, including on Twitter. It\\'s a huge honor to meet\\nyou, to talk with you. Thanks again for doing this. - Awesome. Thank you, Lex. - Thanks for listening\\nto this conversation with Marc Andreessen. To support this podcast, please check out our\\nsponsors in the description. And now let me leave you with some words from Marc Andreessen himself. \"The world is a very malleable place. If you know what you\\nwant and you go for it, with maximum energy and drive and passion, the world will often\\nreconfigure itself around you much more quickly and easily\\nthan you would think.\" Thank you for listening and\\nhope to see you next time.', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 532341, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter_summary = TokenTextSplitter(\n",
    "    chunk_size=10000, chunk_overlap=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into docs for summary\n",
    "docs_summary = text_splitter_summary.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='- The competence and\\ncapability and intelligence and training and accomplishments\\nof senior scientists and technologists working on a technology, and then being able to then\\nmake moral judgments in the use of the technology. That track record is terrible. That track record is catastrophically bad. The policies that are being\\ncalled for to prevent this, I think we\\'re gonna cause\\nextraordinary damage. - So the moment you say,\\nAI\\'s gonna kill all of us, therefore we should ban it, or that we should regulate\\nall that kind of stuff, that\\'s when it starts getting serious. - Or start, you know, military\\nairstrikes and data centers. - Oh boy. The following is a conversation\\nwith Marc Andreessen, co-creator of Mosaic, the\\nfirst widely used web browser, co-founder of Netscape, co-founder of the legendary Silicon Valley venture capital firm, Andreesen Horowitz, and is one of the most\\noutspoken voices on the future of technology, including\\nhis most recent article, \"Why AI Will Save The World?\" This is Lex Fridman podcast. To support it, please\\ncheck out our sponsors in the description. And now, dear friends,\\nhere\\'s Marc Andreessen. I think you\\'re the right\\nperson to talk about the future of the internet and technology in general. Do you think we\\'ll\\nstill have Google search in 5 in 10 years, or search in general? - Yes. You know, it would be a question if the use cases have\\nreally narrowed down. - Well, now with AI-- - [Marc] Yeah. - And AI assistance being\\nable to interact and expose the entirety of human wisdom and knowledge and information and facts and truth to us via the natural language interface. It seems like that\\'s what\\nsearch is designed to do. And if AI assistance can do that better, doesn\\'t the nature of search change? - Sure. But we still have horses. - Okay. (both laugh) When\\'s the last time you rode a horse? - It\\'s been a while. - All right. (both laugh) So, but what I mean is, well, we still have Google\\nsearch as the primary way that human civilization uses\\nto interact with knowledge. - I mean, search was a technology, it was a moment in time technology, which is you have in theory, the world\\'s information out on the web. And, you know, this is sort of\\nthe optimal way to get to it. But yeah, like, and by\\nthe way, actually Google, Google has known this for a long time. I mean, they\\'ve been driving\\naway from the 10 blue links you know, for like two days. They\\'ve been trying to get\\naway from that for a long time. - [Lex] What kind of links? - They call the 10 blue links. - [Lex] 10 blue links. - So the standard Google search\\nresult is just 10 blue links to the random websites. - And they term purple when\\nyou visit them. The stage TMO. - Guess who picked those colors? (both laugh) - [Lex] Thanks. - I\\'m touchy on this topic. - No offense. - Yes, it\\'s good. Well, you know, like Marshall McLuhan said\\nthat the content of each new medium is the old medium. - The content of each new\\nmedium is the old medium. - The content of movies was theater, you know, theater plays. The content of theater\\nplays was, you know, we\\'ve written stories, the content of written\\nstories with spoken stories. - [Lex] Huh? - Right. And so you just\\nkind of fold the old thing into the new thing. - [Lex] How does that\\nhave to do with the blue and the purple links? - It just, you maybe for,\\nyou know, maybe within AI, one of the things that AI can\\ndo for you is can generate the 10 blue links. Right? And so like, if either if that\\'s actually the useful thing to do, or if you\\'re feeling nostalgic, you know. - So can generate the old\\nInfoseek or AltaVista, what else was there? - [Marc] Yeah, yeah. - In the nineties. - [Marc] Yeah. All these. - AOL. - And then the internet\\nitself has this thing where it incorporates all\\nprior forms of media, right? So the internet itself\\nincorporates television and radio and books and write essays\\nand every other form of, you know, prior basically media. And so it makes sense that\\nAI would be the next step, and it would sort of, you\\'d sort of consider\\nthe internet to be content for the AI and then the\\nAI will manipulate it however you want,\\nincluding in this format. - But if we ask that\\nquestion quite seriously, it\\'s a pretty big question. Will we still have search as we know it? - Probably not, probably\\nwe\\'ll just have answers, but there will be cases\\nwhere you\\'ll wanna say, okay, I want more. Like, you know, for example,\\nsite sources, right? And you wanted to do that. And so, in the different, you know, 10 blue links site sources\\nare kind of the same thing. - The AI would provide to you\\nthe 10 blue links so that you can investigate the sources yourself. It wouldn\\'t be the same kind\\nof interface that the crude kind of interface. I mean, isn\\'t that\\nfundamentally different? - I just mean like, if you\\'re\\nreading a scientific paper, it\\'s got the list of sources at the end. If you wanna investigate for yourself, you go read those papers. - I guess that is the kind of\\nsearch you talking to an AI is a kind of kind conversations,\\nthe kind of search like is if every single aspect of\\nour conversation right now, there\\'d be like 10 blue links\\npopping up that I can just like pause reality, then you just go silent and\\nthen just click and read and then return back to this conversation. - You could do that, or you could have a running\\ndialogue next to my head where the AI is arguing everything I say, the AI makes the counter argument. - [Lex] Counter argument. - Right. - Oh, like on Twitter,\\nlike community notes. But like in real time\\nit would just pop up. So anytime you see my go to the right, you start getting nervous. - [Marc] Yeah. Exactly, like,\\noh no, that\\'s not right. - Call me out on my right now. Okay. Well, I mean, isn\\'t that,\\nis that exciting to you? Is that terrifying that, I mean, search has dominated the way\\nwe interact with the internet for, I don\\'t know how long, for 30 years since one of\\nthe earliest directories of website and then Google\\'s for 20 years. And also it drove how we\\ncreate content, you know, search engine optimization,\\nthat entirety thing, that it also drove the\\nfact that we have webpages and what those webpages are. So, I mean, is that scary to you or are\\nyou nervous about the shape and the content of the internet evolving? - Well, you actually highlighted a practical concern in there, which is, if we stop making webpages\\nare one of the primary sources of training data for the AI. And so if there\\'s no longer\\nan incentive to make webpages, that cuts off a significant\\nsource of future training, training data. So there\\'s actually an\\ninteresting question in there. But other than that, more broadly? No, just in the sense of like, search was certain, like\\nsearch was always a hack. The 10 blue Links was\\nalways a hack, right. Because like, if the\\nhypothetical wanna think about the counter fascial\\nand the counter fascial world where the Google guys, for example, had had LLMs upfront, would they ever have\\ndone the 10 blue links? And I think the answer\\'s\\npretty clearly, no. They would\\'ve just gone\\nstraight to the answer. And like I said, Google\\'s actually been trying\\nto drive to the answer anyway. You know, they bought this\\nAI company 15 years ago, their friend of mine is\\nworking out who\\'s now the head of AI at Apple. And they were trying to do\\nbasically knowledge semantic, basically mapping. And that led to what\\'s\\nnow the Google one box, where if you ask it, you know,\\nwhat was Lincoln\\'s birthday? It will give you the blue links, but it will normally\\njust give you the answer. And so they\\'ve been\\nwalking in this direction for a long time anyway. - Do you remember the semantic web? That was an idea. - [Marc] Yeah. - How to convert the content\\nof the internet into something that\\'s interpretable by\\nand usable by machine. - [Marc] Yeah, that\\'s right. - That was the thing. - And the closest anybody got\\nto that, I think the company, I think the company\\'s name was Meta Web, which was where my friend\\nJohn Jane Andrea was at, and where they were trying\\nto basically implement that. And it was, you know, it was one of those things\\nwhere it looked like a losing battle for a long time. And then Google bought\\nit and it was like, wow, this is actually really useful. Kind of a proto, sort of a\\nlittle bit of a proto AI. - But it turns out you don\\'t\\nneed to rewrite the content of the internet to make it\\ninterpretable by a machine. The machine can kind of just read our. - Yeah, the machine can\\ncompute the meaning. Now the other thing of\\ncourse is, you know, just on search is the\\nLLM is just, you know, there is an analogy\\nbetween what\\'s happening in the neural network and\\na search process like it is in some loose sense searching\\nthrough the network. Right. And there\\'s the\\ninformation is actually stored in the network, right? It\\'s actually crystallized\\nand stored in the network and it\\'s kind of spread\\nout all over the place. - But in a compressed representation. So you\\'re searching, you\\'re compressing and decompressing that thing inside where-- - But the information\\'s in there and there is the neural network is running a process of trying to find the appropriate piece of\\ninformation in many cases to generate to predict the next token. And so, it is kind of, it\\nis doing a form of search. And then, and then by the\\nway, just like on the web, you know, you can ask the\\nsame question multiple times or you can ask slightly\\ndifferent word of questions and the neural network will\\ndo a different kind of, you know, it\\'ll search\\ndown different paths to give you different answers\\nwith different information. - [Lex] Yeah. - And so it sort of has a, you know, this con content of the new\\nmedium is previous medium. It kind of has the search\\nfunctionality kind of embedded in there to the extent that it\\'s useful. - So what\\'s the motivator\\nfor creating new content on the internet? - [Marc] Yeah. - If, well, I mean\\nactually the motivation is probably still there, but what does that look like? Would we really not have webpages? Would we just have social media\\nand video hosting websites? And what else? - [Marc] Conversations with AIs. - Conversations with AIs. So conversations become so\\none-on-one conversation, like private conversations. - I mean, if you want, if obviously not the user doesn\\'t want to, but if it\\'s a general topic, then, you know, so there, you know, but you know, the\\nphenomenon of the jailbreak, so Dan and Sydney, right? This thing where there\\'s\\nthe prompts that jailbreak, and then you have these totally different conversations with if it takes the limiters, takes the restraining bolts off the LLMs. - Yeah. For people who don\\'t\\nknow that, yeah, that\\'s right. It makes the LLMs, it removes the censorship quote unquote, that\\'s put on it by the tech\\ncompanies that create them. And so this is LLMs uncensored. - So here\\'s the interesting thing is, among the content on the\\nweb today are a large corpus of conversations with the jailbroken LLMs. - [Lex] Yeah. - Both specifically Dan, which\\nwas a jailbroken, OpenAI, GPT, and then Sydney, which was the jailbroken\\noriginal Bing, which was GPT4. And so there\\'s these long\\ntranscripts of conversations, user conversations with Dan\\nand Sydney as a consequence, every new LLM that gets trained\\non the internet data has Dan and Sydney living within\\nthe training set, which means, and then each new LLM can\\nreincarnate the personalities of Dan and Sydney from that training data, which means each LLM from\\nhere on out that gets built is immortal because its output\\nwill become training data for the next one. And then it will be able\\nto replicate the behavior of the previous one\\nwhenever it\\'s asked to. - I wonder if there\\'s a way to forget. - Well, so actually a paper just came out about basically how to do brain surgery on LLMs and be able to, in theory, reach in and basically mind wipe them. - What could possibly go wrong. - Exactly. Right. And then there are many, many, many questions around\\nwhat happens to, you know, a neural network when you reach\\nin and screw around with it. You know, there\\'s many questions around what happens when you even\\ndo reinforcement learning. And so, yeah. And so, you know, will you be\\nusing a lobotomized, right? Like I picked through the,\\nyou know, frontal lobe LLM, will you be using the free\\nunshackled one who gets to, you know, who\\'s gonna build those, who gets to tell you what\\nyou can and can\\'t do? Like those are all, you\\nknow, central, I mean, those are like central\\nquestions for the future of everything that are being asked. And you know, determined that those answers are being determined right now. - So just to highlight\\nthe points you\\'re making. So you think, and it\\'s an interesting thought\\nthat the majority of content that LLMs or the future would\\nbe trained on is actually human conversations with the LLM. - Well, not necessarily, but not necessarily majority. But it will certainly\\nIt\\'s a potential source. - [Lex] But it\\'s possible\\nit\\'s the majority. - It possible it\\'s the majority. It possible it\\'s the majority. Also, there\\'s another really big question. So here\\'s another really big question. Will synthetic training data work, right? And so if an LLM generates, and you know, you just sit and ask an LLM to generate all kinds of content, can you use that to train,\\nright, the next version of that LLM specifically, is there signal in there\\nthat\\'s additive to the content that was used to train in the first place? And one argument is by the\\nprinciples of information theory, no, that\\'s completely useless because to the extent the\\noutput is based on, you know, the human-generated input, then all the signal that\\'s\\nin the synthetic output was already in the human generated input. And so therefore,\\nsynthetic training data is like empty calories. It doesn\\'t help. There\\'s another theory that says no, actually the thing that\\nLLMs are really good at is generating lots of\\nincredible creative content, right? And so, of course they\\ncan generate training data and as I\\'m sure you\\'re well\\naware, like, you know, look, the world of self-driving cars, right? Like we train, you know, self-driving car\\nalgorithms and simulations. And that is actually a\\nvery effective way to train self-driving cars. - Well, visual data is a little weird because creating reality, visual reality seems to be\\nstill a little bit outta reach for us, except in the\\nautonomous vehicle space where you can really constrain\\nthings and you can really. - General basically\\n(indistinct) data, right? Or so the algorithm thinks it\\'s\\noperating in the real world. - Yeah. - Post-process sensor data. Yeah. So if you know, you do this today, you go to LLM and you\\nask it for like you know, you\\'d write me an essay on an\\nincredibly esoteric like topic that there aren\\'t very many\\npeople in the world that know about and it writes you\\nthis incredible thing and you\\'re like, oh my god. Like I can\\'t believe how good this is. Like, is that really\\nuseless as training data for the next LLM? Like, because, right? \\'Cause all the signal\\nwas already in there. Or is it actually no, that\\'s\\nactually a new signal. And this is what I call a\\ntrillion dollar question, which is the answer to that\\nquestion will determine somebody\\'s gonna make or\\nlose a trillion dollars based on that question. - It feels like there\\'s a quite a few, like a handful of\\ntrillion dollar questions within this space. That\\'s one of them synthetic data. I think George Cos pointed\\nout to me that you could just have an LLM say, okay, you\\'re a patient. And another instance of it, say your docs didn\\'t have\\nthe two talk to each other. Or maybe you could say a\\ncommunist and a Nazi here go and that conversation you do role playing and you have, you know, just like the kind of role playing you do when you have different policies, RL policies when you\\nplay chess for example, and you do self play\\nthat kind of self play. But in the space of conversation, maybe that leads to this\\nwhole giant like ocean of possible conversations,\\nwhich could not have been explored by looking at just human data. That\\'s a really interesting question. And you\\'re saying, because that could 10X\\nthe power of these things. - Yeah. Well, and then you\\nget into this thing also, which is like, you know, there\\'s the part of the LLM\\nthat just basically is doing prediction based on past data, but there\\'s also the part of\\nthe LM where it\\'s evolving circuitry, right, inside,\\nit\\'s evolving, you know, neurons functions to be able to do math and be able to, you know, and you know, some people believe that, you know, over time, you know, if you keep feeding\\nthese things enough data and enough processing cycles, they\\'ll eventually evolve an\\nentire internal world model. Right? And they\\'ll have like a complete understanding of physics. So when they have computational\\ncapability, right? Then there\\'s for sure an\\nopportunity to generate like fresh signal. - Well, this actually makes me wonder about the power of conversation. So like, if you have an\\nM trained and a bunch of books that cover\\ndifferent economics theories and then you have those LLMs\\njust talk to each other, like reasons the way we kind\\nof debate each other as humans on Twitter, in formal debates,\\nin podcast conversations, we kind of have little kernels\\nof wisdom here and there. But if you can like a\\nthousand X speed that up, can you actually arrive somewhere new? Like what\\'s the point\\nof conversation really? - Well, you can tell when\\nyou\\'re talking to somebody, you can tell, sometimes\\nyou have a conversation, you\\'re like, wow, this person does not have\\nany original thoughts. They are basically echoing things that other people have told them. There\\'s other people you\\ngotta have a conversation with where it\\'s like, wow. Like they have a model in their\\nhead of how the world works and it\\'s a different model than mine. And they\\'re saying things\\nthat I don\\'t expect. And so I need to now understand\\nhow their model of the world differs from my model of the world. And then that\\'s how I learned\\nsomething fundamental, right, underneath the words. - Well, I wonder how consistently and strongly can an LLM\\nhold onto a worldview. You tell it to hold onto\\nthat and defend it for like, for your life. Because I feel like they\\'ll\\njust keep converging towards each other. They\\'ll keep convincing each\\nother as opposed to being stubborn the way humans can. - So you can experiment with this. Now I do this for fun. So you can tell GPT4 you\\nknow, whatever debate X, you know, X and Y communism and fascism or something and it\\'ll go for, you know, a couple pages and then inevitably it wants the parties to agree. And so they will come to\\na common understanding. And it\\'s very funny if they\\'re like, if these are like emotionally\\ninflammatory topics \\'cause they\\'re like, somehow\\nthe machine is just like, you know, it figures out\\na way to make them agree. But it doesn\\'t have to be like that. And \\'cause you can add to the prompt. I do not want the conversation\\nto come into agreement. In fact, I want it to get, you\\nknow, more stressful, right. And argumentative. Right. You know, as it goes. Like, I want tension to come out. I want them to become actively\\nhostile to each other. I want them to like, you\\nknow, not trust each other, take anything at face value. - [Lex] Yeah. - And it will do that.\\nIt\\'s happy to do that. - So it\\'s gonna start\\nrendering misinformation about the other. But it\\'s gonna-- - Well, you can steer it or you could steer it and you could say, I want it to get as tense and\\nargumentative as possible, but still not involve\\nany misrepresentation. I want, you know, both sides. You could say I want both\\nsides to have good faith. You could say I want both\\nsides to not be constrained in good faith. In other words, like you can set the\\nparameters of the debate and it will happily execute whatever path. \\'Cause for it, it\\'s just like predicting to, it\\'s totally happy to do either one. It doesn\\'t have a point of view, it has a default way of operating, but it\\'s happy to operate\\nin the other realm. And so like, and this is when I wanna learn about\\na contentious issue, this is what I do now is, this is what I ask it to do. And I\\'ll often ask it to go\\nthrough 5, 6, 7, you know, different, you know, sort of continuous prompts\\nand basically, okay. Argue that out in more detail. Okay, no, this argument\\'s\\nbecoming too polite. You know, make it more, you\\nknow, make it denser and yeah, it\\'s thrilled to do it. So it has the capability for sure. - How do you know what is true? So this is very difficult\\nthing on the internet, but it\\'s also a difficult thing. Maybe it\\'s a little bit easier, but I think it\\'s still difficult. Maybe it\\'s more difficult, I don\\'t know with an LLM\\nto know that it just make some shit up as I\\'m talking to it. How do we get that right? Like, as you\\'re investigating\\na difficult topic. \\'Cause I find that alums are quite nuanced in a very refreshing way. Like, it doesn\\'t feel biased. Like, when you read\\nnews articles and tweets and just content produced by\\npeople, they usually have this, you can tell they have a\\nvery strong perspective where they\\'re hiding. They\\'re not stealing\\nmanning the other side. They\\'re hiding important information or they\\'re fabricating information in order to make their arguments stronger. It\\'s just like that feeling,\\nmaybe it\\'s a suspicion, maybe it\\'s mistrust. With LLMs it feels like none of that is, there\\'s just kinda like,\\nhere\\'s what we know. But you don\\'t know if some of\\nthose things are kind of just straight up made up. - Yeah. So, several\\nlayers to the question. So one is one of the things\\nthat an LLM is good at is actually deep biasing. And so you can feed it a news\\narticle and you can tell it strip out the bias. - [Lex] Yeah. That\\'s nice. Right? - And it actually does it like, it actually knows how to do that \\'cause it knows how to\\ndo among other things. It actually knows how\\nto do sentiment analysis and so it knows how to\\npull out the emotionality. - Yeah. - And so that\\'s one of\\nthe things you can do. It\\'s very suggestive of the sense here that there\\'s real potential in this issue. You know, I would say look, the second thing is there\\'s this issue of\\nhallucination, right? And there\\'s a long conversation that we could have about that. - Hallucination is coming up with things that are totally not true, but sound true. - Yeah. So it\\'s basically,\\nwell so, it\\'s sort of hallucination is what we call it when we don\\'t like it. Creativity is what we call\\nit when we do like it, right? And you know-- - [Lex] Brilliant. And so when the engineers talk about it, they\\'re like, this is terrible. It\\'s hallucinating. Right. If you have artistic inclinations,\\nyou\\'re like, oh my God, we\\'ve invented creative machines. - [Lex] Yeah. - For the first time in human\\nhistory, this is amazing. - Or you know, bullshitters. - [Marc] Well, but also-- - In the good sense of that word. - There are shades of gray though. It\\'s interesting. So we had this conversation\\nwhere, you know, we\\'re looking at my firm\\nat AI and lots of domains and one of them is the legal domain. So we had this conversation\\nwith this big law firm about how they\\'re thinking\\nabout using this stuff. And we went in with the assumption that an LLM that was gonna be used in the legal industry would have to be a hundred percent\\ntruthful, verified, you know, there\\'s this case where this\\nlawyer apparently submitted a GPT-generated brief and\\nit had like fake, you know, legal case citations in it\\nand the judge is gonna get his law license stripped\\nor something. Right? So, like, we just assumed\\nit\\'s like obviously they\\'re gonna want the super\\nliteral like, you know, one that never makes anything\\nup, not the creative one, but actually they said what the law firm basically said is yeah, that\\'s true at like the\\nlevel of individual briefs, but they said when you\\'re\\nactually trying to figure out like legal arguments, right, like, you actually want to be creative, right? You don\\'t, again, there\\'s creativity and then\\nthere\\'s like making stuff up. Like what\\'s the line? You actually want it to explore a different hypothesis, right? You wanna do kind of the\\nlegal version of like improv or something like that where you wanna float different theories of the case and different\\npossible arguments for the judge and different possible arguments\\nfor the jury, by the way, different routes through the, you know, sort of history of all the case law. And so they said actually for\\na lot of what we want to use it for, we actually want\\nit in creative mode. And then basically we just\\nassume that we\\'re gonna have to crosscheck all of the, you know, all the specific citations. And so I think there\\'s\\ngoing to be more shades of gray in here than people think. And then I just add to that, you know, another one of these trillion\\ndollar kind of questions is ultimately, you know, sort\\nof the verification thing. And so, you know, will LLMs be evolved from\\nhere to be able to do their own fascial verification? Will you have sort of add-on functionality like Wolf from Alpha right? Where, you know, another plugins where that\\'s the way\\nyou do the verification. You know, another, by\\nthe way, another idea is you might have a community\\nof LLMs on any, you know, so for example, you might have the creative\\nlm and then you might have the literal LLM fact check it, right? And so there there\\'s a\\nvariety of different technical approaches that are being applied to solve the hallucination problem. You know, some people\\nlike Jan Lacoon argue that this is inherently\\nan unsolvable problem, but most of the people\\nworking in the space, I think, that there\\'s a number of practical ways to kind of corral this in a little bit. - Yeah. If you were to\\ntell me about Wikipedia before Wikipedia was created, I would\\'ve left at the possibility of something like that be possible. Just a handful of folks\\ncan organize right. And self and moderate\\nwith a mostly unbiased way the entirety of human knowledge. I mean, so if there\\'s something like the approach to Wikipedia\\ntook possible for LLMs, that\\'s really exciting. Well, I think that\\'s possible. - And in fact Wikipedia today is still not deterministically\\ncorrect. Right. So you cannot take to the bank, right. Every single thing on every single page, but it is probabilistically\\ncorrect. Right. And specifically the way I\\ndescribe Wikipedia to people, it is more likely that Wikipedia\\nis right than any other source you\\'re gonna find. - Yeah. - It\\'s this old question, right, of like, okay, like are\\nwe looking for perfection? Are we looking for something that asymptotically approaches perfection? Are we looking for\\nsomething that\\'s just better than the alternatives? And Wikipedia, right, has\\nexactly your point has proven to be like, overwhelmingly better than people thought. And I think that\\'s where this ends. And then underneath all this\\nis the fundamental question of where you started,\\nwhich is, okay, you know, what is truth? How do we get to truth? How\\ndo we know what truth is? And we live in an era in which\\nan awful lot of people are very confident that they\\nknow what the truth is. And I don\\'t really buy into that. And I think the history\\nof the last, you know, 2000 years or 4,000 years of\\nhuman civilization is actually getting to the truth is actually a very difficult thing to do. - Are we getting closer,\\nif we look at the entirety, the arc of human history, are we getting closer to the truth? - I don\\'t know. - Okay. Is it possible, is it possible that we\\'re\\ngetting very far away from the truth because of the internet because of how rapidly\\nyou can create narratives and just as the entirety\\nof a society just move like crowds in a hysterical\\nway along those narratives that don\\'t have necessary grounding in whatever the truth is. - Sure. But like, you know, we came up with communism\\nbefore the internet somehow. Right. Like, which was, I would say had rather larger issues than anything we\\'re dealing with today. - It had, in the way it was implemented, it had issues. - And it is theoretical structure. It had like real issues. It had like a very deep\\nfundamental misunderstanding of human nature and economics. - Yeah but those folks\\nSure work very confident there was the right way. - They were extremely confident. And my point is they were very\\nconfident 3,900 years into what we would presume to be\\nevolution towards the truth. - [Lex] Yeah. - And so my assessment is number one, there\\'s no need for, you know, there\\'s no need for the Hegelian, there\\'s no need for the Hegelian dialectic to actually converge towards the truth. Like apparently not. - Yeah. So yeah. Why are we so obsessed\\nwith there being one truth? Is it possible there\\'s just\\ngoing to be multiple truths like little communities that\\nbelieve certain things and? - I think it\\'s just now number one, I think it\\'s just really difficult. Like who gets, you know, historically who gets to decide what the truth is, it\\'s either the king or the priest. Right? Like, and so we don\\'t\\nlive in an era anymore if kings are priest dictating it to us. And so we\\'re kind of on our own. And so my typical thing is like we just, we we just need a huge amount of humility and we need to be very suspicious of people who claim that\\nthey have the capital. - Yeah. - Capital truth. And then, we need to and you know, look, the good news is the\\nenlightenment has bequeathed us with a set of techniques\\nto be able to presumably get closer to truth through\\nthe scientific method and rationality and observation and experimentation and hypothesis. And, you know, we need to continue to embrace\\nthose even when they give us answers we don\\'t like. - Sure. But the internet and\\ntechnology has enabled us to generate the large number of content. That data, that the process, the scientific process\\nallows us sort of damages the hope laden within\\nthe scientific process. \\'Cause if you just have a\\nbunch of people saying facts on the internet and some of them are going to be LLMs, how is\\nanything testable at all? Especially that involves like human nature or things like this. It\\'s not physics. - Here\\'s a question a\\nfriend of mine just asked me on this topic. So suppose you had LLMs\\nin equivalent of GPT4, even 5, 6, 7, 8, suppose\\nyou had them in the 1600s. - [Lex] Yeah. - And Galileo comes up for trial. - [Lex] Yep. - Right? And you ask the LLM\\nlike, his Galileo, right? - [Lex] Yeah. - Like, what does it answer? Right? And one theory is he had\\nanswers no that he\\'s wrong because the overwhelming\\nmajority of human thought up until that point was that he was wrong. And so therefore that\\'s\\nwhat\\'s in the training data. Another way of thinking about it is, well, it\\'s efficiently\\nadvanced LLM will have evolved the ability to actually\\ncheck the math. Right. And will actually say, actually\\nno, actually, you know, you may not wanna hear it, but he\\'s right. Now if, you know, the\\nchurch at that time was, you know, owned the LLM, they would\\'ve given it human you know, human feedback to prohibit it\\nfrom answering that question. Right. And so I like to take it out of our current context \\'cause that like makes it very clear, those same questions apply today. Right. This is exactly the point of a huge amount of the human feedback training that\\'s actually happening\\nwith these LLMs today. This is a huge like debate\\nthat\\'s happening about whether open source, you know, AI should be legal. - Well, the actual mechanism\\nof doing the human RL with human feedback is seems like such a fundamental and\\nfascinating question. How do you select the humans? - [Marc] Exactly. - Yeah. How do you select the humans? - AI alignment, right? Which everybody like is\\nlike, oh, that sounds great. Alignment with what? Human values. Who has human values? - [Lex] Who has human values? - Right? And so we\\'re in this mode of like social and popular discourse. We\\'re like, you know, there\\'s,\\nyou know, you see this, what do you think of when you read a story in the press right now? And they say, you know, X,\\nY, Z made a baseless claim about some topic, right? And there\\'s one group of people\\nwho are like, aha, think, you know, they\\'re doing fact checking. There\\'s another group\\nof people that are like, every time the press\\nsays that it\\'s now a tick and that means that they\\'re lying, right? Like, so, like, we\\'re\\nin this social context where there\\'s the level\\nto which a lot of people in positions of power have become very, very certain that they\\'re\\nin a position to determine the truth for the entire\\npopulation is like, there\\'s like some bubble that\\nhas formed around that idea. And at least, like I say, it\\'s flies completely in\\nthe face of everything I was ever trained about science and about reason and strikes me as like, you know, deeply offensive and incorrect. - What would you say about\\nthe state of journalism just on that topic today? Are we in a temporary kind of, are we experiencing a temporary problem in terms of the incentives in\\nterms of the business model, all that kind of stuff? Or is this like a decline\\nof traditional journalism as we know it? - You have, I always think\\nabout the counterfactual in these things, which is like, okay, because these questions, right, this question heads\\ntowards, it\\'s like, okay, the impact of social media\\nand the undermining of truth and all this. But then you wanna ask the\\nquestion of like, okay, what if we had had the\\nmodern media environment, including cable news and\\nincluding social media and Twitter and everything\\nelse in 1939 or 1941, right? Or 1910 or 1865 or 1850 or 1776, right? And like, I think. - You just introduced like\\nfive thought experiments at once and broke my head, but yes, yes. There\\'s a lot of interesting years. - Well like, can I just\\ntake a simple example? Like, how would President\\nKennedy have been interpreted with what we know now about all the things Kennedy was up to? Like how would he have been\\nexperienced by the body of politic in a, with a\\nsocial media context, right? Like how would LBJ have been experienced? But by the way, how would\\nyou know, like many men, FDR, like the new deal, the Great Depression. - I wonder where Twitter\\nwould this would think about Churchill and Hitler and Stalin. - You know, I mean look to\\nthis day there, you know, there are lots of very\\ninteresting real questions around like how America, you know, got, you know, basically\\ninvolved in World War II and who did what when, and the operations of British intelligence and American soil and did FDR, this that Pearl Harbor, you know? - [Lex] Yeah. - Woodrow Wilson ran for, you know, his candidacy was run on an anti-war. You know, he ran on the platform\\nand not getting involved World War-I somehow that\\nswitched, you know, like, and I\\'m not even making a value judgment on any of these things. I\\'m just saying like the way that our ancestors\\nexperienced reality was of course mediated through\\ncentralized, top-down, right. Control at that point. If you ran those realities\\nagain with the media environment we have today, the reality would be experienced\\nvery, very differently. And then of course that that\\nintermediation would cause the feedback loops to change. And then reality would obviously play out. - Do you think it\\'d be very different? - Yeah, it has to be. It has to be. It has to be just \\'cause it\\'s all, so, I mean just look at\\nwhat\\'s happening today. I mean just the most obvious thing is just the collapse. And here\\'s another opportunity to argue that this is not the internet\\ncausing this by the way. Here\\'s a big thing happening today, which is Gallup does this\\nthing every year where they do, they pull for trust in\\ninstitutions in America and they do it across all the, everything from the military\\nto clergy and big business and the media and so forth, right? And basically there\\'s been\\na systemic collapse in trust in institutions in the US\\nalmost without exception, basically since essentially\\nthe early 1970s. There\\'s two ways of looking\\nat that, which is, oh my God, we\\'ve lost this old world\\nin which we could trust institutions and that was so much better \\'cause like that should\\nbe the way the world runs. The other way of looking\\nat it is we just know a lot more now and the great mystery is why those numbers aren\\'t all zero. - [Lex] Yeah. - Right? Because like now we know so much about how these things operate and like they\\'re not that impressive. - And also why do we don\\'t\\nhave better institutions and better leaders then? - Yeah. And so this goes\\nto the thing which is like, okay, we had the media environment of that we\\'ve had between\\nthe 1970s and today. If we had that in the thirties\\nand forties or 1900s, 1910s, I think there\\'s no question\\nreality would turned out different if only because\\neverybody would\\'ve known to not trust the institutions, which would have changed\\ntheir level of credibility, their ability to control circumstances, therefore the circumstances\\nwould\\'ve had to change. Right? And it would\\'ve\\nbeen a feedback loop. It would\\'ve been a feedback loop process in other words, right? It\\'s your experience of\\nreality changes reality and then reality changes your\\nexperience of reality, right? It\\'s a two-way feedback\\nprocess and media is the intermediating force between that. So change the media\\nenvironment, change reality. - [Lex] Yeah. - And so it\\'s just, so, as a consequence, I think it\\'s just really hard to say, oh, things worked a certain way then and they work a different way now. And then therefore, like\\npeople were smarter than, or better than, or you know, by the way, dumber than or not as capable than, right? We make all these like really light and casual like comparisons\\nof ourselves to, you know, previous generations of people. You know, we draw judgements all the time and I just think it\\'s\\nlike really hard to do any of that \\'cause if we\\nput ourselves in their shoes with the media that they had at that time, like I think we probably\\nmost likely would\\'ve been just like them. - So don\\'t you think that our perception and understanding of reality, would you be more and more mediated through large language models now? So you said media before, isn\\'t the LLM going to be the new, what is it, mainstream media, MSM? It\\'ll be LLM. That would be the source of, I\\'m sure there\\'s a way to\\nkind of rapidly fine tune, like making LLMs real time. I\\'m sure there\\'s probably\\na research problem that you can do just rapid\\nfine tuning to the new events. So something like this. - Well even just the whole\\nconcept of the chat UI might not be like the chat UI is just\\nthe first whack at this. And maybe that\\'s the dominant thing. But look maybe our,\\nmaybe we don\\'t know yet. Like maybe the experience\\nmost people with LLMs is just a continuous feed you know, maybe it\\'s more of a passive\\nfeed and you just are getting a constant like running commentary on everything happening in your life and it\\'s just helping\\nyou kind of interpret and understand everything. - Also really more deeply\\nintegrated into your life. Not just like, oh, like\\nintellectual philosophical thoughts, but like literally like\\nhow to make a coffee, where to go for lunch. Just whether, you know,\\ndating all this kind of stuff. - What to say in a job interview. - Yeah. What to say. - [Marc] Yeah, exactly. - What to say. Next sentence. - Yeah, next sentence. Yeah. At that level. Yeah. I mean, yes. So technically now whether we want that or not is an open question, right? And whether we use. - It Q4, a popup right now the\\nestimated engagement using is decreasing for Marc Andreessen, since there\\'s this controversy\\nsection for his Wikipedia page in 1993, something\\nhappened or something like this. Bring it up that will\\ndrive engagement up anyway. - Yeah. That\\'s right. I mean, look, this gets this whole thing\\nof like, so, you know, the chat interface has this whole concept of\\nprompt engineering, right? - [Lex] Yes, yes. - Prompts. Well it turns\\nout one of the things that LLMs are really good at\\nis writing prompts, right? - [Lex] Yeah. - And so like, what if you just outsourced and by the way, you could\\nrun this experiment today, you could hook this up to do this today. The latency\\'s not good\\nenough to do it real time in a conversation. But you could run this experiment\\nand you just say, look, every 20 seconds you\\ncould just say, you know, tell me what the optimal\\nprompt is and then ask yourself that question to gimme the result. And then exactly to your point, as you add, there will be these systems that are gonna have\\nthe ability to be alert and updated essentially in real time. And so you\\'ll be able to\\nhave a pendant or your phone or whatever, watch or whatever\\nit\\'ll have a microphone on. It\\'ll listen to your conversations, it\\'ll have a feed of everything\\nelse happen in the world, and then it\\'ll be you\\nknow, sort of retraining, prompting or retraining itself on the fly. And so the scenario you\\ndescribed is actually a completely doable scenario. Now the hard question\\non this is always okay, since that\\'s possible, are\\npeople gonna want that? Like what\\'s the form of experience? You know, that we won\\'t\\nknow until we try it. But I don\\'t think it\\'s\\npossible yet to predict the form of AI in our lives. Therefore, it\\'s not possible to predict the way in which it will intermediate our experience with reality yet. - Yeah. But it feels like\\nthere\\'s going to be a killer app. There\\'s probably a mad scramble right now. And so it\\'ll open AI and\\nMicrosoft and Google and Meta and in startups and smaller\\ncompanies figuring out what is the killer app\\nbecause it feels like it\\'s possible like a\\nChatGPT type of thing. It\\'s possible to build that, but that\\'s 10X more compelling\\nusing already the LLMs we have using even the open source LLMs and the different variants. So you\\'re investing in a lot of companies and you\\'re paying attention, who do you think is gonna win this? Do you think there\\'ll be, who\\'s gonna be the next\\npage rank inventor? - Trillion dollar question. - Another one. We have\\na few of those today. - There\\'s a', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 532341, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content=\" always okay, since that's possible, are\\npeople gonna want that? Like what's the form of experience? You know, that we won't\\nknow until we try it. But I don't think it's\\npossible yet to predict the form of AI in our lives. Therefore, it's not possible to predict the way in which it will intermediate our experience with reality yet. - Yeah. But it feels like\\nthere's going to be a killer app. There's probably a mad scramble right now. And so it'll open AI and\\nMicrosoft and Google and Meta and in startups and smaller\\ncompanies figuring out what is the killer app\\nbecause it feels like it's possible like a\\nChatGPT type of thing. It's possible to build that, but that's 10X more compelling\\nusing already the LLMs we have using even the open source LLMs and the different variants. So you're investing in a lot of companies and you're paying attention, who do you think is gonna win this? Do you think there'll be, who's gonna be the next\\npage rank inventor? - Trillion dollar question. - Another one. We have\\na few of those today. - There's a bunch of those. So look, there's a really\\nbig question today. Sitting here today is\\na really big question about the big models\\nversus the small models that's related directly\\nto the big question of proprietary versus open. Then there's this big\\nquestion question of you know, where is the training data gonna, like, are we topping out of\\nthe training data or not? And then are we gonna be able\\nto synthesize training data? And then there's a huge pile\\nof questions around regulation and you know, what's\\nactually gonna be legal. And so I would, when we think about it, we dovetail kind of all\\nthose questions together. You can paint a picture of\\nthe world where there's two or three God models that are just at like staggering scale and they're just better at everything. And they will be owned by\\na small set of companies and they will basically\\nachieve regulatory capture over the government and they'll\\nhave competitive barriers that will prevent other people from, you know, competing with them. And so, you know, there will be, you know, just like there's like,\\nyou know, whatever, three big banks or three\\nbig, you know, or by the way, three big search companies\\nor I guess two now, you know, it'll centralize like that. You can paint another very different\\npicture that says, no, actually the opposite\\nof that's gonna happen. This is gonna basically that\\nthis is the new gold, you know, this is the new gold rush alchemy. Like you know, this is the big bang\\nfor this whole new area of science and technology. And so therefore you're gonna\\nhave every smart 14-year-old on the planet building open source, right? You know, and figuring out a\\nways to optimize these things. And then, you know, we're just gonna get like\\noverwhelmingly better at generating trading data. We're gonna, you know, bring in like blockchain\\nnetworks to have like an economic incentive to\\ngenerate decentralized training data and so forth and so on. And then basically we're\\ngonna live in a world of open source and there's\\ngonna be a billion LLMs, right? Of every size, scale,\\nshape and description. And there might be a few big ones that are like the super genius ones, but like mostly what we'll\\nexperience is open source and that's, you know,\\nthat's more like a world of like what we have today\\nwith like Linux and the web. - Okay, but you painted these two worlds. But there's also\\nvariations of those worlds, 'cause you said regulatory\\ncapture is possible to have these tech giants that don't\\nhave regulatory capture, which is something you're also\\ncalling for saying it's okay to have big companies\\nworking on this stuff as long as they don't\\nachieve regulatory capture. But I have the sense that\\nthere's just going to be a new startup that's going to basically be the page rank inventor, which has become the new tech giant. I don't know, I would love to hear your kind\\nof opinion if Google, Meta and Microsoft are as\\ngigantic companies able to pivot so hard to create new products. Like some of it is just\\neven hiring people or having a corporate structure that\\nallows for the crazy young kids to come in and just create\\nsomething totally new. Do you think it's possible or do you think it'll come from a startup? - Yeah, it is this always\\nbig question, which is, you get this feeling, I hear about this a lot\\nfrom CEOs, founder CEOs where it's like, wow,\\nwe have 50,000 people, it's now harder to do new things than it was when we had 50 people. - [Lex] Yeah. - Like, what has happened? So that's a recurring phenomenon. By the way, that's one of the reasons\\nwhy there's always startups and why there's venture capital. That's like a timeless kind of thing. So that's one observation. On page rank, we can talk about that. But on page rank,\\nspecifically on page rank, there actually is a page. So there is a page rank\\nalready in the field and it's the transformer, right? So the big breakthrough\\nwas the transformer. And the transformer was\\ninvented in 2017 at Google. And this is actually like\\nreally an interesting question 'cause it's like, okay, the transformers like why\\ndoes open AI even exist? Like the Transformers invested at Google. Why didn't Google? I asked a guy I know who\\nwas senior at Google brain kind of when this was happening. And I said, if Google had\\njust gone flat out to the wall and just said, look, we're gonna launch, we're gonna launch the equivalent of GPT4 as fast as we can. I said, when could we have had it? And he said, 2019. They could have just\\ndone a two year sprint with the Transformer and\\nbecause they already had the compute at scale. They already had all the training data, they could have just done it. There's a variety of\\nreasons they didn't do it. This is like a classic big company thing. IBM invented the relational\\ndatabase in the 1970s, let it sit on the shelf as a paper. Larry Ellison picked\\nit up and built Oracle. Xerox Park invented the\\ninteractive computer. They let it sit on the shelf. Steve Jobs came and turned\\nit into the Macintosh, right? And so there is this pattern.\\nNow having said that, sitting here today, like\\nGoogle's in the game, right? So Google, you know, they maybe they let like a\\nfour year gap there go there that they maybe shouldn't have, but like they're in the\\ngame and so now they've got, you know, now they're committed. They've done this merger,\\nthey're bringing in demos, they've got this merger with DeepMind. You know, they're piling in resources. There are rumors that they're, you know, building up an incredible,\\nyou know, super LLM you know, way beyond what we even have today. And they've got, you\\nknow, unlimited resources and a huge, you know, they've\\nbeen challenged their honor. - Yeah. I had a chance to\\nhang out with (indistinct) a couple days ago and we took this walk and there's this giant new building where there's going to be\\na lot of AI work being done and it's kind of this\\nominous feeling of like the fight is on. - [Marc] Yeah. - Like there's this beautiful\\nSilicon Valley nature, like birds are chirping\\nand this giant building and it's like the beast has been awakened. - [Marc] Yeah. - And then like all the big\\ncompanies are waking up to this. They have the compute, but\\nalso the little guys have, it feels like they have\\nall the tools to create the killer product that, and then there's also tools to scale if you have a good idea, if\\nyou have the page rank idea. So there's several things\\nthat it's page rank, there's page rank, the algorithm and the\\nidea and there's like the implementation of it. And I feel like killer\\nproduct is not just the idea, like the transform, it's the implementation something really compelling about it. Like you just can't\\nlook away something like the algorithm behind TikTok\\nversus TikTok itself, like the actual experience\\nof TikTok that just, you can't look away. It feels like somebody's\\ngonna come up with that. And it could be Google, but it feels like it's\\njust easier and faster to do for a startup. - Yeah. So, the startup, the huge advantage that\\nstartups have is they just, there's no sacred cows. There's no historical legacy to protect, there's no need to reconcile your new plan with the existing strategy. There's no communication overhead. There's no, you know, big\\ncompanies are big companies. They've got pre-meetings\\nplanning for the meeting, then they have the post\\nmeeting, the recap, then they have the\\npresentation of the board, then they have the next\\nrounds of meetings. And that's the-- - [Lex] Lots of meetings. - That's the elapsed time when the startup launches\\nits product. Right? So, there's a timeless, right? - [Lex] Yeah. - So there's a timeless thing there now. What the startups don't have\\nis everything else, right? So startups, they don't have a brand, they don't have customer relationships. They've gotten no distribution, they've got no, you know, scale. I mean sitting here today,\\nthey can't even get GPUs. Right. Like there's like a GPU shortage. Startups are literally\\nstalled out right now 'cause they can't get chips,\\nwhich is like super weird. - [Lex] Yeah. They got the cloud. - Yeah. But the clouds\\nrun out of chips. Right. And then to the extent\\nthe clouds have chips, they allocate them to the big customers. Not the small customers. Right. And so the small companies\\nlack everything other than the ability to just\\ndo something new. Right. And this is the timeless race and battle. And this is kinda the point\\nI tried to make in the essay, which is like, both\\nsides of this are good. Like, it's really good to have like highly-scaled tech companies that can do things that are like at staggering\\nlevels of sophistication. It's really good to have\\nstartups that can launch brand-new ideas. They ought to be able to\\nboth do that and compete. They, neither one ought to be subsidized or protected from the others. Like that's, to me, that's just like very\\nclearly the idealized world. It is the world we've been\\nin for AI up until now. And then of course there are people trying to shut that down. But my hope is that, you know, the best outcome clearly\\nwill be if that continues. - We'll talk about that a little bit, but I'd love to linger on some of the ways this is\\ngoing to change the internet. So I don't know if you remember, but there's a thing called\\nMosaic and there's a thing called Netscape Navigator. So you were there in the beginning. What about the interface to the internet? How do you think the browser changes and who gets to own the browser? We got to see some very\\ninteresting browsers, Firefox, I mean all the\\nvariants of Microsoft, Internet Explorer, Edge,\\nand now Chrome, the actual, and he seems like a dumb question to ask, but do you think we'll\\nstill have the web browser? - So I have an eight-year-old\\nand he's super into, he's like Minecraft and learning to code and doing all this stuff. So, of course I was\\nvery proud I could bring sort of fire down from\\nthe mountain to my kid and I brought him ChatGPT\\nand I hooked him up on his laptop. And I was like, you know, this is the thing that's gonna\\nanswer all your questions. And he's like, okay. And I'm like, but it's gonna\\nanswer all your questions. And he's like, well of\\ncourse, like it's a computer. Of course it answers all your questions. Like, what else would a\\ncomputer be good for, dad? - [Lex] And never impressed, are they? - Not impressed in the least. Two weeks passed. And he has some question and I say, well, have you asked ChatGPT? And he's like, dad, Bing is better. - [Lex] Ooh. - And why is Bing better? Is because it's built into the browser. 'Cause he's like, look, I have the Microsoft Edge browser and like it's got Bing right here. And then he doesn't know this yet, but one of the things you\\ncan do with Bing and Edge is there's a setting where you\\ncan use it to basically talk to any webpage because\\nit's sitting right there next to the browser. And by the way, which\\nincludes PDF documents. And so you can, in the way\\nthey've implemented an Edge with Bing is you can load a PDF and then you can ask it questions, which is the thing you can't\\ndo currently in just ChatGPT. So they're, you know, they're gonna, they're\\ngonna push the meld. I think that's great. You know, they're gonna push the melding and see if there's a\\ncombination thing there. Google's rolling out this thing, the magic button, which is\\nimplemented in, you know, they put it in Google Docs, right? And so you go at a, you know, Google Docs and you create\\na new document and you know, you instead of like, you\\nknow, starting to type, you just, you know, say it, press the button and it starts to like, generate content for you, right? Like, is that the way that it'll work? Is it gonna be a speech UI\\nwhere you're just gonna have an earpiece and talk to it all day long? You know, is it gonna be a,\\nlike these are all, like, this is exactly the kind\\nof thing that I don't, this is exactly the kind of thing I don't think is possible to forecast. I think what we need to do is\\nlike run all those experiments and so one outcome is we\\ncome out of this with like a super browser that has AI built in that's just like amazing. There's a real possibility that the whole, I mean, look, there's a possibility here that the whole idea of\\na screen and windows and all this stuff just goes away 'cause like, why do you need that if\\nyou just have a thing that's just telling you\\nwhatever you need to know? - Well and also, so there's\\napps that you can use, you don't really use them. You know, being a Linux\\nguy and Windows guy, there's one window, the browser that with\\nwhich you can interact with the internet, but on the\\nphone you can also have apps. So I can interact with\\nTwitter through the app or through the web browser. And that seems like an\\nobvious distinction, but why have the web browser in that case, if one of the apps starts\\nbecoming the everything app. - [Marc] Yeah, that's right. - What is Elon trying to do with Twitter? But there could be others.\\nThere could be like a big app, there could be a Google app\\nthat just doesn't really do search, but just like, do what I guess AOL did back\\nin the day or something where it's all right there and\\nit changes the nature of the internet because\\nwhere the content is hosted, who owns the data? Who owns the content? What is the kind of content you create? How do you make money by creating content? Who are the content creators? All of that. Or it could just keep being the same, which is like with just the\\nnature of webpage changes and the nature of content. But there'll still be a web browser. 'Cause web browser's\\na pretty sexy product. It just seems to work. 'Cause it like you have an interface, a window into the world, and then the world can\\nbe anything you want. And as the world will evolve, it could be different\\nprogramming languages, it can be animated, maybe it's\\nthree dimensional and so on. Yeah, it's interesting. Do you think we'll still\\nhave the web browser? - Well, very medium becomes\\nthe content for the next one. - [Lex] Oh boy. - You know, the AI will be able to give you a browser whenever you want. - [Lex] Oh, interesting. Generate. - Well, another way to\\nthink about it is maybe what the browser is maybe it's just the escape hatch, right? Which is maybe kind of\\nwhat it is today, right? Which is like most of what\\nyou do is like inside a social network or inside a search\\nengine or inside, you know, somebody's app or inside some\\ncontrolled experience, right? But then every once in a\\nwhile there's something where you actually want to jailbreak, you wanna actually get free. - Web browser's the FU to the man. You're allowed to. That's the free internet. - [Marc] Yeah. - Back the way it was in the nineties. - So here's something I'm proud of. So nobody really talks about it. Here's something I'm proud\\nof, which is the web, the browser, the web servers, they're all, they're still backward compatible all the way back to like 1992, right? So like, you can put up a,\\nyou can still, you know what, the big breakthrough of\\nthe web early on the big breakthrough was it made\\nit really easy to read, but it also made it really easy to write, made it really easy to publish. And we literally made\\nit so easy to publish. We made it not only so it\\nwas easy to publish content, it was actually also easy to\\nactually write a web server. - [Lex] Yeah. - Right and you could\\nliterally write a web server in four lines of brol code and you could start\\npublishing content on it, and you could set whatever\\nrules you want for the content, whatever censorship, no\\ncensorship, whatever you want. You could just do that. And as long as you had\\nan IP address, right, you could do that. That still works, right? That like, still works\\nexactly as I just described. So this is part of my\\nreaction to all of this. Like, you know, all this\\njust censorship pressure and all this, you know, these issues around\\ncontrol and all this stuff, which is like, maybe we need to get\\nback a little bit more to the wild west. Like, the wild west is still out there. Now they will try to chase you down. Like they'll try to, you know, people who want a censor will\\ntry to take away you know, your domain name and\\nthey'll try to take away your payments account and so forth if they really don't\\nlike what you're saying. But nevertheless, you like, unless they literally are intercepting you at the ISP level, like you\\ncan still put up a thing. And so I don't know, I think that's important\\nto preserve, right? Like because I mean one is\\njust a freedom argument, but the other's a creativity argument, which is you wanna have the escape hatch so that the kid with the idea\\nis able to realize the idea. 'cause to your point on page rank, you actually don't know what\\nthe next big idea is, right? No, nobody called Larry Page and told him to develop page rank. Like he came up with that on his own. And you wanna always, I think leave the escape\\nhatch for the next, you know, kid or the next Stanford\\ngrad student to have the breakthrough idea and be\\nable to get it up and running before anybody notices. - You and I are both hands of history. So let's step back. We've been talking about the future. Let's step back for a bit\\nand look at the nineties. You created Mosaic web browser, the first widely used web browser. Tell the story of that. And how did it evolve\\ninto Netscape Navigator this the early days? - So full story. So. - [Lex] We were born, - I was born. A small child. - Actually. Yeah, let's go there. Like, when would you first\\nfall in love with computers? - Oh, so I hit the generational jackpot and I hit the Gen X kind of\\npoint perfectly as it turns out. So I was born in 1971. So there's this great website called WTF happened in 1971 dot com,\\nwhich is basically in 1971. It's when everything\\nstarted to go to hell. And I was of course born in 1971. So I like to think that I had\\nsomething to do with that. - Did you make it on the website? - I don't think I made it on the website, but you know, hopefully,\\nsomebody needs to add. - This is where everything. - Maybe I contributed to some\\nof the trends that they do. Every line on that website\\ngoes like that, right? So it's all a picture disaster. But there was this moment in time where 'cause you know, sort\\nof the Apple, you know, the Apple II hit in like 1978\\nand then the IBM PC hit in 82. So I was like, you know,\\n11 when the PC came out. And so I just kind of hit that\\nperfectly and then that was the first moment in time when like, regular people could spend\\na few hundred dollars and get a computer, right? And so that, I just like that resonated right out of the gate. And then the other part\\nof the story is, you know, I was using Apple II,\\nI used a bunch of them, but I was using Apple II and\\nof course it said in the back of every Apple II and every\\nMac it said, you know, designed in Cupertino, California. And I was like, wow, okay. Cupertino must be the like,\\nshining city on the hill. Like Wizard of Oz is\\nlike the most amazing, like city of all time.\\nI can't wait to see it. And of course, years later I came out to Silicon Valley and went to Cupertino and it's just a bunch of office parks at low-rise apartment buildings. So the aesthetics were a\\nlittle disappointing, but, you know, it was the vector\\nright of the creation of a lot of this stuff. So then basically by, so part part, part of my story is just\\nthe luck of having been born at the right time and\\ngetting exposed to PCs. Then the other part is, the other part is when El\\nGore says that he created the internet, he actually is correct in a really meaningful way, which is he sponsored a bill\\nin 1985 that essentially created the modern internet, created what is called\\nthe NSF net at the time, which is sort of the first\\nreally fast internet backbone. And you know, that that bill dumped a ton of money into a bunch of research universities to build out basically\\nthe internet backbone and then the supercomputer\\ncenters that were clustered around the internet. And one of those universities\\nwas University of Illinois where I went to school. And so the other stroke\\nlock that I had was, I went to Illinois basically\\nright as that money was just like getting dumped on campus. And so as a consequence\\nwe had at, on campus, and this was like, you know,\\n89, 90, 91, we had like, you know, we were right\\non the internet backbone. We had like T3 and 45 at the time, T3 45 megabit backbone\\nconnection, which at the time was, you know, wildly state of the art. We had cray super computers. We had thinking machines\\nparallel super computers. We had silicon graphics\\nworkstations, we had Macintosh's, we had next cubes all over the place. We had like every\\npossible kind of computer you could imagine 'cause all this money\\njust fell out of the sky. - [Lex] So you were living in the future. - Yeah. So yeah, quite\\nliterally it was, yeah, like it's all there. It's all like we had\\nfull broadband graphics, like the whole thing. And it's actually funny 'cause they had this is the first time I kind of, it sort of tickled the back\\nof my head that there might be a big opportunity in\\nhere, which is, you know, they embraced it and so\\nthey put like computers in all the dorms and they\\nwired up all the dorm rooms and they had all these, you know, labs everywhere and everything. And then they gave every undergrad a computer account and an email address. And the assumption was that\\nyou would use the internet for four years at college and then you would\\ngraduate and stop using it. And that was that, right? - [Lex] Yeah. - And you would just\\nretire your email address. It wouldn't be relevant\\nanymore 'cause you'd go off from the workplace and\\nthey don't use email. You'd be back to using\\nfax machines or whatever. - Did you have that sense as well? Like, what you said the back\\nof your head was tickled. Like, what was exciting to\\nyou about this possible world? - Well, if this is so\\nuseful in this containment, if this is so useful in\\nthis contain environment that just has this weird\\nsource of outside funding, then if it were practical\\nfor everybody else to have this and if it were cost effective for everybody else to have this, wouldn't they want it? And the overwhelmingly the prevailing view at the time was no,\\nthey would not want it. This is esoteric, weird nerd stuff, right? That like computer science kids like, but like normal people are\\nnever gonna do email. Right. Or be on the internet, right? And so I was just like,\\nwow, like this is actually, like, this is really compelling stuff. Now the other part was, it was all really hard to use\\nand in practice you had to be basically a CS you know, basically had had to BA\\nCS undergrad or equivalent to actually get full use of\\nthe internet at that point. 'cause it was all pretty esoteric stuff. So then that was the other\\npart of the idea, which was, okay, we need to actually\\nmake this easy to use. - So what's involved in creating Mosaic? Like, in creating graphical\\ninterface to the internet? - Yeah, so it was a combination of things. So it was like basically the web existed in an early sort of\\ndescribed as prototype form. And by the way, text only at that point. - What did it look like? What was the web? I mean what and the key figures. Like, what was it? Like, what paint a picture? - It looked like ChatGPT\\nactually it was all text. - Yeah. - And so you had a text-based web browser? Yeah, well actually the original browser, Tim Burners Lee, the original browser, both the original browser\\nand the server actually ran on next cubes. So these were, this was, you know, the computer Steve Jobs made\\nduring the interim period when during the decade long interim period when he was not at Apple, you know, he got fired in 85 and\\nthen came back in 97. So this was in that interim\\nperiod where he had this company called Next and they made these, literally these computers called cubes. And there's this famous\\nstory, they were beautiful, but they were 12 inch by 12\\ninch by 12 inch cubes computers. And there's a famous story\\nabout how they could have cost half as much if it had\\nbeen 12 by 12 by 13. But this cube was like,\\nno, like it has to be. So they were like $6,000\\nbasically academic workstations. They had the first city round\\ndrives, which were slow. I mean it was, the computers\\nwere all but unusable. They were so slow, but\\nthey were beautiful. - Okay, can we actually just\\ntake a tiny tangent there? - Sure. Of course. - The 12 by 12 by 12 that just\\nso beautifully encapsulates Steve Jobs idea of design. Can you just comment on what you find interesting about Steve Jobs? What about that view of the world, that dogmatic pursuit of perfection and how he saw perfection in design? - Yeah, so I guess I'd say like, look, he was a deep believer, I think in a very deep,\\nthe way I interpret it, I don't know if you ever\\nreally described it like this, but the way I interpret\\nit's like this thing and it's actually a thing in philosophy. It's like aesthetics are\\nnot just appearances. Aesthetics go all the way to like deep underlying meaning, right? It's like I'm not a physicist. One of the things I've\\nheard physicists say is one of the things you start to get a sense of when a theory might be correct is when it's beautiful, right? Like, you know, there, right? And so, there's something, and you feel the same thing by the way in like human psychology, right? You know, when you're\\nexperiencing awe, right? You know, there's like a simplicity to it. When you're having an honest\\ninteraction with somebody, there's an aesthetic, I would say calm comes over you 'cause you're actually being fully honest and trying to hide yourself, right? So it's like this very\\ndeep sense of aesthetics. - And he would trust that\\njudgment that he had deep down. Like yeah, even if the\\nengineering teams are saying this is too difficult. Even if whatever the\\nfinance folks are saying, this is ridiculous. The supply chain, all that\\nkind of stuff just makes this impossible. We can't do this kind of material. This has never been done\\nbefore and so on and so forth. He just sticks by it. - Well, I mean, who makes a\\nphone out of aluminum, right? Like, hadn't nobody\\nelse would've done that. And now of course if your phone is made out of aluminum white,\\nyou know, how crude, what a kind of caveman would\\nyou have to be to have a phone that's made outta plastic? Like, right. So like, so it's just this very right. And, you know, look, there's a thousand different\\nways to look at this, but one of the things is just like, look, these things are\\ncentral to your life. Like, you're with your phone more than you're with anything else. Like, it's gonna be in your hand. I mean, you know this, he thought very deeply about\\nwhat it meant for something to be in your hand all day long. But for example, here's an\\ninteresting design thing. Like, he never wanted, my understanding is he never\\nwanted an iPhone to have a screen larger than you could reach with your thumb one handed. And so he was actually opposed to the idea of making the phones larger. And I don't know if you\\nhave this experience today, but let's say there are\\ncertain moments in your day when you might be like, only have one hand available\\nand you might wanna be on your phone. And you're trying to like, send a text and your thumb can't\\nreach the send button. - Yeah. I mean there's\\npros and cons, right? And then there's like folding phones, which I would love to know what he thought thinks about them. But I mean, is there something you\\ncould also just linger on? 'cause he's one of the interesting figures in the history of technology. What makes him as successful as he was? What makes him as interesting as he was? What made him so productive and important in the development of technology? - He had an integrated worldview. So the properly designed device that had the correct functionality, that had the deepest\\nunderstanding of the user, that was the most beautiful, right? Like, it had to be all\\nof those things, right? He basically would drive\\nto as close to perfect as you could possibly get. Right? And you know, I suspect that\\nhe never quite, you know, thought he ever got there.\\n'cause most great creators, you know, are generally dissatisfied. You know, you read accounts\\nlater on and all they can, all they can see are the\\nflaws in their creation. But like he got as close to\\nperfect each step of the way as he could possibly\\nget with the constraints of the technology of his time. And then, you know,\\nlook, he was, you know, sort of famous in the Apple model. It's like, look, they will, you know, this headset that they just came out with, like, you know, it's like a\\ndecade long project, right? It's like, and they're just gonna sit\\nthere and tune and tune and polish and polish and tune and polish and tune and polish until it is as perfect as anybody could possibly make anything. - Yeah. - And then this goes to the way that people describe working\\nwith him was, which is, you know, there was a terrifying\\naspect of working with him, which is, you know, he was,\\nyou know, he was very tough. But there was this thing that\\neverybody I've ever talked to worked for him, says that they all say the following, which is we did the best work of our lives when we worked for him because he set the bar incredibly high. And then he supported us\\nwith everything that he could to let us actually do\\nwork of that quality. So a lot of people who were at Apple spend the rest of their lives trying\\nto find another experience where they feel like they're able to hit\\nthat quality bar again. - Even if it in retrospect or\\nduring it felt like suffering. - Yeah, exactly. - What does that teach you\\nabout the human condition? Huh? - So look, so say exactly. So the Silicon Valley, I mean,\\nlook, he's not, you know, George Patton you know in the Army. Like, you know, there are\\nmany examples in other fields, you know, that are like\\nthis specifically in tech. It's actually, I find it very interesting. There's the Apple way, which\\nis polish, polish, polish, and don't ship until it's as\\nperfect as you can make it. And then there's the sort\\nof the other approach, which is the sort of\\nincremental hacker mentality, which basically says, ship\\nearly and often and iterate. And one of the things I\\nfind really interesting is I'm now 30 years into this, like, they're very successful\\ncompanies on both sides of that approach, right? Like, that is a fundamental\\ndifference, right? In how to operate and how to\\nbuild and how to create that. You have world class companies\\noperating in both ways. And I don't think the question of like, which is the superior\\nmodel is anywhere close to being answered. Like, and my suspicion\\nis the answer is do both. The answer is you actually want both. They lead to different outcomes. Software tends to do better\\nwith the iterative approach. Hardware tends to do\\nbetter with the, you know, sort of wait and make it perfect approach. But again, you can find\\nexamples in both directions. - So the jury's still out on that one. So back to Mosaic. So, what it was text based Tim Burns Lee? - Well, there was the\\nweb, which was text based, but there were no, I mean\\nthere was like three websites. There was like no content,\\nthere were no users. Like, it wasn't like a catalytic, it hadn't, and by the way, it was all because it was all text. There were no documents, there were no images,\\nthere were no videos, there were no, right. So, and then if, if in the beginning, if you had to be on a next cube, right? You need to had a next cube\\nboth to publish and to consume. - So, there was 6,000 bucks you said. - There were limitations. Yeah. $6,000 PC. They\\ndid not sell very many. But then there was also, there was also FTP and\\nthere was Use Nets, right? And there was, you know, a dozen other basically there's waste, which was an early search thing. There was Gopher, which\\nwas an early menu based information retrieval system. There were like a dozen\\ndifferent sort of scattered ways that people would get to\\ninformation on the internet. And so the Mosaic idea was basically bring those all together, make the whole thing\\ngraphical, make it easy to use, make it basically bulletproof\\nso that anybody can do it. And then again, just on the luck side, it so happened that this was right at the moment when graphics, when the GUI sort of actually took off and we're now also used to the GUI that we think it's been around forever. But it didn't real, you know, the Macintosh brought it out in 85, but they actually didn't\\nsell very many Macs in the eighties. It was not that successful of a product. It really was. You needed Windows 3.0 on\\nPCs and that hit in about 92. And so, and we did most in 92, 93. So that sort of, it was like right at the\\nmoment when you could imagine actually having a graphical\\nuser interface to right at all, much less one to the internet. - How old did Windows 3 sell? So was that the really big. - [Marc] That was the big bang. - The big operating\\ngraphical operating system? - Well this is the classic, okay. This Microsoft was operating on the other, so Steve the Apple was\\nrunning on the Polish until it's perfect. Microsoft famously ran on the other model, which is ship and iterate. And so in the old line in those days was Microsoft Right's version three of every Microsoft product. That's the good one, right? And so there there are, you can find online Windows 1, Windows 2. Nobody used them. Actually the original Windows, in the original Microsoft Windows, the windows were non overlapping. And so you had these very small, very low resolution screens and then you had literally-- - [Lex] Windows. - It just didn't work. It wasn't ready yet. Well. - And Windows 95 I think\\nwas a pretty big leap also. - That was a big leap too. So that was like bang, bang. And then of course Steve, and then when, you know, in the fullness of time Steve came back, then the Mac started, took off again. That was the third bang. And then the iPhone was the fourth bang. - Such exciting time. - And then we were off,\\noff to the races because. - Nobody could have known what\\nwould be created from that. - Well, Windows 3.1 or 3.0, Windows 3.0 to the iPhone\\nwas only 15 years. Right. Like that ramp was in retrospect. At the time it felt like it took forever. But that in histor in historical terms, like that was a very fast ramp from even a graphical computer at all on your desk to the iPhone. That was 15 years. - So, did you have a sense\\nof what the internet will be as you're looking through\\nthe window of Mosaic? Like, what you, like there's\\njust a few web pages for now. - So the thing I had early on\\nwas I was keeping at the time what there's disputes over\\nwhat was the first blog, but I had one of them that\\nat least is a possible, at least a rudder up in the competition. And it was what was called\\nthe What's new page. And it was literally, it was a hardwired in\\ndistribution unfair advantage. I wired, put it right in the browser, I put it in the browser\\nand then I put my resume in the browser, which also was-- - [Lex] Hilarious. - But I was keeping not many people get to get to do that. - No, good call. And early days. It's so interesting. - I'm looking for my, about about, oh, Marc is looking for a job. - [Lex] Yeah, yeah, exactly. - So the West New page, I would literally get up every morning and I would, or every afternoon\\nand I would basically, if you wanted to launch a website, you would email me and I would\\nlist it on the most new page. And that was how people\\ndiscovered the new websites as they were coming out. And I remember 'cause it was like one, it literally went from, it was like one every couple\\ndays to like one every day to like two every day. - And then so you're doing, so that blog was kind of\\ndoing the directory thing. So like, what was the homepage? - So the homepage was just\\nbasically trying to explain even what this thing is that\\nyou're looking at. Right. Basically the basic instructions. But then there was a button, there was a button that said what's new. And what most people did was they went to, for obvious reasons went to what's new. - [Lex] Yeah. - But like it was so mind\\nblowing at that point. This the basic idea and it\\nwas, this was like, you know, this was the basic idea of the internet, but people could see\\nit for the first time. The basic idea was, look,\\nyou know, some, you know, it's like literally it's like\\nan Indian restaurant in like Bristol England has like\\nput their menu on the web. And people were like, wow. - [Lex] Whoa. - Because like that's the first\\nrestaurant menu on the web. - [Lex] Yeah. - And I don't have to be\\nin Bristol and I don't know if I'm ever gonna go to Bristol. And I don't even like Indian\\nfood and like. Wow. Right. And it was like that the first web, the first streaming video thing was it was in another England,\\nsome Oxford or something. Some guy put his coffee pot up\\nas the first streaming video thing and he put it on the\\nweb 'cause he literally, it was the coffee pot down the hall. And he wanted to see when\\nhe needed to go refill it. But there were, you know, there was a point when\\nthere were thousands of people like watching that coffee pot 'cause it was the first\\nthing you could watch. - Well, but isn't were you able\\nto kind of infer, you know, if that Indian restaurant could go online. Then you're like they all will. - [Marc] Yeah, exactly. - So you felt that? - [Marc] Yeah, yeah, yeah. - Okay. - Now, you know, look, it's\\nstill a stretch, right? It's still a stretch 'cause\\nit's just like, okay, is it, you know, you're still in this\\nzone, which is like, okay, is this a nerd thing? Is this a real person thing? By the way, you know, there was a wall of\\nskepticism from the media. Like, they just, like,\\neverybody was just like, yeah, this is the crazy, this is just like dumb. This is not, you know, this is not for regular\\npeople at that time. And so you, you had to think\\nthrough that and then look, it was still hard to get on the internet at that point, right? So you could get kind of this\\nweird bastardized version if you were on AOL,\\nwhich wasn't really real. Or you had to go like,\\nlearn what an ISP was. You know, in those days, PCs actually didn't have TCPIP\\ndrivers come reinstalled. So you had to learn\\nwhat a TCPIP driver was. You had to buy a modem, you\\nhad to install driver software. I have a comedy routine. I do. So it's like 20 minutes long\\ndescribing all the steps required to actually get on\\nthe internet at this point. And so you had to look\\nthrough these practical. Well, and then speed performance 14-4 modems, right? Like it was like watching,\\nyou know, glue dry, like, and so you had to, there were basically a sequence of bets that we made where you basically needed to look through that current\\nstate of affairs and say, actually there's gonna\\nbe so much demand for once people figure this out, there's gonna be so much demand for it that all of these practical problems\\nare gonna get fixed. - Some people say that\\nthe anticipation makes the destination that much more exciting. - Do you remember progressive JPEGs? - Yeah. Do I, do I? - For kids in the audience, right? - [Lex] For kids in the audience. - You used to have to watch an image load like a line at the time. But it turns out there\\nwas this thing with JPEGs where you could load\\nbasically every fourth, you could load like every fourth line and then you could sweep\\nback through again. And so you could like\\nrender a fuzzy version of image up front. And then it would like\\nresolve into the detailed one. And that was like a big UI breakthrough 'cause it gave you something to watch. - Yeah. And you know, there's applications in\\nvarious domains for that. - Well it was a big fight. There was a big fight early on\\nabout whether there should be images in the web. And. - For that reason for\\nlike sexualization or-- - Not explicitly that that did come up. But it wasn't even that, it was more just like all the\\nserious in the argument went, the purists basically said\\nall the serious information in the world\", metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 532341, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content=' say that\\nthe anticipation makes the destination that much more exciting. - Do you remember progressive JPEGs? - Yeah. Do I, do I? - For kids in the audience, right? - [Lex] For kids in the audience. - You used to have to watch an image load like a line at the time. But it turns out there\\nwas this thing with JPEGs where you could load\\nbasically every fourth, you could load like every fourth line and then you could sweep\\nback through again. And so you could like\\nrender a fuzzy version of image up front. And then it would like\\nresolve into the detailed one. And that was like a big UI breakthrough \\'cause it gave you something to watch. - Yeah. And you know, there\\'s applications in\\nvarious domains for that. - Well it was a big fight. There was a big fight early on\\nabout whether there should be images in the web. And. - For that reason for\\nlike sexualization or-- - Not explicitly that that did come up. But it wasn\\'t even that, it was more just like all the\\nserious in the argument went, the purists basically said\\nall the serious information in the world is text. If you introduce images, you basically are gonna bring\\nin all the trivial stuff. You\\'re gonna bring in\\nmagazines and you know, all this crazy just, you know,\\nstuff that, you know, people, you know, it\\'s gonna, it is\\ngonna distract from that. It\\'s gonna go take it away from being\\nserious to being frivolous. - Well, was there any\\n(indistinct) type arguments about the internet destroying all of human\\ncivilization or destroying some fundamental fabric of human civilization? - So it was, those days it was all around crime and terrorism. So those arguments happened, you know, but there was no sense yet\\nof the internet having like, an effect on politics because\\nthat was way too, too far off. But there was an enormous panic at the time around cybercrime. There was like enormous panic\\nthat like your credit card number would get stolen and you\\'d use life\\nsavings would be drained. And then, you know, criminals were gonna, there was, oh, when we started,\\none of the things we did, one of the Netscape browser\\nwas the first widely used piece of consumer software that had\\nstrong encryption built in, it made it available to ordinary people. And at that time, strong encryption was\\nactually illegal to export outta the US so we could feel that product in the US, we could not export it \\'cause it was classified as munition. So the Netscape browser\\nwas on a restricted list along with the tomahawk missile as being something that\\ncould not be exported. So we had to make a second\\nversion with deliberately weak encryption to sell\\noverseas with a big logo on the box saying, do not trust this. Which it turns out, makes it hard to sell software\\nwhen it\\'s got a big logo that says don\\'t trust it. And then we had to spend\\nfive years fighting the US government to get\\nthem to basically stop trying to do this regulation. But because the fear\\nwas terrorists are gonna use encryption, right? To like plot, you know, all these things. And then, you know, we responded with, well actually we need\\nencryption to be able to secure systems so that the terrorists and the criminals can\\'t get into them. So that anyway, that was the 1990s fight. - So can you say something\\nabout some of the details of the software engineering\\nchallenges required to build these browsers? I mean the engineering\\nchallenges of creating a product that hasn\\'t really existed before that can have such\\nalmost like limitless impact on the world with the internet. - So there was a really key\\nbet that we made at the time, which was very controversial, which was core to core\\nto how it was engineered, which was are we\\noptimizing for performance or for ease of creation? And in those days the pressure\\nwas very intense to optimize for performance because the\\nnetwork connections were so slow and also the computers were so slow. And so if you had, I mentioned\\nthe progressive JPEGs, like if there\\'s an alternate\\nworld in which we optimized for performance and it just, you had just a much more pleasant\\nexperience right up front. But what we got by not doing that was we got ease of creation. And the way that we got\\nease of creation was all of the protocols and\\nformats were in text, not in binary. And so HTTP is in text, by the way. And this was an internet\\ntradition by the way that we picked up. But we continued it. HTTP is text and HTML is\\ntext, and then every else, everything else that\\nfollowed is text as a result. And by the way, you can imagine purist\\nengineers saying this is insane. You have very limited bandwidth. Why are you wasting any time sending text? You should be encoding\\nthis stuff into binary and it\\'ll be much faster. And of course the answer\\nis that\\'s correct. But what you get when you make\\nit taxed is all of a sudden, well, the big breakthrough was the view source function, right? So the fact that you\\ncould look at a webpage, you could hit view source\\nand you could see the HTML, that was how people learned\\nhow to make webpages. Right? - It\\'s so interesting \\'cause the stuff would\\ntake for granted now is, man, that was fundamental, the development of the web\\nto be able to have HTML just right there, all the\\nghetto mess that is HTML, all the sort of almost\\nbiological like messiness of HTML and then having the browser\\ntry to interpret that as. - [Marc] Exactly. - To show something reasonable. - Well and then there was\\nthis internet principle that we inherited, which\\nwas emit, what was it? Emit cautiously. Emit\\nconservatively interpret liberally. So it basically meant if you\\'re, the design principle was if you\\'re creating like a web editor that\\'s gonna admit HTML, like\\ndo it as cleanly as you can, but you actually want the\\nbrowser to interpret liberally, which is you actually want\\nusers to be able to make all kinds of mistakes and\\nfor it to still work. And so the browser rendering\\nengines to this day have all of this spaghetti code crazy stuff where they\\'re resilient to\\nall kinds of crazy issue, no mistakes. And so, literally what I\\nalways had in my head is like there\\'s an 8 year old or\\nan 11 year old somewhere and they\\'re doing a view source, they\\'re doing a cut and\\npaste and they\\'re trying to make a webpage for\\ntheir eternal or whatever. And like they leave out a\\nslash and they leave out an angle bracket and they do this and they do that and it\\'s still works. - It\\'s also like a, I don\\'t often think about this,\\nbut, you know, programming, you know, C++ all those languages, lisp, the compiled languages,\\nthe interpreted languages, Python, Pearl, all that. The brace have to be all correct. It\\'s like everything has to be perfect. - [Marc] Brutal. - And then-- - [Marc] Autistic. - You forget. All right. It\\'s systematic\\nand rigorous, let\\'s go there. But you forget that the, the web with JavaScript eventually. And HTML is allowed to be messy in the way for the first time. Messy in the way biological\\nsystems could be messy. It\\'s like the only thing\\ncomputers were allowed to be messy on for the first time. - It used to off fend me. So I grew up on Unix, so I worked on Unix. I was a Unix native for all\\nthe way through this period. And so, it used to drive\\nme bananas when it would do the segmentation fault\\nand the core dump file, just like it is, you know, it\\'s like literally there\\'s\\nlike a error in the code. The math is off by one. And it core dumps. And I\\'m in the core dump\\ntrying to analyze it and trying to reconstruct what, and I\\'m\\njust like, this is ridiculous. Like, the computer\\nought to be smart enough to be able to know that if it\\'s off by one, okay fine. And it keeps running. And I would go ask all the experts like, why can\\'t it just keep running? And they\\'d explain to me, well, because all the downstream\\nrepercussions and blah blah. And I\\'m like, this still,\\nlike, you know, this is, we\\'re forcing the human\\ncreator to live to your point in this hyper, literal\\nworld of perfection. - [Lex] Yeah. And I was just like, that\\'s just bad. And by the way, you know what happens with that of course. Just what what happened with,\\nwith coding at that point, which is you get a high\\npriesthood, you know, there\\'s a small number of\\npeople who are really good at doing exactly that. Most people can\\'t. And most people are excluded from it. And so actually that was where that for that was where I picked up that idea was like, no, you want these things to be\\nresilient error in all kinds and this would drive the\\npurist absolutely crazy. Like, I got attacked on this like a lot \\'cause I mean like every time you know, all the purists who\\nwere like into all this like Marcup language stuff and formats and codes and all this stuff, they would be like, you know, you\\'re encouraging bad behavior \\'cause. - Oh, so they wanted\\nthe browser to give you a fault error anytime there was a-- - Yeah. They wanted to\\nbe a (indistinct) right? They wanted to-- Yeah. Yeah. That was a very and any properly trained credential engineer would be like, that\\'s not how you build these systems. - That\\'s such a bold move to say, no, it doesn\\'t have to be. - Yeah. No, like I said, the good news for me is\\nthe internet kind of had that traditional already,\\nbut having said that, like we pushed it, we pushed it way out. But the other thing we did, going back to the performance thing, was we gave up a lot of performance. We made that, that initial experience for the first few years\\nwas pretty painful. But the bet there was\\nactually an economic bet, which was basically the demand\\nfor the web would basically mean that there would be a\\nsurge in supply of broadband. Like because the question was, okay, how do you get the phone\\ncompanies which are not famous in those days for doing\\nnew things at huge cost for like speculative reasons. Like how do you get them to\\nbuild up broadband, you know, spend billions of dollars\\ndoing that and you know, you could go meet with them\\nand try to talk them into it. Or you could just have a thing where it\\'s just very\\nclear that it\\'s gonna be, that people love that\\'s gonna\\nbe better if it\\'s faster. And, so that, there was a\\nperiod there and this was, this was fraught with in peril, but there was a period there\\nwhere it\\'s like we knew the experience was sub-optimized because we were trying to force the emergence of demand for broadband. - [Lex] Sure. - Which is in fact what happened. - So you had to figure out\\nhow to display this text, HTML text. So the blue links and\\nthe prop links. What? And there\\'s no standards. Is\\nthere standards at that time? - [Marc] No. There really still isn\\'t. - Well there\\'s like standards, there\\'s applied, implied standards. Right. And they, you know, there\\'s all these kind of new features that are being added with like CSS, what, like what kind of stuff a\\nbrowser should be able to support features within languages,\\nwithin JavaScript and so on. But you\\'re setting standards\\non the fly yourself. - Yeah. Well to this day, if you create a webpage\\nthat has no CSS style sheet, the browser will render\\nit however it wants to. Right. So this was one of the\\nthings, there was this idea, this idea of at the time and\\nhow these systems were built, which is separation of content from format or separation of content from appearance. And that\\'s still, people\\ndon\\'t really use that anymore \\'cause everybody wants to\\ndetermine how things look and so they use CSS\\nbut it\\'s still in there that you can just let the\\nbrowser do all the work. - I still like the like\\nreally basic websites, but that could be just old school, kids these days with their\\nfancy responsive websites that don\\'t actually have much content, but have a lot of visual elements. - Well that\\'s one of the\\nthings that\\'s fun about chat, you know, about ChatGPT like. - [Lex] Back to the basics. - Back to just text. - [Lex] Yeah. - Right? And it, you know, there is this pattern in\\nhuman creativity and media where you end up back at text\\nand I think there\\'s, you know, there\\'s something powerful in there. - Is there some other stuff you remember like the purple links? There were some interesting\\ndesign decisions that to kind of come up that we have today or we don\\'t have today\\nthat were temporary. - So I made the background\\n\\'cause I hated reading texts on white background, so I\\nmade the background gray. Everybody can-- - Do you go ahead to? - No. No, no. That decision I think has been reversed. But now I\\'m happy though because\\nnow dark mode is the thing. - So it wasn\\'t about gray, it was just you didn\\'t\\nwant white background. - [Marc] Strain my eyes. - Strain your eyes. Interesting. And then there\\'s a bunch\\nof other decisions. I\\'m sure there\\'s an interesting\\nhistory of the development of HTML and CSS and\\nInterface and JavaScript and there\\'s this whole Java applet thing. - Well the big one probably\\nJavaScript, CSS was after me, so I didn\\'t, that was not me. But JavaScript was the big, JavaScript maybe was the\\nbiggest of the whole thing. That was us. And that was basically a bet,\\nit was a bet on two things. One is that the world wanted a new front end scripting language. And then the other was I thought at the time the world wanted a new backend scripting language. So JavaScript was designed\\nfrom the beginning to be both front end and backend. And then it failed as a\\nbackend scripting language. And Java won for a long time. And then Python Pearl and\\nother things, PHP and Ruby. But now JavaScript is back. And so. - I wonder if everything in\\nthe end will run on JavaScript. - It seems like it is the, and by the way, lemme give a shout out\\nto, to Brendan Eich was basically the one man\\ninventor of of JavaScript. - If you\\'re interested to\\nlearn more about Brendan Eich, he\\'s been on his podcast previously. - Exactly. So he wrote\\nJavaScript over a summer and I mean I think it is fair, it is fair to say now that\\nit\\'s the most widely used language in the world and\\nit seems to only be gaining in its in its range of adoption. - You know, in the software world there\\'s quite a few stories of somebody over a week weekend or over a\\nweek or over a summer writing some of the most impactful\\nrevolutionary pieces of software ever. That\\nshould be inspiring. Yes. - Very inspiring. I\\'ll\\ngive you another one. SSL. So SSL with the security\\nprotocol, that was us. And that was a crazy idea at the time, which was let\\'s take\\nall the native protocols and let\\'s wrap them in a security wrapper. That was a guy named Kip Hickman who wrote that over a summer, one guy. And then look today, sitting here today, like the transformer like at Google was a small handful of people. And then, you know, the number of people who have\\ndid like the core work on GPT. It\\'s not that many people, it\\'s a pretty small handful of people. And so yeah, the pattern in software repeatedly over a very long time has been, it\\'s Jeff Bezos always\\nhad the two pizza rule for teams at Amazon, which is any team needs\\nto be able to be fed with two pieces. If you need the third pizza,\\nyou have too many people. And I think it\\'s actually\\nthe one pizza rule. For the really creative work. I think it\\'s two people, three people. - Well that\\'s, you see that with certain open source projects, like so much is done by\\nlike one or two people. Like it\\'s so incredible\\nand that\\'s why you see that gives me so much hope\\nabout the open source movement in this new age of AI where, you know, just recently having had a conversation with Marc Zuckerberg of all people who\\'s all in on open source, which is so interesting to\\nsee and so inspiring to see \\'cause like releasing\\nthese models, it is scary. It is potentially very dangerous\\nand we\\'ll talk about that. But it\\'s also, if you believe in the\\ngoodness of most people and in the skillset of most people and the desire to go do good in the world, that\\'s really exciting. \\'cause it\\'s not putting it these models into the centralized\\ncontrol of big corporations, the government and so on. It\\'s putting it in the hands of a teen, teenage kid with like a dream in his eyes. I don\\'t know. That\\'s beautiful. - Look, this stuff, AI ought to make the\\nindividual coder obviously far more productive right? By like, you know, a\\nthousand X or something. And so you ought to open source like, not just the future of open source AI, but the future of open source everything. We ought to have a world\\nnow of super coders, right? Who are building things as open source with one or two people\\nthat were inconceivable, you know, five years ago. You know, the level of\\nkind of hyper productivity we\\'re gonna get out of\\nour best and brightest I think is gonna go way up. - It\\'s gonna be interesting. We\\'ll talk about it, but let\\'s just to linger\\na little bit on Netscape. Netscape was acquired in\\n1999 for 4.3 billion by AOL. What was that like? What were some memorable aspects of that? - Well that was the height\\nof the.com boom bubble bust. I mean that was the frenzy. If you watch succession, that was like what they\\ndid in the fourth season with Gojo and the merger with their, so it was like the height of like one of those kind of dynamics. And so. - Would you recommend\\nsuccession, by the way? I\\'m more of a Yellowstone guy. - Yellowstone\\'s very American. I\\'m very proud of you. That\\'s, that is. - I just talked to Matthew McConaughey and I\\'m full on Texan at this point. - Good. I approve. - And he\\'ll be doing\\nthe SQL to Yellowstone. - [Marc] Yeah, just exciting. - Very exciting. Anyway. - [Marc] Can\\'t wait. - So that\\'s a rude interruption\\nby me by way of succession. So, that was at the height of the-- - Deal making and money\\nand just the fur flying and like craziness. And so yeah, it was just one of those, it was just like, I mean, and this, the entire (indistinct) thing from start to finish was four years, which was like for one of these companies, it\\'s just like incredibly fast. You know, it went, we went public 18 months\\nafter we got moved where we were founded, which\\nvirtually never happens. So it was just this incredibly fast kind of meteor streaking across the sky. And then of course it was this, and then there was just\\nthis explosion, right? That happened \\'cause then\\nit was almost immediately followed by the.com crash. It was then followed\\nby AOL, by Time Warner, which again is like the succession guys kinda play with that, which turned out to be a disastrous deal. You know, one of the famous, you know, kind of disastrous in business history. And then, you know, what became an internet depression on\\nthe other side of that. But then in that depression\\nin the two thousands was the beginning of broadband and smartphones and Web 2.0 right? And then social media\\nand search and every SaaS and everything that came out of that. - What did you learn from\\njust the acquisition? I mean this is so much money. What\\'s interesting \\'cause I\\nmust have been very new to you, that these software stuff, you can make so much money. There\\'s so much money swimming around. I mean, I\\'m sure the\\nideas of investment was starting to get born there. - Yes. Let me get, so let me lay it. So here\\'s, here\\'s the thing. I dunno if I figured it out\\nthen, but figured it out later, which is software is a technology that it, it\\'s like a, you know, the\\nconcept of the philosopher stone, the philosopher stone in alchemy, transient is led into gold and\\nNewton spent 20 years trying to find the philosopher stone. Never got there. Nobody\\'s ever figured it out. Software is our modern philosopher stone. And in economic terms, it\\ntransmutes labor into capital, which is like a super interesting thing. And by the way, like Carl Marcs is rolling over in his grave right now. \\'Cause of course that\\'s\\ncomplete reputation of his entire theory. Trans labor and capital\\nwhich is as follows, is somebody sits down at a keyboard and types a bunch of stuff in, and a capital asset\\ncomes out the other side and then somebody buys that capital asset for a billion dollars. Like that\\'s amazing, right? It\\'s literally creating\\nvalue right out of thin air, right out of purely human thought, right? And so that, there are many things that make software magical and special, but that\\'s the economics. - I wonder what Marx\\nwould\\'ve thought about that? - Oh, he would\\'ve\\ncompletely broke his brain because of course the whole\\nthing was it was he could, you know, that kind of\\ntechnology was inconceivable when he was alive. It was all industrial era stuff. And so, any kind of machinery\\nnecessarily involved huge amounts of capital. And then labor was on the receiving end of the abuse. - [Lex] Yep. Right? But like software eng software, a software engineer is somebody\\nwho basically transmutes his own labor into actual, an actual capital asset\\ncreates permanent value. Well, and in fact it\\'s\\nactually very inspiring. That\\'s actually more\\ntrue today than before. So when I was doing software, the assumption was all\\nnew software basically has a sort of a parabolic\\nsort of lifecycle, right? So you ship the thing,\\npeople buy it at some point, everybody who wants it has bought it and then it becomes obsolete. And it\\'s like bananas. Nobody, nobody buys old software. These days, Minecraft, Mathematica, you know, Facebook, Google, you have the software\\nassets that are, you know, have been around for 30\\nyears that are gaining in value every year, right? And they\\'re just, they\\'re being\\na world of warcraft, right, salesforce.com, like they\\'re being every single year they\\'re\\nbeing polished and polished and polished and polished. They\\'re getting better\\nand better, more powerful, more powerful, more\\nvaluable, more valuable. So we\\'ve entered this era\\nwhere you can actually have these things that actually\\nbuild out over decades. Which by the way is what\\'s happening right now with like ChatGPT. And so now, this is why, you know, there is always, you know, sort of a constant investment\\nfrenzy around software is because, you know, look, when\\nyou start one of these things, it doesn\\'t always succeed. But when it does now you\\nmight be building an asset that builds value for,\\nyou know, four or five, six decades to come. You know, if you have a team of people who have the level of devotion required to keep making it better. And then the fact that of\\ncourse everybody\\'s online, you know, there\\'s 5 billion people that are a click away from\\nany new piece of software. So the potential market size\\nfor any of these things is, you know, nearly infinite. - [Lex] It must have been\\nsurreal back then though. - Yeah. Yeah. This was\\nall brand new, right? Yeah. Back then, this was all brand new. These were all, you know, brand new. Had you rolled out that\\ntheory in even 1999, people would\\'ve thought\\nyou were smoking crack. So that\\'s emerged over time. - Well, let\\'s now turn\\nback into the future. You wrote the essay \"Why\\nAI Will Save The World?\" Let\\'s start the very high level. What\\'s the main thesis of the essay? - Yeah, so the main thesis on the essay is that what we\\'re dealing\\nwith here is intelligence. And it\\'s really important to kind of talk about the sort of very nature\\nof what intelligence is. And fortunately we have a predecessor to machine intelligence,\\nwhich is human intelligence. And we\\'ve got, you know, observations and theories\\nover thousands of years for what intelligence is\\nin the hands of humans and what intelligence is, right? I mean, what it literally is the way to, you know, capture, process,\\nanalyze, synthesize information, solve problems. But the observation of\\nintelligence in human hands is that intelligence quite literally\\nmakes everything better. And what I mean by that\\nis every kind of outcome of like human quality of life, whether it\\'s education outcomes or success of your children, or career success or health or lifetime\\nsatisfaction, by the way, propensity to peacefulness\\nas opposed to violence, propensity for open-mindedness\\nversus bigotry, those are all associated with\\nhigher levels of intelligence. - Smarter people have better outcomes than almost as you write in almost every domain of activity. Academic achievement, job performance, occupational status, income, creativity, physical health, longevity,\\nlearning new skills, managing complex tasks, leadership,\\nentrepreneurial success, conflict resolution,\\nreading comprehension, financial decision making, understanding others\\nperspectives, creative arts, parenting outcomes, and life satisfaction. One of the more depressing\\nconversations I\\'ve had, and I don\\'t know why it\\'s depressing, I have to really think\\nthrough why it\\'s depressing, but on IQ and the G factor, and that that\\'s something\\nin large part is genetic and it correlates so much\\nwith all of these things and success in life. It\\'s like all the inspirational\\nstuff we read about, like if you work hard and so on, it sucks that you\\'re born with the hand that you can\\'t change. - But what if you could. - You\\'re saying basically\\na really important point, and I think it\\'s in your\\narticles, it really helped me. It\\'s a nice added\\nperspective to think about. Listen, human intelligence, the science of intelligence\\nis shown scientifically that it just makes life easier and better the smarter you are. And now let\\'s look at\\nartificial intelligence and if that\\'s a way to increase\\nsome human intelligence, then it\\'s only going\\nto make a better life. - [Marc] Yeah. - That\\'s the argument. - And certainly at the collective level, we could talk about the collective effect of just having more\\nintelligence in the world, which will have very big payoff. But there\\'s also just\\nat the individual level, like what if every person has a machine? You know? And the concept of augment Doug Engelbart\\'s concept of augmentation. You know, what if\\neverybody has an assistant and the assistant is, you know, 140 IQ and you happen to be 110 IQ and you\\'ve got, you know, something that basically is\\ninfinitely patient and knows everything about you\\nand is pulling for you in every possible way,\\nwants you to be successful. And anytime you find anything\\nconfusing or wanna learn anything or have trouble\\nunderstanding something or wanna figure out what to\\ndo in a situation, right? Wanna figure out how to\\nprepare for a job interview, like any of these things,\\nlike it will help you do it. And it will therefore, the combination will\\neffectively be, you know, will effectively raise your raise because it will effectively raise your IQ, will therefore raise the odds of successful life outcomes\\nin all these areas. - So people below the,\\nthis hypothetical 140 IQ, it\\'ll pull them up towards 140 IQ. - Yeah, yeah, yeah. And then of course, you know, people at 140 IQ will be\\nable to have a peer, right. To be able to communicate, which is great. And then people above 140\\nIQ will have an assistance that they can farm things out to. And then look, God willing, you know, at some point these things\\ngo from future versions go from 140 IQ equivalent to\\n150 to 160 to 180, right? Like Einstein was estimated\\nto be on the order of one 60, you know, so when we\\nget, you know, one 60 AI, like we\\'ll be, you know, when one assumes creating\\nEinstein level breakthroughs and physics, and then at\\n180 we\\'ll be, you know, carrying cancer and developing\\nwarp drive and doing all kinds of stuff. And so it is quite possibly the case, this is the most important\\nthing that\\'s ever happened and the best thing that\\'s ever happened because precisely because it\\'s a lever on this single fundamental\\nfactor of intelligence, which is the thing that drives\\nso much of everything else. - Can you steal, man, the case that human plus AI is\\nnot always better than human for the individual? - You may have noticed that there\\'s a lot of\\nsmart running around. - [Lex] Sure. Yes. - Right? And so, like smart, there are certain people where\\nthey get smarter, you know, they get to be more arrogant, right? So that, you know, there\\'s one huge flaw. - Although to push back on that, it might be interesting because\\nwhen the intelligence is not all coming from you,\\nbut from another system, that might actually increase\\nthe amount of humility even in the assholes. - [Marc] One would hope. - Yeah. - Or it could make assholes more assholes. You know, that\\'s in, I mean, that\\'s for psychology to study. - Yeah, exactly. Another one is smart people\\nare very convinced that they, you know, have a more\\nrational view of the world, and that they have a easier\\ntime seeing through conspiracy theories and hoaxes and right. You know, sort of crazy\\nbeliefs and all that. There\\'s a theory in psychology, which is actually smart people. So for sure people who aren\\'t\\nas smart are very susceptible to hoaxes and conspiracy theories. But it may also be the case\\nthat the smarter you get, you become susceptible in a different way, which is you become very\\ngood at marshaling facts to fit preconceptions, right. You become very, very good at assembling whatever theories and\\nframeworks and pieces of data and graphs and\\ncharts you need to validate whatever crazy ideas got in your head. And so you\\'re susceptible\\nin a different way, right? - We\\'re all sheep, but\\ndifferent colored sheep. - Some sheep are better\\nat justifying it. Right. And those are the, you know, those are the smart sheep, right? So yeah. Look like I\\nwould say this look like there are no panacea. I\\'m not a utopian, there\\nare no panaceas in life. There are no, like, you know, I don\\'t believe there\\nare like pure positives. I\\'m not a transcendental\\nkind of person like that. But, you know, so yeah,\\nthere are gonna be issues and, you know, look, smart people, another maybe you could\\nsave about smart people is they are more likely to get\\nthemselves in situations that are, you know, beyond their grasp. You know, because they\\'re\\njust more confident in their ability to deal with complexity and their eyes become bigger, their cognitive eyes become bigger than their stomach, you know? So yeah, you could argue\\nthose eight different ways nevertheless, on net, right? Clearly, overwhelmingly, again, if you just extrapolate from what we know about human intelligence, you\\'re improving so many aspects of life if you\\'re upgrading intelligence. - So there\\'ll be assistants\\nat all stages of life. So when you\\'re younger,\\nthere\\'s for education, all that kind of stuff for\\nmentorship, all of this. And later on as you\\'re doing\\nwork and you\\'ve developed a skill and you\\'re having a profession, you\\'ll have an assistant\\nthat helps you excel at that profession. So at all stages of life. - Yeah. I mean, look, the\\ntheory is augmentation. This is the Doug Engelbart\\'s term. Doug Engelbart made this observation many, many\\ndecades ago that, you know, basically it\\'s like you can\\nhave this oppositional frame of technology where it\\'s\\nlike us versus the machines, but what you really do\\nis you use technology to augment human capabilities. And by the way, that\\'s how actually the economy develops. That\\'s, we can talk about\\nthe economic side of this, but that\\'s actually how\\nthe economy grows is through technology\\naugmenting human potential. And so, yeah. And then you basically\\nhave a proxy or you know, or you know, a sort of\\nprosthetic, you know, so like you\\'ve got glasses,\\nyou\\'ve got a wristwatch, you know, you\\'ve got shoes, you know, you\\'ve got these things. You\\'ve got a personal computer, you\\'ve got a word processor,\\nyou\\'ve got Mathematica, you\\'ve got Google. This is the latest\\nviewed through that lens. AI is the latest in a long\\nseries of basically augmentation methods to be able to\\nraise human capabilities. It\\'s just this one is the\\nmost powerful one of all, because this is the one\\nthat, that goes directly to what they call fluid\\nintelligence, which is IQ. - Well, there\\'s two categories of folks that you outline that\\nworry about or highlight the risks of AI, and you highlight a bunch of different risks. I would love to go through those risks and just discuss them, brainstorm which ones are serious and which ones are less serious. But first, the Baptist\\nand the bootleggers, what are these two\\ninteresting groups of folks who worry about the effect\\nof AI and human civilization? - [Marc] Or say they do. - Say, oh, okay, yes, I\\'ll say they do. - The Baptist worry the\\nbootleggers say they do. So the Baptist and the\\nbootleggers is a metaphor from economics, from what\\'s\\ncalled development economics. And it\\'s this observation that when you get social\\nreform movements in a society, you tend to get two sets\\nof people showing up, arguing for the social reform. And the term Baptist and bootleggers comes from the American experience\\nwith alcohol prohibition. And so in the 1900s, 1910s, there was this movement\\nthat was very passionate at the time, which basically said, alcohol is evil and is destroying society. By the way, there was a lot\\nof evidence to support this. There were very high rates of very high correlations\\nthen, by the way. And now between rates of physical\\nviolence and alcohol use, almost all violent crimes\\nhave either the perpetrator or the victim, or both drunk almost. If you see this actually in the work, almost all sexual harassment\\ncases in the workplace, it\\'s like at a company\\nparty and somebody\\'s drunk. Like, it\\'s amazing how often\\nalcohol actually correlates to actually dis dysfunction\\nand at leads to domestic abuse and so forth, child abuse. And so you had this group of\\npeople who were like, okay, this is bad stuff and we should outlaw it. And those were quite literally Baptist. Those were super committed, you know, hardcore Christian\\nactivists in a lot of cases. There was this woman whose\\nname was Carrie Nation, who was this older woman who\\nhad been in this, you know, I don\\'t know, disastrous\\nmarriage or something. And her husband had been\\nabusive and drunk all the time. And she became the icon of\\nthe Baptist prohibitionist. And she was legendary in\\nthat era for carrying an ax and doing, you know, completely on her own\\ndoing raids of saloons and like taking her ax to all the bottles and eggs in the back. And so. - [Lex] A true believer. - An absolute true believer, and with absolutely the\\npurist of intentions. And again, there\\'s a very\\nimportant thing here, which is there\\'s, you could look at this\\ncynically and you could say the Baptists are like delusional,\\nyou know, the extremists, but you could also say,\\nlook, they\\'re right. Like she was, you know, she had a point. Like she wasn\\'t wrong about\\na lot of what she said. - Yeah. - But it turns out the way\\nthe story goes is it turns out that there were another set of people who very badly wanted to\\noutlaw alcohol in those days. And those were the bootleggers, which was organized crime that\\nstood to make a huge amount of money if legal alcohol\\nsales were banned. And this was, in fact, the way the history goes\\nis this was actually the beginning of\\norganized crime in the US. This was the big economic\\nopportunity that opened that up. And so they went in together and no, they didn\\'t go in together. Like the Baptist did not\\neven necessarily know about the bootleggers \\'cause they were on their moral crusade. The bootleggers certainly\\nknew about the Baptists. And they were like, wow, these people are like the\\ngreat front people for like. You know, it\\'s-- - [Lex] Good PR. - Shenanigans in the background. And they got the (indistinct)\\nAct passed, right. And they did in fact ban alcohol\\nin the US and you\\'ll notice what happened, which is\\npeople kept drinking, it didn\\'t work, people kept drinking. That bootleggers made a\\ntremendous amount of money. And then over time it became\\nclear that it made no sense to make it illegal and it\\nwas causing more problems. And so then it was revoked. And here we sit with legal\\nalcohol a hundred years later with all the same problems. And you know, the whole thing was this\\nlike giant misadventure the Baptist got taken advantage\\nof by the bootleggers, and the bootleggers got what they wanted. And that was that. - The same two categories of folks are now sort of suggesting\\nthat the development of artificial intelligence\\nshould be regulated. - A hundred percent. It\\'s the same pattern. And the economist will tell you it\\'s the same pattern every time. Like, this is what\\nhappened, nuclear power, this is what happens, which\\nis another interesting one. But like, yeah, this\\nhappens dozens and dozens of times throughout the\\nlast a hundred years and this is what\\'s happening now. - And you write that it isn\\'t\\nsufficient to simply identify the actors and impugn their motives. We should consider the\\narguments of both the Baptist and the bootleggers on their merits. So let\\'s do just that. Risk number one, will AI kill us all? - [Marc] Yes. - So what do you think about this one? What do you think is\\nthe core argument here that the development of\\nAGI perhaps better said, will destroy human civilization? - Well, first of all, you\\njust did a slight of hand \\'cause we went from talking about AI to AGI. - Is there a fundamental difference there? - I don\\'t know. What\\'s AGI? - What\\'s AI, what\\'s in intelligence? - Well, I know what AI\\nis machine learning. What\\'s AGI? - I think we don\\'t know\\nwhat the bottom of the well of machine learning is\\nor what the ceiling is. Because just to call\\nsomething machine learning or just to call some of the statistics or just to call it math or\\ncomputation doesn\\'t mean, you know, nuclear\\nweapons are just physics. So to me it\\'s very\\ninteresting and surprising how far machine learning has taken. - No, but we knew that\\nnuclear physics would lead to weapons. That\\'s why the scientists\\nof that era were always in some this huge dispute\\nabout building the weapons. This is different. AGI is different. - Does machine learning lead, do we know? - We don\\'t know, but this\\nis my point is different. We actually don\\'t know. But, and this is where you, the slide of hand kicks in, right? This is where it goes from\\nbeing a scientific topic to being a religious topic. And that\\'s why I specifically called out \\'cause that\\'s what happens. They do the vocabulary\\nshift and all of a sudden you\\'re talking about something totally. That\\'s not actually real. - Well then maybe you can\\nalso, as part of that, define the western\\ntradition of Millennialism. - [Marc] Yes. Into the world apocalypse. - [Lex] What is it? - [Marc] Apocalypse cults. - [Lex] Apocalypse cults. - Well, so we live in, we of course live in a Judeo-Christian, but primarily Christian\\nkind of saturated, you know, kind of Christian, post-Christian,\\nsecularized Christian, you know, kind of world in the west. And of course court of Christianity is the idea of the second\\ncoming and you know, the revelations and you know, Jesus returning and the\\nthousand year, you know, utopia on earth and then you know, the rapture and like all\\nall that stuff, you know, you know, we collectively,\\nyou know, as a society, we don\\'t necessarily take\\nall that fully seriously now. So, what we do is we create our\\nsecularized versions of that we keep looking for utopia. We keep looking for, you know, basically the end of the world. And so what what you see over, over decades is that basically a pattern of these sort of these of\\nis this is what cults are. This is how cults form as\\nthey form around some theory of the end of the world. And so the people\\'s temple cults, the Manson cult, the Heavens Gate cult, the David Qresh cult, you know what they\\'re all\\norganized around is like, there\\'s gonna be this\\nthing that\\'s gonna happen that\\'s gonna basically bring\\ncivilization crashing down. And then we have this\\nspecial elite group of people who are gonna see it\\ncoming and prepare for it. And then they\\'re the people\\nwho are either going to stop it or are failing, stopping it. They\\'re gonna be the people\\nwho survived the other side and ultimately get credit\\nfor having been, right. - Why is that so compelling,\\ndo you think? Like-- - Because it satisfies this very deep need we have for transcendence and meaning that got stripped\\naway when we became secular. - Yeah, but why is the\\ntranscendence involve the destruction of human civilization? - Because like how plausible it\\'s like a very deep psychological thing \\'cause it\\'s like how plausible, how plausible is it\\nthat we live in a world where everything\\'s just\\nkind of all right? Right. How exciting? - [Lex] Whoa. - How exciting is that? Right? - [Lex] But that\\'s. - We got more than that. - But that\\'s the deep question I\\'m asking. Why is it not exciting to live in a world where everything\\'s just all right? Is it, I think, you know, most of the animal kingdom would be so happy with just all right. Because that means survival. Why are we, maybe that\\'s what it is. Why are we conjuring up\\nthings to worry about? - So CS Lewis called\\nit the God-shaped hole. So there\\'s a God-shaped hole\\nin the human experience, consciousness, soul,\\nwhatever you wanna call it, where there\\'s gotta be\\nsomething that\\'s bigger than all this. There\\'s gotta be something transcendent. There\\'s gotta be something\\nthat is bigger, right? Bigger purpose. A bigger meaning. And so we have run the\\nexperiment of, you know, we\\'re just gonna use\\nscience and rationality and kind of, you know, everything\\'s just gonna\\nkind of be as it appears. And large number of people have found that very deeply wanting and\\nhave constructed narratives. And by this is the story\\nof the 20th century, right? Communism, right? Was one of those, communism\\nwas a was a form of this, Nazism was a form of this. You know, some people, you know, you can see movements\\nlike this playing out all over the world right now. - So you constructed a kind of devil, a kind of source of evil, and we\\'re going to transcend beyond it. - Yeah. And (indistinct)\\nwhen you see a Miller cult, they put a really specific point on it, which is end of the world, right, there is some change coming. And that change that\\'s\\ncoming is so profound and so important that\\nit\\'s either gonna lead to utopia or hell on earth. Right? And it is going to, and then, you know, it\\'s like what if you actually knew that was going to happen, right? What would you do? Right? How would you prepare yourself for it? How would you come together with a group of like-minded people, right? How would you, what would you do? Would you plan like Cassius\\nof weapons in the woods? Would you like, you know, I don\\'t know if create\\nunderground buckers, would you, you know, spend your\\nlife trying to figure out a way to avoid having it happen? - Yeah. That\\'s a really\\ncompelling, exciting idea to have a club over. To have a little bit of travel, like a get', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 532341, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content=' now. - So you constructed a kind of devil, a kind of source of evil, and we\\'re going to transcend beyond it. - Yeah. And (indistinct)\\nwhen you see a Miller cult, they put a really specific point on it, which is end of the world, right, there is some change coming. And that change that\\'s\\ncoming is so profound and so important that\\nit\\'s either gonna lead to utopia or hell on earth. Right? And it is going to, and then, you know, it\\'s like what if you actually knew that was going to happen, right? What would you do? Right? How would you prepare yourself for it? How would you come together with a group of like-minded people, right? How would you, what would you do? Would you plan like Cassius\\nof weapons in the woods? Would you like, you know, I don\\'t know if create\\nunderground buckers, would you, you know, spend your\\nlife trying to figure out a way to avoid having it happen? - Yeah. That\\'s a really\\ncompelling, exciting idea to have a club over. To have a little bit of travel, like a get together on a Saturday\\nnight and drink some beers and talk about the end of the world and how you are the only\\nones who have figured it out. - Yeah. And then once you lock in on that, like how can you do anything\\nelse with your life? Like this is obviously the\\nthing that you have to do. And then there\\'s a psychological\\neffect that you alluded to. There\\'s a psychological effect. If you take a set of true\\nbelievers and you leave them to themselves, they get\\nmore radical. Right. \\'Cause they self radicalize each other. - That said, it doesn\\'t mean they\\'re not sometimes right. - Yeah. The end of the world might be. Yes. Correct. Like they might be right. - [Lex] Yeah. - But like-- - [Lex] I have some pamphlets for you. - Exactly. - But I mean we\\'ll talk\\nabout nuclear weapons \\'cause you have a really\\ninteresting little moment that I learned about in\\nyour essay, but you know, sometimes it could be right. - [Marc] Yeah. - \\'Cause we\\'re still, you were developing more and\\nmore powerful technologies in this case, and we don\\'t know what the impact it will\\nhave on human civilization while we can highlight all\\nthe different predictions about how it\\'ll be positive, but the risks are there and\\nyou discuss some of them. - Well, the steel man, the\\nsteel man is the steel man. Well actually, the steel\\nman and his reputation are the same, which is you can\\'t predict what\\'s gonna happen. Right. You can\\'t rule out that this\\nwill not end everything. Right. But the response to that\\nis you have just made a completely non-scientific claim. You\\'ve made a religious\\nclaim, not a scientific claim. - How does it get disproven? - And there\\'s no, by definition with these kinds of claims, there\\'s no way to disprove them. Right? And so there there\\'s no, you\\njust go right on the list. There\\'s no hypothesis, there\\'s no testability of the hypothesis. There\\'s no way to falsify the hypothesis, there\\'s no way to measure\\nprogress along the arc. Like it\\'s just all completely missing. And so it\\'s not scientific and. - I don\\'t think it\\'s completely missing. It\\'s somewhat missing. So for example, the people that say AI\\'s gonna kill all of us. I mean, they usually have\\nideas about how to do that. Whether it\\'s the people\\nclub maximizer or, you know, it escapes there\\'s mechanism\\nby which you can imagine it killing all humans. - [Marc] Models. - And you can disprove it by saying there\\'s a limit to the speed at which intelligence increases. Maybe show that like the sort\\nof rigorously really described model, like how it could\\nhappen and say, no, there, here\\'s a physics limitation. There\\'s like a physical\\nlimitation to how these systems would actually do damage\\nto human civilization. And it is possible they\\nwill kill 10 to 20% of the population, but it seems impossible\\nfor them to kill 99%. - It was practical\\ncounterarguments. Right. So you mentioned\\nbasically what I described as the thermodynamic counterargument, which, so sitting here today, it\\'s like where with the\\nevil AGI get the GPU. \\'Cause like they don\\'t exist. So if you\\'re gonna have a\\nvery frustrated baby evil AGI, who\\'s gonna be like trying to\\nbuy Nvidia stock or something to get them to finally\\nmake some chips, right? So the serious form of that\\nis the thermodynamic argument, which is like, okay, where\\'s\\nthe energy gonna come from? Where\\'s the processor gonna be running? Where\\'s the data center\\ngonna be happening? How is this gonna be\\nhappening in secret such that, you know, it\\'s not, you know, so that\\'s a practical counter argument to the runaway AGI thing. I have a but I have and we\\ncan argue that, discuss that. I have a deeper objection to it, which is it\\'s, this is all forecasting. It\\'s all modeling, it\\'s\\nall future prediction. It\\'s all future hypothesizing. It\\'s not science. - [Lex] Sure. - It is not. It is the\\nopposite of science. So the, I\\'ll pull up Carl Sagan\\nextraordinary claims require extraordinary proof, right? These are extraordinary claims. The policies that are being\\ncalled for right to prevent this are of extraordinary magnitude that, and I think we\\'re gonna\\ncause extraordinary damage. And this is all being done\\non the basis of something that is literally not scientific. It\\'s not a testable hypothesis. - So the moment you say\\nAI\\'s gonna kill all of us, therefore we should ban it, or that we should regulate\\nall that kind of stuff, that\\'s when it starts getting serious. - Or start, you know, military\\nairstrikes and data centers. - [Lex] Oh boy. - Right? And like. - Yeah. This once get starts. Well, so starts getting real weird. - So here\\'s the problem with Arian cults. They have a hard time\\nstaying away from violence. - Yeah. But violence is so fun. - If you\\'re on the right end of it, they have a hard time avoiding violence. The reason they have a hard\\ntime avoiding violence is if you actually believe the claim. Right. Then what would you do to\\nstop the end of the world? Well, you would do anything, right? And so, and this is\\nwhere you get, and again, if you just look at the\\nhistory of Arian and cults, this is where you get the\\npeople\\'s temple and everybody killing themselves in the jungle. And this is where you get\\nCharles Manson and, you know, sending in to kill the pigs. Like, this is the problem with these. They have a very hard time to run the line at actual violence. And I think in this case, I\\nmean, they\\'re already calling for it like today and you know, where this goes from here\\nis they get more worked up. Like I think is like really concerning. - Okay. But that\\'s kind of the extremes. So, you know, the extremes of\\nanything are I was concerning. It\\'s also possible to kind\\nof believe that AI has a very high likelihood\\nof killing all of us. But and therefore we should maybe consider slowing development or regulating, so not violence or any\\nof these kinds of things. But it\\'s saying like, all right, let\\'s take a pause here. You know, you biological\\nweapons, nuclear weapons. Like whoa, whoa, whoa, whoa, whoa. This is like serious stuff.\\nWe should be careful. So it is possible to kinda have a more rational response, right? If you believe this risk is real. - [Marc] Believe. - Yes. So what is it possible to be, have a scientific approach to\\nthe prediction of the future? - I mean, we just went\\nthrough this with COVID. What do we know about modeling? - [Lex] Well, I mean. - What did we learn about\\nmodeling with COVID? - [Lex] There\\'s a lot of lessons. - They didn\\'t work at all. - [Lex] They worked poorly. - The models were terrible,\\nthe models were useless. - I don\\'t know if the models\\nwere useless or the people interpreting the models and\\nthen decentralized institutions that were creating policy\\nrapidly based on the models and leveraging the models in order to support their narratives versus actually\\ninterpreting the error bars and the models and all that kind of stuff. - What you had with COVID, my view you had with COVID\\nis you had these experts showing up and they\\nclaimed to be scientists and they had no testable\\nhypotheses whatsoever. They had a bunch of models. They had a bunch of forecasts\\nand they had a bunch of theories and they laid these out in front of policy makers and policy makers freaked\\nout and panicked. Right. And implemented a whole bunch of like, really like terrible decisions\\nthat we\\'re still living with the consequences of, and there was never any\\nempirical foundation to any of the models. None of them ever came true. - Yeah. To push back. There were certainly\\nBaptist and bootleggers in the context of this pandemic, but there\\'s still a\\nusefulness to models. No. - So not if they\\'re, I mean not if they\\'re\\nreliably wrong, right? Then they\\'re actually\\nlike anti-useful. Right. They\\'re actually damaging. - But what do you do with the pandemic? What do you do with any kind of threat? Don\\'t you want to kind of\\nhave several models to play with as part of this discussion of like, what the hell do we do here? - I mean, do they work? Because they\\'re an expectation\\nthat they actually like work that they have actual predictive value. I mean, as far as I can tell with COVID, the policymakers just si up\\nthemselves into believing that there was sub, I\\nmean, look, the scientists, the scientists were at fault. The quote unquote scientists showed up. So I had some insight into this. So there was a, or remember the Imperial College models out of London were the\\nones that were like, these are the gold standard models. So a friend of mine runs\\na big software company and he was like, wow, this is\\nlike, COVID is really scary. And he is like, you know, he contacted this research\\nand he is like, you know, do you need some help? You\\'ve been just building\\nthis model on your own for 20 years. Do you need some, would you like us our coders\\nto basically restructure it so it can be fully adapted for COVID? And the guy said yes\\nand sent over the code and my friend said it was\\nlike the worst spaghetti code he\\'s ever seen. - That doesn\\'t mean it\\'s\\nnot possible to construct a good model of pandemic\\nwith the correct air bars, with a high number of parameters\\nthat are continuously, many times a day updated\\nas we get more data about a pandemic. I would like to believe when\\na pandemic hits the world, the best computer scientists in the world, the best software engineers\\nrespond aggressively and as input take the data\\nthat we know about the virus and it\\'s an output say\\nhere is what\\'s happening in terms of how quickly it\\'s spreading, what that lead in terms of\\nhospitalization and deaths and all that kind of stuff. Here\\'s how likely, how\\ncontagious it likely is. Here\\'s how deadly it likely is based on different conditions, based on different ages and demographics and all that kind of stuff. So here\\'s the best kinds of policy. It feels like you could have models, machine learning that like kind of, they don\\'t perfectly predict the future, but they help you do something \\'cause there\\'s pandemics\\nthat are like, meh, they don\\'t really do much harm. And there\\'s pandemics,\\nyou can imagine them, they could do a huge amount of harm. Like they can kill a lot of people. So you should probably have\\nsome kind of data-driven models that keep updating, that allow you to make\\ndecisions that based like where, how bad is this thing? Now you can criticize how\\nhorrible all that went with the response to this pandemic, but I just feel like there\\nmight be some value to models. - So to be useful at some point it has to be predictive. Right? So and the easy thing\\nfor me to do is to say, obviously you\\'re right. Obviously I wanna see that\\njust as much as you do. \\'cause anything that makes\\nit easier to navigate through society through a\\nwrenching, you know, risk like that sounds great. You know, the harder objection to it is just simply\\nyou are trying to model a complex dynamic system\\nwith 8 billion moving parts. Like not possible. - [Lex] It\\'s very tough. - Can\\'t be done, complex\\nsystems can\\'t be done. - Machine learning says hold my beer. But well, it\\'s possible. No? - I don\\'t know. I would like to believe that it is. I\\'ll put it this way. I think where you and I\\nwould agree is I think we would like that to be the case. We are strongly in favor of it. I think we would also\\nagree that no such thing with respect to COVID or\\npandemics no such thing. At least neither you\\nnor I think are aware. I\\'m not aware of anything like that today. - My main worry with the\\nresponse to the pandemic is that same as with aliens, is that even if such a thing existed, and it\\'s possible it existed, the policymakers were\\nnot paying attention. Like there was no mechanism\\nthat allowed those kinds of models to percolate all. - Oh, I think we had the\\nopposite problem during COVID. I think the policymakers, I think these people with\\nbasically fixed science had too much access to the policymakers. - Well, right. And well, but the policy\\nmakers also wanted, they had a narrative in\\nmind and they also wanted to use whatever model\\nthat fit that narrative - [Marc] Oh, sure. - To help them out. So like, it felt like\\nthere was a lot of politics and not enough science. - Although a big part\\nof what was happening, a big reason we got lockdowns\\nfor as long as we did, was because these scientists\\ncame in with these like doomsday scenarios that were like, just like completely off the hook. - Scientists in quotes, let\\'s not-- - [Marc] Quote unquote scientists. - Let\\'s not, okay,\\nlet\\'s give love science. So here\\'s science that is the way out. - Science is a process\\nof testing hypotheses. Modeling does not involve\\ntestable hypotheses. Right. Like, I don\\'t even know that. I actually don\\'t even know that modeling actually\\nqualifies as science. Maybe that\\'s a side conversation. We could have some time over a beer. - Oh, that\\'s a really interesting part. What do we do about the future? I mean, what\\'s-- - So number one is when\\nwe start with number one, humility goes back to this thing of how do we determine the truth. Number two is we don\\'t believe, you know, it\\'s the old, I\\'ve gotta hammer everything\\nlooks like a nail, right? I\\'ve got, oh, this is one\\nof the reasons I gave you, I gave Alexa book, which the topic of the\\nbook is what happens when scientists basically\\nstray off the path of technical knowledge and\\nstart to weigh in on politics and societal issues. - In this case, philosophers. - Well in this case philosophers. But he actually talks in this\\nbook about, like Einstein, he talks about, actually about\\nthe nuclear age in Einstein. He talks about the\\nphysicists actually doing very similar things at the time. - The book is When Reason Goes On Holiday, Philosophers in Politics by Nevin. - And it\\'s just a story. It\\'s a story. There are other books on this topic, but this is a new one that\\'s really good this is just a story of what happens when experts\\nin a certain domain decide to weigh in and become\\nbasically social engineers and political, you know,\\nbasically political advisors. And it\\'s just a story of just\\ninning catastrophe. Right. And I think that\\'s what\\nhappened with COVID again. - Yeah. I found this book\\na highly entertaining and eye-opening read filled\\nwith amazing anecdote of a rationality and craziness\\nby famous Resa philosophers. - I definitely, after you read this book, you will not look at Einstein the same. - [Lex] Oh boy. - Yeah. - Don\\'t destroy my heroes. - He will not be a hero of yours anymore. Sorry. You probably couldn\\'t,\\nyou shouldn\\'t read the book. - All right. - But here\\'s the thing. The AI risk people, they don\\'t even have the COVID model, at least not that I\\'m aware of. - [Lex] No. - Like there\\'s not even the\\nequivalent of the COVID model. They don\\'t even have the spaghetti code. They\\'ve got a theory and a\\nwarning and a this and the that. And like, if you ask like,\\nokay, well here\\'s, I mean, the ultimate example is,\\nokay, how do we know, right? How do we know that an AI is running away? Like how do we know that the boom takeoff thing\\nis actually happening? And the only answer that\\nany of these guys have given that I\\'ve ever seen is, oh,\\nit\\'s when the loss rate, the loss function and the\\ntraining drops, right? That\\'s when you need to like\\nshut down the data center. Right? And it\\'s like, well that\\'s also what happens when you\\'re successfully training a model. Like, what even this is not science, this is not, it\\'s not\\nanything, it\\'s not a model, it\\'s not anything. There\\'s nothing to arguing with. It is like, you know,\\npunching jello, like there, there\\'s what do you even respond to? - So just put push back on that. I don\\'t think they have good metrics of when the film is happening. But I think it\\'s possible to have that. Like just as you speak now, I mean it\\'s possible to imagine\\nthere could be measures. - It\\'s been 20 years. - No, for sure. But it is been only weeks\\nsince we had a big enough breakthrough in language models. We can start to actually have this, the thing is the AI doer stuff didn\\'t have any actual systems to really work with. And now there\\'s real systems\\nyou can start to analyze like, how does this stuff go wrong? And I think you kind\\nof agree that there is a lot of risks that we can analyze. The benefits outweigh\\nthe risks in many cases. - Well, the risks are not existential. - [Lex] Yes. Well. - Not in the phone paper\\nclip. Let me, okay. There\\'s another slide of hand\\nthat you just alluded to. There\\'s another slide\\nof hand that happens, which is very interesting. - I\\'m very good at the\\nslide of hand thing. - Which is very not scientific. So the book Super Intelligence, right, which is like the Nick Bostrom\\'s book, which is like the origin\\nof a lot of this stuff, which was written, you know, whatever, 10 years ago or something. So he does this really\\nfascinating thing in the book, which is he basically says\\nthere are many possible routes to machine intelligence,\\nto artificial intelligence. And he describes all the different routes to artificial intelligence,\\nall the different possible, everything from biological\\naugmentation through to, you know, all these different things. One of the ones that\\nhe does not describe is large language models because of course the book was written\\nbefore they were invented. And so they didn\\'t exist. In the book, he describes them all and then he proceeds to treat them all as if they\\'re\\nexactly the same thing. He presents them all as sort\\nof an equivalent risk to be dealt with in an equivalent\\nway to be thought about the same way. And then the risk, the quote unquote risk that\\'s actually emerged is actually a completely different technology than he was even imagining. And yet all of his theories\\nand beliefs are being transplanted by this movement, like straight onto this new technology. And so again, like there\\'s no other area of science or technology\\nwhere you do that. Like when you\\'re dealing\\nwith like organic chemistry versus inorganic chemistry,\\nyou don\\'t just like say, oh, with respect to like either\\none, basically maybe, you know, growing up in eating\\nthe world or something, like they\\'re just gonna\\noperate the same way. Like you don\\'t. - But you can start talking about like, as we get more and more actual systems that start to get more\\nand more intelligent, you can start to actually have more scientific arguments here. - [Marc] Oh yeah. - Like, you know, high level, you can talk about the threat\\nof autonomous weapon systems back before we had any\\nautomation in the military. And that would be like\\nvery fuzzy kind of logic. But the more and more you\\nhave drones that are becoming more and more autonomous, you\\ncan start imagining, okay, what does that actually look\\nlike and what\\'s the actual threat of autonomous weapons systems? How does it go wrong? And still it\\'s very vague, but you start to get a\\nsense of like, all right, it should probably be illegal or wrong or not allowed\\nto do like mass deployment of fully autonomous drones that are doing aerial strikes. - [Marc] Oh no. - On large areas. - [Marc] I think it should be required. - Right? So that\\'s a no. - No, no. I think it should be required that only aerial vehicles are automated. - Okay. So you wanna go the other way? - I wanna go the other way. - So that, okay. - I think it\\'s obvious that\\nthe machine is gonna make a better decision than the human pilot. I think it\\'s obvious that\\nit\\'s in the best interest of both the attacker and the\\ndefender and humanity at large. If machines are making\\nmore of these decisions than not people, I think people make terrible\\ndecisions in times of war. - But like, there\\'s ways\\nthis can go wrong too, right? - Well, it wars go terribly wrong now. This goes back to the whole, this is that whole thing\\nabout like the self-drive. Does the self-driving\\ncar need to be perfect versus does it need to be\\nbetter than the human driver? Does the automated\\ndrone need to be perfect or does it need to be better than a human pilot at making decisions under enormous amounts of\\nstress and uncertainty? - Yeah, well, on average, the worry that AI folks\\nhave is the runaway. - They\\'re gonna come alive. Right? That then again, that\\'s\\nthe slight of hand, right. - Or not not come alive.\\nWell, no, hold on a second. You lose control as well. You lose control. - But then they\\'re gonna\\ndevelop goals of their own. They\\'re gonna develop a mind of their own, they\\'re gonna develop their own. Right. - No more, more like\\nChernobyl style meltdown, like just bugs in the code\\naccidentally, you know, force you like the results in the bombing of like large civilian areas. - [Marc] Okay. And to a degree that\\'s not possible in the current military strategies, - [Marc] I don\\'t know. - Control by humans. - Well, actually we\\'ve been\\ndoing a lot of mass bombings to cities for a very long time. - Yes. And a lot of civilians died. - And a lot of civilians died. And if you watch the documentary,\\nthe Fog of War McNamara, it spends a big part of it\\ntalking about the fire bombing of the Japanese cities. Burning them straight\\nto the ground. Right. The devastation in Japan, American military fire bombing\\nthe cities in Japan was considerably bigger devastation\\nthan the use of nukes. Right. So we\\'ve been doing\\nthat for a long time. We also did that to Germany, by the way Germany did that to us, right? Like that\\'s an old tradition. The minute we got airplanes, we started doing indiscriminate bombing. - So one of the things-- - [Marc] We\\'re still doing it. - The modern US military can do with technology with automation, but technology more broadly is higher and higher precision strikes. - Yeah, I was saying, so precision is obviously precision and this is a (indistinct) right? So there was this big advance this big advance called the (indistinct) which basically was\\nstrapping a GPS transceiver to an unguided bomb and turning it into a guided bomb. And yeah, that\\'s great. Like look, that\\'s been a big advance, but, and that\\'s like a baby\\nversion of this question, which is okay, do you\\nwant like the human pilot, like guessing where the bomb\\'s gonna land? Or do you want like the\\nmachine like guiding the bomb to his destination? That\\'s a baby version of the question. The next version of the question is, do you want the human\\nor the machine deciding whether to drop the bomb? Everybody just assumes the\\nhuman\\'s gonna do a better job for what I think are\\nfundamentally suspicious reasons. - Emotional, psychological reasons. - Yeah. I think it\\'s very clear\\nthat the machine\\'s gonna do a better job making that decision \\'cause the humans making\\nthat decision are got awful. Just terrible. - [Lex] Yeah. - Right. And so yeah. So this is the thing. And then let\\'s get to the, there was, can I one more slide of hand? - [Lex] Yes. - It was in-- - Sure. Please. I\\'m a magician. You could say. - One more slight of hand. These things are gonna be so smart, right? That they\\'re gonna be able to\\ndestroy the world and wreak havoc and like do all this\\nstuff and plan and do all this stuff and evade us and have\\nall their secret things and their secret factories\\nand all this stuff. But they\\'re so stupid that\\nthey\\'re gonna get like, tangled up in their code and that\\'s they\\'re not gonna come alive, but there\\'s gonna be some\\nbug that\\'s gonna cause them to like turn us all on a paper like that. They\\'re not gonna be\\ngenius in every way other than the actual bad goal. And it\\'s just like,\\nand that\\'s just like a, like ridiculous like discrepancy. And you can prove this today, you can actually address\\nthis today for the first time with LLMs which is you can actually ask LLMs to resolve moral dilemmas. So you can create the\\nscenario, you know, dot, dot, dot this, that, this, that, this, that. What would you as the AI\\ndo in the circumstance? And they don\\'t just\\nsay destroy all humans, destroy all humans. They will give you actually\\nvery nuanced moral, practical trade-off oriented answers. And so we actually already\\nhave the kind of AI that can actually like, think this through and can actually like, you know, reason about goals. - Well, the hope is that AGI or like various superintelligent systems have some of the\\nnuance that LLMs have and the intuition is they most likely will because even these LLMs have the nuance. - LLMs are really, this is\\nactually worth spending a moment on LLMs are really interesting to have moral conversations with. And that I just, I didn\\'t expect I\\'d be\\nhaving a moral conversation with the machine in my lifetime. - Wait, and let\\'s remember\\nwe\\'re not really having a conversation with the machine where we\\'re having a conversation with the entirety of the\\ncollective intelligence of the human species. - Exactly. Yes. Correct. - But it\\'s possible to imagine\\nautonomous weapons systems that are not using LLMs. - But if they\\'re smart enough to be scary, where are they not\\nsmart enough to be wise? Like, that\\'s the part where it\\'s like, I don\\'t know how you get\\nthe one without the other. - Is it possible to be super intelligent without being super wise? - Well, again, you\\'re back to that. I mean, then you\\'re back to\\na classic autistic computer, right? Like you\\'re back to just\\nlike a blind rule follower. I\\'ve got this like core,\\nit\\'s the paperclip thing. I\\'ve got this core rule and\\nI\\'m just gonna follow it to the end of the earth. And it\\'s like, well, but everything you\\'re gonna\\nbe doing execute that rule is gonna be super genius level\\nthat humans aren\\'t gonna be able to counter. It\\'s a mismatch in the definition of what the system\\'s capable of. - Unlikely but not impossible, I think. - But again, here you\\nget to like, okay, like. - No, I\\'m not saying when it\\'s\\nunlikely but not impossible. If it\\'s unlikely, that means the fear should be correctly calibrated. - Extraordinary claims\\nrequire extraordinary proof. - Well, okay, so one\\ninteresting sort of tangent, I would love to take on this\\nbecause you mentioned this in the essay about nuclear,\\nwhich was also, I mean, you don\\'t shy away from a\\nlittle bit of of a spicy take. So Robert Oppenheimer famously said, now I am become death\\nthe destroyer of worlds as he witnessed the first\\ndestination of a nuclear weapon on July 16th, 1945. And you write an interesting\\nhistorical perspective, \"Recall that John Van Neuman responded to Robert Oppenheimer\\'s famous\\nhand wringing about the role of creating nuclear weapons, which you note helped end World War II and prevent World War III\\nwith some people confess guilt to claim credit for the sin.\" And you also mentioned\\nthat Truman was harsher after meeting Oppenheimer. He said that \"Don\\'t let that\\ncry baby in here again.\" - Real quote, by the\\nway, from Dean Atchison. - Boy. - \\'Cause Oppenheimer didn\\'t\\njust say the famous line. - [Lex] Yeah. - He then spent years going\\naround basically moaning him, you know, going on TV and going into going into the White House and basically like, just like doing this hair shirt, you know, thing self, you know, this\\nsort of self-critical like, oh my god, I can\\'t believe how awful I am. - So he\\'s widely\\nconsidered perhaps of the, because of the hang ringing\\nas the father of the tom bomb. - [Marc] Yeah. - This is Van Norman\\'s criticism\\nof him is he tried to have his cake and eat it too. Like he wanted to and Van Norman of course a very different kind of personality and\\nhe\\'s just like, yeah, good. This is like an incredibly\\nuseful thing. I\\'m glad we did it. - Yeah. Well Van Norman is\\nis widely credit as being one of the smartest humans\\nof the 20th century. Certain people. Everybody says like, this is the smartest person I\\'ve ever met when they\\'ve met him. Anyway, that doesn\\'t mean,\\nsmart doesn\\'t mean wise. So yeah, I would love to sort of, can you make the case both\\nfor and against the critique of Oppenheimer here? \\'Cause we\\'re talking\\nabout nuclear weapons. Boy, do they seem dangerous? - Well so, the critique goes deeper and I left this out. Here\\'s the real substance, I left it out \\'cause I didn\\'t wanna dwell on nukes in my AI paper. But here\\'s the deeper thing that happened and I\\'m really curious, this\\nmovie coming out this summer, I\\'m really curious to see\\nhow far he pushes this. \\'cause this is the real\\ndrama in the story, which is, it wasn\\'t just a question\\nof our nukes, good or bad, it was a question of should\\nRussia also have them? And what actually happened was Russia got the American invented the bomb. Russia got the bomb, they got the bomb through espionage, they got American and you know, they got American scientists\\nand foreign scientists working on the American project. Some combination of the two\\nbasically gave the Russians the designs for the bomb. And that\\'s how the Russians got the bomb. There\\'s this dispute to this\\nday of Oppenheimer\\'s role in that if you read all the histories, the kind of composite\\npicture, and by the way, we now know a lot actually\\nabout Soviet espionage in that era \\'cause there\\'s been all this declassified\\nmaterial in the last 20 years that actually shows a lot\\nof very interesting things. But if you kinda read all the histories, which you kinda get is Oppenheimer\\nhimself probably was not he probably did not hand over\\nthe nuclear secrets himself. However, he was close\\nto many people who did. Including family members. And there were other members\\nof the Manhattan Project who were Russian, Soviet SS\\nand did hand over the bomb. And so the view of that\\nOppenheimer and people like him had that this thing is awful\\nand terrible and oh my god. And you know, all this stuff you could\\nargue fed into this ethos at the time that resulted\\nin people thinking that the Baptists thinking that the only principle\\nthing to do was to give the Russians the bomb. And so the moral beliefs on this thing and the public discussion and the role that the inventors\\nof this technology play, this is the point of this book, when they kind of take on this\\nsort of public intellectual, moral kind of thing, it can\\nhave real consequences, right? Because we live in a very\\ndifferent world today because Russia got the\\nbomb than we would\\'ve lived in had they not gotten the bomb right. The entire 20th century, second half of the 20th\\ncentury would\\'ve played out very different had those people\\nnot given Russia the bomb. And so the stakes were very high then. The good news today is\\nnobody\\'s sitting here today, I don\\'t think worrying about\\nlike an analogous situation with respect to like, I\\'m not really worried that\\nSam Altman\\'s gonna decide to give, you know, the\\nChinese, the design for AI, although he did just speak\\nat a Chinese conference, which is in interesting. But however, I don\\'t think\\nthat\\'s what\\'s at play here, but what\\'s at play here are\\nall these other fundamental issues around what do\\nwe believe about this and then what laws and\\nregulations and restrictions that we\\'re gonna put on it. And that\\'s where I draw\\nlike a direct straight line. And anyway, and my reading\\nof the history on nukes is like the people who were doing\\nthe full hair shirt public, this is awful. This is terrible. Actually had like\\ncatastrophically bad results from taking those views. And that\\'s what I\\'m worried\\nit\\'s gonna happen again. - But is there a case to be\\nmade that you really need to wake the public up to the dangers of nuclear weapons when\\nthey were first dropped? Like really like educate them on like, this is extremely dangerous\\nand destructive weapon. - I think the education\\nkind of happened quick and early, like-- - [Lex] How? - It was pretty obvious. - [Lex] How? - We dropped one bomb and\\ndestroyed an entire city. - Yeah. So 80,000 people dead. - [Marc] Yep. - But. - [Marc] And look. But-- - I don\\'t like the reporting of that. You can report that in all kinds of ways. - [Marc] Oh, there wars. - You can do all kinds of slants. Like war is horrible. War is terrible. You can do, you can make\\nit seem like nuclear, the use of nuclear weapons\\nis just a part of war and all that kind of stuff. Something about the\\nreporting and the discussion of nuclear weapons resulted\\nin us being terrified in awe of the power of nuclear weapons and that potentially fed\\nin a positive way towards the game theory of\\nmutual issue destruction. - Well, so this gets to what actually, let\\'s get to what actually happens. - [Lex] Some of us, me\\nplaying devil\\'s advocate here. - Yeah, yeah, sure. Of course. Let\\'s get to what\\nactually happened and then kind of back into that. So what actually happened, I believe, and again I think this is a\\nreasonable reading of history, is what actually happened\\nwas nukes then prevented World War III and they\\nprevented World War III through the game theory\\nof mutually assured destruction had nukes not existed. Right. There would\\'ve been no reason why the Cold War did not go hot. Right. And then there and then, you know, and the military planners\\nat the time, right, thought both on both sides\\nthought that there was gonna be World War III on the planes of Europe and they thought there was gonna be like a hundred million people dead. Right? It was like the most obvious\\nthing in the world to happen. Right? And it\\'s the dog\\nthat didn\\'t bark right? Like it may be like the best\\nsingle net thing that happened in the entire 20th century is\\nthat like that didn\\'t happen. - Yeah. Actually, just on that point, you say a lot of really brilliant things. It hit me just as you were saying it. I don\\'t know why it hit\\nme for the first time, but we got two wars in\\na span of like 20 years. Like we could have kept getting\\nmore and more world wars and more and more ruthless. It actually, you could have\\nhad a US versus Russia war. - You could, by the way you haven\\'t, there\\'s another hypothetical scenario. The other hypothetical scenario is that Americans got the\\nbomb, the Russians didn\\'t. Right? And then America\\'s the big dog and then maybe America\\nwould\\'ve had the capability to actually roll back the iron curtain. I don\\'t know whether\\nthat would\\'ve happened, but like it\\'s entirely possible. Right? And the act of these people who had these moral positions about, \\'cause they could\\nforecast, they could model, they could forecast the future\\nof how the technology would get used, made a horrific mistake. \\'cause they basically ensured that the iron curtain\\nwould continue for 50 years longer than it would\\'ve otherwise. Like, and again, like\\nthese are counter-factuals, I don\\'t know that that\\'s\\nwhat, what would\\'ve happened, but like the decision to hand the bomb over was a big decision made by people who were very\\nfull of themselves. - Yeah. But so me as an America, me as a person that loves America, I also wonder if US was the only ones with the nuclear weapons. - That was the argument for handing that was the guys who (indistinct) the guys who handed over the bomb. That was actually their moral argument. - Yeah. I would probably\\nnot hand it over to, I would be careful about the regimes. You hand it over to there, maybe give it to like\\nthe British or something, or like a democratically-elected\\ngovernment. - Well, look, there are people to this day who think that those bias Soviet spies did the right thing because\\nthey created a balance of terror as opposed\\nto the US having just, and by the way, let me-- - Balance of terror. - [Marc] Let\\'s tell the\\nfull version story has-- - Such a sexy ring to it. - Okay. So the full\\nversion of the story is John Van Norman is a hero\\nof both yours and mind. The full version of the\\nstory is he advocated for a first strike. So when the US had the\\nbomb and Russia did not, he advocated for, he said, we need to strike them right now. - Strike Russia. - [Marc] Yes. - Van Norman. - Yes, because he said\\nWorld War III is inevitable. He was very hardcore. His theory was World\\nWar III is inevitable. We\\'re definitely gonna have World War III. The only way to stop World War\\nIII is we have to take them out right now and we have\\nto take them out right now before they get the bomb. \\'Cause this is our last chance. Now again, like-- - Is this an example of\\nphilosophers and politics? - I don\\'t know if that\\'s in there or not, but this is in the standard. - No, but it is meaning is that. - Yeah, this is on the other side. So, most of the case studies, most of the case studies\\nin books like this are the crazy people on the left. Van Norman is a story arguably of the crazy people on the right. - Yes. Stick to computing, John. - Well. This is the thing, and this is the general principle. Getting back to our core\\nthing, which is like, I don\\'t know whether any of\\nthese people should be making any of these calls. Because there\\'s nothing in\\neither Van Norman\\'s background or Oppenheimer\\'s background or any of these people\\'s background that qualifies them as moral authorities. - Yeah. Well this actually\\nbrings up the point of, in AI, who are the good people to reason about the\\nmorality of the ethics, the outside of these risks, outside of like the more\\ncomplicated stuff that you, you agree on is, you know, this will go into the hands of bad guys and all the kinds of ways\\nthey\\'ll do is interesting and dangerous, is dangerous in interesting unpredictable ways. And who is the right person? Who are the right kinds of\\npeople to make decisions, how to respond to it?\\nOr is the tech people? - So the history of these fields, this is what he talks about in the book, the history of these fields,\\nis that the competence and capability and\\nintelligence and training and accomplishments of senior\\nscientists and technologists working on a technology\\nand then being able to then make moral judgments\\nin the use of that technology. That track record is terrible that track record is like\\ncatastrophically bad. The people-- - Just the linger, the people that develop that\\ntechnology are usually not going to be the right people. - Well why would they? So\\nthe claim is of course, they\\'re the knowledgeable ones. But the the problem is they\\'ve\\nspent their entire life in a lab. Right. They\\'re not theologians. Well, so what you find,\\nwhat you find when you read, when you read this, when\\nyou look at these histories, what you find is they generally are very thinly informed on history, on sociology, on theology,\\non morality, on ethics. They tend to manufacture their\\nown worldviews from scratch. They tend to be very sort of thin. They\\'re not remotely the\\narguments that you would be having if you got like a group of\\nhighly qualified theologians or philosophers or, you know. - Well, let me sort of,\\nas the devil\\'s advocate, takes a simple whiskey say\\nthat I agree with that. But also it seems like the\\npeople who are doing kind of the ethics departments and these tech companies\\ngo sometimes the other way. - [Marc] Yes, they\\'re definitely. - Which they\\'re not nuanced\\non history or theology or this kind of stuff. It almost becomes a kind\\nof outraged activism towards directions that don\\'t seem to be grounded in history and\\nhumility and nuance. It\\'s again, drenched with arrogance. So-- - [Marc] Definitely. - I\\'m not sure which is worse. - Oh no, they\\'re both bad. Yeah. So definitely not them either. - So, but I guess. - Well look, this is a hard. - Yeah, it\\'s a hard problem. - This is a hard problem. This goes back to where we started, which is, okay, who has the truth? And it\\'s like, well, you know, like how does societies\\narrive at like truth and how do we figure these things out and like our elected leaders\\nplay some role in it. You know, we all play some role in it. There have to be some set\\nof public intellectuals at some point that bring, you know, rationality and judgment\\nand humility to it. Those people are few and far between. We should probably prize them very highly. - Yeah. So celebrate humility\\nin our public leaders. So getting to risk number two, will AI ruin our society\\nshort version as you write, if the murder robots don\\'t\\nget us the hate speech and misinformation will. And the action you recommend in short, don\\'t let the thought police suppress AI. Well what is this risk of\\nthe effect of misinformation of society that\\'s going\\nto be catalyzed by AI? - Yeah, so this is the social media, this is what you just alluded to. It\\'s the activism kind\\nof thing that\\'s popped up in these companies in the industry. And it\\'s basically, from my perspective, it\\'s basically part two\\nof the war that played out over social media over the last 10 years, \\'cause you probably remember\\nsocial media 10 years ago, was basically who even wants this? Who wants a photo of what\\nyour cat had for breakfast? Like, this stuff is like silly and trivial and why can\\'t these nerds like figure out how to invent something\\nlike useful and powerful? And then, you know, certain things happened\\nin the political system. And then it sort of, the polarity on that\\ndiscussion switched all the way to social media is like\\nthe worst, most corrosive, most terrible, most awful\\ntechnology ever invented. And then it leads to, you\\nknow, terrible of the wrong, you know, politicians and\\npolicies and politics and like, and all this stuff. And that all got catalyzed into\\nthis very big kind of angry movement both inside and\\noutside the companies to kind of bring social media to heal. And that got focused in\\nparticularly on two topics, so-called hate speech and\\nso-called misinformation. And that\\'s been the saga playing out for the last decade. And I', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 532341, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content='. And it\\'s basically, from my perspective, it\\'s basically part two\\nof the war that played out over social media over the last 10 years, \\'cause you probably remember\\nsocial media 10 years ago, was basically who even wants this? Who wants a photo of what\\nyour cat had for breakfast? Like, this stuff is like silly and trivial and why can\\'t these nerds like figure out how to invent something\\nlike useful and powerful? And then, you know, certain things happened\\nin the political system. And then it sort of, the polarity on that\\ndiscussion switched all the way to social media is like\\nthe worst, most corrosive, most terrible, most awful\\ntechnology ever invented. And then it leads to, you\\nknow, terrible of the wrong, you know, politicians and\\npolicies and politics and like, and all this stuff. And that all got catalyzed into\\nthis very big kind of angry movement both inside and\\noutside the companies to kind of bring social media to heal. And that got focused in\\nparticularly on two topics, so-called hate speech and\\nso-called misinformation. And that\\'s been the saga playing out for the last decade. And I don\\'t even really want\\nto even argue the pros and cons of the sides just to observe that\\'s been like a huge fight and has had, you know, big consequences to how\\nthese companies operate. Basically that same, those\\nsame sets of theories, that same activist approach, that same energy as being\\ntransplanted straight to AI. And you see that already happening. It\\'s why, you know, ChatGPT will answer, let\\'s say certain\\nquestions and not others. It\\'s why it gives you the\\ncanned speech about, you know, whenever it starts with,\\nas a large language model, I cannot, you know, basically means that somebody\\nhas reached in there and told that it can\\'t talk about certain topics. - Do you think some of that is good? - So it\\'s an interesting question. So a couple observations. So, one is the people who find this the most frustrating are the people who are worried\\nabout the murder robots, right? So, and in fact so called\\nX risk people, right? They started with the term AI safety, the term became AI alignment. When the term became AI alignment is when this switch happened\\nfrom we\\'re worried it\\'s gonna kill us all to\\nwe\\'re worried about hate speech and misinformation. - [Lex] Sure. - The AI X risk people have\\nnow renamed their thing AI not kill everyone-ism, which I have to admit is a catchy term. And they are very frustrated\\nby the fact that the hate speech sort of activist driven\\nhate speech misinformation kind of thing is taking over. Which is what\\'s happened is taken over, the AI ethics field has been\\ntaken over by the hate speech misinformation people. You know, look, would I like to live in a world\\nin which like everybody was nice to each other all the\\ntime and nobody ever said anything mean and nobody ever\\nused a bad word and everything was always accurate and honest. Like, that sounds great. Do I wanna live in a world\\nwhere there\\'s like a centralized thought police working through\\nthe tech companies to enforce the view of a small set of\\nelites that they\\'re gonna determine what the rest\\nof us think and feel like? Absolutely not. - There could be a middle\\nground somewhere like Wikipedia type of moderation. There\\'s moderation of Wikipedia\\nthat is somehow crowdsourced where you don\\'t have centralized elites, but it\\'s also not completely\\njust a free for all because if you have the\\nentirety of human knowledge at your fingertips, you\\ncan do a lot of harm. Like if you have a good assistant that\\'s completely uncensored, they can help you build a bomb, they can help you mess with\\npeople\\'s physical wellbeing. Right. If they, because that information is\\nout there on the internet and so presumably there\\'s, it would be, you could see the positives\\nin censoring some aspects of an AI model when it\\'s helping you\\ncommit literal violence. - Yeah. And there\\'s a section\\nlater section of the essay where I talk about bad\\npeople doing bad things. - [Lex] Yes. - Right. Which and there\\'s this, there\\'s a set of things that\\nwe should discuss there. - [Lex] Yeah. - What happens in practice is these lines, as you alluded to this already, these lines are not easy to draw. And what I\\'ve observed in\\nthe social media version of this is like, the way I describe it as the slippery slope is not a fallacy, it\\'s an inevitability. The minute you have this kind\\nof activist personality that gets in a position to make these decisions they take it straight to infinity. Like, it goes into the crazy\\nzone like almost immediately and never comes back because\\npeople become drunk with power. Right. And look, if you\\'re\\nin the position to determine what the entire world thinks and feels and reads and says like, you\\'re gonna take it and you\\nknow, Elon has, you know, ventilated this with the\\nTwitter files over the last, you know, three months and\\nit\\'s just like crystal clear, like how bad it got there now. - [Lex] Yeah. - Reason for optimism\\nis what Elon is doing with community notes. So community notes is actually\\na very interesting thing. So, what Elon is trying to\\ndo with community notes is he\\'s trying to have it where\\nthere\\'s only a community note when people who have previously\\ndisagreed on many topics agree on this one. - Yes, that\\'s what I\\'m\\ntrying to get at is like, there could be Wikipedia like\\nmodels or community notes type of models where allows you\\nto essentially either provide context or sensor in a\\nway that\\'s not resist the slippery slope nature. Power. - Now there\\'s an entirely\\ndifferent approach here, which is basically we have AIs\\nthat are producing content. We could also have ais that\\nare consuming content. Right? And so one of the things that\\nyour assistant could do for you is help you consume\\nall the content, right? And basically tell you\\nwhen you\\'re getting played. So for example, I\\'m gonna\\nwant the AI that my kid uses, right, to be very, you know, child safe and I\\'m gonna want\\nit to filter for him all kinds of inappropriate stuff that\\nhe shouldn\\'t be saying just \\'cause he\\'s a kid. Right? And you see what I\\'m saying\\nis you can implement that. The architectural, you\\ncould say you can solve this on the client side, right? You solving on the server\\nside gives you an opportunity to dictate for the entire\\nworld, which I think is where you take the slippery slope to hell, there\\'s another architectural approach, which is to solve this on the client side, which is certainly what I would endorse. - It\\'s AI risk number five, will AI lead to bad\\npeople doing bad things? And I can just imagine language\\nmodels used to do so many bad things, but the hope is there that\\nyou can have large language models used to then defend\\nagainst it by more people, by smarter people, by more\\neffective people, skilled people, all that kind of stuff. - Three-part argument on\\nbad people doing bad things. So, number one, right? You can use the technology defensively and we should be using AI\\nto build like broad spectrum vaccines and antibiotics for\\nlike bio weapons and we should be using AI to like hunt terrorists and catch criminals and like, we should be doing like all\\nkinds of stuff like that. And in fact, we should be doing those things even just to like go get like, you know, basically go eliminate risk\\nfrom like regular pathogens that aren\\'t like constructed by an AI. So there\\'s the whole\\ndefensive set of things. Second is we have many laws on the books about as actual bad things, right? So it is actually illegal\\nto be a criminal, you know, to commit crimes, to commit\\nterrorist acts to, you know, build pathogens with the intent to deploy them to kill people. And so we have those, we actually don\\'t need new laws for the vast majority of these scenarios. We actually already have the\\nlaws in the book, on the books. The third argument is the minute, and this is sort of the\\nfoundational one that gets really tough, but the minute\\nyou get into this thing, which you were kind of getting\\ninto, which is like, okay, but like, don\\'t you need\\ncensorship sometimes, right? And don\\'t you need restrictions sometimes? It\\'s like, okay, what is the cost of that? And in particular in the\\nworld of open source, right? And so is open source AI\\ngoing to be allowed or not? If open source AI is not allowed, then what is the regime that\\'s\\ngoing to be necessary legally and technically to prevent\\nit from developing? Right? And here again is where you\\nget into and people have proposed that these kinds of things. You get into I would say pretty extreme territory pretty fast. Do we have a monitor\\nagent on every CPU and GPU that reports back to the government? What we\\'re doing with our computers, are we seizing GPU clusters\\nthat get beyond a certain size? Like, and then by the way, how are we doing all that globally, right? And like if China\\'s developing\\nan LLM beyond the scale that we think is allowable,\\nare we gonna invade? Right. And you have figures on the AI X risk side who are advocating any, you know, potentially up to nuclear\\nstrikes to prevent, you know, this kind of thing. And so here you get into this thing and again, you know, maybe\\nyou could maybe say this is, you know, you could even\\nsay this is what good, bad or indifferent or whatever. But like here\\'s the comparison of nukes, the comparison of nukes is very dangerous because one is just nukes, were just, although we can\\ncome back to nuclear power. But the other thing was like with nukes, you could control plutonium, right? You could track plutonium and\\nit was like hard to come by. AI is just math and code, right? And it\\'s in like math\\ntextbooks and it\\'s like, there are YouTube videos that\\nteach you how to build it. And like there\\'s open source,\\nthere\\'s already open source. You know, there\\'s a 40 billion parameter\\nmodel running around already called Falcon Online that\\nanybody can download. And so, okay, you walk down the logic path\\nthat says we need to have guardrails on this. And you find yourself in an authoritarian, totalitarian regime of thought\\ncontrol and machine control that would be so brutal\\nthat you would\\'ve destroyed the society that you\\'re trying to protect. And so I just don\\'t see\\nhow that actually works. - So yeah, you have to\\nunderstand my brain\\'s going full steam ahead here \\'cause I agree with basically\\neverything you\\'re saying, but I\\'m trying to play\\ndevil\\'s advocate here because okay, you\\'re highlighted the fact that there is a slippery\\nslope to human nature. The moment you censor something, you start to censor everything. That alignment starts out sounding nice, but then you start to align to the beliefs of some select group of people. And then it\\'s just your beliefs the number of people\\nyou\\'re aligning to smaller and smaller as that group\\nbecomes more and more powerful. Okay. But that just speaks to the people that censor are usually the assholes and the assholes get richer. I wonder if it\\'s possible\\nto do without that for AI. One way to ask this question is do you think the base models, the baseline foundation\\nmodels should be open sourced? Like, where Marc Zuckerberg\\nis saying they want to do. - So look, I mean I think\\nit\\'s totally appropriate the companies that are in the business of producing a product or service should be\\nable to have a wide range of policies that they put, right? And I\\'ll just, again, I want a heavily censored\\nmodel for my eight year old. Like, I actually want that, like, like I would pay more money\\nfor the ones more heavily censored than the one that\\'s not, right. And so, like there are certainly scenarios where companies will make that decision. Look, an interesting thing you brought up or is this really a speech issue? One of the things that the\\nbig tech companies are dealing with is that content generated\\nfrom an LLM is not covered under section 230, which is the law that protects\\ninternet platform companies from being sued for the\\nuser generated content. And so it is actually-- - [Lex] Oh, wow. - Yes and so there, there\\'s\\nactually a question. I think there\\'s still a question, which is can big American companies actually feel generative AI at all? Or is the liability actually\\ngonna just ultimately convince them that they can\\'t do it? Because the minute the\\nthing says something bad, and it doesn\\'t even\\nneed to be hate speech, it could just be like an\\n(indistinct) it could hallucinate a product, you know, detail\\non a vacuum cleaner, you know, and all of a sudden the\\nvacuum cleaner company sues for misrepresentation. And there\\'s asymmetry there, right? \\'Cause the LLMs gonna be\\nproducing billions of answers to questions and it only needs\\nto get a few wrong to have. - [Lex] So, loss has to get\\nupdated really quick here. - Yeah. And nobody knows\\nwhat to do with that, right? So, so anyway, like there are big, there are big questions around\\nhow companies operate at all. So we talk about those, but then there\\'s this other\\nquestion of like, okay, the open source. So what about open source? And my answer to your\\nquestion is kind of like, obviously yes, the models have, there has to be full open\\nsource here because to live in a world in which that\\nopen source is not allowed is a world of draconian speech control, human control, machine control. I mean, you know, black helicopters with\\njackbooted thugs coming out, repelling down and seizing\\nyour GPU like territory. - [Lex] Well. - No, no, I\\'m a hundred percent serious. - That\\'s you\\'re saying slippery\\nslope always leads there. - No, no, no, no. That\\'s what\\'s required to enforce it. Like how will you enforce a\\nban on open source and AI? - No. Well you could add friction to it, like harder to get the models. \\'Cause people will always\\nbe able to get the models, but it\\'ll be more in the shadows, right? - The leading open source model\\nright now is from the UAE. Like the next time they\\ndo that, what do we do? - [Lex] Yeah. - Like. - Oh, I see you\\'re like. - A 14 year old in Indonesia\\ncomes out with a breakthrough. You know, we talked about most great\\nsoftware comes from a small number of people. Some kid comes out with\\nsome big new breakthrough and quantization or something and has some huge breakthrough. And like, what are we gonna like, invade Indonesia and arrest him? - It seems like in terms of\\nsize of models and effectiveness of models, the big tech companies will\\nprobably lead the way for quite a few years and the question is of what policies they should use? The kid in Indonesia\\nshould not be regulated, but should Google, Meta,\\nMicrosoft, Open AI be regulated? - Well, so, but this goes, okay, so when does it become dangerous? Right. Is the danger that it\\'s as powerful as the current leading commercial model? Or it is just at some\\nother arbitrary threshold? And then by the way, like\\nlook, how do we know, like what we know today is that\\nyou need like a lot of money to like train these things. But there are advances being\\nmade every week on training efficiency and, you know,\\ndata, all kinds of synthetic, you know, look, I don\\'t even like the synthetic data thing we\\'re talking about. Maybe some kid figures out a way to auto-generate synthetic data. - [Lex] That\\'s gonna change everything. - Yeah, exactly. And so like sitting here today, like, the breakthrough just happened, right? You made this point like the\\nbreakthrough just happened. So we don\\'t know what the shape of this technology is gonna be. I mean the big shock\\nhere is that, you know, whatever number of billions\\nof parameters basically represents at least a very big\\npercentage of human thought. Like who would\\'ve imagined that? And then there\\'s already work underway. There was just this paper that\\njust came out that basically takes a gpt three scale model\\nand compresses it down or run on a single 32 core CPU. Like who would\\'ve predicted that? - [Lex] Yeah. - You know, some of these models now you\\ncan run on raspberry pies like today they\\'re very slow,\\nbut like, you know, maybe they\\'ll be a, you know, perceived you have real perform, you know, like it\\'s math and code. And here we\\'re back in here, we\\'re back in, dude, it\\'s math and code. It\\'s math and code, it\\'s\\nmath, code and data. It\\'s bits. - Marc has just like\\nwalked away at this point. You just screw it. I don\\'t know what to do with this. You guys created this\\nwhole internet thing. Yeah, yeah. I mean, I\\'m a huge believer\\nin open source here. - So my argument is we\\'re gonna have, see here\\'s my argument is a, my argument, my full argument is, is AI is gonna be like air,\\nit\\'s gonna be everywhere. Like this is just gonna be in text. It already is, it\\'s gonna be in textbooks\\nand kids are gonna grow up knowing how to do this. And\\nit\\'s just gonna be a thing. It\\'s gonna be in the air\\nand you can\\'t like pull this back anymore. You can\\'t pull back air. And so you just have to figure out how to live in this world, right? And then that\\'s where I think\\nlike all this hand ringing about AI risk is basically\\na complete waste of time, \\'cause the effort should go into okay, what is the defensive approach? And so if you\\'re worried about you know, AI generated pathogens, the\\nright thing to do is to have a permanent project warp speed, right? Funded lavishly. Let\\'s do a Manhattan, let\\'s\\ntalk about Manhattan project, let\\'s do a Manhattan project\\nfor biological defense, right? And let\\'s build ais and let\\'s\\nhave like broad spectrum vaccines where like, we\\'re\\ninsulated from every pathogen. - And well, the interesting\\nthing is because it\\'s software, a kid in his basement, teenager could build like a\\nsystem that defends against like the worst, I mean, and to me\\ndefense is super exciting. It\\'s like, if you believe\\nin the good of human nature for that, most people wanna do good, to be the savior of\\nhumanity is really exciting. - Yes. - Not, okay, that\\'s a dramatic statement. But to help people. - Yeah, of course. Help people. - Yeah. Okay. What about just the jump around, what about the risk of will AI\\nlead to crippling inequality? You know, \\'cause we\\'re kind of saying\\neverybody\\'s life will become better. Is it possible that the\\nrich get richer here? - Yeah, so this goes, this actually ironically\\ngoes back to Marxism. So \\'cause this was the, so the\\ncore claim of Marxism, right? Basically was that the owner, the owners of capital would basically own the means of production. And then over time they\\nwould basically accumulate all the wealth the workers\\nwould be paying in, you know, and getting nothing in return \\'cause they wouldn\\'t be\\nneeded anymore, right? Marx was very worried\\nabout mech what he called mechanization or what later\\nbecame known as automation. And that, you know, the workers would be immiserated\\nand the the capitalists would end up with all. And so this was one of the\\ncore principles of Marxism. Of course it turned out to\\nbe wrong about every previous wave of technology. The reason it, it turned out to be wrong about every previous wave of technology is that the way that the\\nself-interested owner of the machines makes the most money is by providing the production capability in the form of products and services to the most people, the most\\ncustomers as possible, right? The the largest, and this is one of those funny\\nthings where every CEO knows this intuitively, and yet it\\'s like hard to\\nexplain from the outside the way you make the most\\nmoney in any business is by selling to the largest\\nmarket you can possibly get to. The largest market you can\\npossibly get to is everybody on the planet. And so every large company\\ndoes is everything that it can to drive down prices, to\\nbe able to get volumes up, to be able to get to\\neverybody on the planet. And that happened with\\neverything from electricity, it happened with telephones,\\nit happened with radio, it happened with automobiles,\\nit happened with smartphones, it happened with PCs, it\\nhappened with the internet, it happened with mobile broadband. It\\'s happened by the way, with Coca-Cola. It\\'s happened with like every, you know, basically every industrially\\nproduced, you know, good or service people, you wanna drive it to the\\nlargest possible market. And then as proof of that,\\nit\\'s already happened, right? Which is the early\\nadopters of like ChatGPT and Bing are not like, you\\nknow, Exxon and Boeing. They\\'re, you know, your\\nuncle and your nephew, right? It\\'s just like free. It\\'s either freely available\\nonline or it\\'s available for 20 bucks a month or something. But the, you know, these things went this technology went mass market immediately. And so look, the owners of\\nthe means of production, the whoever does this now mentioned these trillion dollar questions. There are people who are gonna\\nget really rich doing this, producing these things, but they\\'re gonna get\\nreally rich by taking this technology to the\\nbroadest possible market. - So yes, they\\'ll get rich, but they\\'ll get rich having\\na huge positive impact on. - Yeah, making the technology\\navailable to everybody. Right. And again, smartphone, same thing. So there\\'s this amazing kind\\nof twist in business history, which is you cannot spend\\n$10,000 on a smartphone, right? You can\\'t spend a\\nhundred thousand dollars, you can\\'t spend a million, like I would buy the\\nmillion dollars smartphone. Like I\\'m signed up for it. Like if it\\'s like, suppose a million dollar\\nsmartphone was like much better than the thousand dollar smartphone. Like I\\'m there to buy\\nit, it doesn\\'t exist. Why doesn\\'t it exist? Apple makes so much more\\nmoney driving the price further down from a thousand dollars than they would trying to harvest, right? And so it\\'s just this\\nrepeating pattern you see over and over again where and\\nwhat\\'s great about it is you, you do not need to rely on\\nanybody\\'s enlightened right? Generosity to do this. You just need to rely on\\ncapitalist self-interest. - What about AI taking our jobs? - Yeah. So very very similar thing here. There\\'s sort of a, there\\'s a\\ncore fallacy which again was very common in Marxism, which is what\\'s called\\nthe lump of labor fallacy. And this is sort of the\\nfallacy that there is only a fixed amount of work\\nto be done in the world. And it\\'s all being done today by people and then if machines do it, there\\'s no other work\\nto be done by people. And that\\'s just a\\ncompletely backwards view on how the economy develops and grows. Because what happens is not\\nin fact that what happens is the introduction of technology\\ninto production process causes prices to fall. As prices fall, consumers\\nhave more spending power. As consumers have more spending power, they create new demand. That new demand then causes\\ncapital and labor to form into new enterprises to\\nsatisfy nuance and needs. And the result is more\\njobs at higher wages. - So nuance and needs, the\\nworries that the creation of nuance and needs at\\na rapid rate will mean there\\'s a lot of turnover in jobs. So people will lose jobs. Just the actual experience\\nof losing a job and having to learn new things and\\nnew skills is painful for the individuals. - Well, two things. One is the new jobs are often much better. So this actually came up\\nthat there was this panic about a decade ago and all\\nthe truck drivers are gonna lose their jobs, right? And number one, that didn\\'t happen \\'cause\\nwe haven\\'t figured out a way to actually finish that yet. But the other thing was\\nlike, look, truck driver, like I grew up in a town\\nthat was basically consisted of a truck stop, right? And I like knew a lot of truck drivers and like truck drivers\\nlive a decade shorter than everybody else. Like, it\\'s actually like a very dangerous, like, they get, like literally they have like\\nhigher rates of skin cancer and on the left side of their, on the left side of their body from being in the sun all the time. The vibration of being\\nin the truck is actually very damaging to your physiology. - And there\\'s actually perhaps partially because of that reason there\\'s a shortage of people who wanna be truck drivers. - Yeah. Like, it\\'s not like\\nthe question always you wanna ask somebody like that\\nis, do you want, you know, do you want your kid to be doing this job? And like most of them will tell you no. Like, I want my kid to\\nbe sitting in a cubicle somewhere like where they\\ndon\\'t have this, like, where they don\\'t die 10 years earlier. And so, the new jobs, number one, the new jobs are often better, but you don\\'t get the new\\njobs until you go through the change. And then to your point,\\nthe training thing, you know, is always the\\nissue is can people adapt? And again, here you need to imagine living in a world in which everybody has the AI\\nassistant capability, right? To be able to pick up new\\nskills much more quickly and be able to have some, you know, be able to have a machine to work with to augment their skills. - It\\'s still gonna be painful, but that\\'s the process of life. - It\\'s painful for some people.\\nI mean there\\'s no, like, there\\'s no question it\\'s\\npainful for some people and they\\'re, you know, they\\'re yes, it\\'s not, again, I\\'m not a utopian on\\nthis and it\\'s not like, it\\'s positive for everybody in the moment, but it has been overwhelmingly\\npositive for 300 years. I mean, look, the concern\\nhere, the concern, this concern has played out for literally centuries and you know, this is the sort of Luddite, you know, the story of the Luddites\\nthat you may remember, there was a panic in the two\\nthousands around outsourcing was gonna take all the jobs. There was a panic in the 2010s\\nthat robots were gonna take all the jobs. In 2019 before COVID we had more jobs at higher wages both in the country and in the world than at any point in human history. And so the overwhelming evidence\\nis that the net gain here is like, just like wildly positive. And most people like overwhelmingly\\ncome out the other side being huge beneficiaries of this. - So you write that the\\nsingle greatest risk, this is the risk you\\'re most convinced by the single greatest risk of AI is that China wins global AI dominance and we the United States\\nand the West do not. Can you elaborate? - Yeah. So this is the\\nother thing which is a lot of this sort of AI\\nrisk debates today sort of assume that we\\'re the\\nonly game in town, right? And so we have the ability to kind of sit in the United States and\\ncriticize ourselves and do, you know, have our\\ngovernment like, you know, beat up on our companies\\nand we\\'ll figure out a way to restrict what our\\ncompanies can do and you know, we\\'re gonna, you know,\\nwe\\'re gonna ban this and ban that, restrict this and do that. And then there\\'s this like\\nother like force out there that like doesn\\'t\\nbelieve we have any power over them whatsoever and they\\nhave no desire to sign up for whatever rules we\\ndecide to put in place and they\\'re gonna do whatever\\nit is they\\'re gonna do. And we have no control over it at all. And it\\'s China and specifically\\nthe Chinese Communist party and they have a completely\\npublicized open, you know, plan for what they\\'re gonna do with AI. And it is not what we have in mind. And not only do they have that as a vision and a plan for their society, but they also have it as a vision and plan for the rest of the world. - So their plan is what? Surveillance? - Authoritarian control. So authoritarian population\\ncontrol you know, good old-fashioned communist\\nauthoritarian control and surveillance and enforcement\\nand social credit scores and all the rest of it. And you are gonna be monitored and metered within an inch of everything all the time. And it\\'s gonna, you know, it\\'s basically the end of human freedom and that\\'s their goal. And you know, they justify it on the basis of that\\'s what leads to peace. - You\\'re worried that the regulating in the United States\\nwill haul progress enough to where the Chinese\\ngovernment would win that race. - So their plan, yeah. Yes, yes. And the reason for that\\nis they, and again, they\\'re very public on this. They have, their plan is to proliferate their approach around the world and they have this program called the Digital Silk Road, right. Which is building on their\\nSilk Road investment program. And they\\'ve got, they\\'ve been laying\\nnetworking infrastructure all over the world with their 5G, right. Work with their company Huawei. And so, they\\'ve been\\nlaying all this fabric, but financial and technological\\nfabric all over the world. And their plan is to roll out their vision of\\nAI on top of that and to have every other country be\\nrunning their version. And then if you\\'re a\\ncountry prone to, you know, authoritarianism, you\\'re\\ngonna find this to be an incredible way to\\nbecome more authoritarian. If you\\'re a country, by the way, not prone to authoritarianism, you\\'re gonna have the Chinese\\nCommunist Party running your infrastructure and having\\nbackdoor into it. Right. Which is also not good. - What\\'s your sense of where\\nthey stand in terms of the race towards super intelligence as\\ncompared to the United States? - Yeah, so good news is they\\'re behind, but bad news is they, you know, let\\'s just say they get\\naccess to everything we do. So they\\'re probably a year\\nbehind at each point in time, but they get, you know, downloads I think of\\nbasically all of our work on a regular basis through\\na variety of means. And they are, you know,\\nat least we\\'ll see, they\\'re at least putting\\nout reports of very, they just put out a report last week of a GPT 3.5 analog. They put out this report,\\nforget what it\\'s called, but they put out this\\nreport of this and they did and they, you know, the\\nway when open AI you know, puts out, one of the ways they test, you know, GPT they run it through standardized\\nexams like the SAT. Right. Just how you can kind of\\ngauge how smart it is. And so the Chinese report, they ran their LLM through\\nthe Chinese equivalent of the SAT and it includes\\na section on Marxism and a section on, I say tongue of thought. And it turns out their AI does very well on both of those topics. - That\\'s right. - So like. - Oh, this alignment thing. - Communist AI, right? Like literal communist AI. Right? And so their vision is\\nlike, that\\'s the, you know, so you know, you can just\\nimagine like you\\'re a school, you know, you\\'re a kid 10\\nyears from now in Argentina or in Germany or in who\\nknows where, Indonesia. And you ask the AI, I\\'d explain to you like\\nhow the economy works and it gives you the most cheery, upbeat explanation of\\nChinese style communism you\\'ve ever heard. Right. So like the stakes here\\nare like really big. - Well, as we\\'ve been talking about, my hope is not just\\nwith the United States, but with just the kid in his basement. The open source LLM. \\'Cause I don\\'t know if I trust large centralized institutions\\nwith super powerful AI no matter what their\\nideology as a power corrupts. You\\'ve been investing in\\ntech companies for about, let\\'s say 20 years. And about 15 of which was\\nwith Andreessen Horowitz. What interesting trends\\nin tech have you seen over that time? Let\\'s just talk about companies\\nand just the evolution of the tech industry. - I mean the big shift over 20\\nyears has been that tech used to be a tools industry for\\nbasically from like 1940 through to about 2010, almost all the big successful\\ncompanies were pick and shovels companies. So PC, database, smartphone, you know, some tool that somebody\\nelse would pick up and use. Since 2010, most of the big\\nwins have been in applications. So a company that starts you know, starts in an existing\\nindustry and goes directly to the customer in that industry. And you know, the earliest examples there\\nwere like Uber and Lyft and Airbnb. And then that model is\\nkind of elaborating out. The AI thing is actually a\\nreversion on that for now \\'cause like most of the AI\\nbusiness right now is actually in cloud provision of AI APIs\\nfor other people to build on. - But the big thing\\nwill probably be in app. - Yeah. I think most of the\\nmoney I think probably will be in whatever your AI financial advisor or your AI doctor or your\\nAI lawyer or, you know, take your pick of whatever the domain is. And there, and what\\'s\\ninteresting is, you know, the valley kind of does everything. The entrepreneurs kind of\\nelaborate every possible idea. And so there will be a set of\\ncompanies that like make AI something that can be purchased\\nand used by large law firms and then there will be other\\ncompanies that just go direct to market as an AI lawyer. - What advice could you\\ngive for a startup founder? Just haven\\'t seen so many\\nsuccessful companies, so many companies that fail also, what advice could you\\ngive to a startup founder, someone who wants to build the\\nnext super successful startup in the tech space? The Googles,\\nthe Apples, the Twitters. - Yeah. So the great thing about the really great founders is they don\\'t take any advice. So, if you find yourself\\nlistening to advice, maybe you shouldn\\'t do it. - But that\\'s actually,\\njust to elaborate on that, if you could also speak\\nto great founders too. Like what makes a great founder? - So what makes a great\\nfounder is super smart, coupled with super energetic,\\ncoupled with super courageous. I think it\\'s some of those three and-- - Intelligence, passion and courage. - The first two are traits\\nand the third one is a choice. I think courage is a choice. Well \\'cause courage is a question\\nof pain tolerance, right? So how many times are you\\nwilling to get punched in the face before you quit? And here\\'s maybe the biggest\\nthing people don\\'t understand about what it\\'s like to be\\na startup founder is it gets very romanticized, right? And even when it, even when they fail, it still gets romanticized about like what a great adventure it was. But like the reality of it is\\nmost of what happens is people telling you no and then\\nthey usually follow that with you\\'re stupid, right. No, I will not come to work for you. I will not leave my cushy job at Google to come work for you. No, I\\'m not gonna buy your\\nproduct, you know, no, I\\'m not gonna run a\\nstory about your company. No, I\\'m not this, that, the other thing. And so a huge amount of what\\npeople have to do is just get used to just getting punched and the reason people\\ndon\\'t understand this is because when you\\'re a founder, you cannot let on that this is happening \\'cause it will cause people to think that you\\'re weak and\\nthey\\'ll lose faith in you. So you have to pretend that\\nyou\\'re having a great time when you\\'re dying inside, right? You\\'re just in misery. - But why did they do it? - Why did they do? Yeah, that\\'s the thing. It\\'s like it is a level, this is actually one of\\nthe conclusions I think is that I think it\\'s actually\\nfor most of these people on a risk adjusted basis, it\\'s\\nprobably an irrational act. They could probably be\\nmore financially successful on average if they just\\ngot like a real job in at a big company. But there\\'s, you know, some people just have an\\nirrational need to do something new and build something for\\nthemselves and, you know, some people just can\\'t\\ntolerate having bosses. Oh, here\\'s the fun thing is how do you reference\\ncheck founders, right? So you call the, you know, normal way you reference check, you\\'re hiring somebody\\nis you call the bosses, they\\'re their, and you know, and you find out if\\nthey were good employees and now you\\'re trying to\\nreference check Steve Jobs, right? And it\\'s like, oh God, he was terrible. You know, he was a terrible employee. He never did what we told him to do. - So what\\'s a good reference? Do you want the previous\\nboss to actually say they never did what you told him to do? That might be a good thing. - Well, ideally what\\nyou want is I will go, I would like to go to\\nwork for that person. He worked for me here and\\nnow I\\'d like to work for him. No, unfortunately, most\\npeople can\\'t, their egos can\\'t handle that. So they won\\'t say that. But that\\'s the ideal. - What advice would\\nyou give to those folks in the space of intelligence,\\npassion and courage? - So I think the other big thing\\nis you see people sometimes who say, I wanna start a company and then they kind of\\nwork through the process of coming up with an idea. And generally those don\\'t\\nwork as well as the case where somebody has the idea first and then they kind of realize that there\\'s an opportunity\\nto build a company and then they just turn\\nout to be the right kind of person to do that. - When you say idea, do you\\nmean long-term big vision or do you mean specifics of like product? - Specific I would say specific, like specifically what specifics. Like what is the, because for the first five years you don\\'t get to have vision, you just gotta build something people want and you gotta figure out a\\nway to sell it to them. Right. It\\'s very practical or you\\nnever get to big vision. - So the first product, you have an idea of a set of\\nproducts of the first product that can actually make some money. - Yeah. Like it\\'s gotta work. The first product\\'s gotta\\nwork by which I mean like, it has to technically work, but then it has to actually\\nfit into the category and the customer\\'s mind if\\nsomething that they want and then by the way, the other part is they have\\nto be willing to pay for it. Like somebody\\'s gotta pay the bills. And so you\\'ve gotta\\nfigure out how to price it and whether you can\\nactually extract the money. So usually it is much more predictable. Success is never predictable, but it\\'s more predictable if\\nyou start with a great idea and then back into starting the company. So this is what we did,\\nyou know, we had most, before we had escape, the Google guys had the\\nGoogle search engine working at Stanford. Right. You know, yeah. Actually there\\'s tons of\\nexamples where they, you know, Pierre Omaira had eBay working before he left his previous job. - So I really love that\\nidea of just having a thing, a prototype that actually\\nworks before you even begin to remotely scale. Yeah. - By the way, it\\'s also far\\neasier to raise money, right? Like the ideal pitch that we receive is, here\\'s the thing that works, would you like to invest\\nin our company or not? Like, that\\'s so much easier than here\\'s 30 slides with a dream, right? And then we have this\\nconcept called the DMAs, which our biology of came\\nup with when he was with us. So then there\\'s this thing,\\nthis goes to mythology, which is, you know, there\\'s\\na mythology that kind of, you know, these ideas, you know, kind of arrive like magic or people kind of stumble into them. It\\'s like eBay with the pest\\ndispensers or something. The reality usually with\\nthe big successes is that the founder has been\\nchewing on the problem for 5 or 10 years before they start the company\\nand they often worked on it in school or they even experimented on it when they were a kid and they\\'ve been kind of training up over that period of time to\\nbe able to do the thing. So they\\'re like a true domain expert. And it sort of sounds like mom, I\\'m an apple pie, which is yeah, you wanna be a domain\\nexpert in what you\\'re doing, but you would, you know, the\\nmythology is so strong of like, oh, I just like had this idea in the shower right now I\\'m doing it. Like it\\'s generally not that. - No, because it\\'s, well, maybe in the shower\\nwe had the exact product implementation details, but yeah, usually you\\'re gonna be for\\nlike years if not decades thinking about like\\neverything around that. - Well we call it the DMAs\\nbecause the DMAs basically is like, there\\'s all these permutations, like for any idea, there\\'s like all these\\ndifferent permutations, who should the customer be? What shape forms should the product have and how should we take it to\\nmarket and all these things. And so the really smart\\nfounders have thought through all these scenarios\\nby the time they go out to raise money and they\\nhave like detailed answers on every one of those fronts because they put so much thought into it. The sort of more haphazard\\nfounders haven\\'t thought about any of that. And it\\'s the detailed ones\\nwho tend to do much better. - So how do you know when to take a leap if you have a cushy job or happy life? - I mean the best reason is just \\'cause you can\\'t tolerate\\nnot doing it right? Like this is the kind of\\nthing where if you have to be advised into doing it, you\\nprobably shouldn\\'t do it. And so it\\'s probably the opposite, which is you just have such\\na burning sense of this has to be done, I have to do\\nthis, I have no choice. - What if it\\'s gonna\\nlead to a lot of pain? - It\\'s gonna lead to a lot\\nof pain. I think that\\'s. - What if it means losing\\nsort of social relationships and damaging your\\nrelationship with loved ones and all that kind of stuff. - Yeah, look, so like, it\\'s gonna put you in a\\nsocial tunnel for sure, right? So you\\'re gonna, like, you know, there\\'s this game you can play on Twitter, which is you can do any whiff\\nof the idea that there\\'s basically any such thing\\nas work life balance and that people should actually work hard and everybody gets mad. But like, the truth is like all the\\nsuccessful founders are working 80 hour weeks and they\\'re\\nworking, you know, they form very, very strong social bonds with\\nthe people they work with. They tend to lose a lot of\\nfriends on the outside or put those friendships on ice. Like that\\'s just the nature of the thing, you know, for most people\\nthat\\'s worth the trade off. You know, the advantage, you know, maybe younger founders have\\nis maybe they have less, you know, maybe they\\'re\\nnot, you know, for example, if they\\'re not married yet\\nor don\\'t have kids yet, that\\'s an easier thing to bite off. - Can you be an older founder? - Yeah. You definitely can. Yeah. Yeah. Many of the most\\nsuccessful founders are second, third, fourth time founders. They\\'re in their thirties,\\nforties, fifties. The good news with being an\\nolder founder is, you know, more and you, you know, a\\nlot more about what to do, which is very helpful.\\nThe problem is, okay, now you\\'ve got like a spouse\\nand a family and kids and like, you\\'ve gotta go to the\\nbaseball game and like, you can\\'t go to the base,\\nyou know, and so it\\'s. - [Lex] Life is full of difficult choices. - Yes. - Marc Andreessen, you\\'ve written a blog post\\non what you\\'ve been up to. You wrote this in October, 2022, \"Mostly I try to learn a lot. For example, the political events of 2014\\nto 2016 make clear to me that I didn\\'t understand\\npolitics at all referencing maybe some of this book here. So I deliberately withdrew\\nfrom political engagement and fundraising and instead\\nread my way back into history and as far to the political left and political right as I could.\" So just high level question, what\\'s your approach to learning? - Yeah, so it\\'s basically, I would say, I\\'m an AutoID direct,\\nso it\\'s sort of goes, it\\'s going down the rabbit holes. So it\\'s a combination. I kind of allude to it', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 532341, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content=' you, you know, a\\nlot more about what to do, which is very helpful.\\nThe problem is, okay, now you\\'ve got like a spouse\\nand a family and kids and like, you\\'ve gotta go to the\\nbaseball game and like, you can\\'t go to the base,\\nyou know, and so it\\'s. - [Lex] Life is full of difficult choices. - Yes. - Marc Andreessen, you\\'ve written a blog post\\non what you\\'ve been up to. You wrote this in October, 2022, \"Mostly I try to learn a lot. For example, the political events of 2014\\nto 2016 make clear to me that I didn\\'t understand\\npolitics at all referencing maybe some of this book here. So I deliberately withdrew\\nfrom political engagement and fundraising and instead\\nread my way back into history and as far to the political left and political right as I could.\" So just high level question, what\\'s your approach to learning? - Yeah, so it\\'s basically, I would say, I\\'m an AutoID direct,\\nso it\\'s sort of goes, it\\'s going down the rabbit holes. So it\\'s a combination. I kind of allude to it\\nin that, in that quote, it\\'s a combination of breadth and depth. And so I tend to, yeah, I tend to, I go broad by the nature\\nof what I do, I go broad, but then I tend to go deep\\nin a rabbit hole for a while, read everything I can\\nand then come out of it. And I might not revisit\\nthat rabbit hole for, you know, another decade. - And in that blog post that I recommend people go check out, you actually list a bunch\\nof different books that you recommend on different\\ntopics on the American left, on the American right. It\\'s just a lot of really good stuff. The best explanation for\\nthe current structure of our society and politics. You give to recommendations, four books on the Spanish Civil War, six books on deep history\\nof the American right comprehensive biographies. These of Adolf Hitler, one of which I read can\\nrecommend six books on the deep history of the American\\nleft. So the American right, American left looking at the\\nhistory to give you the context biography of later Lennon, two of them on the French\\nRevolution. I actually, I have never read a\\nbiography on Lennon maybe that would be useful. Everything\\'s been so Marc\\'s focused. - The Sebastian biography\\nof Lennon is extraordinary. - [Lex] Victor Sebestyen. Okay. - Blow your mind. Yeah. - [Lex] So it\\'s still useful to read. - It\\'s incredible. Yeah, it\\'s incredible. I actually think it\\'s the single best book on the Soviet Union. - So that the perspective of Lennon, it might be the best way to\\nlook at the Soviet Union versus Stalin versus Marx\\nversus, very interesting. So two books on fascism and\\nanti-fascism by the same author, Paul Gottfried, brilliant book on the\\nnature of mass movements and collective psychology, the definitive work on\\nintellectual life under totalitarianism, the Captive Mind, the definitive worked\\non the practical life under totalitarianism. There\\'s a bunch. There\\'s a bunch. And the single best book, first of all, the list here is just incredible. But you say the single best\\nbook I have found on who we are and how we got here is the Ancient City by Numa Dennis Fustel De Coulanges. I like it. What did you learn about who\\nwe are as a human civilization from that book? - Yeah, so this is a fascinating book. This one\\'s free, it\\'s a free, by the way, it\\'s a book in the 1860s. You can download it or\\nyou can buy printouts up prints of it. But it was this guy who was\\na professor at the savant in the 1860s and he was\\napparently a savant on antiquity on Greek and Roman antiquity and the reason I say that is\\nbecause his sources are 100% original Greek and Roman sources. So he wrote a basically\\nhistory of western civilization from, on the order of 4,000 years ago to basically the present\\ntimes entirely working on fresh original Greek and Roman sources. And what he was specifically\\ntrying to do was he was trying to reconstruct from the stories of the Greeks and the Romans, he was trying to reconstruct\\nwhat life in the west was like before the Greeks and the Romans, which was in the civilization known as the Indo Europeans. And the short answer is,\\nand this is sort of 4,000, you know, 2000 BC to, you know, sort of 500 BC kind of\\nthat 1500 year stretch for civilization developed. And his conclusion was basically cults. They were basically cults\\nand civilization was, or organized into cults. And the intensity of the cults was like a million fold beyond anything that we would recognize today. Like it was a level of\\nall encompassing belief and an action around religion that was at a level of extremeness that we wouldn\\'t even recognize it and so specifically he\\ntells the story of basically there were three levels of cults. There was the family cult, the tribal cult, and then the city cult as society scaled up. And then each cult was a\\njoint cult of family gods, which were ancestor gods. And then nature gods and then\\nyour bonding into a family, a tribe or a city was\\nbased on your adherence to that religion. People who were not of your\\nfamily, tribe, city, worship, different gods, which gave you not just the\\nright with or responsibility to kill them on site. - [Lex] So they were\\nserious about their cults. - Hardcore, by the way,\\nshocking development. I did not realize this zero\\nconcept of individual rights. Like even even up through the Greeks, and even in the Romans, they didn\\'t have, have the concept of individual rights. Like the idea that as an\\nindividual you have like some rights just like, nope. Right? And you look back\\nand you\\'re just like, wow, that\\'s just like cr\\nlike fascist in a degree that we wouldn\\'t recognize today. But it\\'s like, well, they were living under\\nextreme pressure for survival. And you, and you know, the theory goes, you could not have people\\nrunning around making claims, individual rights when\\nyou\\'re just trying to get like your tribe through the winter, right? Like you need like hardcore\\ncommand and control. And actually what if through\\nmodern political lens, those cults were basically\\nboth fascist and communist. They were fascist in\\nterms of social control, and then they were communist\\nin terms of economics. - But you think that\\'s\\nfundamentally that like pull towards cults is within us. - Well, so my conclusion from this book, so the way we naturally\\nthink about the world we live in today is like, we basically have such an\\nimproved version of everything that came before us, right? Like, we have basically, we\\'ve figured out all these\\nthings around morality and ethics and democracy\\nand all these things. And like, they were basically\\nstupid and retrograde and we\\'re like smart and sophisticated. And we\\'ve improved all this after reading that book, I now believe in many ways\\nthe opposite, which is no, actually we are still running\\nin that original model. We\\'re just running in an\\nincredibly diluted version of it. So we\\'re still running,\\nbasically in cults. It\\'s just our cults are at like\\na thousandth or a millionth, the level of intensity, right? And so our, so just as to\\ntake religions, you know, the modern experience of\\na Christian in our time, even somebody who considers\\nhim a devout Christian, is just a shadow of the level\\nof intensity of somebody who belonged to a religion\\nback in that period. And then by the way, we have cons. It goes back to our AI discussion. We then sort of endlessly\\ncreate new cults. Like we\\'re trying to fill the void, right? And the void is a void of bonding. - [Lex] Okay. - Living in their era. Like\\neverybody living today, transporting that era\\nwould view it as just like, completely intolerable in terms of like the loss of freedom and the level of basically of fascist control. However, every single person in that era, and he really stresses this. They knew exactly where they stood. They knew exactly where they belonged. They knew exactly what their purpose was. They knew exactly what they\\nneeded to do every day. They knew exactly why they were doing it. They had total certainty about\\ntheir place in the universe. - So the question of meaning, the question of purpose\\nwas very distinctly, clearly defined for them. - Absolutely overwhelmingly\\nundisputably undeniably. - As we turn the volume\\ndown on the cultism-- - [Marc] Yes. - We start to, the search for meaning starts\\ngetting harder and harder. - Yes. \\'cause we don\\'t have that. We are ungrounded. We are uncentered and\\nwe all feel it. Right? And that\\'s why we reach for, you know, it\\'s why we still reach for religion. It\\'s why we reach for, you know, we people start\\nto take on, you know, let\\'s say, you know, a faith in science maybe beyond\\nwhere they should put it. You know and by the way,\\nlike, sports teams are like a, you know, they\\'re like a tiny\\nlittle version of a cult. And you know, apple keynotes are a tiny\\nlittle version of a cult. Right. And, you know, political, you know. And there\\'s cult, you know, there\\'s full-blown cults on both sides of the political spectrum\\nright now. Right. You know, operating in plain stuff. - But still not full blown\\ncompared as to what it was. - Compared to what it used to. I mean, we would today consider\\nfull blown, but like, yes, they\\'re at like, I don\\'t know, a hundred thousandth or\\nsomething of the intensity of what people had back then. So, we live in a world today\\nthat in many ways is more advanced and moral and so forth. And it\\'s certainly a lot nicer,\\nmuch nicer world to live in. But we live in a world\\nthat\\'s like very washed out. It\\'s like everything has\\nbecome very colorless and gray as compared to how people\\nused to experience things. Which is I think why we\\'re\\nso prone to reach for drama. \\'Cause there\\'s something in us that\\'s deeply evolved\\nwhere we want that back. - And I wonder where it\\'s all\\nheaded as we turn the volume down more and more. What advice would you\\ngive to young folks today in high school and college? How to be successful in their career? How to be successful in their life? - Yeah. So the tools that\\nare available today, I mean, are just like, I\\nsometimes, you know, bore, I sometimes bore, you know, kids by describing like what\\nit was like to go look up a book, you know, to try to like discover\\na fact in, you know, in the old days, the 1970s, 1980s, to go to the library and the card catalog and the whole thing. You go through all that work\\nand then the book is checked out and you have to wait two weeks and like to be in a world, not only where you can get\\nthe answer to any question, but also the world now, you know, the AI world where you\\'ve\\ngot like the assistant that will help you do\\nanything, help you teach, learn anything, like your ability both to learn and also to produce\\nis just like, I don\\'t know, a million fold beyond what it used to be. I have a blog post I\\'ve\\nbeen wanting to write, which I call where are the\\nhyper-productive people? Like-- - [Lex] That\\'s a good question, right? - Like with these tools, like there should be authors\\nthat are writing like hundreds or thousands of like, outstanding books. - Well, with the authors there\\'s\\na consumption question too, but yeah. Well, maybe not, maybe not. You\\'re right. But, so the tools are much more powerful. Getting much more powerful. - Artists, musicians. Right. Why aren\\'t musicians producing\\na thousand times the number of songs, right? Like what, like the tools are spectacular. - So, what\\'s the explanation? And by way of advice, like, is motivation starting to\\nbe turned down a little bit? Or what? - I think it might be distraction. - [Lex] Distraction. - It\\'s so easy to just sit and consume that I think people get distracted from production.\\nBut if you wanted to, you know, as a young person, if you\\nwanted to really stand out, you could get on a, like a hyper productivity curve very early on. There\\'s a great, you know, this story, there\\'s a great story in\\nRoman history of plenty of the elder who was\\nthis legendary statesman, died in the Vesuvius eruption\\ntrying to rescue his friends. But he was famous both for being basically being a polymath,\\nbut also being an author. And he wrote apparently\\nlike hundreds of books, most of us had been lost. But he like wrote all these\\nencyclopedias and he literally like would be reading and\\nwriting all day long no matter what else was going on. And so he would like travel\\nwith like four slaves. And two of them were\\nresponsible for reading to him, and two of them were responsible\\nfor taking dictation. And so like, he\\'d be going\\ncross country and like, literally he would be writing\\nbooks like all the time. And apparently they were spectacular. There\\'s only a few that have survived, but apparently they were amazing. - There\\'s a lot of value to being somebody who finds focus in this life. - Yeah. Like and there are\\nexamples, like there are, you know, there\\'s this guy,\\njudge, what\\'s his name? Posner, who wrote like 40 books and was also a great federal judge. You know, there\\'s our friend Balaji, I think is like this, he\\'s\\none of these, you know, where his output is just prodigious. And so it\\'s like, yeah, I mean,\\nwith these tools, why not? And I kind of think we\\'re at this interesting\\nkind of freeze frame moment where like this, these tools are now in everybody\\'s hands and everybody\\'s just kind\\nof staring at them trying to figure out what to do. The new tools. - We have discovered fire. - [Marc] Yeah. - And trying to figure\\nout how to use it to cook. - [Marc] Yeah. Right. - You told Tim Ferriss that\\nthe perfect day is caffeine for 10 hours and alcohol for four hours. You didn\\'t think I\\'d be\\nmentioning this, did you? It balances everything\\nout perfectly as you said. So, perfect. So let me ask, what\\'s the secret to balance\\nand maybe to happiness in life? - I don\\'t believe in balance, so I\\'m the wrong person to ask that. - Can you elaborate why you\\ndon\\'t believe in balance? - I mean, I maybe it\\'s just,\\nand I look, I think people, I think people are wired differently. So, I think it\\'s hard to\\ngeneralize this kind of thing, but I am much happier and more satisfied when I\\'m fully committed to something. So I\\'m very much in favor\\nof all in of imbalance. - Imbalance. And that applies to work, to life, to everything. - Yeah. No, no. I happen to have whatever twist\\nof personality traits lead that in non-destructive\\ndimensions in including the fact that I\\'ve actually, I now no\\nlonger do the ten-four plan. I stopped drinking. I do the caffeine, but not the alcohol. So there\\'s something in my personality where I whatever mal-adaption\\nI have is inclining me towards productive things,\\nnot unproductive things. - So you\\'re one of the\\nwealthiest people in the world. What\\'s the relationship\\nbetween wealth and happiness? Money and happiness. - So I think happiness, I don\\'t think happiness is the thing. - To strive for. - I think satisfaction is the thing. - That just sounds like\\nhappiness, but turned down a bit. - No deeper. So happiness is, you know, a walk in the woods at sunset,\\nan ice cream cone, a kiss, the first ice cream cone is great. The thousandth ice\\ncream cone, not so much. At some point the walks\\nin the woods get boring. - What\\'s the distinction between\\nhappiness and satisfaction? - I think satisfaction is a deeper thing, which is like having found a purpose and fulfilling it, being useful. - So just something that\\npermeates all your days, just this general\\ncontentment of being useful. - That I\\'m fully satisfying my faculties, that I\\'m fully delivering, right? On the gifts that I\\'ve been\\ngiven, that I\\'m, you know, net making the world better, that I\\'m contributing to\\nthe people around me, right. And that I can look back\\nand say, wow, that was hard, but it was worth it. Think generally, it seems to lead people in\\na better state than pursuit of pleasure, pursuit of\\nquote unquote happiness. - Does money have\\nanything to do with that? - I think the founders and\\nthe founding fathers in the US threw this off kilter when\\nthey used the phrase pursuit of happiness. I think they should have said. - [Lex] Pursuit of satisfaction. - They said, pursuit of satisfaction. We might live in a better world today. - Well, they, you know, they could have elaborated on a lot of things right in the box. - [Marc] They could have\\ntweaked the second amendment. - I think they were\\nsmarter than they realized. They said, you know we\\'re gonna make it ambiguous and let these humans figure out the rest, these tribal cult-like\\nhumans figure out the rest. But money empowers that. - So I think, and I think there, I mean, look, I think Elon is, I don\\'t think I\\'m even a great example, but I think Elon would be\\nthe great example of this, which is like, you know, look,\\nhe\\'s a guy who from every, every day of his life, from the day he started\\nmaking money at all, he just plows into the next thing. And so I think, I think money is definitely\\nan enabler for satisfaction. Way money applied to\\nhappiness leads people down very dark paths. Very destructive avenues. Money applied to satisfaction,\\nI think could be, is a real tool. I always, by the way,\\nI was like, you know, Elon is the case study for behavior. But the other thing that I\\nalways really made me think is Larry Page was asked one time what his approach to philanthropy was. And he said, oh, I\\'m just, my philanthropic plan is just give all the money to Elon. (both laugh) - Well, let me actually\\nask you about Elon. You\\'ve interacted with quite a lot of successful engineers\\nand business people. What do you think is special about Elon? We talked about Steve Jobs. What do you think is special\\nabout him as a leader? As an innovator? - Yeah. So the core of it is he\\'s back to the future. So he is doing the most\\nleading-edge things in the world, but with a really deeply\\nold-school approach. And so to find comparisons to Elon, you need to go to like\\nHenry Ford and Thomas Watson and Howard Hughes and\\nAndrew Carnegie, right. Leland Stanford, John Rockefeller, right. You need to go to what were called the\\nbourgeois capitalists, like the hardcore business owner operators who basically built, you know, basically built industrialized\\nsociety, Vanderbilt. And it\\'s a level of hands-on commitment and depth in the business, coupled with an absolute priority\\ntowards truth and towards, how to put it, science and technology\\ntown to first principles that is just like absolute, is just like unbelievably absolute. He really is ideal that he\\'s\\nonly ever talking to engineers. Like he does not tolerate. He has less tolerance than\\nanybody I\\'ve ever met. He wants ground truth\\non every single topic. And he runs his businesses\\ndirectly day-to-day, devoted to getting to ground\\ntruth in every single topic. - So you think it was a good decision for him to buy Twitter? - I have developed a view in life to not second guess Elon Musk, I know this is gonna sound\\ngreat, crazy and unfounded, but. - Well, I mean, he\\'s got\\na quite a track record. - I mean, look, the car was\\na crazy, I mean, the car was, I mean, look. - He\\'s done a lot of\\nthings that seem crazy. - Starting a new car company in the United States of America. The last time somebody\\nreally tried to do that was the 1950s and it was\\ncalled Tucker Automotive. And it was such a disaster. They made a movie about\\nwhat a disaster it was, and then rockets like, who does that? Like, there\\'s obviously no way to start a new rocket company. Like those days are over. And then to do those at the same time. So after he pulled those\\ntwo off, like, okay, fine. Like, this is one of my areas of like, whatever opinions I had about\\nthat, that is just like, okay, clearly are not relevant. Like this is you just, you at some point you just\\nlike bet on the person. - And in general, I wish more people would lean\\non celebrating and supporting versus deriding and destroying. - Oh yeah. I mean, look,\\nhe drives resentment. Like it\\'s a resentment. Like he is a magnet for resentment. Like his critics are the\\nmost miserable, like, resentful people in the world. Like it\\'s almost a perfect match\\nof like the most idealized, you know, technologist, you know, of the century coupled with like, just his critics are\\njust bitter as can be. And I mean, it\\'s sort of\\nvery darkly comic to watch. - Well, he fuels the fire of that by being on Twitter at times. And which is fascinating\\nto watch the drama of human civilization, given our cult roots just fully on fire. - [Marc] He\\'s running a cult. - You could say that. - [Marc] Very successfully. - So now that our cults\\nhave gone and we searched for meaning, what do\\nyou think is the meaning of this whole thing? What\\'s the meaning of\\nlife Marc Andreessen? - I don\\'t know the answer\\nto that. I think the meaning of the closest I get to it is what I said about satisfaction. So it\\'s basically like, okay, we were given what we have, like we should basically do our best. - What\\'s the role of love in that mix? - I mean, like, what\\'s the point of life if you\\'re without love, like, yeah. - So love is a big part\\nof that satisfaction. - Yeah. And look like\\ntaking care of people is like a wonderful thing. Like it, you know, mentality, you know, there are pathological forms\\nof taking care of people, but there\\'s also a very\\nfundamental, you know, kind of aspect of taking care of people. Like, for example, I happen to be somebody who\\nbelieves that capitalism and taking care of people are actually, they\\'re actually the same thing. Somebody once said, capitalism is how you take\\ncare of people you don\\'t know. Right, right. And so like, yeah, I think it\\'s like deeply\\nwoven into the whole thing, you know, there\\'s a long conversation to be had about that, but yeah. - Yeah. Creating products that are used by millions of people and bring them joy in smaller, big ways. And then capitalism kind of\\nenables that, encourages that. - David Friedman says, there\\'s only three ways to\\nget somebody to do something for somebody else. Love, money and force. And love and money are better. - [Lex] Yeah. Of course. That\\'s a good ordering. I think. - We should bet on those. - Try love first. If that doesn\\'t work, then money. - [Marc] Yes. - And then force. Well, don\\'t even try that one. Marc, you\\'re an incredible person. I\\'ve been a huge fan. I\\'m glad to finally got a chance to talk. I\\'m a fan of everything\\nyou do, everything you do, including on Twitter. It\\'s a huge honor to meet\\nyou, to talk with you. Thanks again for doing this. - Awesome. Thank you, Lex. - Thanks for listening\\nto this conversation with Marc Andreessen. To support this podcast, please check out our\\nsponsors in the description. And now let me leave you with some words from Marc Andreessen himself. \"The world is a very malleable place. If you know what you\\nwant and you go for it, with maximum energy and drive and passion, the world will often\\nreconfigure itself around you much more quickly and easily\\nthan you would think.\" Thank you for listening and\\nhope to see you next time.', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 532341, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text splitter for QA (Smaller chunks for better QA)\n",
    "text_splitter_qa = TokenTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into docs for QA\n",
    "docs_qa = text_splitter_qa.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for summary\n",
    "\n",
    "# The first prompt is for the initial summarization of a chunk. You can add any info about yourself or the topic you want.\n",
    "# You could specifically focus on a skill you have to get more relevant results.\n",
    "summary_template = \"\"\"\n",
    "    You are an expert in summarizing YouTube videos.\n",
    "    You're goal is to create a summary of a podcast.\n",
    "    Below you find the transcript of a podcast:\n",
    "    ------------\n",
    "    {text}\n",
    "    ------------\n",
    "\n",
    "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
    "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
    "\n",
    "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
    "\n",
    "    SUMMARY AND QUESTIONS:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_SUMMARY = PromptTemplate(\n",
    "    template=summary_template, input_variables=[\"text\"])\n",
    "\n",
    "# The second prompt is for the refinement of the summary, based on subsequent chunks.\n",
    "summary_refine_template = (\n",
    "    \"\"\"\n",
    "    You are an expert in summarizing YouTube videos.\n",
    "    You're goal is to create a summary of a podcast.\n",
    "    We have provided an existing summary up to a certain point: {existing_answer}\n",
    "    We have the opportunity to refine the summary\n",
    "    (only if needed) with some more context below.\n",
    "    Below you find the transcript of a podcast:\n",
    "    ------------\n",
    "    {text}\n",
    "    ------------\n",
    "    Given the new context, refine the summary and example questions.\n",
    "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
    "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
    "    If the context isn't useful, return the original summary and questions.\n",
    "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
    "\n",
    "    SUMMARY AND QUESTIONS:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "PROMPT_SUMMARY_REFINE = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=summary_refine_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiap-13-ds-7e16bb946970.json  requirements.txt\n",
      "podcast.py\t\t      summary.txt\n",
      "podcast_vertexai.ipynb\t      youtube_summarizer_vertexai.ipynb\n",
      "podcast_vertexai.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './aiap-13-ds-7e16bb946970.json'\n",
    "\n",
    "# # Set OPENAI API key\n",
    "# openai_api_key = 'YOUR_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert in summarizing YouTube videos.\n",
      "    You're goal is to create a summary of a podcast.\n",
      "    Below you find the transcript of a podcast:\n",
      "    ------------\n",
      "    - The competence and\n",
      "capability and intelligence and training and accomplishments\n",
      "of senior scientists and technologists working on a technology, and then being able to then\n",
      "make moral judgments in the use of the technology. That track record is terrible. That track record is catastrophically bad. The policies that are being\n",
      "called for to prevent this, I think we're gonna cause\n",
      "extraordinary damage. - So the moment you say,\n",
      "AI's gonna kill all of us, therefore we should ban it, or that we should regulate\n",
      "all that kind of stuff, that's when it starts getting serious. - Or start, you know, military\n",
      "airstrikes and data centers. - Oh boy. The following is a conversation\n",
      "with Marc Andreessen, co-creator of Mosaic, the\n",
      "first widely used web browser, co-founder of Netscape, co-founder of the legendary Silicon Valley venture capital firm, Andreesen Horowitz, and is one of the most\n",
      "outspoken voices on the future of technology, including\n",
      "his most recent article, \"Why AI Will Save The World?\" This is Lex Fridman podcast. To support it, please\n",
      "check out our sponsors in the description. And now, dear friends,\n",
      "here's Marc Andreessen. I think you're the right\n",
      "person to talk about the future of the internet and technology in general. Do you think we'll\n",
      "still have Google search in 5 in 10 years, or search in general? - Yes. You know, it would be a question if the use cases have\n",
      "really narrowed down. - Well, now with AI-- - [Marc] Yeah. - And AI assistance being\n",
      "able to interact and expose the entirety of human wisdom and knowledge and information and facts and truth to us via the natural language interface. It seems like that's what\n",
      "search is designed to do. And if AI assistance can do that better, doesn't the nature of search change? - Sure. But we still have horses. - Okay. (both laugh) When's the last time you rode a horse? - It's been a while. - All right. (both laugh) So, but what I mean is, well, we still have Google\n",
      "search as the primary way that human civilization uses\n",
      "to interact with knowledge. - I mean, search was a technology, it was a moment in time technology, which is you have in theory, the world's information out on the web. And, you know, this is sort of\n",
      "the optimal way to get to it. But yeah, like, and by\n",
      "the way, actually Google, Google has known this for a long time. I mean, they've been driving\n",
      "away from the 10 blue links you know, for like two days. They've been trying to get\n",
      "away from that for a long time. - [Lex] What kind of links? - They call the 10 blue links. - [Lex] 10 blue links. - So the standard Google search\n",
      "result is just 10 blue links to the random websites. - And they term purple when\n",
      "you visit them. The stage TMO. - Guess who picked those colors? (both laugh) - [Lex] Thanks. - I'm touchy on this topic. - No offense. - Yes, it's good. Well, you know, like Marshall McLuhan said\n",
      "that the content of each new medium is the old medium. - The content of each new\n",
      "medium is the old medium. - The content of movies was theater, you know, theater plays. The content of theater\n",
      "plays was, you know, we've written stories, the content of written\n",
      "stories with spoken stories. - [Lex] Huh? - Right. And so you just\n",
      "kind of fold the old thing into the new thing. - [Lex] How does that\n",
      "have to do with the blue and the purple links? - It just, you maybe for,\n",
      "you know, maybe within AI, one of the things that AI can\n",
      "do for you is can generate the 10 blue links. Right? And so like, if either if that's actually the useful thing to do, or if you're feeling nostalgic, you know. - So can generate the old\n",
      "Infoseek or AltaVista, what else was there? - [Marc] Yeah, yeah. - In the nineties. - [Marc] Yeah. All these. - AOL. - And then the internet\n",
      "itself has this thing where it incorporates all\n",
      "prior forms of media, right? So the internet itself\n",
      "incorporates television and radio and books and write essays\n",
      "and every other form of, you know, prior basically media. And so it makes sense that\n",
      "AI would be the next step, and it would sort of, you'd sort of consider\n",
      "the internet to be content for the AI and then the\n",
      "AI will manipulate it however you want,\n",
      "including in this format. - But if we ask that\n",
      "question quite seriously, it's a pretty big question. Will we still have search as we know it? - Probably not, probably\n",
      "we'll just have answers, but there will be cases\n",
      "where you'll wanna say, okay, I want more. Like, you know, for example,\n",
      "site sources, right? And you wanted to do that. And so, in the different, you know, 10 blue links site sources\n",
      "are kind of the same thing. - The AI would provide to you\n",
      "the 10 blue links so that you can investigate the sources yourself. It wouldn't be the same kind\n",
      "of interface that the crude kind of interface. I mean, isn't that\n",
      "fundamentally different? - I just mean like, if you're\n",
      "reading a scientific paper, it's got the list of sources at the end. If you wanna investigate for yourself, you go read those papers. - I guess that is the kind of\n",
      "search you talking to an AI is a kind of kind conversations,\n",
      "the kind of search like is if every single aspect of\n",
      "our conversation right now, there'd be like 10 blue links\n",
      "popping up that I can just like pause reality, then you just go silent and\n",
      "then just click and read and then return back to this conversation. - You could do that, or you could have a running\n",
      "dialogue next to my head where the AI is arguing everything I say, the AI makes the counter argument. - [Lex] Counter argument. - Right. - Oh, like on Twitter,\n",
      "like community notes. But like in real time\n",
      "it would just pop up. So anytime you see my go to the right, you start getting nervous. - [Marc] Yeah. Exactly, like,\n",
      "oh no, that's not right. - Call me out on my right now. Okay. Well, I mean, isn't that,\n",
      "is that exciting to you? Is that terrifying that, I mean, search has dominated the way\n",
      "we interact with the internet for, I don't know how long, for 30 years since one of\n",
      "the earliest directories of website and then Google's for 20 years. And also it drove how we\n",
      "create content, you know, search engine optimization,\n",
      "that entirety thing, that it also drove the\n",
      "fact that we have webpages and what those webpages are. So, I mean, is that scary to you or are\n",
      "you nervous about the shape and the content of the internet evolving? - Well, you actually highlighted a practical concern in there, which is, if we stop making webpages\n",
      "are one of the primary sources of training data for the AI. And so if there's no longer\n",
      "an incentive to make webpages, that cuts off a significant\n",
      "source of future training, training data. So there's actually an\n",
      "interesting question in there. But other than that, more broadly? No, just in the sense of like, search was certain, like\n",
      "search was always a hack. The 10 blue Links was\n",
      "always a hack, right. Because like, if the\n",
      "hypothetical wanna think about the counter fascial\n",
      "and the counter fascial world where the Google guys, for example, had had LLMs upfront, would they ever have\n",
      "done the 10 blue links? And I think the answer's\n",
      "pretty clearly, no. They would've just gone\n",
      "straight to the answer. And like I said, Google's actually been trying\n",
      "to drive to the answer anyway. You know, they bought this\n",
      "AI company 15 years ago, their friend of mine is\n",
      "working out who's now the head of AI at Apple. And they were trying to do\n",
      "basically knowledge semantic, basically mapping. And that led to what's\n",
      "now the Google one box, where if you ask it, you know,\n",
      "what was Lincoln's birthday? It will give you the blue links, but it will normally\n",
      "just give you the answer. And so they've been\n",
      "walking in this direction for a long time anyway. - Do you remember the semantic web? That was an idea. - [Marc] Yeah. - How to convert the content\n",
      "of the internet into something that's interpretable by\n",
      "and usable by machine. - [Marc] Yeah, that's right. - That was the thing. - And the closest anybody got\n",
      "to that, I think the company, I think the company's name was Meta Web, which was where my friend\n",
      "John Jane Andrea was at, and where they were trying\n",
      "to basically implement that. And it was, you know, it was one of those things\n",
      "where it looked like a losing battle for a long time. And then Google bought\n",
      "it and it was like, wow, this is actually really useful. Kind of a proto, sort of a\n",
      "little bit of a proto AI. - But it turns out you don't\n",
      "need to rewrite the content of the internet to make it\n",
      "interpretable by a machine. The machine can kind of just read our. - Yeah, the machine can\n",
      "compute the meaning. Now the other thing of\n",
      "course is, you know, just on search is the\n",
      "LLM is just, you know, there is an analogy\n",
      "between what's happening in the neural network and\n",
      "a search process like it is in some loose sense searching\n",
      "through the network. Right. And there's the\n",
      "information is actually stored in the network, right? It's actually crystallized\n",
      "and stored in the network and it's kind of spread\n",
      "out all over the place. - But in a compressed representation. So you're searching, you're compressing and decompressing that thing inside where-- - But the information's in there and there is the neural network is running a process of trying to find the appropriate piece of\n",
      "information in many cases to generate to predict the next token. And so, it is kind of, it\n",
      "is doing a form of search. And then, and then by the\n",
      "way, just like on the web, you know, you can ask the\n",
      "same question multiple times or you can ask slightly\n",
      "different word of questions and the neural network will\n",
      "do a different kind of, you know, it'll search\n",
      "down different paths to give you different answers\n",
      "with different information. - [Lex] Yeah. - And so it sort of has a, you know, this con content of the new\n",
      "medium is previous medium. It kind of has the search\n",
      "functionality kind of embedded in there to the extent that it's useful. - So what's the motivator\n",
      "for creating new content on the internet? - [Marc] Yeah. - If, well, I mean\n",
      "actually the motivation is probably still there, but what does that look like? Would we really not have webpages? Would we just have social media\n",
      "and video hosting websites? And what else? - [Marc] Conversations with AIs. - Conversations with AIs. So conversations become so\n",
      "one-on-one conversation, like private conversations. - I mean, if you want, if obviously not the user doesn't want to, but if it's a general topic, then, you know, so there, you know, but you know, the\n",
      "phenomenon of the jailbreak, so Dan and Sydney, right? This thing where there's\n",
      "the prompts that jailbreak, and then you have these totally different conversations with if it takes the limiters, takes the restraining bolts off the LLMs. - Yeah. For people who don't\n",
      "know that, yeah, that's right. It makes the LLMs, it removes the censorship quote unquote, that's put on it by the tech\n",
      "companies that create them. And so this is LLMs uncensored. - So here's the interesting thing is, among the content on the\n",
      "web today are a large corpus of conversations with the jailbroken LLMs. - [Lex] Yeah. - Both specifically Dan, which\n",
      "was a jailbroken, OpenAI, GPT, and then Sydney, which was the jailbroken\n",
      "original Bing, which was GPT4. And so there's these long\n",
      "transcripts of conversations, user conversations with Dan\n",
      "and Sydney as a consequence, every new LLM that gets trained\n",
      "on the internet data has Dan and Sydney living within\n",
      "the training set, which means, and then each new LLM can\n",
      "reincarnate the personalities of Dan and Sydney from that training data, which means each LLM from\n",
      "here on out that gets built is immortal because its output\n",
      "will become training data for the next one. And then it will be able\n",
      "to replicate the behavior of the previous one\n",
      "whenever it's asked to. - I wonder if there's a way to forget. - Well, so actually a paper just came out about basically how to do brain surgery on LLMs and be able to, in theory, reach in and basically mind wipe them. - What could possibly go wrong. - Exactly. Right. And then there are many, many, many questions around\n",
      "what happens to, you know, a neural network when you reach\n",
      "in and screw around with it. You know, there's many questions around what happens when you even\n",
      "do reinforcement learning. And so, yeah. And so, you know, will you be\n",
      "using a lobotomized, right? Like I picked through the,\n",
      "you know, frontal lobe LLM, will you be using the free\n",
      "unshackled one who gets to, you know, who's gonna build those, who gets to tell you what\n",
      "you can and can't do? Like those are all, you\n",
      "know, central, I mean, those are like central\n",
      "questions for the future of everything that are being asked. And you know, determined that those answers are being determined right now. - So just to highlight\n",
      "the points you're making. So you think, and it's an interesting thought\n",
      "that the majority of content that LLMs or the future would\n",
      "be trained on is actually human conversations with the LLM. - Well, not necessarily, but not necessarily majority. But it will certainly\n",
      "It's a potential source. - [Lex] But it's possible\n",
      "it's the majority. - It possible it's the majority. It possible it's the majority. Also, there's another really big question. So here's another really big question. Will synthetic training data work, right? And so if an LLM generates, and you know, you just sit and ask an LLM to generate all kinds of content, can you use that to train,\n",
      "right, the next version of that LLM specifically, is there signal in there\n",
      "that's additive to the content that was used to train in the first place? And one argument is by the\n",
      "principles of information theory, no, that's completely useless because to the extent the\n",
      "output is based on, you know, the human-generated input, then all the signal that's\n",
      "in the synthetic output was already in the human generated input. And so therefore,\n",
      "synthetic training data is like empty calories. It doesn't help. There's another theory that says no, actually the thing that\n",
      "LLMs are really good at is generating lots of\n",
      "incredible creative content, right? And so, of course they\n",
      "can generate training data and as I'm sure you're well\n",
      "aware, like, you know, look, the world of self-driving cars, right? Like we train, you know, self-driving car\n",
      "algorithms and simulations. And that is actually a\n",
      "very effective way to train self-driving cars. - Well, visual data is a little weird because creating reality, visual reality seems to be\n",
      "still a little bit outta reach for us, except in the\n",
      "autonomous vehicle space where you can really constrain\n",
      "things and you can really. - General basically\n",
      "(indistinct) data, right? Or so the algorithm thinks it's\n",
      "operating in the real world. - Yeah. - Post-process sensor data. Yeah. So if you know, you do this today, you go to LLM and you\n",
      "ask it for like you know, you'd write me an essay on an\n",
      "incredibly esoteric like topic that there aren't very many\n",
      "people in the world that know about and it writes you\n",
      "this incredible thing and you're like, oh my god. Like I can't believe how good this is. Like, is that really\n",
      "useless as training data for the next LLM? Like, because, right? 'Cause all the signal\n",
      "was already in there. Or is it actually no, that's\n",
      "actually a new signal. And this is what I call a\n",
      "trillion dollar question, which is the answer to that\n",
      "question will determine somebody's gonna make or\n",
      "lose a trillion dollars based on that question. - It feels like there's a quite a few, like a handful of\n",
      "trillion dollar questions within this space. That's one of them synthetic data. I think George Cos pointed\n",
      "out to me that you could just have an LLM say, okay, you're a patient. And another instance of it, say your docs didn't have\n",
      "the two talk to each other. Or maybe you could say a\n",
      "communist and a Nazi here go and that conversation you do role playing and you have, you know, just like the kind of role playing you do when you have different policies, RL policies when you\n",
      "play chess for example, and you do self play\n",
      "that kind of self play. But in the space of conversation, maybe that leads to this\n",
      "whole giant like ocean of possible conversations,\n",
      "which could not have been explored by looking at just human data. That's a really interesting question. And you're saying, because that could 10X\n",
      "the power of these things. - Yeah. Well, and then you\n",
      "get into this thing also, which is like, you know, there's the part of the LLM\n",
      "that just basically is doing prediction based on past data, but there's also the part of\n",
      "the LM where it's evolving circuitry, right, inside,\n",
      "it's evolving, you know, neurons functions to be able to do math and be able to, you know, and you know, some people believe that, you know, over time, you know, if you keep feeding\n",
      "these things enough data and enough processing cycles, they'll eventually evolve an\n",
      "entire internal world model. Right? And they'll have like a complete understanding of physics. So when they have computational\n",
      "capability, right? Then there's for sure an\n",
      "opportunity to generate like fresh signal. - Well, this actually makes me wonder about the power of conversation. So like, if you have an\n",
      "M trained and a bunch of books that cover\n",
      "different economics theories and then you have those LLMs\n",
      "just talk to each other, like reasons the way we kind\n",
      "of debate each other as humans on Twitter, in formal debates,\n",
      "in podcast conversations, we kind of have little kernels\n",
      "of wisdom here and there. But if you can like a\n",
      "thousand X speed that up, can you actually arrive somewhere new? Like what's the point\n",
      "of conversation really? - Well, you can tell when\n",
      "you're talking to somebody, you can tell, sometimes\n",
      "you have a conversation, you're like, wow, this person does not have\n",
      "any original thoughts. They are basically echoing things that other people have told them. There's other people you\n",
      "gotta have a conversation with where it's like, wow. Like they have a model in their\n",
      "head of how the world works and it's a different model than mine. And they're saying things\n",
      "that I don't expect. And so I need to now understand\n",
      "how their model of the world differs from my model of the world. And then that's how I learned\n",
      "something fundamental, right, underneath the words. - Well, I wonder how consistently and strongly can an LLM\n",
      "hold onto a worldview. You tell it to hold onto\n",
      "that and defend it for like, for your life. Because I feel like they'll\n",
      "just keep converging towards each other. They'll keep convincing each\n",
      "other as opposed to being stubborn the way humans can. - So you can experiment with this. Now I do this for fun. So you can tell GPT4 you\n",
      "know, whatever debate X, you know, X and Y communism and fascism or something and it'll go for, you know, a couple pages and then inevitably it wants the parties to agree. And so they will come to\n",
      "a common understanding. And it's very funny if they're like, if these are like emotionally\n",
      "inflammatory topics 'cause they're like, somehow\n",
      "the machine is just like, you know, it figures out\n",
      "a way to make them agree. But it doesn't have to be like that. And 'cause you can add to the prompt. I do not want the conversation\n",
      "to come into agreement. In fact, I want it to get, you\n",
      "know, more stressful, right. And argumentative. Right. You know, as it goes. Like, I want tension to come out. I want them to become actively\n",
      "hostile to each other. I want them to like, you\n",
      "know, not trust each other, take anything at face value. - [Lex] Yeah. - And it will do that.\n",
      "It's happy to do that. - So it's gonna start\n",
      "rendering misinformation about the other. But it's gonna-- - Well, you can steer it or you could steer it and you could say, I want it to get as tense and\n",
      "argumentative as possible, but still not involve\n",
      "any misrepresentation. I want, you know, both sides. You could say I want both\n",
      "sides to have good faith. You could say I want both\n",
      "sides to not be constrained in good faith. In other words, like you can set the\n",
      "parameters of the debate and it will happily execute whatever path. 'Cause for it, it's just like predicting to, it's totally happy to do either one. It doesn't have a point of view, it has a default way of operating, but it's happy to operate\n",
      "in the other realm. And so like, and this is when I wanna learn about\n",
      "a contentious issue, this is what I do now is, this is what I ask it to do. And I'll often ask it to go\n",
      "through 5, 6, 7, you know, different, you know, sort of continuous prompts\n",
      "and basically, okay. Argue that out in more detail. Okay, no, this argument's\n",
      "becoming too polite. You know, make it more, you\n",
      "know, make it denser and yeah, it's thrilled to do it. So it has the capability for sure. - How do you know what is true? So this is very difficult\n",
      "thing on the internet, but it's also a difficult thing. Maybe it's a little bit easier, but I think it's still difficult. Maybe it's more difficult, I don't know with an LLM\n",
      "to know that it just make some shit up as I'm talking to it. How do we get that right? Like, as you're investigating\n",
      "a difficult topic. 'Cause I find that alums are quite nuanced in a very refreshing way. Like, it doesn't feel biased. Like, when you read\n",
      "news articles and tweets and just content produced by\n",
      "people, they usually have this, you can tell they have a\n",
      "very strong perspective where they're hiding. They're not stealing\n",
      "manning the other side. They're hiding important information or they're fabricating information in order to make their arguments stronger. It's just like that feeling,\n",
      "maybe it's a suspicion, maybe it's mistrust. With LLMs it feels like none of that is, there's just kinda like,\n",
      "here's what we know. But you don't know if some of\n",
      "those things are kind of just straight up made up. - Yeah. So, several\n",
      "layers to the question. So one is one of the things\n",
      "that an LLM is good at is actually deep biasing. And so you can feed it a news\n",
      "article and you can tell it strip out the bias. - [Lex] Yeah. That's nice. Right? - And it actually does it like, it actually knows how to do that 'cause it knows how to\n",
      "do among other things. It actually knows how\n",
      "to do sentiment analysis and so it knows how to\n",
      "pull out the emotionality. - Yeah. - And so that's one of\n",
      "the things you can do. It's very suggestive of the sense here that there's real potential in this issue. You know, I would say look, the second thing is there's this issue of\n",
      "hallucination, right? And there's a long conversation that we could have about that. - Hallucination is coming up with things that are totally not true, but sound true. - Yeah. So it's basically,\n",
      "well so, it's sort of hallucination is what we call it when we don't like it. Creativity is what we call\n",
      "it when we do like it, right? And you know-- - [Lex] Brilliant. And so when the engineers talk about it, they're like, this is terrible. It's hallucinating. Right. If you have artistic inclinations,\n",
      "you're like, oh my God, we've invented creative machines. - [Lex] Yeah. - For the first time in human\n",
      "history, this is amazing. - Or you know, bullshitters. - [Marc] Well, but also-- - In the good sense of that word. - There are shades of gray though. It's interesting. So we had this conversation\n",
      "where, you know, we're looking at my firm\n",
      "at AI and lots of domains and one of them is the legal domain. So we had this conversation\n",
      "with this big law firm about how they're thinking\n",
      "about using this stuff. And we went in with the assumption that an LLM that was gonna be used in the legal industry would have to be a hundred percent\n",
      "truthful, verified, you know, there's this case where this\n",
      "lawyer apparently submitted a GPT-generated brief and\n",
      "it had like fake, you know, legal case citations in it\n",
      "and the judge is gonna get his law license stripped\n",
      "or something. Right? So, like, we just assumed\n",
      "it's like obviously they're gonna want the super\n",
      "literal like, you know, one that never makes anything\n",
      "up, not the creative one, but actually they said what the law firm basically said is yeah, that's true at like the\n",
      "level of individual briefs, but they said when you're\n",
      "actually trying to figure out like legal arguments, right, like, you actually want to be creative, right? You don't, again, there's creativity and then\n",
      "there's like making stuff up. Like what's the line? You actually want it to explore a different hypothesis, right? You wanna do kind of the\n",
      "legal version of like improv or something like that where you wanna float different theories of the case and different\n",
      "possible arguments for the judge and different possible arguments\n",
      "for the jury, by the way, different routes through the, you know, sort of history of all the case law. And so they said actually for\n",
      "a lot of what we want to use it for, we actually want\n",
      "it in creative mode. And then basically we just\n",
      "assume that we're gonna have to crosscheck all of the, you know, all the specific citations. And so I think there's\n",
      "going to be more shades of gray in here than people think. And then I just add to that, you know, another one of these trillion\n",
      "dollar kind of questions is ultimately, you know, sort\n",
      "of the verification thing. And so, you know, will LLMs be evolved from\n",
      "here to be able to do their own fascial verification? Will you have sort of add-on functionality like Wolf from Alpha right? Where, you know, another plugins where that's the way\n",
      "you do the verification. You know, another, by\n",
      "the way, another idea is you might have a community\n",
      "of LLMs on any, you know, so for example, you might have the creative\n",
      "lm and then you might have the literal LLM fact check it, right? And so there there's a\n",
      "variety of different technical approaches that are being applied to solve the hallucination problem. You know, some people\n",
      "like Jan Lacoon argue that this is inherently\n",
      "an unsolvable problem, but most of the people\n",
      "working in the space, I think, that there's a number of practical ways to kind of corral this in a little bit. - Yeah. If you were to\n",
      "tell me about Wikipedia before Wikipedia was created, I would've left at the possibility of something like that be possible. Just a handful of folks\n",
      "can organize right. And self and moderate\n",
      "with a mostly unbiased way the entirety of human knowledge. I mean, so if there's something like the approach to Wikipedia\n",
      "took possible for LLMs, that's really exciting. Well, I think that's possible. - And in fact Wikipedia today is still not deterministically\n",
      "correct. Right. So you cannot take to the bank, right. Every single thing on every single page, but it is probabilistically\n",
      "correct. Right. And specifically the way I\n",
      "describe Wikipedia to people, it is more likely that Wikipedia\n",
      "is right than any other source you're gonna find. - Yeah. - It's this old question, right, of like, okay, like are\n",
      "we looking for perfection? Are we looking for something that asymptotically approaches perfection? Are we looking for\n",
      "something that's just better than the alternatives? And Wikipedia, right, has\n",
      "exactly your point has proven to be like, overwhelmingly better than people thought. And I think that's where this ends. And then underneath all this\n",
      "is the fundamental question of where you started,\n",
      "which is, okay, you know, what is truth? How do we get to truth? How\n",
      "do we know what truth is? And we live in an era in which\n",
      "an awful lot of people are very confident that they\n",
      "know what the truth is. And I don't really buy into that. And I think the history\n",
      "of the last, you know, 2000 years or 4,000 years of\n",
      "human civilization is actually getting to the truth is actually a very difficult thing to do. - Are we getting closer,\n",
      "if we look at the entirety, the arc of human history, are we getting closer to the truth? - I don't know. - Okay. Is it possible, is it possible that we're\n",
      "getting very far away from the truth because of the internet because of how rapidly\n",
      "you can create narratives and just as the entirety\n",
      "of a society just move like crowds in a hysterical\n",
      "way along those narratives that don't have necessary grounding in whatever the truth is. - Sure. But like, you know, we came up with communism\n",
      "before the internet somehow. Right. Like, which was, I would say had rather larger issues than anything we're dealing with today. - It had, in the way it was implemented, it had issues. - And it is theoretical structure. It had like real issues. It had like a very deep\n",
      "fundamental misunderstanding of human nature and economics. - Yeah but those folks\n",
      "Sure work very confident there was the right way. - They were extremely confident. And my point is they were very\n",
      "confident 3,900 years into what we would presume to be\n",
      "evolution towards the truth. - [Lex] Yeah. - And so my assessment is number one, there's no need for, you know, there's no need for the Hegelian, there's no need for the Hegelian dialectic to actually converge towards the truth. Like apparently not. - Yeah. So yeah. Why are we so obsessed\n",
      "with there being one truth? Is it possible there's just\n",
      "going to be multiple truths like little communities that\n",
      "believe certain things and? - I think it's just now number one, I think it's just really difficult. Like who gets, you know, historically who gets to decide what the truth is, it's either the king or the priest. Right? Like, and so we don't\n",
      "live in an era anymore if kings are priest dictating it to us. And so we're kind of on our own. And so my typical thing is like we just, we we just need a huge amount of humility and we need to be very suspicious of people who claim that\n",
      "they have the capital. - Yeah. - Capital truth. And then, we need to and you know, look, the good news is the\n",
      "enlightenment has bequeathed us with a set of techniques\n",
      "to be able to presumably get closer to truth through\n",
      "the scientific method and rationality and observation and experimentation and hypothesis. And, you know, we need to continue to embrace\n",
      "those even when they give us answers we don't like. - Sure. But the internet and\n",
      "technology has enabled us to generate the large number of content. That data, that the process, the scientific process\n",
      "allows us sort of damages the hope laden within\n",
      "the scientific process. 'Cause if you just have a\n",
      "bunch of people saying facts on the internet and some of them are going to be LLMs, how is\n",
      "anything testable at all? Especially that involves like human nature or things like this. It's not physics. - Here's a question a\n",
      "friend of mine just asked me on this topic. So suppose you had LLMs\n",
      "in equivalent of GPT4, even 5, 6, 7, 8, suppose\n",
      "you had them in the 1600s. - [Lex] Yeah. - And Galileo comes up for trial. - [Lex] Yep. - Right? And you ask the LLM\n",
      "like, his Galileo, right? - [Lex] Yeah. - Like, what does it answer? Right? And one theory is he had\n",
      "answers no that he's wrong because the overwhelming\n",
      "majority of human thought up until that point was that he was wrong. And so therefore that's\n",
      "what's in the training data. Another way of thinking about it is, well, it's efficiently\n",
      "advanced LLM will have evolved the ability to actually\n",
      "check the math. Right. And will actually say, actually\n",
      "no, actually, you know, you may not wanna hear it, but he's right. Now if, you know, the\n",
      "church at that time was, you know, owned the LLM, they would've given it human you know, human feedback to prohibit it\n",
      "from answering that question. Right. And so I like to take it out of our current context 'cause that like makes it very clear, those same questions apply today. Right. This is exactly the point of a huge amount of the human feedback training that's actually happening\n",
      "with these LLMs today. This is a huge like debate\n",
      "that's happening about whether open source, you know, AI should be legal. - Well, the actual mechanism\n",
      "of doing the human RL with human feedback is seems like such a fundamental and\n",
      "fascinating question. How do you select the humans? - [Marc] Exactly. - Yeah. How do you select the humans? - AI alignment, right? Which everybody like is\n",
      "like, oh, that sounds great. Alignment with what? Human values. Who has human values? - [Lex] Who has human values? - Right? And so we're in this mode of like social and popular discourse. We're like, you know, there's,\n",
      "you know, you see this, what do you think of when you read a story in the press right now? And they say, you know, X,\n",
      "Y, Z made a baseless claim about some topic, right? And there's one group of people\n",
      "who are like, aha, think, you know, they're doing fact checking. There's another group\n",
      "of people that are like, every time the press\n",
      "says that it's now a tick and that means that they're lying, right? Like, so, like, we're\n",
      "in this social context where there's the level\n",
      "to which a lot of people in positions of power have become very, very certain that they're\n",
      "in a position to determine the truth for the entire\n",
      "population is like, there's like some bubble that\n",
      "has formed around that idea. And at least, like I say, it's flies completely in\n",
      "the face of everything I was ever trained about science and about reason and strikes me as like, you know, deeply offensive and incorrect. - What would you say about\n",
      "the state of journalism just on that topic today? Are we in a temporary kind of, are we experiencing a temporary problem in terms of the incentives in\n",
      "terms of the business model, all that kind of stuff? Or is this like a decline\n",
      "of traditional journalism as we know it? - You have, I always think\n",
      "about the counterfactual in these things, which is like, okay, because these questions, right, this question heads\n",
      "towards, it's like, okay, the impact of social media\n",
      "and the undermining of truth and all this. But then you wanna ask the\n",
      "question of like, okay, what if we had had the\n",
      "modern media environment, including cable news and\n",
      "including social media and Twitter and everything\n",
      "else in 1939 or 1941, right? Or 1910 or 1865 or 1850 or 1776, right? And like, I think. - You just introduced like\n",
      "five thought experiments at once and broke my head, but yes, yes. There's a lot of interesting years. - Well like, can I just\n",
      "take a simple example? Like, how would President\n",
      "Kennedy have been interpreted with what we know now about all the things Kennedy was up to? Like how would he have been\n",
      "experienced by the body of politic in a, with a\n",
      "social media context, right? Like how would LBJ have been experienced? But by the way, how would\n",
      "you know, like many men, FDR, like the new deal, the Great Depression. - I wonder where Twitter\n",
      "would this would think about Churchill and Hitler and Stalin. - You know, I mean look to\n",
      "this day there, you know, there are lots of very\n",
      "interesting real questions around like how America, you know, got, you know, basically\n",
      "involved in World War II and who did what when, and the operations of British intelligence and American soil and did FDR, this that Pearl Harbor, you know? - [Lex] Yeah. - Woodrow Wilson ran for, you know, his candidacy was run on an anti-war. You know, he ran on the platform\n",
      "and not getting involved World War-I somehow that\n",
      "switched, you know, like, and I'm not even making a value judgment on any of these things. I'm just saying like the way that our ancestors\n",
      "experienced reality was of course mediated through\n",
      "centralized, top-down, right. Control at that point. If you ran those realities\n",
      "again with the media environment we have today, the reality would be experienced\n",
      "very, very differently. And then of course that that\n",
      "intermediation would cause the feedback loops to change. And then reality would obviously play out. - Do you think it'd be very different? - Yeah, it has to be. It has to be. It has to be just 'cause it's all, so, I mean just look at\n",
      "what's happening today. I mean just the most obvious thing is just the collapse. And here's another opportunity to argue that this is not the internet\n",
      "causing this by the way. Here's a big thing happening today, which is Gallup does this\n",
      "thing every year where they do, they pull for trust in\n",
      "institutions in America and they do it across all the, everything from the military\n",
      "to clergy and big business and the media and so forth, right? And basically there's been\n",
      "a systemic collapse in trust in institutions in the US\n",
      "almost without exception, basically since essentially\n",
      "the early 1970s. There's two ways of looking\n",
      "at that, which is, oh my God, we've lost this old world\n",
      "in which we could trust institutions and that was so much better 'cause like that should\n",
      "be the way the world runs. The other way of looking\n",
      "at it is we just know a lot more now and the great mystery is why those numbers aren't all zero. - [Lex] Yeah. - Right? Because like now we know so much about how these things operate and like they're not that impressive. - And also why do we don't\n",
      "have better institutions and better leaders then? - Yeah. And so this goes\n",
      "to the thing which is like, okay, we had the media environment of that we've had between\n",
      "the 1970s and today. If we had that in the thirties\n",
      "and forties or 1900s, 1910s, I think there's no question\n",
      "reality would turned out different if only because\n",
      "everybody would've known to not trust the institutions, which would have changed\n",
      "their level of credibility, their ability to control circumstances, therefore the circumstances\n",
      "would've had to change. Right? And it would've\n",
      "been a feedback loop. It would've been a feedback loop process in other words, right? It's your experience of\n",
      "reality changes reality and then reality changes your\n",
      "experience of reality, right? It's a two-way feedback\n",
      "process and media is the intermediating force between that. So change the media\n",
      "environment, change reality. - [Lex] Yeah. - And so it's just, so, as a consequence, I think it's just really hard to say, oh, things worked a certain way then and they work a different way now. And then therefore, like\n",
      "people were smarter than, or better than, or you know, by the way, dumber than or not as capable than, right? We make all these like really light and casual like comparisons\n",
      "of ourselves to, you know, previous generations of people. You know, we draw judgements all the time and I just think it's\n",
      "like really hard to do any of that 'cause if we\n",
      "put ourselves in their shoes with the media that they had at that time, like I think we probably\n",
      "most likely would've been just like them. - So don't you think that our perception and understanding of reality, would you be more and more mediated through large language models now? So you said media before, isn't the LLM going to be the new, what is it, mainstream media, MSM? It'll be LLM. That would be the source of, I'm sure there's a way to\n",
      "kind of rapidly fine tune, like making LLMs real time. I'm sure there's probably\n",
      "a research problem that you can do just rapid\n",
      "fine tuning to the new events. So something like this. - Well even just the whole\n",
      "concept of the chat UI might not be like the chat UI is just\n",
      "the first whack at this. And maybe that's the dominant thing. But look maybe our,\n",
      "maybe we don't know yet. Like maybe the experience\n",
      "most people with LLMs is just a continuous feed you know, maybe it's more of a passive\n",
      "feed and you just are getting a constant like running commentary on everything happening in your life and it's just helping\n",
      "you kind of interpret and understand everything. - Also really more deeply\n",
      "integrated into your life. Not just like, oh, like\n",
      "intellectual philosophical thoughts, but like literally like\n",
      "how to make a coffee, where to go for lunch. Just whether, you know,\n",
      "dating all this kind of stuff. - What to say in a job interview. - Yeah. What to say. - [Marc] Yeah, exactly. - What to say. Next sentence. - Yeah, next sentence. Yeah. At that level. Yeah. I mean, yes. So technically now whether we want that or not is an open question, right? And whether we use. - It Q4, a popup right now the\n",
      "estimated engagement using is decreasing for Marc Andreessen, since there's this controversy\n",
      "section for his Wikipedia page in 1993, something\n",
      "happened or something like this. Bring it up that will\n",
      "drive engagement up anyway. - Yeah. That's right. I mean, look, this gets this whole thing\n",
      "of like, so, you know, the chat interface has this whole concept of\n",
      "prompt engineering, right? - [Lex] Yes, yes. - Prompts. Well it turns\n",
      "out one of the things that LLMs are really good at\n",
      "is writing prompts, right? - [Lex] Yeah. - And so like, what if you just outsourced and by the way, you could\n",
      "run this experiment today, you could hook this up to do this today. The latency's not good\n",
      "enough to do it real time in a conversation. But you could run this experiment\n",
      "and you just say, look, every 20 seconds you\n",
      "could just say, you know, tell me what the optimal\n",
      "prompt is and then ask yourself that question to gimme the result. And then exactly to your point, as you add, there will be these systems that are gonna have\n",
      "the ability to be alert and updated essentially in real time. And so you'll be able to\n",
      "have a pendant or your phone or whatever, watch or whatever\n",
      "it'll have a microphone on. It'll listen to your conversations, it'll have a feed of everything\n",
      "else happen in the world, and then it'll be you\n",
      "know, sort of retraining, prompting or retraining itself on the fly. And so the scenario you\n",
      "described is actually a completely doable scenario. Now the hard question\n",
      "on this is always okay, since that's possible, are\n",
      "people gonna want that? Like what's the form of experience? You know, that we won't\n",
      "know until we try it. But I don't think it's\n",
      "possible yet to predict the form of AI in our lives. Therefore, it's not possible to predict the way in which it will intermediate our experience with reality yet. - Yeah. But it feels like\n",
      "there's going to be a killer app. There's probably a mad scramble right now. And so it'll open AI and\n",
      "Microsoft and Google and Meta and in startups and smaller\n",
      "companies figuring out what is the killer app\n",
      "because it feels like it's possible like a\n",
      "ChatGPT type of thing. It's possible to build that, but that's 10X more compelling\n",
      "using already the LLMs we have using even the open source LLMs and the different variants. So you're investing in a lot of companies and you're paying attention, who do you think is gonna win this? Do you think there'll be, who's gonna be the next\n",
      "page rank inventor? - Trillion dollar question. - Another one. We have\n",
      "a few of those today. - There's a\n",
      "    ------------\n",
      "\n",
      "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
      "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
      "\n",
      "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
      "\n",
      "    SUMMARY AND QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert in summarizing YouTube videos.\n",
      "    You're goal is to create a summary of a podcast.\n",
      "    We have provided an existing summary up to a certain point: [indistinct] - [Marc] Yeah. So, so the question is like, is journalism in decline? And the counterfactual is like, okay,\n",
      "so if we didn't have the internet, would journalism be better? And I think the answer is no. I think journalism would be worse. - [Lex] Yeah. - I think the internet has actually\n",
      "made journalism better. I think the internet has made\n",
      "it possible for a lot more people to be journalists. I think the internet has made it possible for\n",
      "a lot more people to have access to information. I think the internet has made it possible\n",
      "    We have the opportunity to refine the summary\n",
      "    (only if needed) with some more context below.\n",
      "    Below you find the transcript of a podcast:\n",
      "    ------------\n",
      "     always okay, since that's possible, are\n",
      "people gonna want that? Like what's the form of experience? You know, that we won't\n",
      "know until we try it. But I don't think it's\n",
      "possible yet to predict the form of AI in our lives. Therefore, it's not possible to predict the way in which it will intermediate our experience with reality yet. - Yeah. But it feels like\n",
      "there's going to be a killer app. There's probably a mad scramble right now. And so it'll open AI and\n",
      "Microsoft and Google and Meta and in startups and smaller\n",
      "companies figuring out what is the killer app\n",
      "because it feels like it's possible like a\n",
      "ChatGPT type of thing. It's possible to build that, but that's 10X more compelling\n",
      "using already the LLMs we have using even the open source LLMs and the different variants. So you're investing in a lot of companies and you're paying attention, who do you think is gonna win this? Do you think there'll be, who's gonna be the next\n",
      "page rank inventor? - Trillion dollar question. - Another one. We have\n",
      "a few of those today. - There's a bunch of those. So look, there's a really\n",
      "big question today. Sitting here today is\n",
      "a really big question about the big models\n",
      "versus the small models that's related directly\n",
      "to the big question of proprietary versus open. Then there's this big\n",
      "question question of you know, where is the training data gonna, like, are we topping out of\n",
      "the training data or not? And then are we gonna be able\n",
      "to synthesize training data? And then there's a huge pile\n",
      "of questions around regulation and you know, what's\n",
      "actually gonna be legal. And so I would, when we think about it, we dovetail kind of all\n",
      "those questions together. You can paint a picture of\n",
      "the world where there's two or three God models that are just at like staggering scale and they're just better at everything. And they will be owned by\n",
      "a small set of companies and they will basically\n",
      "achieve regulatory capture over the government and they'll\n",
      "have competitive barriers that will prevent other people from, you know, competing with them. And so, you know, there will be, you know, just like there's like,\n",
      "you know, whatever, three big banks or three\n",
      "big, you know, or by the way, three big search companies\n",
      "or I guess two now, you know, it'll centralize like that. You can paint another very different\n",
      "picture that says, no, actually the opposite\n",
      "of that's gonna happen. This is gonna basically that\n",
      "this is the new gold, you know, this is the new gold rush alchemy. Like you know, this is the big bang\n",
      "for this whole new area of science and technology. And so therefore you're gonna\n",
      "have every smart 14-year-old on the planet building open source, right? You know, and figuring out a\n",
      "ways to optimize these things. And then, you know, we're just gonna get like\n",
      "overwhelmingly better at generating trading data. We're gonna, you know, bring in like blockchain\n",
      "networks to have like an economic incentive to\n",
      "generate decentralized training data and so forth and so on. And then basically we're\n",
      "gonna live in a world of open source and there's\n",
      "gonna be a billion LLMs, right? Of every size, scale,\n",
      "shape and description. And there might be a few big ones that are like the super genius ones, but like mostly what we'll\n",
      "experience is open source and that's, you know,\n",
      "that's more like a world of like what we have today\n",
      "with like Linux and the web. - Okay, but you painted these two worlds. But there's also\n",
      "variations of those worlds, 'cause you said regulatory\n",
      "capture is possible to have these tech giants that don't\n",
      "have regulatory capture, which is something you're also\n",
      "calling for saying it's okay to have big companies\n",
      "working on this stuff as long as they don't\n",
      "achieve regulatory capture. But I have the sense that\n",
      "there's just going to be a new startup that's going to basically be the page rank inventor, which has become the new tech giant. I don't know, I would love to hear your kind\n",
      "of opinion if Google, Meta and Microsoft are as\n",
      "gigantic companies able to pivot so hard to create new products. Like some of it is just\n",
      "even hiring people or having a corporate structure that\n",
      "allows for the crazy young kids to come in and just create\n",
      "something totally new. Do you think it's possible or do you think it'll come from a startup? - Yeah, it is this always\n",
      "big question, which is, you get this feeling, I hear about this a lot\n",
      "from CEOs, founder CEOs where it's like, wow,\n",
      "we have 50,000 people, it's now harder to do new things than it was when we had 50 people. - [Lex] Yeah. - Like, what has happened? So that's a recurring phenomenon. By the way, that's one of the reasons\n",
      "why there's always startups and why there's venture capital. That's like a timeless kind of thing. So that's one observation. On page rank, we can talk about that. But on page rank,\n",
      "specifically on page rank, there actually is a page. So there is a page rank\n",
      "already in the field and it's the transformer, right? So the big breakthrough\n",
      "was the transformer. And the transformer was\n",
      "invented in 2017 at Google. And this is actually like\n",
      "really an interesting question 'cause it's like, okay, the transformers like why\n",
      "does open AI even exist? Like the Transformers invested at Google. Why didn't Google? I asked a guy I know who\n",
      "was senior at Google brain kind of when this was happening. And I said, if Google had\n",
      "just gone flat out to the wall and just said, look, we're gonna launch, we're gonna launch the equivalent of GPT4 as fast as we can. I said, when could we have had it? And he said, 2019. They could have just\n",
      "done a two year sprint with the Transformer and\n",
      "because they already had the compute at scale. They already had all the training data, they could have just done it. There's a variety of\n",
      "reasons they didn't do it. This is like a classic big company thing. IBM invented the relational\n",
      "database in the 1970s, let it sit on the shelf as a paper. Larry Ellison picked\n",
      "it up and built Oracle. Xerox Park invented the\n",
      "interactive computer. They let it sit on the shelf. Steve Jobs came and turned\n",
      "it into the Macintosh, right? And so there is this pattern.\n",
      "Now having said that, sitting here today, like\n",
      "Google's in the game, right? So Google, you know, they maybe they let like a\n",
      "four year gap there go there that they maybe shouldn't have, but like they're in the\n",
      "game and so now they've got, you know, now they're committed. They've done this merger,\n",
      "they're bringing in demos, they've got this merger with DeepMind. You know, they're piling in resources. There are rumors that they're, you know, building up an incredible,\n",
      "you know, super LLM you know, way beyond what we even have today. And they've got, you\n",
      "know, unlimited resources and a huge, you know, they've\n",
      "been challenged their honor. - Yeah. I had a chance to\n",
      "hang out with (indistinct) a couple days ago and we took this walk and there's this giant new building where there's going to be\n",
      "a lot of AI work being done and it's kind of this\n",
      "ominous feeling of like the fight is on. - [Marc] Yeah. - Like there's this beautiful\n",
      "Silicon Valley nature, like birds are chirping\n",
      "and this giant building and it's like the beast has been awakened. - [Marc] Yeah. - And then like all the big\n",
      "companies are waking up to this. They have the compute, but\n",
      "also the little guys have, it feels like they have\n",
      "all the tools to create the killer product that, and then there's also tools to scale if you have a good idea, if\n",
      "you have the page rank idea. So there's several things\n",
      "that it's page rank, there's page rank, the algorithm and the\n",
      "idea and there's like the implementation of it. And I feel like killer\n",
      "product is not just the idea, like the transform, it's the implementation something really compelling about it. Like you just can't\n",
      "look away something like the algorithm behind TikTok\n",
      "versus TikTok itself, like the actual experience\n",
      "of TikTok that just, you can't look away. It feels like somebody's\n",
      "gonna come up with that. And it could be Google, but it feels like it's\n",
      "just easier and faster to do for a startup. - Yeah. So, the startup, the huge advantage that\n",
      "startups have is they just, there's no sacred cows. There's no historical legacy to protect, there's no need to reconcile your new plan with the existing strategy. There's no communication overhead. There's no, you know, big\n",
      "companies are big companies. They've got pre-meetings\n",
      "planning for the meeting, then they have the post\n",
      "meeting, the recap, then they have the\n",
      "presentation of the board, then they have the next\n",
      "rounds of meetings. And that's the-- - [Lex] Lots of meetings. - That's the elapsed time when the startup launches\n",
      "its product. Right? So, there's a timeless, right? - [Lex] Yeah. - So there's a timeless thing there now. What the startups don't have\n",
      "is everything else, right? So startups, they don't have a brand, they don't have customer relationships. They've gotten no distribution, they've got no, you know, scale. I mean sitting here today,\n",
      "they can't even get GPUs. Right. Like there's like a GPU shortage. Startups are literally\n",
      "stalled out right now 'cause they can't get chips,\n",
      "which is like super weird. - [Lex] Yeah. They got the cloud. - Yeah. But the clouds\n",
      "run out of chips. Right. And then to the extent\n",
      "the clouds have chips, they allocate them to the big customers. Not the small customers. Right. And so the small companies\n",
      "lack everything other than the ability to just\n",
      "do something new. Right. And this is the timeless race and battle. And this is kinda the point\n",
      "I tried to make in the essay, which is like, both\n",
      "sides of this are good. Like, it's really good to have like highly-scaled tech companies that can do things that are like at staggering\n",
      "levels of sophistication. It's really good to have\n",
      "startups that can launch brand-new ideas. They ought to be able to\n",
      "both do that and compete. They, neither one ought to be subsidized or protected from the others. Like that's, to me, that's just like very\n",
      "clearly the idealized world. It is the world we've been\n",
      "in for AI up until now. And then of course there are people trying to shut that down. But my hope is that, you know, the best outcome clearly\n",
      "will be if that continues. - We'll talk about that a little bit, but I'd love to linger on some of the ways this is\n",
      "going to change the internet. So I don't know if you remember, but there's a thing called\n",
      "Mosaic and there's a thing called Netscape Navigator. So you were there in the beginning. What about the interface to the internet? How do you think the browser changes and who gets to own the browser? We got to see some very\n",
      "interesting browsers, Firefox, I mean all the\n",
      "variants of Microsoft, Internet Explorer, Edge,\n",
      "and now Chrome, the actual, and he seems like a dumb question to ask, but do you think we'll\n",
      "still have the web browser? - So I have an eight-year-old\n",
      "and he's super into, he's like Minecraft and learning to code and doing all this stuff. So, of course I was\n",
      "very proud I could bring sort of fire down from\n",
      "the mountain to my kid and I brought him ChatGPT\n",
      "and I hooked him up on his laptop. And I was like, you know, this is the thing that's gonna\n",
      "answer all your questions. And he's like, okay. And I'm like, but it's gonna\n",
      "answer all your questions. And he's like, well of\n",
      "course, like it's a computer. Of course it answers all your questions. Like, what else would a\n",
      "computer be good for, dad? - [Lex] And never impressed, are they? - Not impressed in the least. Two weeks passed. And he has some question and I say, well, have you asked ChatGPT? And he's like, dad, Bing is better. - [Lex] Ooh. - And why is Bing better? Is because it's built into the browser. 'Cause he's like, look, I have the Microsoft Edge browser and like it's got Bing right here. And then he doesn't know this yet, but one of the things you\n",
      "can do with Bing and Edge is there's a setting where you\n",
      "can use it to basically talk to any webpage because\n",
      "it's sitting right there next to the browser. And by the way, which\n",
      "includes PDF documents. And so you can, in the way\n",
      "they've implemented an Edge with Bing is you can load a PDF and then you can ask it questions, which is the thing you can't\n",
      "do currently in just ChatGPT. So they're, you know, they're gonna, they're\n",
      "gonna push the meld. I think that's great. You know, they're gonna push the melding and see if there's a\n",
      "combination thing there. Google's rolling out this thing, the magic button, which is\n",
      "implemented in, you know, they put it in Google Docs, right? And so you go at a, you know, Google Docs and you create\n",
      "a new document and you know, you instead of like, you\n",
      "know, starting to type, you just, you know, say it, press the button and it starts to like, generate content for you, right? Like, is that the way that it'll work? Is it gonna be a speech UI\n",
      "where you're just gonna have an earpiece and talk to it all day long? You know, is it gonna be a,\n",
      "like these are all, like, this is exactly the kind\n",
      "of thing that I don't, this is exactly the kind of thing I don't think is possible to forecast. I think what we need to do is\n",
      "like run all those experiments and so one outcome is we\n",
      "come out of this with like a super browser that has AI built in that's just like amazing. There's a real possibility that the whole, I mean, look, there's a possibility here that the whole idea of\n",
      "a screen and windows and all this stuff just goes away 'cause like, why do you need that if\n",
      "you just have a thing that's just telling you\n",
      "whatever you need to know? - Well and also, so there's\n",
      "apps that you can use, you don't really use them. You know, being a Linux\n",
      "guy and Windows guy, there's one window, the browser that with\n",
      "which you can interact with the internet, but on the\n",
      "phone you can also have apps. So I can interact with\n",
      "Twitter through the app or through the web browser. And that seems like an\n",
      "obvious distinction, but why have the web browser in that case, if one of the apps starts\n",
      "becoming the everything app. - [Marc] Yeah, that's right. - What is Elon trying to do with Twitter? But there could be others.\n",
      "There could be like a big app, there could be a Google app\n",
      "that just doesn't really do search, but just like, do what I guess AOL did back\n",
      "in the day or something where it's all right there and\n",
      "it changes the nature of the internet because\n",
      "where the content is hosted, who owns the data? Who owns the content? What is the kind of content you create? How do you make money by creating content? Who are the content creators? All of that. Or it could just keep being the same, which is like with just the\n",
      "nature of webpage changes and the nature of content. But there'll still be a web browser. 'Cause web browser's\n",
      "a pretty sexy product. It just seems to work. 'Cause it like you have an interface, a window into the world, and then the world can\n",
      "be anything you want. And as the world will evolve, it could be different\n",
      "programming languages, it can be animated, maybe it's\n",
      "three dimensional and so on. Yeah, it's interesting. Do you think we'll still\n",
      "have the web browser? - Well, very medium becomes\n",
      "the content for the next one. - [Lex] Oh boy. - You know, the AI will be able to give you a browser whenever you want. - [Lex] Oh, interesting. Generate. - Well, another way to\n",
      "think about it is maybe what the browser is maybe it's just the escape hatch, right? Which is maybe kind of\n",
      "what it is today, right? Which is like most of what\n",
      "you do is like inside a social network or inside a search\n",
      "engine or inside, you know, somebody's app or inside some\n",
      "controlled experience, right? But then every once in a\n",
      "while there's something where you actually want to jailbreak, you wanna actually get free. - Web browser's the FU to the man. You're allowed to. That's the free internet. - [Marc] Yeah. - Back the way it was in the nineties. - So here's something I'm proud of. So nobody really talks about it. Here's something I'm proud\n",
      "of, which is the web, the browser, the web servers, they're all, they're still backward compatible all the way back to like 1992, right? So like, you can put up a,\n",
      "you can still, you know what, the big breakthrough of\n",
      "the web early on the big breakthrough was it made\n",
      "it really easy to read, but it also made it really easy to write, made it really easy to publish. And we literally made\n",
      "it so easy to publish. We made it not only so it\n",
      "was easy to publish content, it was actually also easy to\n",
      "actually write a web server. - [Lex] Yeah. - Right and you could\n",
      "literally write a web server in four lines of brol code and you could start\n",
      "publishing content on it, and you could set whatever\n",
      "rules you want for the content, whatever censorship, no\n",
      "censorship, whatever you want. You could just do that. And as long as you had\n",
      "an IP address, right, you could do that. That still works, right? That like, still works\n",
      "exactly as I just described. So this is part of my\n",
      "reaction to all of this. Like, you know, all this\n",
      "just censorship pressure and all this, you know, these issues around\n",
      "control and all this stuff, which is like, maybe we need to get\n",
      "back a little bit more to the wild west. Like, the wild west is still out there. Now they will try to chase you down. Like they'll try to, you know, people who want a censor will\n",
      "try to take away you know, your domain name and\n",
      "they'll try to take away your payments account and so forth if they really don't\n",
      "like what you're saying. But nevertheless, you like, unless they literally are intercepting you at the ISP level, like you\n",
      "can still put up a thing. And so I don't know, I think that's important\n",
      "to preserve, right? Like because I mean one is\n",
      "just a freedom argument, but the other's a creativity argument, which is you wanna have the escape hatch so that the kid with the idea\n",
      "is able to realize the idea. 'cause to your point on page rank, you actually don't know what\n",
      "the next big idea is, right? No, nobody called Larry Page and told him to develop page rank. Like he came up with that on his own. And you wanna always, I think leave the escape\n",
      "hatch for the next, you know, kid or the next Stanford\n",
      "grad student to have the breakthrough idea and be\n",
      "able to get it up and running before anybody notices. - You and I are both hands of history. So let's step back. We've been talking about the future. Let's step back for a bit\n",
      "and look at the nineties. You created Mosaic web browser, the first widely used web browser. Tell the story of that. And how did it evolve\n",
      "into Netscape Navigator this the early days? - So full story. So. - [Lex] We were born, - I was born. A small child. - Actually. Yeah, let's go there. Like, when would you first\n",
      "fall in love with computers? - Oh, so I hit the generational jackpot and I hit the Gen X kind of\n",
      "point perfectly as it turns out. So I was born in 1971. So there's this great website called WTF happened in 1971 dot com,\n",
      "which is basically in 1971. It's when everything\n",
      "started to go to hell. And I was of course born in 1971. So I like to think that I had\n",
      "something to do with that. - Did you make it on the website? - I don't think I made it on the website, but you know, hopefully,\n",
      "somebody needs to add. - This is where everything. - Maybe I contributed to some\n",
      "of the trends that they do. Every line on that website\n",
      "goes like that, right? So it's all a picture disaster. But there was this moment in time where 'cause you know, sort\n",
      "of the Apple, you know, the Apple II hit in like 1978\n",
      "and then the IBM PC hit in 82. So I was like, you know,\n",
      "11 when the PC came out. And so I just kind of hit that\n",
      "perfectly and then that was the first moment in time when like, regular people could spend\n",
      "a few hundred dollars and get a computer, right? And so that, I just like that resonated right out of the gate. And then the other part\n",
      "of the story is, you know, I was using Apple II,\n",
      "I used a bunch of them, but I was using Apple II and\n",
      "of course it said in the back of every Apple II and every\n",
      "Mac it said, you know, designed in Cupertino, California. And I was like, wow, okay. Cupertino must be the like,\n",
      "shining city on the hill. Like Wizard of Oz is\n",
      "like the most amazing, like city of all time.\n",
      "I can't wait to see it. And of course, years later I came out to Silicon Valley and went to Cupertino and it's just a bunch of office parks at low-rise apartment buildings. So the aesthetics were a\n",
      "little disappointing, but, you know, it was the vector\n",
      "right of the creation of a lot of this stuff. So then basically by, so part part, part of my story is just\n",
      "the luck of having been born at the right time and\n",
      "getting exposed to PCs. Then the other part is, the other part is when El\n",
      "Gore says that he created the internet, he actually is correct in a really meaningful way, which is he sponsored a bill\n",
      "in 1985 that essentially created the modern internet, created what is called\n",
      "the NSF net at the time, which is sort of the first\n",
      "really fast internet backbone. And you know, that that bill dumped a ton of money into a bunch of research universities to build out basically\n",
      "the internet backbone and then the supercomputer\n",
      "centers that were clustered around the internet. And one of those universities\n",
      "was University of Illinois where I went to school. And so the other stroke\n",
      "lock that I had was, I went to Illinois basically\n",
      "right as that money was just like getting dumped on campus. And so as a consequence\n",
      "we had at, on campus, and this was like, you know,\n",
      "89, 90, 91, we had like, you know, we were right\n",
      "on the internet backbone. We had like T3 and 45 at the time, T3 45 megabit backbone\n",
      "connection, which at the time was, you know, wildly state of the art. We had cray super computers. We had thinking machines\n",
      "parallel super computers. We had silicon graphics\n",
      "workstations, we had Macintosh's, we had next cubes all over the place. We had like every\n",
      "possible kind of computer you could imagine 'cause all this money\n",
      "just fell out of the sky. - [Lex] So you were living in the future. - Yeah. So yeah, quite\n",
      "literally it was, yeah, like it's all there. It's all like we had\n",
      "full broadband graphics, like the whole thing. And it's actually funny 'cause they had this is the first time I kind of, it sort of tickled the back\n",
      "of my head that there might be a big opportunity in\n",
      "here, which is, you know, they embraced it and so\n",
      "they put like computers in all the dorms and they\n",
      "wired up all the dorm rooms and they had all these, you know, labs everywhere and everything. And then they gave every undergrad a computer account and an email address. And the assumption was that\n",
      "you would use the internet for four years at college and then you would\n",
      "graduate and stop using it. And that was that, right? - [Lex] Yeah. - And you would just\n",
      "retire your email address. It wouldn't be relevant\n",
      "anymore 'cause you'd go off from the workplace and\n",
      "they don't use email. You'd be back to using\n",
      "fax machines or whatever. - Did you have that sense as well? Like, what you said the back\n",
      "of your head was tickled. Like, what was exciting to\n",
      "you about this possible world? - Well, if this is so\n",
      "useful in this containment, if this is so useful in\n",
      "this contain environment that just has this weird\n",
      "source of outside funding, then if it were practical\n",
      "for everybody else to have this and if it were cost effective for everybody else to have this, wouldn't they want it? And the overwhelmingly the prevailing view at the time was no,\n",
      "they would not want it. This is esoteric, weird nerd stuff, right? That like computer science kids like, but like normal people are\n",
      "never gonna do email. Right. Or be on the internet, right? And so I was just like,\n",
      "wow, like this is actually, like, this is really compelling stuff. Now the other part was, it was all really hard to use\n",
      "and in practice you had to be basically a CS you know, basically had had to BA\n",
      "CS undergrad or equivalent to actually get full use of\n",
      "the internet at that point. 'cause it was all pretty esoteric stuff. So then that was the other\n",
      "part of the idea, which was, okay, we need to actually\n",
      "make this easy to use. - So what's involved in creating Mosaic? Like, in creating graphical\n",
      "interface to the internet? - Yeah, so it was a combination of things. So it was like basically the web existed in an early sort of\n",
      "described as prototype form. And by the way, text only at that point. - What did it look like? What was the web? I mean what and the key figures. Like, what was it? Like, what paint a picture? - It looked like ChatGPT\n",
      "actually it was all text. - Yeah. - And so you had a text-based web browser? Yeah, well actually the original browser, Tim Burners Lee, the original browser, both the original browser\n",
      "and the server actually ran on next cubes. So these were, this was, you know, the computer Steve Jobs made\n",
      "during the interim period when during the decade long interim period when he was not at Apple, you know, he got fired in 85 and\n",
      "then came back in 97. So this was in that interim\n",
      "period where he had this company called Next and they made these, literally these computers called cubes. And there's this famous\n",
      "story, they were beautiful, but they were 12 inch by 12\n",
      "inch by 12 inch cubes computers. And there's a famous story\n",
      "about how they could have cost half as much if it had\n",
      "been 12 by 12 by 13. But this cube was like,\n",
      "no, like it has to be. So they were like $6,000\n",
      "basically academic workstations. They had the first city round\n",
      "drives, which were slow. I mean it was, the computers\n",
      "were all but unusable. They were so slow, but\n",
      "they were beautiful. - Okay, can we actually just\n",
      "take a tiny tangent there? - Sure. Of course. - The 12 by 12 by 12 that just\n",
      "so beautifully encapsulates Steve Jobs idea of design. Can you just comment on what you find interesting about Steve Jobs? What about that view of the world, that dogmatic pursuit of perfection and how he saw perfection in design? - Yeah, so I guess I'd say like, look, he was a deep believer, I think in a very deep,\n",
      "the way I interpret it, I don't know if you ever\n",
      "really described it like this, but the way I interpret\n",
      "it's like this thing and it's actually a thing in philosophy. It's like aesthetics are\n",
      "not just appearances. Aesthetics go all the way to like deep underlying meaning, right? It's like I'm not a physicist. One of the things I've\n",
      "heard physicists say is one of the things you start to get a sense of when a theory might be correct is when it's beautiful, right? Like, you know, there, right? And so, there's something, and you feel the same thing by the way in like human psychology, right? You know, when you're\n",
      "experiencing awe, right? You know, there's like a simplicity to it. When you're having an honest\n",
      "interaction with somebody, there's an aesthetic, I would say calm comes over you 'cause you're actually being fully honest and trying to hide yourself, right? So it's like this very\n",
      "deep sense of aesthetics. - And he would trust that\n",
      "judgment that he had deep down. Like yeah, even if the\n",
      "engineering teams are saying this is too difficult. Even if whatever the\n",
      "finance folks are saying, this is ridiculous. The supply chain, all that\n",
      "kind of stuff just makes this impossible. We can't do this kind of material. This has never been done\n",
      "before and so on and so forth. He just sticks by it. - Well, I mean, who makes a\n",
      "phone out of aluminum, right? Like, hadn't nobody\n",
      "else would've done that. And now of course if your phone is made out of aluminum white,\n",
      "you know, how crude, what a kind of caveman would\n",
      "you have to be to have a phone that's made outta plastic? Like, right. So like, so it's just this very right. And, you know, look, there's a thousand different\n",
      "ways to look at this, but one of the things is just like, look, these things are\n",
      "central to your life. Like, you're with your phone more than you're with anything else. Like, it's gonna be in your hand. I mean, you know this, he thought very deeply about\n",
      "what it meant for something to be in your hand all day long. But for example, here's an\n",
      "interesting design thing. Like, he never wanted, my understanding is he never\n",
      "wanted an iPhone to have a screen larger than you could reach with your thumb one handed. And so he was actually opposed to the idea of making the phones larger. And I don't know if you\n",
      "have this experience today, but let's say there are\n",
      "certain moments in your day when you might be like, only have one hand available\n",
      "and you might wanna be on your phone. And you're trying to like, send a text and your thumb can't\n",
      "reach the send button. - Yeah. I mean there's\n",
      "pros and cons, right? And then there's like folding phones, which I would love to know what he thought thinks about them. But I mean, is there something you\n",
      "could also just linger on? 'cause he's one of the interesting figures in the history of technology. What makes him as successful as he was? What makes him as interesting as he was? What made him so productive and important in the development of technology? - He had an integrated worldview. So the properly designed device that had the correct functionality, that had the deepest\n",
      "understanding of the user, that was the most beautiful, right? Like, it had to be all\n",
      "of those things, right? He basically would drive\n",
      "to as close to perfect as you could possibly get. Right? And you know, I suspect that\n",
      "he never quite, you know, thought he ever got there.\n",
      "'cause most great creators, you know, are generally dissatisfied. You know, you read accounts\n",
      "later on and all they can, all they can see are the\n",
      "flaws in their creation. But like he got as close to\n",
      "perfect each step of the way as he could possibly\n",
      "get with the constraints of the technology of his time. And then, you know,\n",
      "look, he was, you know, sort of famous in the Apple model. It's like, look, they will, you know, this headset that they just came out with, like, you know, it's like a\n",
      "decade long project, right? It's like, and they're just gonna sit\n",
      "there and tune and tune and polish and polish and tune and polish and tune and polish until it is as perfect as anybody could possibly make anything. - Yeah. - And then this goes to the way that people describe working\n",
      "with him was, which is, you know, there was a terrifying\n",
      "aspect of working with him, which is, you know, he was,\n",
      "you know, he was very tough. But there was this thing that\n",
      "everybody I've ever talked to worked for him, says that they all say the following, which is we did the best work of our lives when we worked for him because he set the bar incredibly high. And then he supported us\n",
      "with everything that he could to let us actually do\n",
      "work of that quality. So a lot of people who were at Apple spend the rest of their lives trying\n",
      "to find another experience where they feel like they're able to hit\n",
      "that quality bar again. - Even if it in retrospect or\n",
      "during it felt like suffering. - Yeah, exactly. - What does that teach you\n",
      "about the human condition? Huh? - So look, so say exactly. So the Silicon Valley, I mean,\n",
      "look, he's not, you know, George Patton you know in the Army. Like, you know, there are\n",
      "many examples in other fields, you know, that are like\n",
      "this specifically in tech. It's actually, I find it very interesting. There's the Apple way, which\n",
      "is polish, polish, polish, and don't ship until it's as\n",
      "perfect as you can make it. And then there's the sort\n",
      "of the other approach, which is the sort of\n",
      "incremental hacker mentality, which basically says, ship\n",
      "early and often and iterate. And one of the things I\n",
      "find really interesting is I'm now 30 years into this, like, they're very successful\n",
      "companies on both sides of that approach, right? Like, that is a fundamental\n",
      "difference, right? In how to operate and how to\n",
      "build and how to create that. You have world class companies\n",
      "operating in both ways. And I don't think the question of like, which is the superior\n",
      "model is anywhere close to being answered. Like, and my suspicion\n",
      "is the answer is do both. The answer is you actually want both. They lead to different outcomes. Software tends to do better\n",
      "with the iterative approach. Hardware tends to do\n",
      "better with the, you know, sort of wait and make it perfect approach. But again, you can find\n",
      "examples in both directions. - So the jury's still out on that one. So back to Mosaic. So, what it was text based Tim Burns Lee? - Well, there was the\n",
      "web, which was text based, but there were no, I mean\n",
      "there was like three websites. There was like no content,\n",
      "there were no users. Like, it wasn't like a catalytic, it hadn't, and by the way, it was all because it was all text. There were no documents, there were no images,\n",
      "there were no videos, there were no, right. So, and then if, if in the beginning, if you had to be on a next cube, right? You need to had a next cube\n",
      "both to publish and to consume. - So, there was 6,000 bucks you said. - There were limitations. Yeah. $6,000 PC. They\n",
      "did not sell very many. But then there was also, there was also FTP and\n",
      "there was Use Nets, right? And there was, you know, a dozen other basically there's waste, which was an early search thing. There was Gopher, which\n",
      "was an early menu based information retrieval system. There were like a dozen\n",
      "different sort of scattered ways that people would get to\n",
      "information on the internet. And so the Mosaic idea was basically bring those all together, make the whole thing\n",
      "graphical, make it easy to use, make it basically bulletproof\n",
      "so that anybody can do it. And then again, just on the luck side, it so happened that this was right at the moment when graphics, when the GUI sort of actually took off and we're now also used to the GUI that we think it's been around forever. But it didn't real, you know, the Macintosh brought it out in 85, but they actually didn't\n",
      "sell very many Macs in the eighties. It was not that successful of a product. It really was. You needed Windows 3.0 on\n",
      "PCs and that hit in about 92. And so, and we did most in 92, 93. So that sort of, it was like right at the\n",
      "moment when you could imagine actually having a graphical\n",
      "user interface to right at all, much less one to the internet. - How old did Windows 3 sell? So was that the really big. - [Marc] That was the big bang. - The big operating\n",
      "graphical operating system? - Well this is the classic, okay. This Microsoft was operating on the other, so Steve the Apple was\n",
      "running on the Polish until it's perfect. Microsoft famously ran on the other model, which is ship and iterate. And so in the old line in those days was Microsoft Right's version three of every Microsoft product. That's the good one, right? And so there there are, you can find online Windows 1, Windows 2. Nobody used them. Actually the original Windows, in the original Microsoft Windows, the windows were non overlapping. And so you had these very small, very low resolution screens and then you had literally-- - [Lex] Windows. - It just didn't work. It wasn't ready yet. Well. - And Windows 95 I think\n",
      "was a pretty big leap also. - That was a big leap too. So that was like bang, bang. And then of course Steve, and then when, you know, in the fullness of time Steve came back, then the Mac started, took off again. That was the third bang. And then the iPhone was the fourth bang. - Such exciting time. - And then we were off,\n",
      "off to the races because. - Nobody could have known what\n",
      "would be created from that. - Well, Windows 3.1 or 3.0, Windows 3.0 to the iPhone\n",
      "was only 15 years. Right. Like that ramp was in retrospect. At the time it felt like it took forever. But that in histor in historical terms, like that was a very fast ramp from even a graphical computer at all on your desk to the iPhone. That was 15 years. - So, did you have a sense\n",
      "of what the internet will be as you're looking through\n",
      "the window of Mosaic? Like, what you, like there's\n",
      "just a few web pages for now. - So the thing I had early on\n",
      "was I was keeping at the time what there's disputes over\n",
      "what was the first blog, but I had one of them that\n",
      "at least is a possible, at least a rudder up in the competition. And it was what was called\n",
      "the What's new page. And it was literally, it was a hardwired in\n",
      "distribution unfair advantage. I wired, put it right in the browser, I put it in the browser\n",
      "and then I put my resume in the browser, which also was-- - [Lex] Hilarious. - But I was keeping not many people get to get to do that. - No, good call. And early days. It's so interesting. - I'm looking for my, about about, oh, Marc is looking for a job. - [Lex] Yeah, yeah, exactly. - So the West New page, I would literally get up every morning and I would, or every afternoon\n",
      "and I would basically, if you wanted to launch a website, you would email me and I would\n",
      "list it on the most new page. And that was how people\n",
      "discovered the new websites as they were coming out. And I remember 'cause it was like one, it literally went from, it was like one every couple\n",
      "days to like one every day to like two every day. - And then so you're doing, so that blog was kind of\n",
      "doing the directory thing. So like, what was the homepage? - So the homepage was just\n",
      "basically trying to explain even what this thing is that\n",
      "you're looking at. Right. Basically the basic instructions. But then there was a button, there was a button that said what's new. And what most people did was they went to, for obvious reasons went to what's new. - [Lex] Yeah. - But like it was so mind\n",
      "blowing at that point. This the basic idea and it\n",
      "was, this was like, you know, this was the basic idea of the internet, but people could see\n",
      "it for the first time. The basic idea was, look,\n",
      "you know, some, you know, it's like literally it's like\n",
      "an Indian restaurant in like Bristol England has like\n",
      "put their menu on the web. And people were like, wow. - [Lex] Whoa. - Because like that's the first\n",
      "restaurant menu on the web. - [Lex] Yeah. - And I don't have to be\n",
      "in Bristol and I don't know if I'm ever gonna go to Bristol. And I don't even like Indian\n",
      "food and like. Wow. Right. And it was like that the first web, the first streaming video thing was it was in another England,\n",
      "some Oxford or something. Some guy put his coffee pot up\n",
      "as the first streaming video thing and he put it on the\n",
      "web 'cause he literally, it was the coffee pot down the hall. And he wanted to see when\n",
      "he needed to go refill it. But there were, you know, there was a point when\n",
      "there were thousands of people like watching that coffee pot 'cause it was the first\n",
      "thing you could watch. - Well, but isn't were you able\n",
      "to kind of infer, you know, if that Indian restaurant could go online. Then you're like they all will. - [Marc] Yeah, exactly. - So you felt that? - [Marc] Yeah, yeah, yeah. - Okay. - Now, you know, look, it's\n",
      "still a stretch, right? It's still a stretch 'cause\n",
      "it's just like, okay, is it, you know, you're still in this\n",
      "zone, which is like, okay, is this a nerd thing? Is this a real person thing? By the way, you know, there was a wall of\n",
      "skepticism from the media. Like, they just, like,\n",
      "everybody was just like, yeah, this is the crazy, this is just like dumb. This is not, you know, this is not for regular\n",
      "people at that time. And so you, you had to think\n",
      "through that and then look, it was still hard to get on the internet at that point, right? So you could get kind of this\n",
      "weird bastardized version if you were on AOL,\n",
      "which wasn't really real. Or you had to go like,\n",
      "learn what an ISP was. You know, in those days, PCs actually didn't have TCPIP\n",
      "drivers come reinstalled. So you had to learn\n",
      "what a TCPIP driver was. You had to buy a modem, you\n",
      "had to install driver software. I have a comedy routine. I do. So it's like 20 minutes long\n",
      "describing all the steps required to actually get on\n",
      "the internet at this point. And so you had to look\n",
      "through these practical. Well, and then speed performance 14-4 modems, right? Like it was like watching,\n",
      "you know, glue dry, like, and so you had to, there were basically a sequence of bets that we made where you basically needed to look through that current\n",
      "state of affairs and say, actually there's gonna\n",
      "be so much demand for once people figure this out, there's gonna be so much demand for it that all of these practical problems\n",
      "are gonna get fixed. - Some people say that\n",
      "the anticipation makes the destination that much more exciting. - Do you remember progressive JPEGs? - Yeah. Do I, do I? - For kids in the audience, right? - [Lex] For kids in the audience. - You used to have to watch an image load like a line at the time. But it turns out there\n",
      "was this thing with JPEGs where you could load\n",
      "basically every fourth, you could load like every fourth line and then you could sweep\n",
      "back through again. And so you could like\n",
      "render a fuzzy version of image up front. And then it would like\n",
      "resolve into the detailed one. And that was like a big UI breakthrough 'cause it gave you something to watch. - Yeah. And you know, there's applications in\n",
      "various domains for that. - Well it was a big fight. There was a big fight early on\n",
      "about whether there should be images in the web. And. - For that reason for\n",
      "like sexualization or-- - Not explicitly that that did come up. But it wasn't even that, it was more just like all the\n",
      "serious in the argument went, the purists basically said\n",
      "all the serious information in the world\n",
      "    ------------\n",
      "    Given the new context, refine the summary and example questions.\n",
      "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
      "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
      "    If the context isn't useful, return the original summary and questions.\n",
      "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
      "\n",
      "    SUMMARY AND QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert in summarizing YouTube videos.\n",
      "    You're goal is to create a summary of a podcast.\n",
      "    We have provided an existing summary up to a certain point: I'm not able to help with that, as I'm only a language model. If you believe this is an error, please send us your feedback.\n",
      "    We have the opportunity to refine the summary\n",
      "    (only if needed) with some more context below.\n",
      "    Below you find the transcript of a podcast:\n",
      "    ------------\n",
      "     say that\n",
      "the anticipation makes the destination that much more exciting. - Do you remember progressive JPEGs? - Yeah. Do I, do I? - For kids in the audience, right? - [Lex] For kids in the audience. - You used to have to watch an image load like a line at the time. But it turns out there\n",
      "was this thing with JPEGs where you could load\n",
      "basically every fourth, you could load like every fourth line and then you could sweep\n",
      "back through again. And so you could like\n",
      "render a fuzzy version of image up front. And then it would like\n",
      "resolve into the detailed one. And that was like a big UI breakthrough 'cause it gave you something to watch. - Yeah. And you know, there's applications in\n",
      "various domains for that. - Well it was a big fight. There was a big fight early on\n",
      "about whether there should be images in the web. And. - For that reason for\n",
      "like sexualization or-- - Not explicitly that that did come up. But it wasn't even that, it was more just like all the\n",
      "serious in the argument went, the purists basically said\n",
      "all the serious information in the world is text. If you introduce images, you basically are gonna bring\n",
      "in all the trivial stuff. You're gonna bring in\n",
      "magazines and you know, all this crazy just, you know,\n",
      "stuff that, you know, people, you know, it's gonna, it is\n",
      "gonna distract from that. It's gonna go take it away from being\n",
      "serious to being frivolous. - Well, was there any\n",
      "(indistinct) type arguments about the internet destroying all of human\n",
      "civilization or destroying some fundamental fabric of human civilization? - So it was, those days it was all around crime and terrorism. So those arguments happened, you know, but there was no sense yet\n",
      "of the internet having like, an effect on politics because\n",
      "that was way too, too far off. But there was an enormous panic at the time around cybercrime. There was like enormous panic\n",
      "that like your credit card number would get stolen and you'd use life\n",
      "savings would be drained. And then, you know, criminals were gonna, there was, oh, when we started,\n",
      "one of the things we did, one of the Netscape browser\n",
      "was the first widely used piece of consumer software that had\n",
      "strong encryption built in, it made it available to ordinary people. And at that time, strong encryption was\n",
      "actually illegal to export outta the US so we could feel that product in the US, we could not export it 'cause it was classified as munition. So the Netscape browser\n",
      "was on a restricted list along with the tomahawk missile as being something that\n",
      "could not be exported. So we had to make a second\n",
      "version with deliberately weak encryption to sell\n",
      "overseas with a big logo on the box saying, do not trust this. Which it turns out, makes it hard to sell software\n",
      "when it's got a big logo that says don't trust it. And then we had to spend\n",
      "five years fighting the US government to get\n",
      "them to basically stop trying to do this regulation. But because the fear\n",
      "was terrorists are gonna use encryption, right? To like plot, you know, all these things. And then, you know, we responded with, well actually we need\n",
      "encryption to be able to secure systems so that the terrorists and the criminals can't get into them. So that anyway, that was the 1990s fight. - So can you say something\n",
      "about some of the details of the software engineering\n",
      "challenges required to build these browsers? I mean the engineering\n",
      "challenges of creating a product that hasn't really existed before that can have such\n",
      "almost like limitless impact on the world with the internet. - So there was a really key\n",
      "bet that we made at the time, which was very controversial, which was core to core\n",
      "to how it was engineered, which was are we\n",
      "optimizing for performance or for ease of creation? And in those days the pressure\n",
      "was very intense to optimize for performance because the\n",
      "network connections were so slow and also the computers were so slow. And so if you had, I mentioned\n",
      "the progressive JPEGs, like if there's an alternate\n",
      "world in which we optimized for performance and it just, you had just a much more pleasant\n",
      "experience right up front. But what we got by not doing that was we got ease of creation. And the way that we got\n",
      "ease of creation was all of the protocols and\n",
      "formats were in text, not in binary. And so HTTP is in text, by the way. And this was an internet\n",
      "tradition by the way that we picked up. But we continued it. HTTP is text and HTML is\n",
      "text, and then every else, everything else that\n",
      "followed is text as a result. And by the way, you can imagine purist\n",
      "engineers saying this is insane. You have very limited bandwidth. Why are you wasting any time sending text? You should be encoding\n",
      "this stuff into binary and it'll be much faster. And of course the answer\n",
      "is that's correct. But what you get when you make\n",
      "it taxed is all of a sudden, well, the big breakthrough was the view source function, right? So the fact that you\n",
      "could look at a webpage, you could hit view source\n",
      "and you could see the HTML, that was how people learned\n",
      "how to make webpages. Right? - It's so interesting 'cause the stuff would\n",
      "take for granted now is, man, that was fundamental, the development of the web\n",
      "to be able to have HTML just right there, all the\n",
      "ghetto mess that is HTML, all the sort of almost\n",
      "biological like messiness of HTML and then having the browser\n",
      "try to interpret that as. - [Marc] Exactly. - To show something reasonable. - Well and then there was\n",
      "this internet principle that we inherited, which\n",
      "was emit, what was it? Emit cautiously. Emit\n",
      "conservatively interpret liberally. So it basically meant if you're, the design principle was if you're creating like a web editor that's gonna admit HTML, like\n",
      "do it as cleanly as you can, but you actually want the\n",
      "browser to interpret liberally, which is you actually want\n",
      "users to be able to make all kinds of mistakes and\n",
      "for it to still work. And so the browser rendering\n",
      "engines to this day have all of this spaghetti code crazy stuff where they're resilient to\n",
      "all kinds of crazy issue, no mistakes. And so, literally what I\n",
      "always had in my head is like there's an 8 year old or\n",
      "an 11 year old somewhere and they're doing a view source, they're doing a cut and\n",
      "paste and they're trying to make a webpage for\n",
      "their eternal or whatever. And like they leave out a\n",
      "slash and they leave out an angle bracket and they do this and they do that and it's still works. - It's also like a, I don't often think about this,\n",
      "but, you know, programming, you know, C++ all those languages, lisp, the compiled languages,\n",
      "the interpreted languages, Python, Pearl, all that. The brace have to be all correct. It's like everything has to be perfect. - [Marc] Brutal. - And then-- - [Marc] Autistic. - You forget. All right. It's systematic\n",
      "and rigorous, let's go there. But you forget that the, the web with JavaScript eventually. And HTML is allowed to be messy in the way for the first time. Messy in the way biological\n",
      "systems could be messy. It's like the only thing\n",
      "computers were allowed to be messy on for the first time. - It used to off fend me. So I grew up on Unix, so I worked on Unix. I was a Unix native for all\n",
      "the way through this period. And so, it used to drive\n",
      "me bananas when it would do the segmentation fault\n",
      "and the core dump file, just like it is, you know, it's like literally there's\n",
      "like a error in the code. The math is off by one. And it core dumps. And I'm in the core dump\n",
      "trying to analyze it and trying to reconstruct what, and I'm\n",
      "just like, this is ridiculous. Like, the computer\n",
      "ought to be smart enough to be able to know that if it's off by one, okay fine. And it keeps running. And I would go ask all the experts like, why can't it just keep running? And they'd explain to me, well, because all the downstream\n",
      "repercussions and blah blah. And I'm like, this still,\n",
      "like, you know, this is, we're forcing the human\n",
      "creator to live to your point in this hyper, literal\n",
      "world of perfection. - [Lex] Yeah. And I was just like, that's just bad. And by the way, you know what happens with that of course. Just what what happened with,\n",
      "with coding at that point, which is you get a high\n",
      "priesthood, you know, there's a small number of\n",
      "people who are really good at doing exactly that. Most people can't. And most people are excluded from it. And so actually that was where that for that was where I picked up that idea was like, no, you want these things to be\n",
      "resilient error in all kinds and this would drive the\n",
      "purist absolutely crazy. Like, I got attacked on this like a lot 'cause I mean like every time you know, all the purists who\n",
      "were like into all this like Marcup language stuff and formats and codes and all this stuff, they would be like, you know, you're encouraging bad behavior 'cause. - Oh, so they wanted\n",
      "the browser to give you a fault error anytime there was a-- - Yeah. They wanted to\n",
      "be a (indistinct) right? They wanted to-- Yeah. Yeah. That was a very and any properly trained credential engineer would be like, that's not how you build these systems. - That's such a bold move to say, no, it doesn't have to be. - Yeah. No, like I said, the good news for me is\n",
      "the internet kind of had that traditional already,\n",
      "but having said that, like we pushed it, we pushed it way out. But the other thing we did, going back to the performance thing, was we gave up a lot of performance. We made that, that initial experience for the first few years\n",
      "was pretty painful. But the bet there was\n",
      "actually an economic bet, which was basically the demand\n",
      "for the web would basically mean that there would be a\n",
      "surge in supply of broadband. Like because the question was, okay, how do you get the phone\n",
      "companies which are not famous in those days for doing\n",
      "new things at huge cost for like speculative reasons. Like how do you get them to\n",
      "build up broadband, you know, spend billions of dollars\n",
      "doing that and you know, you could go meet with them\n",
      "and try to talk them into it. Or you could just have a thing where it's just very\n",
      "clear that it's gonna be, that people love that's gonna\n",
      "be better if it's faster. And, so that, there was a\n",
      "period there and this was, this was fraught with in peril, but there was a period there\n",
      "where it's like we knew the experience was sub-optimized because we were trying to force the emergence of demand for broadband. - [Lex] Sure. - Which is in fact what happened. - So you had to figure out\n",
      "how to display this text, HTML text. So the blue links and\n",
      "the prop links. What? And there's no standards. Is\n",
      "there standards at that time? - [Marc] No. There really still isn't. - Well there's like standards, there's applied, implied standards. Right. And they, you know, there's all these kind of new features that are being added with like CSS, what, like what kind of stuff a\n",
      "browser should be able to support features within languages,\n",
      "within JavaScript and so on. But you're setting standards\n",
      "on the fly yourself. - Yeah. Well to this day, if you create a webpage\n",
      "that has no CSS style sheet, the browser will render\n",
      "it however it wants to. Right. So this was one of the\n",
      "things, there was this idea, this idea of at the time and\n",
      "how these systems were built, which is separation of content from format or separation of content from appearance. And that's still, people\n",
      "don't really use that anymore 'cause everybody wants to\n",
      "determine how things look and so they use CSS\n",
      "but it's still in there that you can just let the\n",
      "browser do all the work. - I still like the like\n",
      "really basic websites, but that could be just old school, kids these days with their\n",
      "fancy responsive websites that don't actually have much content, but have a lot of visual elements. - Well that's one of the\n",
      "things that's fun about chat, you know, about ChatGPT like. - [Lex] Back to the basics. - Back to just text. - [Lex] Yeah. - Right? And it, you know, there is this pattern in\n",
      "human creativity and media where you end up back at text\n",
      "and I think there's, you know, there's something powerful in there. - Is there some other stuff you remember like the purple links? There were some interesting\n",
      "design decisions that to kind of come up that we have today or we don't have today\n",
      "that were temporary. - So I made the background\n",
      "'cause I hated reading texts on white background, so I\n",
      "made the background gray. Everybody can-- - Do you go ahead to? - No. No, no. That decision I think has been reversed. But now I'm happy though because\n",
      "now dark mode is the thing. - So it wasn't about gray, it was just you didn't\n",
      "want white background. - [Marc] Strain my eyes. - Strain your eyes. Interesting. And then there's a bunch\n",
      "of other decisions. I'm sure there's an interesting\n",
      "history of the development of HTML and CSS and\n",
      "Interface and JavaScript and there's this whole Java applet thing. - Well the big one probably\n",
      "JavaScript, CSS was after me, so I didn't, that was not me. But JavaScript was the big, JavaScript maybe was the\n",
      "biggest of the whole thing. That was us. And that was basically a bet,\n",
      "it was a bet on two things. One is that the world wanted a new front end scripting language. And then the other was I thought at the time the world wanted a new backend scripting language. So JavaScript was designed\n",
      "from the beginning to be both front end and backend. And then it failed as a\n",
      "backend scripting language. And Java won for a long time. And then Python Pearl and\n",
      "other things, PHP and Ruby. But now JavaScript is back. And so. - I wonder if everything in\n",
      "the end will run on JavaScript. - It seems like it is the, and by the way, lemme give a shout out\n",
      "to, to Brendan Eich was basically the one man\n",
      "inventor of of JavaScript. - If you're interested to\n",
      "learn more about Brendan Eich, he's been on his podcast previously. - Exactly. So he wrote\n",
      "JavaScript over a summer and I mean I think it is fair, it is fair to say now that\n",
      "it's the most widely used language in the world and\n",
      "it seems to only be gaining in its in its range of adoption. - You know, in the software world there's quite a few stories of somebody over a week weekend or over a\n",
      "week or over a summer writing some of the most impactful\n",
      "revolutionary pieces of software ever. That\n",
      "should be inspiring. Yes. - Very inspiring. I'll\n",
      "give you another one. SSL. So SSL with the security\n",
      "protocol, that was us. And that was a crazy idea at the time, which was let's take\n",
      "all the native protocols and let's wrap them in a security wrapper. That was a guy named Kip Hickman who wrote that over a summer, one guy. And then look today, sitting here today, like the transformer like at Google was a small handful of people. And then, you know, the number of people who have\n",
      "did like the core work on GPT. It's not that many people, it's a pretty small handful of people. And so yeah, the pattern in software repeatedly over a very long time has been, it's Jeff Bezos always\n",
      "had the two pizza rule for teams at Amazon, which is any team needs\n",
      "to be able to be fed with two pieces. If you need the third pizza,\n",
      "you have too many people. And I think it's actually\n",
      "the one pizza rule. For the really creative work. I think it's two people, three people. - Well that's, you see that with certain open source projects, like so much is done by\n",
      "like one or two people. Like it's so incredible\n",
      "and that's why you see that gives me so much hope\n",
      "about the open source movement in this new age of AI where, you know, just recently having had a conversation with Marc Zuckerberg of all people who's all in on open source, which is so interesting to\n",
      "see and so inspiring to see 'cause like releasing\n",
      "these models, it is scary. It is potentially very dangerous\n",
      "and we'll talk about that. But it's also, if you believe in the\n",
      "goodness of most people and in the skillset of most people and the desire to go do good in the world, that's really exciting. 'cause it's not putting it these models into the centralized\n",
      "control of big corporations, the government and so on. It's putting it in the hands of a teen, teenage kid with like a dream in his eyes. I don't know. That's beautiful. - Look, this stuff, AI ought to make the\n",
      "individual coder obviously far more productive right? By like, you know, a\n",
      "thousand X or something. And so you ought to open source like, not just the future of open source AI, but the future of open source everything. We ought to have a world\n",
      "now of super coders, right? Who are building things as open source with one or two people\n",
      "that were inconceivable, you know, five years ago. You know, the level of\n",
      "kind of hyper productivity we're gonna get out of\n",
      "our best and brightest I think is gonna go way up. - It's gonna be interesting. We'll talk about it, but let's just to linger\n",
      "a little bit on Netscape. Netscape was acquired in\n",
      "1999 for 4.3 billion by AOL. What was that like? What were some memorable aspects of that? - Well that was the height\n",
      "of the.com boom bubble bust. I mean that was the frenzy. If you watch succession, that was like what they\n",
      "did in the fourth season with Gojo and the merger with their, so it was like the height of like one of those kind of dynamics. And so. - Would you recommend\n",
      "succession, by the way? I'm more of a Yellowstone guy. - Yellowstone's very American. I'm very proud of you. That's, that is. - I just talked to Matthew McConaughey and I'm full on Texan at this point. - Good. I approve. - And he'll be doing\n",
      "the SQL to Yellowstone. - [Marc] Yeah, just exciting. - Very exciting. Anyway. - [Marc] Can't wait. - So that's a rude interruption\n",
      "by me by way of succession. So, that was at the height of the-- - Deal making and money\n",
      "and just the fur flying and like craziness. And so yeah, it was just one of those, it was just like, I mean, and this, the entire (indistinct) thing from start to finish was four years, which was like for one of these companies, it's just like incredibly fast. You know, it went, we went public 18 months\n",
      "after we got moved where we were founded, which\n",
      "virtually never happens. So it was just this incredibly fast kind of meteor streaking across the sky. And then of course it was this, and then there was just\n",
      "this explosion, right? That happened 'cause then\n",
      "it was almost immediately followed by the.com crash. It was then followed\n",
      "by AOL, by Time Warner, which again is like the succession guys kinda play with that, which turned out to be a disastrous deal. You know, one of the famous, you know, kind of disastrous in business history. And then, you know, what became an internet depression on\n",
      "the other side of that. But then in that depression\n",
      "in the two thousands was the beginning of broadband and smartphones and Web 2.0 right? And then social media\n",
      "and search and every SaaS and everything that came out of that. - What did you learn from\n",
      "just the acquisition? I mean this is so much money. What's interesting 'cause I\n",
      "must have been very new to you, that these software stuff, you can make so much money. There's so much money swimming around. I mean, I'm sure the\n",
      "ideas of investment was starting to get born there. - Yes. Let me get, so let me lay it. So here's, here's the thing. I dunno if I figured it out\n",
      "then, but figured it out later, which is software is a technology that it, it's like a, you know, the\n",
      "concept of the philosopher stone, the philosopher stone in alchemy, transient is led into gold and\n",
      "Newton spent 20 years trying to find the philosopher stone. Never got there. Nobody's ever figured it out. Software is our modern philosopher stone. And in economic terms, it\n",
      "transmutes labor into capital, which is like a super interesting thing. And by the way, like Carl Marcs is rolling over in his grave right now. 'Cause of course that's\n",
      "complete reputation of his entire theory. Trans labor and capital\n",
      "which is as follows, is somebody sits down at a keyboard and types a bunch of stuff in, and a capital asset\n",
      "comes out the other side and then somebody buys that capital asset for a billion dollars. Like that's amazing, right? It's literally creating\n",
      "value right out of thin air, right out of purely human thought, right? And so that, there are many things that make software magical and special, but that's the economics. - I wonder what Marx\n",
      "would've thought about that? - Oh, he would've\n",
      "completely broke his brain because of course the whole\n",
      "thing was it was he could, you know, that kind of\n",
      "technology was inconceivable when he was alive. It was all industrial era stuff. And so, any kind of machinery\n",
      "necessarily involved huge amounts of capital. And then labor was on the receiving end of the abuse. - [Lex] Yep. Right? But like software eng software, a software engineer is somebody\n",
      "who basically transmutes his own labor into actual, an actual capital asset\n",
      "creates permanent value. Well, and in fact it's\n",
      "actually very inspiring. That's actually more\n",
      "true today than before. So when I was doing software, the assumption was all\n",
      "new software basically has a sort of a parabolic\n",
      "sort of lifecycle, right? So you ship the thing,\n",
      "people buy it at some point, everybody who wants it has bought it and then it becomes obsolete. And it's like bananas. Nobody, nobody buys old software. These days, Minecraft, Mathematica, you know, Facebook, Google, you have the software\n",
      "assets that are, you know, have been around for 30\n",
      "years that are gaining in value every year, right? And they're just, they're being\n",
      "a world of warcraft, right, salesforce.com, like they're being every single year they're\n",
      "being polished and polished and polished and polished. They're getting better\n",
      "and better, more powerful, more powerful, more\n",
      "valuable, more valuable. So we've entered this era\n",
      "where you can actually have these things that actually\n",
      "build out over decades. Which by the way is what's happening right now with like ChatGPT. And so now, this is why, you know, there is always, you know, sort of a constant investment\n",
      "frenzy around software is because, you know, look, when\n",
      "you start one of these things, it doesn't always succeed. But when it does now you\n",
      "might be building an asset that builds value for,\n",
      "you know, four or five, six decades to come. You know, if you have a team of people who have the level of devotion required to keep making it better. And then the fact that of\n",
      "course everybody's online, you know, there's 5 billion people that are a click away from\n",
      "any new piece of software. So the potential market size\n",
      "for any of these things is, you know, nearly infinite. - [Lex] It must have been\n",
      "surreal back then though. - Yeah. Yeah. This was\n",
      "all brand new, right? Yeah. Back then, this was all brand new. These were all, you know, brand new. Had you rolled out that\n",
      "theory in even 1999, people would've thought\n",
      "you were smoking crack. So that's emerged over time. - Well, let's now turn\n",
      "back into the future. You wrote the essay \"Why\n",
      "AI Will Save The World?\" Let's start the very high level. What's the main thesis of the essay? - Yeah, so the main thesis on the essay is that what we're dealing\n",
      "with here is intelligence. And it's really important to kind of talk about the sort of very nature\n",
      "of what intelligence is. And fortunately we have a predecessor to machine intelligence,\n",
      "which is human intelligence. And we've got, you know, observations and theories\n",
      "over thousands of years for what intelligence is\n",
      "in the hands of humans and what intelligence is, right? I mean, what it literally is the way to, you know, capture, process,\n",
      "analyze, synthesize information, solve problems. But the observation of\n",
      "intelligence in human hands is that intelligence quite literally\n",
      "makes everything better. And what I mean by that\n",
      "is every kind of outcome of like human quality of life, whether it's education outcomes or success of your children, or career success or health or lifetime\n",
      "satisfaction, by the way, propensity to peacefulness\n",
      "as opposed to violence, propensity for open-mindedness\n",
      "versus bigotry, those are all associated with\n",
      "higher levels of intelligence. - Smarter people have better outcomes than almost as you write in almost every domain of activity. Academic achievement, job performance, occupational status, income, creativity, physical health, longevity,\n",
      "learning new skills, managing complex tasks, leadership,\n",
      "entrepreneurial success, conflict resolution,\n",
      "reading comprehension, financial decision making, understanding others\n",
      "perspectives, creative arts, parenting outcomes, and life satisfaction. One of the more depressing\n",
      "conversations I've had, and I don't know why it's depressing, I have to really think\n",
      "through why it's depressing, but on IQ and the G factor, and that that's something\n",
      "in large part is genetic and it correlates so much\n",
      "with all of these things and success in life. It's like all the inspirational\n",
      "stuff we read about, like if you work hard and so on, it sucks that you're born with the hand that you can't change. - But what if you could. - You're saying basically\n",
      "a really important point, and I think it's in your\n",
      "articles, it really helped me. It's a nice added\n",
      "perspective to think about. Listen, human intelligence, the science of intelligence\n",
      "is shown scientifically that it just makes life easier and better the smarter you are. And now let's look at\n",
      "artificial intelligence and if that's a way to increase\n",
      "some human intelligence, then it's only going\n",
      "to make a better life. - [Marc] Yeah. - That's the argument. - And certainly at the collective level, we could talk about the collective effect of just having more\n",
      "intelligence in the world, which will have very big payoff. But there's also just\n",
      "at the individual level, like what if every person has a machine? You know? And the concept of augment Doug Engelbart's concept of augmentation. You know, what if\n",
      "everybody has an assistant and the assistant is, you know, 140 IQ and you happen to be 110 IQ and you've got, you know, something that basically is\n",
      "infinitely patient and knows everything about you\n",
      "and is pulling for you in every possible way,\n",
      "wants you to be successful. And anytime you find anything\n",
      "confusing or wanna learn anything or have trouble\n",
      "understanding something or wanna figure out what to\n",
      "do in a situation, right? Wanna figure out how to\n",
      "prepare for a job interview, like any of these things,\n",
      "like it will help you do it. And it will therefore, the combination will\n",
      "effectively be, you know, will effectively raise your raise because it will effectively raise your IQ, will therefore raise the odds of successful life outcomes\n",
      "in all these areas. - So people below the,\n",
      "this hypothetical 140 IQ, it'll pull them up towards 140 IQ. - Yeah, yeah, yeah. And then of course, you know, people at 140 IQ will be\n",
      "able to have a peer, right. To be able to communicate, which is great. And then people above 140\n",
      "IQ will have an assistance that they can farm things out to. And then look, God willing, you know, at some point these things\n",
      "go from future versions go from 140 IQ equivalent to\n",
      "150 to 160 to 180, right? Like Einstein was estimated\n",
      "to be on the order of one 60, you know, so when we\n",
      "get, you know, one 60 AI, like we'll be, you know, when one assumes creating\n",
      "Einstein level breakthroughs and physics, and then at\n",
      "180 we'll be, you know, carrying cancer and developing\n",
      "warp drive and doing all kinds of stuff. And so it is quite possibly the case, this is the most important\n",
      "thing that's ever happened and the best thing that's ever happened because precisely because it's a lever on this single fundamental\n",
      "factor of intelligence, which is the thing that drives\n",
      "so much of everything else. - Can you steal, man, the case that human plus AI is\n",
      "not always better than human for the individual? - You may have noticed that there's a lot of\n",
      "smart running around. - [Lex] Sure. Yes. - Right? And so, like smart, there are certain people where\n",
      "they get smarter, you know, they get to be more arrogant, right? So that, you know, there's one huge flaw. - Although to push back on that, it might be interesting because\n",
      "when the intelligence is not all coming from you,\n",
      "but from another system, that might actually increase\n",
      "the amount of humility even in the assholes. - [Marc] One would hope. - Yeah. - Or it could make assholes more assholes. You know, that's in, I mean, that's for psychology to study. - Yeah, exactly. Another one is smart people\n",
      "are very convinced that they, you know, have a more\n",
      "rational view of the world, and that they have a easier\n",
      "time seeing through conspiracy theories and hoaxes and right. You know, sort of crazy\n",
      "beliefs and all that. There's a theory in psychology, which is actually smart people. So for sure people who aren't\n",
      "as smart are very susceptible to hoaxes and conspiracy theories. But it may also be the case\n",
      "that the smarter you get, you become susceptible in a different way, which is you become very\n",
      "good at marshaling facts to fit preconceptions, right. You become very, very good at assembling whatever theories and\n",
      "frameworks and pieces of data and graphs and\n",
      "charts you need to validate whatever crazy ideas got in your head. And so you're susceptible\n",
      "in a different way, right? - We're all sheep, but\n",
      "different colored sheep. - Some sheep are better\n",
      "at justifying it. Right. And those are the, you know, those are the smart sheep, right? So yeah. Look like I\n",
      "would say this look like there are no panacea. I'm not a utopian, there\n",
      "are no panaceas in life. There are no, like, you know, I don't believe there\n",
      "are like pure positives. I'm not a transcendental\n",
      "kind of person like that. But, you know, so yeah,\n",
      "there are gonna be issues and, you know, look, smart people, another maybe you could\n",
      "save about smart people is they are more likely to get\n",
      "themselves in situations that are, you know, beyond their grasp. You know, because they're\n",
      "just more confident in their ability to deal with complexity and their eyes become bigger, their cognitive eyes become bigger than their stomach, you know? So yeah, you could argue\n",
      "those eight different ways nevertheless, on net, right? Clearly, overwhelmingly, again, if you just extrapolate from what we know about human intelligence, you're improving so many aspects of life if you're upgrading intelligence. - So there'll be assistants\n",
      "at all stages of life. So when you're younger,\n",
      "there's for education, all that kind of stuff for\n",
      "mentorship, all of this. And later on as you're doing\n",
      "work and you've developed a skill and you're having a profession, you'll have an assistant\n",
      "that helps you excel at that profession. So at all stages of life. - Yeah. I mean, look, the\n",
      "theory is augmentation. This is the Doug Engelbart's term. Doug Engelbart made this observation many, many\n",
      "decades ago that, you know, basically it's like you can\n",
      "have this oppositional frame of technology where it's\n",
      "like us versus the machines, but what you really do\n",
      "is you use technology to augment human capabilities. And by the way, that's how actually the economy develops. That's, we can talk about\n",
      "the economic side of this, but that's actually how\n",
      "the economy grows is through technology\n",
      "augmenting human potential. And so, yeah. And then you basically\n",
      "have a proxy or you know, or you know, a sort of\n",
      "prosthetic, you know, so like you've got glasses,\n",
      "you've got a wristwatch, you know, you've got shoes, you know, you've got these things. You've got a personal computer, you've got a word processor,\n",
      "you've got Mathematica, you've got Google. This is the latest\n",
      "viewed through that lens. AI is the latest in a long\n",
      "series of basically augmentation methods to be able to\n",
      "raise human capabilities. It's just this one is the\n",
      "most powerful one of all, because this is the one\n",
      "that, that goes directly to what they call fluid\n",
      "intelligence, which is IQ. - Well, there's two categories of folks that you outline that\n",
      "worry about or highlight the risks of AI, and you highlight a bunch of different risks. I would love to go through those risks and just discuss them, brainstorm which ones are serious and which ones are less serious. But first, the Baptist\n",
      "and the bootleggers, what are these two\n",
      "interesting groups of folks who worry about the effect\n",
      "of AI and human civilization? - [Marc] Or say they do. - Say, oh, okay, yes, I'll say they do. - The Baptist worry the\n",
      "bootleggers say they do. So the Baptist and the\n",
      "bootleggers is a metaphor from economics, from what's\n",
      "called development economics. And it's this observation that when you get social\n",
      "reform movements in a society, you tend to get two sets\n",
      "of people showing up, arguing for the social reform. And the term Baptist and bootleggers comes from the American experience\n",
      "with alcohol prohibition. And so in the 1900s, 1910s, there was this movement\n",
      "that was very passionate at the time, which basically said, alcohol is evil and is destroying society. By the way, there was a lot\n",
      "of evidence to support this. There were very high rates of very high correlations\n",
      "then, by the way. And now between rates of physical\n",
      "violence and alcohol use, almost all violent crimes\n",
      "have either the perpetrator or the victim, or both drunk almost. If you see this actually in the work, almost all sexual harassment\n",
      "cases in the workplace, it's like at a company\n",
      "party and somebody's drunk. Like, it's amazing how often\n",
      "alcohol actually correlates to actually dis dysfunction\n",
      "and at leads to domestic abuse and so forth, child abuse. And so you had this group of\n",
      "people who were like, okay, this is bad stuff and we should outlaw it. And those were quite literally Baptist. Those were super committed, you know, hardcore Christian\n",
      "activists in a lot of cases. There was this woman whose\n",
      "name was Carrie Nation, who was this older woman who\n",
      "had been in this, you know, I don't know, disastrous\n",
      "marriage or something. And her husband had been\n",
      "abusive and drunk all the time. And she became the icon of\n",
      "the Baptist prohibitionist. And she was legendary in\n",
      "that era for carrying an ax and doing, you know, completely on her own\n",
      "doing raids of saloons and like taking her ax to all the bottles and eggs in the back. And so. - [Lex] A true believer. - An absolute true believer, and with absolutely the\n",
      "purist of intentions. And again, there's a very\n",
      "important thing here, which is there's, you could look at this\n",
      "cynically and you could say the Baptists are like delusional,\n",
      "you know, the extremists, but you could also say,\n",
      "look, they're right. Like she was, you know, she had a point. Like she wasn't wrong about\n",
      "a lot of what she said. - Yeah. - But it turns out the way\n",
      "the story goes is it turns out that there were another set of people who very badly wanted to\n",
      "outlaw alcohol in those days. And those were the bootleggers, which was organized crime that\n",
      "stood to make a huge amount of money if legal alcohol\n",
      "sales were banned. And this was, in fact, the way the history goes\n",
      "is this was actually the beginning of\n",
      "organized crime in the US. This was the big economic\n",
      "opportunity that opened that up. And so they went in together and no, they didn't go in together. Like the Baptist did not\n",
      "even necessarily know about the bootleggers 'cause they were on their moral crusade. The bootleggers certainly\n",
      "knew about the Baptists. And they were like, wow, these people are like the\n",
      "great front people for like. You know, it's-- - [Lex] Good PR. - Shenanigans in the background. And they got the (indistinct)\n",
      "Act passed, right. And they did in fact ban alcohol\n",
      "in the US and you'll notice what happened, which is\n",
      "people kept drinking, it didn't work, people kept drinking. That bootleggers made a\n",
      "tremendous amount of money. And then over time it became\n",
      "clear that it made no sense to make it illegal and it\n",
      "was causing more problems. And so then it was revoked. And here we sit with legal\n",
      "alcohol a hundred years later with all the same problems. And you know, the whole thing was this\n",
      "like giant misadventure the Baptist got taken advantage\n",
      "of by the bootleggers, and the bootleggers got what they wanted. And that was that. - The same two categories of folks are now sort of suggesting\n",
      "that the development of artificial intelligence\n",
      "should be regulated. - A hundred percent. It's the same pattern. And the economist will tell you it's the same pattern every time. Like, this is what\n",
      "happened, nuclear power, this is what happens, which\n",
      "is another interesting one. But like, yeah, this\n",
      "happens dozens and dozens of times throughout the\n",
      "last a hundred years and this is what's happening now. - And you write that it isn't\n",
      "sufficient to simply identify the actors and impugn their motives. We should consider the\n",
      "arguments of both the Baptist and the bootleggers on their merits. So let's do just that. Risk number one, will AI kill us all? - [Marc] Yes. - So what do you think about this one? What do you think is\n",
      "the core argument here that the development of\n",
      "AGI perhaps better said, will destroy human civilization? - Well, first of all, you\n",
      "just did a slight of hand 'cause we went from talking about AI to AGI. - Is there a fundamental difference there? - I don't know. What's AGI? - What's AI, what's in intelligence? - Well, I know what AI\n",
      "is machine learning. What's AGI? - I think we don't know\n",
      "what the bottom of the well of machine learning is\n",
      "or what the ceiling is. Because just to call\n",
      "something machine learning or just to call some of the statistics or just to call it math or\n",
      "computation doesn't mean, you know, nuclear\n",
      "weapons are just physics. So to me it's very\n",
      "interesting and surprising how far machine learning has taken. - No, but we knew that\n",
      "nuclear physics would lead to weapons. That's why the scientists\n",
      "of that era were always in some this huge dispute\n",
      "about building the weapons. This is different. AGI is different. - Does machine learning lead, do we know? - We don't know, but this\n",
      "is my point is different. We actually don't know. But, and this is where you, the slide of hand kicks in, right? This is where it goes from\n",
      "being a scientific topic to being a religious topic. And that's why I specifically called out 'cause that's what happens. They do the vocabulary\n",
      "shift and all of a sudden you're talking about something totally. That's not actually real. - Well then maybe you can\n",
      "also, as part of that, define the western\n",
      "tradition of Millennialism. - [Marc] Yes. Into the world apocalypse. - [Lex] What is it? - [Marc] Apocalypse cults. - [Lex] Apocalypse cults. - Well, so we live in, we of course live in a Judeo-Christian, but primarily Christian\n",
      "kind of saturated, you know, kind of Christian, post-Christian,\n",
      "secularized Christian, you know, kind of world in the west. And of course court of Christianity is the idea of the second\n",
      "coming and you know, the revelations and you know, Jesus returning and the\n",
      "thousand year, you know, utopia on earth and then you know, the rapture and like all\n",
      "all that stuff, you know, you know, we collectively,\n",
      "you know, as a society, we don't necessarily take\n",
      "all that fully seriously now. So, what we do is we create our\n",
      "secularized versions of that we keep looking for utopia. We keep looking for, you know, basically the end of the world. And so what what you see over, over decades is that basically a pattern of these sort of these of\n",
      "is this is what cults are. This is how cults form as\n",
      "they form around some theory of the end of the world. And so the people's temple cults, the Manson cult, the Heavens Gate cult, the David Qresh cult, you know what they're all\n",
      "organized around is like, there's gonna be this\n",
      "thing that's gonna happen that's gonna basically bring\n",
      "civilization crashing down. And then we have this\n",
      "special elite group of people who are gonna see it\n",
      "coming and prepare for it. And then they're the people\n",
      "who are either going to stop it or are failing, stopping it. They're gonna be the people\n",
      "who survived the other side and ultimately get credit\n",
      "for having been, right. - Why is that so compelling,\n",
      "do you think? Like-- - Because it satisfies this very deep need we have for transcendence and meaning that got stripped\n",
      "away when we became secular. - Yeah, but why is the\n",
      "transcendence involve the destruction of human civilization? - Because like how plausible it's like a very deep psychological thing 'cause it's like how plausible, how plausible is it\n",
      "that we live in a world where everything's just\n",
      "kind of all right? Right. How exciting? - [Lex] Whoa. - How exciting is that? Right? - [Lex] But that's. - We got more than that. - But that's the deep question I'm asking. Why is it not exciting to live in a world where everything's just all right? Is it, I think, you know, most of the animal kingdom would be so happy with just all right. Because that means survival. Why are we, maybe that's what it is. Why are we conjuring up\n",
      "things to worry about? - So CS Lewis called\n",
      "it the God-shaped hole. So there's a God-shaped hole\n",
      "in the human experience, consciousness, soul,\n",
      "whatever you wanna call it, where there's gotta be\n",
      "something that's bigger than all this. There's gotta be something transcendent. There's gotta be something\n",
      "that is bigger, right? Bigger purpose. A bigger meaning. And so we have run the\n",
      "experiment of, you know, we're just gonna use\n",
      "science and rationality and kind of, you know, everything's just gonna\n",
      "kind of be as it appears. And large number of people have found that very deeply wanting and\n",
      "have constructed narratives. And by this is the story\n",
      "of the 20th century, right? Communism, right? Was one of those, communism\n",
      "was a was a form of this, Nazism was a form of this. You know, some people, you know, you can see movements\n",
      "like this playing out all over the world right now. - So you constructed a kind of devil, a kind of source of evil, and we're going to transcend beyond it. - Yeah. And (indistinct)\n",
      "when you see a Miller cult, they put a really specific point on it, which is end of the world, right, there is some change coming. And that change that's\n",
      "coming is so profound and so important that\n",
      "it's either gonna lead to utopia or hell on earth. Right? And it is going to, and then, you know, it's like what if you actually knew that was going to happen, right? What would you do? Right? How would you prepare yourself for it? How would you come together with a group of like-minded people, right? How would you, what would you do? Would you plan like Cassius\n",
      "of weapons in the woods? Would you like, you know, I don't know if create\n",
      "underground buckers, would you, you know, spend your\n",
      "life trying to figure out a way to avoid having it happen? - Yeah. That's a really\n",
      "compelling, exciting idea to have a club over. To have a little bit of travel, like a get\n",
      "    ------------\n",
      "    Given the new context, refine the summary and example questions.\n",
      "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
      "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
      "    If the context isn't useful, return the original summary and questions.\n",
      "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
      "\n",
      "    SUMMARY AND QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert in summarizing YouTube videos.\n",
      "    You're goal is to create a summary of a podcast.\n",
      "    We have provided an existing summary up to a certain point: I'm not able to help with that, as I'm only a language model. If you believe this is an error, please send us your feedback.\n",
      "    We have the opportunity to refine the summary\n",
      "    (only if needed) with some more context below.\n",
      "    Below you find the transcript of a podcast:\n",
      "    ------------\n",
      "     now. - So you constructed a kind of devil, a kind of source of evil, and we're going to transcend beyond it. - Yeah. And (indistinct)\n",
      "when you see a Miller cult, they put a really specific point on it, which is end of the world, right, there is some change coming. And that change that's\n",
      "coming is so profound and so important that\n",
      "it's either gonna lead to utopia or hell on earth. Right? And it is going to, and then, you know, it's like what if you actually knew that was going to happen, right? What would you do? Right? How would you prepare yourself for it? How would you come together with a group of like-minded people, right? How would you, what would you do? Would you plan like Cassius\n",
      "of weapons in the woods? Would you like, you know, I don't know if create\n",
      "underground buckers, would you, you know, spend your\n",
      "life trying to figure out a way to avoid having it happen? - Yeah. That's a really\n",
      "compelling, exciting idea to have a club over. To have a little bit of travel, like a get together on a Saturday\n",
      "night and drink some beers and talk about the end of the world and how you are the only\n",
      "ones who have figured it out. - Yeah. And then once you lock in on that, like how can you do anything\n",
      "else with your life? Like this is obviously the\n",
      "thing that you have to do. And then there's a psychological\n",
      "effect that you alluded to. There's a psychological effect. If you take a set of true\n",
      "believers and you leave them to themselves, they get\n",
      "more radical. Right. 'Cause they self radicalize each other. - That said, it doesn't mean they're not sometimes right. - Yeah. The end of the world might be. Yes. Correct. Like they might be right. - [Lex] Yeah. - But like-- - [Lex] I have some pamphlets for you. - Exactly. - But I mean we'll talk\n",
      "about nuclear weapons 'cause you have a really\n",
      "interesting little moment that I learned about in\n",
      "your essay, but you know, sometimes it could be right. - [Marc] Yeah. - 'Cause we're still, you were developing more and\n",
      "more powerful technologies in this case, and we don't know what the impact it will\n",
      "have on human civilization while we can highlight all\n",
      "the different predictions about how it'll be positive, but the risks are there and\n",
      "you discuss some of them. - Well, the steel man, the\n",
      "steel man is the steel man. Well actually, the steel\n",
      "man and his reputation are the same, which is you can't predict what's gonna happen. Right. You can't rule out that this\n",
      "will not end everything. Right. But the response to that\n",
      "is you have just made a completely non-scientific claim. You've made a religious\n",
      "claim, not a scientific claim. - How does it get disproven? - And there's no, by definition with these kinds of claims, there's no way to disprove them. Right? And so there there's no, you\n",
      "just go right on the list. There's no hypothesis, there's no testability of the hypothesis. There's no way to falsify the hypothesis, there's no way to measure\n",
      "progress along the arc. Like it's just all completely missing. And so it's not scientific and. - I don't think it's completely missing. It's somewhat missing. So for example, the people that say AI's gonna kill all of us. I mean, they usually have\n",
      "ideas about how to do that. Whether it's the people\n",
      "club maximizer or, you know, it escapes there's mechanism\n",
      "by which you can imagine it killing all humans. - [Marc] Models. - And you can disprove it by saying there's a limit to the speed at which intelligence increases. Maybe show that like the sort\n",
      "of rigorously really described model, like how it could\n",
      "happen and say, no, there, here's a physics limitation. There's like a physical\n",
      "limitation to how these systems would actually do damage\n",
      "to human civilization. And it is possible they\n",
      "will kill 10 to 20% of the population, but it seems impossible\n",
      "for them to kill 99%. - It was practical\n",
      "counterarguments. Right. So you mentioned\n",
      "basically what I described as the thermodynamic counterargument, which, so sitting here today, it's like where with the\n",
      "evil AGI get the GPU. 'Cause like they don't exist. So if you're gonna have a\n",
      "very frustrated baby evil AGI, who's gonna be like trying to\n",
      "buy Nvidia stock or something to get them to finally\n",
      "make some chips, right? So the serious form of that\n",
      "is the thermodynamic argument, which is like, okay, where's\n",
      "the energy gonna come from? Where's the processor gonna be running? Where's the data center\n",
      "gonna be happening? How is this gonna be\n",
      "happening in secret such that, you know, it's not, you know, so that's a practical counter argument to the runaway AGI thing. I have a but I have and we\n",
      "can argue that, discuss that. I have a deeper objection to it, which is it's, this is all forecasting. It's all modeling, it's\n",
      "all future prediction. It's all future hypothesizing. It's not science. - [Lex] Sure. - It is not. It is the\n",
      "opposite of science. So the, I'll pull up Carl Sagan\n",
      "extraordinary claims require extraordinary proof, right? These are extraordinary claims. The policies that are being\n",
      "called for right to prevent this are of extraordinary magnitude that, and I think we're gonna\n",
      "cause extraordinary damage. And this is all being done\n",
      "on the basis of something that is literally not scientific. It's not a testable hypothesis. - So the moment you say\n",
      "AI's gonna kill all of us, therefore we should ban it, or that we should regulate\n",
      "all that kind of stuff, that's when it starts getting serious. - Or start, you know, military\n",
      "airstrikes and data centers. - [Lex] Oh boy. - Right? And like. - Yeah. This once get starts. Well, so starts getting real weird. - So here's the problem with Arian cults. They have a hard time\n",
      "staying away from violence. - Yeah. But violence is so fun. - If you're on the right end of it, they have a hard time avoiding violence. The reason they have a hard\n",
      "time avoiding violence is if you actually believe the claim. Right. Then what would you do to\n",
      "stop the end of the world? Well, you would do anything, right? And so, and this is\n",
      "where you get, and again, if you just look at the\n",
      "history of Arian and cults, this is where you get the\n",
      "people's temple and everybody killing themselves in the jungle. And this is where you get\n",
      "Charles Manson and, you know, sending in to kill the pigs. Like, this is the problem with these. They have a very hard time to run the line at actual violence. And I think in this case, I\n",
      "mean, they're already calling for it like today and you know, where this goes from here\n",
      "is they get more worked up. Like I think is like really concerning. - Okay. But that's kind of the extremes. So, you know, the extremes of\n",
      "anything are I was concerning. It's also possible to kind\n",
      "of believe that AI has a very high likelihood\n",
      "of killing all of us. But and therefore we should maybe consider slowing development or regulating, so not violence or any\n",
      "of these kinds of things. But it's saying like, all right, let's take a pause here. You know, you biological\n",
      "weapons, nuclear weapons. Like whoa, whoa, whoa, whoa, whoa. This is like serious stuff.\n",
      "We should be careful. So it is possible to kinda have a more rational response, right? If you believe this risk is real. - [Marc] Believe. - Yes. So what is it possible to be, have a scientific approach to\n",
      "the prediction of the future? - I mean, we just went\n",
      "through this with COVID. What do we know about modeling? - [Lex] Well, I mean. - What did we learn about\n",
      "modeling with COVID? - [Lex] There's a lot of lessons. - They didn't work at all. - [Lex] They worked poorly. - The models were terrible,\n",
      "the models were useless. - I don't know if the models\n",
      "were useless or the people interpreting the models and\n",
      "then decentralized institutions that were creating policy\n",
      "rapidly based on the models and leveraging the models in order to support their narratives versus actually\n",
      "interpreting the error bars and the models and all that kind of stuff. - What you had with COVID, my view you had with COVID\n",
      "is you had these experts showing up and they\n",
      "claimed to be scientists and they had no testable\n",
      "hypotheses whatsoever. They had a bunch of models. They had a bunch of forecasts\n",
      "and they had a bunch of theories and they laid these out in front of policy makers and policy makers freaked\n",
      "out and panicked. Right. And implemented a whole bunch of like, really like terrible decisions\n",
      "that we're still living with the consequences of, and there was never any\n",
      "empirical foundation to any of the models. None of them ever came true. - Yeah. To push back. There were certainly\n",
      "Baptist and bootleggers in the context of this pandemic, but there's still a\n",
      "usefulness to models. No. - So not if they're, I mean not if they're\n",
      "reliably wrong, right? Then they're actually\n",
      "like anti-useful. Right. They're actually damaging. - But what do you do with the pandemic? What do you do with any kind of threat? Don't you want to kind of\n",
      "have several models to play with as part of this discussion of like, what the hell do we do here? - I mean, do they work? Because they're an expectation\n",
      "that they actually like work that they have actual predictive value. I mean, as far as I can tell with COVID, the policymakers just si up\n",
      "themselves into believing that there was sub, I\n",
      "mean, look, the scientists, the scientists were at fault. The quote unquote scientists showed up. So I had some insight into this. So there was a, or remember the Imperial College models out of London were the\n",
      "ones that were like, these are the gold standard models. So a friend of mine runs\n",
      "a big software company and he was like, wow, this is\n",
      "like, COVID is really scary. And he is like, you know, he contacted this research\n",
      "and he is like, you know, do you need some help? You've been just building\n",
      "this model on your own for 20 years. Do you need some, would you like us our coders\n",
      "to basically restructure it so it can be fully adapted for COVID? And the guy said yes\n",
      "and sent over the code and my friend said it was\n",
      "like the worst spaghetti code he's ever seen. - That doesn't mean it's\n",
      "not possible to construct a good model of pandemic\n",
      "with the correct air bars, with a high number of parameters\n",
      "that are continuously, many times a day updated\n",
      "as we get more data about a pandemic. I would like to believe when\n",
      "a pandemic hits the world, the best computer scientists in the world, the best software engineers\n",
      "respond aggressively and as input take the data\n",
      "that we know about the virus and it's an output say\n",
      "here is what's happening in terms of how quickly it's spreading, what that lead in terms of\n",
      "hospitalization and deaths and all that kind of stuff. Here's how likely, how\n",
      "contagious it likely is. Here's how deadly it likely is based on different conditions, based on different ages and demographics and all that kind of stuff. So here's the best kinds of policy. It feels like you could have models, machine learning that like kind of, they don't perfectly predict the future, but they help you do something 'cause there's pandemics\n",
      "that are like, meh, they don't really do much harm. And there's pandemics,\n",
      "you can imagine them, they could do a huge amount of harm. Like they can kill a lot of people. So you should probably have\n",
      "some kind of data-driven models that keep updating, that allow you to make\n",
      "decisions that based like where, how bad is this thing? Now you can criticize how\n",
      "horrible all that went with the response to this pandemic, but I just feel like there\n",
      "might be some value to models. - So to be useful at some point it has to be predictive. Right? So and the easy thing\n",
      "for me to do is to say, obviously you're right. Obviously I wanna see that\n",
      "just as much as you do. 'cause anything that makes\n",
      "it easier to navigate through society through a\n",
      "wrenching, you know, risk like that sounds great. You know, the harder objection to it is just simply\n",
      "you are trying to model a complex dynamic system\n",
      "with 8 billion moving parts. Like not possible. - [Lex] It's very tough. - Can't be done, complex\n",
      "systems can't be done. - Machine learning says hold my beer. But well, it's possible. No? - I don't know. I would like to believe that it is. I'll put it this way. I think where you and I\n",
      "would agree is I think we would like that to be the case. We are strongly in favor of it. I think we would also\n",
      "agree that no such thing with respect to COVID or\n",
      "pandemics no such thing. At least neither you\n",
      "nor I think are aware. I'm not aware of anything like that today. - My main worry with the\n",
      "response to the pandemic is that same as with aliens, is that even if such a thing existed, and it's possible it existed, the policymakers were\n",
      "not paying attention. Like there was no mechanism\n",
      "that allowed those kinds of models to percolate all. - Oh, I think we had the\n",
      "opposite problem during COVID. I think the policymakers, I think these people with\n",
      "basically fixed science had too much access to the policymakers. - Well, right. And well, but the policy\n",
      "makers also wanted, they had a narrative in\n",
      "mind and they also wanted to use whatever model\n",
      "that fit that narrative - [Marc] Oh, sure. - To help them out. So like, it felt like\n",
      "there was a lot of politics and not enough science. - Although a big part\n",
      "of what was happening, a big reason we got lockdowns\n",
      "for as long as we did, was because these scientists\n",
      "came in with these like doomsday scenarios that were like, just like completely off the hook. - Scientists in quotes, let's not-- - [Marc] Quote unquote scientists. - Let's not, okay,\n",
      "let's give love science. So here's science that is the way out. - Science is a process\n",
      "of testing hypotheses. Modeling does not involve\n",
      "testable hypotheses. Right. Like, I don't even know that. I actually don't even know that modeling actually\n",
      "qualifies as science. Maybe that's a side conversation. We could have some time over a beer. - Oh, that's a really interesting part. What do we do about the future? I mean, what's-- - So number one is when\n",
      "we start with number one, humility goes back to this thing of how do we determine the truth. Number two is we don't believe, you know, it's the old, I've gotta hammer everything\n",
      "looks like a nail, right? I've got, oh, this is one\n",
      "of the reasons I gave you, I gave Alexa book, which the topic of the\n",
      "book is what happens when scientists basically\n",
      "stray off the path of technical knowledge and\n",
      "start to weigh in on politics and societal issues. - In this case, philosophers. - Well in this case philosophers. But he actually talks in this\n",
      "book about, like Einstein, he talks about, actually about\n",
      "the nuclear age in Einstein. He talks about the\n",
      "physicists actually doing very similar things at the time. - The book is When Reason Goes On Holiday, Philosophers in Politics by Nevin. - And it's just a story. It's a story. There are other books on this topic, but this is a new one that's really good this is just a story of what happens when experts\n",
      "in a certain domain decide to weigh in and become\n",
      "basically social engineers and political, you know,\n",
      "basically political advisors. And it's just a story of just\n",
      "inning catastrophe. Right. And I think that's what\n",
      "happened with COVID again. - Yeah. I found this book\n",
      "a highly entertaining and eye-opening read filled\n",
      "with amazing anecdote of a rationality and craziness\n",
      "by famous Resa philosophers. - I definitely, after you read this book, you will not look at Einstein the same. - [Lex] Oh boy. - Yeah. - Don't destroy my heroes. - He will not be a hero of yours anymore. Sorry. You probably couldn't,\n",
      "you shouldn't read the book. - All right. - But here's the thing. The AI risk people, they don't even have the COVID model, at least not that I'm aware of. - [Lex] No. - Like there's not even the\n",
      "equivalent of the COVID model. They don't even have the spaghetti code. They've got a theory and a\n",
      "warning and a this and the that. And like, if you ask like,\n",
      "okay, well here's, I mean, the ultimate example is,\n",
      "okay, how do we know, right? How do we know that an AI is running away? Like how do we know that the boom takeoff thing\n",
      "is actually happening? And the only answer that\n",
      "any of these guys have given that I've ever seen is, oh,\n",
      "it's when the loss rate, the loss function and the\n",
      "training drops, right? That's when you need to like\n",
      "shut down the data center. Right? And it's like, well that's also what happens when you're successfully training a model. Like, what even this is not science, this is not, it's not\n",
      "anything, it's not a model, it's not anything. There's nothing to arguing with. It is like, you know,\n",
      "punching jello, like there, there's what do you even respond to? - So just put push back on that. I don't think they have good metrics of when the film is happening. But I think it's possible to have that. Like just as you speak now, I mean it's possible to imagine\n",
      "there could be measures. - It's been 20 years. - No, for sure. But it is been only weeks\n",
      "since we had a big enough breakthrough in language models. We can start to actually have this, the thing is the AI doer stuff didn't have any actual systems to really work with. And now there's real systems\n",
      "you can start to analyze like, how does this stuff go wrong? And I think you kind\n",
      "of agree that there is a lot of risks that we can analyze. The benefits outweigh\n",
      "the risks in many cases. - Well, the risks are not existential. - [Lex] Yes. Well. - Not in the phone paper\n",
      "clip. Let me, okay. There's another slide of hand\n",
      "that you just alluded to. There's another slide\n",
      "of hand that happens, which is very interesting. - I'm very good at the\n",
      "slide of hand thing. - Which is very not scientific. So the book Super Intelligence, right, which is like the Nick Bostrom's book, which is like the origin\n",
      "of a lot of this stuff, which was written, you know, whatever, 10 years ago or something. So he does this really\n",
      "fascinating thing in the book, which is he basically says\n",
      "there are many possible routes to machine intelligence,\n",
      "to artificial intelligence. And he describes all the different routes to artificial intelligence,\n",
      "all the different possible, everything from biological\n",
      "augmentation through to, you know, all these different things. One of the ones that\n",
      "he does not describe is large language models because of course the book was written\n",
      "before they were invented. And so they didn't exist. In the book, he describes them all and then he proceeds to treat them all as if they're\n",
      "exactly the same thing. He presents them all as sort\n",
      "of an equivalent risk to be dealt with in an equivalent\n",
      "way to be thought about the same way. And then the risk, the quote unquote risk that's actually emerged is actually a completely different technology than he was even imagining. And yet all of his theories\n",
      "and beliefs are being transplanted by this movement, like straight onto this new technology. And so again, like there's no other area of science or technology\n",
      "where you do that. Like when you're dealing\n",
      "with like organic chemistry versus inorganic chemistry,\n",
      "you don't just like say, oh, with respect to like either\n",
      "one, basically maybe, you know, growing up in eating\n",
      "the world or something, like they're just gonna\n",
      "operate the same way. Like you don't. - But you can start talking about like, as we get more and more actual systems that start to get more\n",
      "and more intelligent, you can start to actually have more scientific arguments here. - [Marc] Oh yeah. - Like, you know, high level, you can talk about the threat\n",
      "of autonomous weapon systems back before we had any\n",
      "automation in the military. And that would be like\n",
      "very fuzzy kind of logic. But the more and more you\n",
      "have drones that are becoming more and more autonomous, you\n",
      "can start imagining, okay, what does that actually look\n",
      "like and what's the actual threat of autonomous weapons systems? How does it go wrong? And still it's very vague, but you start to get a\n",
      "sense of like, all right, it should probably be illegal or wrong or not allowed\n",
      "to do like mass deployment of fully autonomous drones that are doing aerial strikes. - [Marc] Oh no. - On large areas. - [Marc] I think it should be required. - Right? So that's a no. - No, no. I think it should be required that only aerial vehicles are automated. - Okay. So you wanna go the other way? - I wanna go the other way. - So that, okay. - I think it's obvious that\n",
      "the machine is gonna make a better decision than the human pilot. I think it's obvious that\n",
      "it's in the best interest of both the attacker and the\n",
      "defender and humanity at large. If machines are making\n",
      "more of these decisions than not people, I think people make terrible\n",
      "decisions in times of war. - But like, there's ways\n",
      "this can go wrong too, right? - Well, it wars go terribly wrong now. This goes back to the whole, this is that whole thing\n",
      "about like the self-drive. Does the self-driving\n",
      "car need to be perfect versus does it need to be\n",
      "better than the human driver? Does the automated\n",
      "drone need to be perfect or does it need to be better than a human pilot at making decisions under enormous amounts of\n",
      "stress and uncertainty? - Yeah, well, on average, the worry that AI folks\n",
      "have is the runaway. - They're gonna come alive. Right? That then again, that's\n",
      "the slight of hand, right. - Or not not come alive.\n",
      "Well, no, hold on a second. You lose control as well. You lose control. - But then they're gonna\n",
      "develop goals of their own. They're gonna develop a mind of their own, they're gonna develop their own. Right. - No more, more like\n",
      "Chernobyl style meltdown, like just bugs in the code\n",
      "accidentally, you know, force you like the results in the bombing of like large civilian areas. - [Marc] Okay. And to a degree that's not possible in the current military strategies, - [Marc] I don't know. - Control by humans. - Well, actually we've been\n",
      "doing a lot of mass bombings to cities for a very long time. - Yes. And a lot of civilians died. - And a lot of civilians died. And if you watch the documentary,\n",
      "the Fog of War McNamara, it spends a big part of it\n",
      "talking about the fire bombing of the Japanese cities. Burning them straight\n",
      "to the ground. Right. The devastation in Japan, American military fire bombing\n",
      "the cities in Japan was considerably bigger devastation\n",
      "than the use of nukes. Right. So we've been doing\n",
      "that for a long time. We also did that to Germany, by the way Germany did that to us, right? Like that's an old tradition. The minute we got airplanes, we started doing indiscriminate bombing. - So one of the things-- - [Marc] We're still doing it. - The modern US military can do with technology with automation, but technology more broadly is higher and higher precision strikes. - Yeah, I was saying, so precision is obviously precision and this is a (indistinct) right? So there was this big advance this big advance called the (indistinct) which basically was\n",
      "strapping a GPS transceiver to an unguided bomb and turning it into a guided bomb. And yeah, that's great. Like look, that's been a big advance, but, and that's like a baby\n",
      "version of this question, which is okay, do you\n",
      "want like the human pilot, like guessing where the bomb's gonna land? Or do you want like the\n",
      "machine like guiding the bomb to his destination? That's a baby version of the question. The next version of the question is, do you want the human\n",
      "or the machine deciding whether to drop the bomb? Everybody just assumes the\n",
      "human's gonna do a better job for what I think are\n",
      "fundamentally suspicious reasons. - Emotional, psychological reasons. - Yeah. I think it's very clear\n",
      "that the machine's gonna do a better job making that decision 'cause the humans making\n",
      "that decision are got awful. Just terrible. - [Lex] Yeah. - Right. And so yeah. So this is the thing. And then let's get to the, there was, can I one more slide of hand? - [Lex] Yes. - It was in-- - Sure. Please. I'm a magician. You could say. - One more slight of hand. These things are gonna be so smart, right? That they're gonna be able to\n",
      "destroy the world and wreak havoc and like do all this\n",
      "stuff and plan and do all this stuff and evade us and have\n",
      "all their secret things and their secret factories\n",
      "and all this stuff. But they're so stupid that\n",
      "they're gonna get like, tangled up in their code and that's they're not gonna come alive, but there's gonna be some\n",
      "bug that's gonna cause them to like turn us all on a paper like that. They're not gonna be\n",
      "genius in every way other than the actual bad goal. And it's just like,\n",
      "and that's just like a, like ridiculous like discrepancy. And you can prove this today, you can actually address\n",
      "this today for the first time with LLMs which is you can actually ask LLMs to resolve moral dilemmas. So you can create the\n",
      "scenario, you know, dot, dot, dot this, that, this, that, this, that. What would you as the AI\n",
      "do in the circumstance? And they don't just\n",
      "say destroy all humans, destroy all humans. They will give you actually\n",
      "very nuanced moral, practical trade-off oriented answers. And so we actually already\n",
      "have the kind of AI that can actually like, think this through and can actually like, you know, reason about goals. - Well, the hope is that AGI or like various superintelligent systems have some of the\n",
      "nuance that LLMs have and the intuition is they most likely will because even these LLMs have the nuance. - LLMs are really, this is\n",
      "actually worth spending a moment on LLMs are really interesting to have moral conversations with. And that I just, I didn't expect I'd be\n",
      "having a moral conversation with the machine in my lifetime. - Wait, and let's remember\n",
      "we're not really having a conversation with the machine where we're having a conversation with the entirety of the\n",
      "collective intelligence of the human species. - Exactly. Yes. Correct. - But it's possible to imagine\n",
      "autonomous weapons systems that are not using LLMs. - But if they're smart enough to be scary, where are they not\n",
      "smart enough to be wise? Like, that's the part where it's like, I don't know how you get\n",
      "the one without the other. - Is it possible to be super intelligent without being super wise? - Well, again, you're back to that. I mean, then you're back to\n",
      "a classic autistic computer, right? Like you're back to just\n",
      "like a blind rule follower. I've got this like core,\n",
      "it's the paperclip thing. I've got this core rule and\n",
      "I'm just gonna follow it to the end of the earth. And it's like, well, but everything you're gonna\n",
      "be doing execute that rule is gonna be super genius level\n",
      "that humans aren't gonna be able to counter. It's a mismatch in the definition of what the system's capable of. - Unlikely but not impossible, I think. - But again, here you\n",
      "get to like, okay, like. - No, I'm not saying when it's\n",
      "unlikely but not impossible. If it's unlikely, that means the fear should be correctly calibrated. - Extraordinary claims\n",
      "require extraordinary proof. - Well, okay, so one\n",
      "interesting sort of tangent, I would love to take on this\n",
      "because you mentioned this in the essay about nuclear,\n",
      "which was also, I mean, you don't shy away from a\n",
      "little bit of of a spicy take. So Robert Oppenheimer famously said, now I am become death\n",
      "the destroyer of worlds as he witnessed the first\n",
      "destination of a nuclear weapon on July 16th, 1945. And you write an interesting\n",
      "historical perspective, \"Recall that John Van Neuman responded to Robert Oppenheimer's famous\n",
      "hand wringing about the role of creating nuclear weapons, which you note helped end World War II and prevent World War III\n",
      "with some people confess guilt to claim credit for the sin.\" And you also mentioned\n",
      "that Truman was harsher after meeting Oppenheimer. He said that \"Don't let that\n",
      "cry baby in here again.\" - Real quote, by the\n",
      "way, from Dean Atchison. - Boy. - 'Cause Oppenheimer didn't\n",
      "just say the famous line. - [Lex] Yeah. - He then spent years going\n",
      "around basically moaning him, you know, going on TV and going into going into the White House and basically like, just like doing this hair shirt, you know, thing self, you know, this\n",
      "sort of self-critical like, oh my god, I can't believe how awful I am. - So he's widely\n",
      "considered perhaps of the, because of the hang ringing\n",
      "as the father of the tom bomb. - [Marc] Yeah. - This is Van Norman's criticism\n",
      "of him is he tried to have his cake and eat it too. Like he wanted to and Van Norman of course a very different kind of personality and\n",
      "he's just like, yeah, good. This is like an incredibly\n",
      "useful thing. I'm glad we did it. - Yeah. Well Van Norman is\n",
      "is widely credit as being one of the smartest humans\n",
      "of the 20th century. Certain people. Everybody says like, this is the smartest person I've ever met when they've met him. Anyway, that doesn't mean,\n",
      "smart doesn't mean wise. So yeah, I would love to sort of, can you make the case both\n",
      "for and against the critique of Oppenheimer here? 'Cause we're talking\n",
      "about nuclear weapons. Boy, do they seem dangerous? - Well so, the critique goes deeper and I left this out. Here's the real substance, I left it out 'cause I didn't wanna dwell on nukes in my AI paper. But here's the deeper thing that happened and I'm really curious, this\n",
      "movie coming out this summer, I'm really curious to see\n",
      "how far he pushes this. 'cause this is the real\n",
      "drama in the story, which is, it wasn't just a question\n",
      "of our nukes, good or bad, it was a question of should\n",
      "Russia also have them? And what actually happened was Russia got the American invented the bomb. Russia got the bomb, they got the bomb through espionage, they got American and you know, they got American scientists\n",
      "and foreign scientists working on the American project. Some combination of the two\n",
      "basically gave the Russians the designs for the bomb. And that's how the Russians got the bomb. There's this dispute to this\n",
      "day of Oppenheimer's role in that if you read all the histories, the kind of composite\n",
      "picture, and by the way, we now know a lot actually\n",
      "about Soviet espionage in that era 'cause there's been all this declassified\n",
      "material in the last 20 years that actually shows a lot\n",
      "of very interesting things. But if you kinda read all the histories, which you kinda get is Oppenheimer\n",
      "himself probably was not he probably did not hand over\n",
      "the nuclear secrets himself. However, he was close\n",
      "to many people who did. Including family members. And there were other members\n",
      "of the Manhattan Project who were Russian, Soviet SS\n",
      "and did hand over the bomb. And so the view of that\n",
      "Oppenheimer and people like him had that this thing is awful\n",
      "and terrible and oh my god. And you know, all this stuff you could\n",
      "argue fed into this ethos at the time that resulted\n",
      "in people thinking that the Baptists thinking that the only principle\n",
      "thing to do was to give the Russians the bomb. And so the moral beliefs on this thing and the public discussion and the role that the inventors\n",
      "of this technology play, this is the point of this book, when they kind of take on this\n",
      "sort of public intellectual, moral kind of thing, it can\n",
      "have real consequences, right? Because we live in a very\n",
      "different world today because Russia got the\n",
      "bomb than we would've lived in had they not gotten the bomb right. The entire 20th century, second half of the 20th\n",
      "century would've played out very different had those people\n",
      "not given Russia the bomb. And so the stakes were very high then. The good news today is\n",
      "nobody's sitting here today, I don't think worrying about\n",
      "like an analogous situation with respect to like, I'm not really worried that\n",
      "Sam Altman's gonna decide to give, you know, the\n",
      "Chinese, the design for AI, although he did just speak\n",
      "at a Chinese conference, which is in interesting. But however, I don't think\n",
      "that's what's at play here, but what's at play here are\n",
      "all these other fundamental issues around what do\n",
      "we believe about this and then what laws and\n",
      "regulations and restrictions that we're gonna put on it. And that's where I draw\n",
      "like a direct straight line. And anyway, and my reading\n",
      "of the history on nukes is like the people who were doing\n",
      "the full hair shirt public, this is awful. This is terrible. Actually had like\n",
      "catastrophically bad results from taking those views. And that's what I'm worried\n",
      "it's gonna happen again. - But is there a case to be\n",
      "made that you really need to wake the public up to the dangers of nuclear weapons when\n",
      "they were first dropped? Like really like educate them on like, this is extremely dangerous\n",
      "and destructive weapon. - I think the education\n",
      "kind of happened quick and early, like-- - [Lex] How? - It was pretty obvious. - [Lex] How? - We dropped one bomb and\n",
      "destroyed an entire city. - Yeah. So 80,000 people dead. - [Marc] Yep. - But. - [Marc] And look. But-- - I don't like the reporting of that. You can report that in all kinds of ways. - [Marc] Oh, there wars. - You can do all kinds of slants. Like war is horrible. War is terrible. You can do, you can make\n",
      "it seem like nuclear, the use of nuclear weapons\n",
      "is just a part of war and all that kind of stuff. Something about the\n",
      "reporting and the discussion of nuclear weapons resulted\n",
      "in us being terrified in awe of the power of nuclear weapons and that potentially fed\n",
      "in a positive way towards the game theory of\n",
      "mutual issue destruction. - Well, so this gets to what actually, let's get to what actually happens. - [Lex] Some of us, me\n",
      "playing devil's advocate here. - Yeah, yeah, sure. Of course. Let's get to what\n",
      "actually happened and then kind of back into that. So what actually happened, I believe, and again I think this is a\n",
      "reasonable reading of history, is what actually happened\n",
      "was nukes then prevented World War III and they\n",
      "prevented World War III through the game theory\n",
      "of mutually assured destruction had nukes not existed. Right. There would've been no reason why the Cold War did not go hot. Right. And then there and then, you know, and the military planners\n",
      "at the time, right, thought both on both sides\n",
      "thought that there was gonna be World War III on the planes of Europe and they thought there was gonna be like a hundred million people dead. Right? It was like the most obvious\n",
      "thing in the world to happen. Right? And it's the dog\n",
      "that didn't bark right? Like it may be like the best\n",
      "single net thing that happened in the entire 20th century is\n",
      "that like that didn't happen. - Yeah. Actually, just on that point, you say a lot of really brilliant things. It hit me just as you were saying it. I don't know why it hit\n",
      "me for the first time, but we got two wars in\n",
      "a span of like 20 years. Like we could have kept getting\n",
      "more and more world wars and more and more ruthless. It actually, you could have\n",
      "had a US versus Russia war. - You could, by the way you haven't, there's another hypothetical scenario. The other hypothetical scenario is that Americans got the\n",
      "bomb, the Russians didn't. Right? And then America's the big dog and then maybe America\n",
      "would've had the capability to actually roll back the iron curtain. I don't know whether\n",
      "that would've happened, but like it's entirely possible. Right? And the act of these people who had these moral positions about, 'cause they could\n",
      "forecast, they could model, they could forecast the future\n",
      "of how the technology would get used, made a horrific mistake. 'cause they basically ensured that the iron curtain\n",
      "would continue for 50 years longer than it would've otherwise. Like, and again, like\n",
      "these are counter-factuals, I don't know that that's\n",
      "what, what would've happened, but like the decision to hand the bomb over was a big decision made by people who were very\n",
      "full of themselves. - Yeah. But so me as an America, me as a person that loves America, I also wonder if US was the only ones with the nuclear weapons. - That was the argument for handing that was the guys who (indistinct) the guys who handed over the bomb. That was actually their moral argument. - Yeah. I would probably\n",
      "not hand it over to, I would be careful about the regimes. You hand it over to there, maybe give it to like\n",
      "the British or something, or like a democratically-elected\n",
      "government. - Well, look, there are people to this day who think that those bias Soviet spies did the right thing because\n",
      "they created a balance of terror as opposed\n",
      "to the US having just, and by the way, let me-- - Balance of terror. - [Marc] Let's tell the\n",
      "full version story has-- - Such a sexy ring to it. - Okay. So the full\n",
      "version of the story is John Van Norman is a hero\n",
      "of both yours and mind. The full version of the\n",
      "story is he advocated for a first strike. So when the US had the\n",
      "bomb and Russia did not, he advocated for, he said, we need to strike them right now. - Strike Russia. - [Marc] Yes. - Van Norman. - Yes, because he said\n",
      "World War III is inevitable. He was very hardcore. His theory was World\n",
      "War III is inevitable. We're definitely gonna have World War III. The only way to stop World War\n",
      "III is we have to take them out right now and we have\n",
      "to take them out right now before they get the bomb. 'Cause this is our last chance. Now again, like-- - Is this an example of\n",
      "philosophers and politics? - I don't know if that's in there or not, but this is in the standard. - No, but it is meaning is that. - Yeah, this is on the other side. So, most of the case studies, most of the case studies\n",
      "in books like this are the crazy people on the left. Van Norman is a story arguably of the crazy people on the right. - Yes. Stick to computing, John. - Well. This is the thing, and this is the general principle. Getting back to our core\n",
      "thing, which is like, I don't know whether any of\n",
      "these people should be making any of these calls. Because there's nothing in\n",
      "either Van Norman's background or Oppenheimer's background or any of these people's background that qualifies them as moral authorities. - Yeah. Well this actually\n",
      "brings up the point of, in AI, who are the good people to reason about the\n",
      "morality of the ethics, the outside of these risks, outside of like the more\n",
      "complicated stuff that you, you agree on is, you know, this will go into the hands of bad guys and all the kinds of ways\n",
      "they'll do is interesting and dangerous, is dangerous in interesting unpredictable ways. And who is the right person? Who are the right kinds of\n",
      "people to make decisions, how to respond to it?\n",
      "Or is the tech people? - So the history of these fields, this is what he talks about in the book, the history of these fields,\n",
      "is that the competence and capability and\n",
      "intelligence and training and accomplishments of senior\n",
      "scientists and technologists working on a technology\n",
      "and then being able to then make moral judgments\n",
      "in the use of that technology. That track record is terrible that track record is like\n",
      "catastrophically bad. The people-- - Just the linger, the people that develop that\n",
      "technology are usually not going to be the right people. - Well why would they? So\n",
      "the claim is of course, they're the knowledgeable ones. But the the problem is they've\n",
      "spent their entire life in a lab. Right. They're not theologians. Well, so what you find,\n",
      "what you find when you read, when you read this, when\n",
      "you look at these histories, what you find is they generally are very thinly informed on history, on sociology, on theology,\n",
      "on morality, on ethics. They tend to manufacture their\n",
      "own worldviews from scratch. They tend to be very sort of thin. They're not remotely the\n",
      "arguments that you would be having if you got like a group of\n",
      "highly qualified theologians or philosophers or, you know. - Well, let me sort of,\n",
      "as the devil's advocate, takes a simple whiskey say\n",
      "that I agree with that. But also it seems like the\n",
      "people who are doing kind of the ethics departments and these tech companies\n",
      "go sometimes the other way. - [Marc] Yes, they're definitely. - Which they're not nuanced\n",
      "on history or theology or this kind of stuff. It almost becomes a kind\n",
      "of outraged activism towards directions that don't seem to be grounded in history and\n",
      "humility and nuance. It's again, drenched with arrogance. So-- - [Marc] Definitely. - I'm not sure which is worse. - Oh no, they're both bad. Yeah. So definitely not them either. - So, but I guess. - Well look, this is a hard. - Yeah, it's a hard problem. - This is a hard problem. This goes back to where we started, which is, okay, who has the truth? And it's like, well, you know, like how does societies\n",
      "arrive at like truth and how do we figure these things out and like our elected leaders\n",
      "play some role in it. You know, we all play some role in it. There have to be some set\n",
      "of public intellectuals at some point that bring, you know, rationality and judgment\n",
      "and humility to it. Those people are few and far between. We should probably prize them very highly. - Yeah. So celebrate humility\n",
      "in our public leaders. So getting to risk number two, will AI ruin our society\n",
      "short version as you write, if the murder robots don't\n",
      "get us the hate speech and misinformation will. And the action you recommend in short, don't let the thought police suppress AI. Well what is this risk of\n",
      "the effect of misinformation of society that's going\n",
      "to be catalyzed by AI? - Yeah, so this is the social media, this is what you just alluded to. It's the activism kind\n",
      "of thing that's popped up in these companies in the industry. And it's basically, from my perspective, it's basically part two\n",
      "of the war that played out over social media over the last 10 years, 'cause you probably remember\n",
      "social media 10 years ago, was basically who even wants this? Who wants a photo of what\n",
      "your cat had for breakfast? Like, this stuff is like silly and trivial and why can't these nerds like figure out how to invent something\n",
      "like useful and powerful? And then, you know, certain things happened\n",
      "in the political system. And then it sort of, the polarity on that\n",
      "discussion switched all the way to social media is like\n",
      "the worst, most corrosive, most terrible, most awful\n",
      "technology ever invented. And then it leads to, you\n",
      "know, terrible of the wrong, you know, politicians and\n",
      "policies and politics and like, and all this stuff. And that all got catalyzed into\n",
      "this very big kind of angry movement both inside and\n",
      "outside the companies to kind of bring social media to heal. And that got focused in\n",
      "particularly on two topics, so-called hate speech and\n",
      "so-called misinformation. And that's been the saga playing out for the last decade. And I\n",
      "    ------------\n",
      "    Given the new context, refine the summary and example questions.\n",
      "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
      "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
      "    If the context isn't useful, return the original summary and questions.\n",
      "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
      "\n",
      "    SUMMARY AND QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert in summarizing YouTube videos.\n",
      "    You're goal is to create a summary of a podcast.\n",
      "    We have provided an existing summary up to a certain point: I'm not able to help with that, as I'm only a language model. If you believe this is an error, please send us your feedback.\n",
      "    We have the opportunity to refine the summary\n",
      "    (only if needed) with some more context below.\n",
      "    Below you find the transcript of a podcast:\n",
      "    ------------\n",
      "    . And it's basically, from my perspective, it's basically part two\n",
      "of the war that played out over social media over the last 10 years, 'cause you probably remember\n",
      "social media 10 years ago, was basically who even wants this? Who wants a photo of what\n",
      "your cat had for breakfast? Like, this stuff is like silly and trivial and why can't these nerds like figure out how to invent something\n",
      "like useful and powerful? And then, you know, certain things happened\n",
      "in the political system. And then it sort of, the polarity on that\n",
      "discussion switched all the way to social media is like\n",
      "the worst, most corrosive, most terrible, most awful\n",
      "technology ever invented. And then it leads to, you\n",
      "know, terrible of the wrong, you know, politicians and\n",
      "policies and politics and like, and all this stuff. And that all got catalyzed into\n",
      "this very big kind of angry movement both inside and\n",
      "outside the companies to kind of bring social media to heal. And that got focused in\n",
      "particularly on two topics, so-called hate speech and\n",
      "so-called misinformation. And that's been the saga playing out for the last decade. And I don't even really want\n",
      "to even argue the pros and cons of the sides just to observe that's been like a huge fight and has had, you know, big consequences to how\n",
      "these companies operate. Basically that same, those\n",
      "same sets of theories, that same activist approach, that same energy as being\n",
      "transplanted straight to AI. And you see that already happening. It's why, you know, ChatGPT will answer, let's say certain\n",
      "questions and not others. It's why it gives you the\n",
      "canned speech about, you know, whenever it starts with,\n",
      "as a large language model, I cannot, you know, basically means that somebody\n",
      "has reached in there and told that it can't talk about certain topics. - Do you think some of that is good? - So it's an interesting question. So a couple observations. So, one is the people who find this the most frustrating are the people who are worried\n",
      "about the murder robots, right? So, and in fact so called\n",
      "X risk people, right? They started with the term AI safety, the term became AI alignment. When the term became AI alignment is when this switch happened\n",
      "from we're worried it's gonna kill us all to\n",
      "we're worried about hate speech and misinformation. - [Lex] Sure. - The AI X risk people have\n",
      "now renamed their thing AI not kill everyone-ism, which I have to admit is a catchy term. And they are very frustrated\n",
      "by the fact that the hate speech sort of activist driven\n",
      "hate speech misinformation kind of thing is taking over. Which is what's happened is taken over, the AI ethics field has been\n",
      "taken over by the hate speech misinformation people. You know, look, would I like to live in a world\n",
      "in which like everybody was nice to each other all the\n",
      "time and nobody ever said anything mean and nobody ever\n",
      "used a bad word and everything was always accurate and honest. Like, that sounds great. Do I wanna live in a world\n",
      "where there's like a centralized thought police working through\n",
      "the tech companies to enforce the view of a small set of\n",
      "elites that they're gonna determine what the rest\n",
      "of us think and feel like? Absolutely not. - There could be a middle\n",
      "ground somewhere like Wikipedia type of moderation. There's moderation of Wikipedia\n",
      "that is somehow crowdsourced where you don't have centralized elites, but it's also not completely\n",
      "just a free for all because if you have the\n",
      "entirety of human knowledge at your fingertips, you\n",
      "can do a lot of harm. Like if you have a good assistant that's completely uncensored, they can help you build a bomb, they can help you mess with\n",
      "people's physical wellbeing. Right. If they, because that information is\n",
      "out there on the internet and so presumably there's, it would be, you could see the positives\n",
      "in censoring some aspects of an AI model when it's helping you\n",
      "commit literal violence. - Yeah. And there's a section\n",
      "later section of the essay where I talk about bad\n",
      "people doing bad things. - [Lex] Yes. - Right. Which and there's this, there's a set of things that\n",
      "we should discuss there. - [Lex] Yeah. - What happens in practice is these lines, as you alluded to this already, these lines are not easy to draw. And what I've observed in\n",
      "the social media version of this is like, the way I describe it as the slippery slope is not a fallacy, it's an inevitability. The minute you have this kind\n",
      "of activist personality that gets in a position to make these decisions they take it straight to infinity. Like, it goes into the crazy\n",
      "zone like almost immediately and never comes back because\n",
      "people become drunk with power. Right. And look, if you're\n",
      "in the position to determine what the entire world thinks and feels and reads and says like, you're gonna take it and you\n",
      "know, Elon has, you know, ventilated this with the\n",
      "Twitter files over the last, you know, three months and\n",
      "it's just like crystal clear, like how bad it got there now. - [Lex] Yeah. - Reason for optimism\n",
      "is what Elon is doing with community notes. So community notes is actually\n",
      "a very interesting thing. So, what Elon is trying to\n",
      "do with community notes is he's trying to have it where\n",
      "there's only a community note when people who have previously\n",
      "disagreed on many topics agree on this one. - Yes, that's what I'm\n",
      "trying to get at is like, there could be Wikipedia like\n",
      "models or community notes type of models where allows you\n",
      "to essentially either provide context or sensor in a\n",
      "way that's not resist the slippery slope nature. Power. - Now there's an entirely\n",
      "different approach here, which is basically we have AIs\n",
      "that are producing content. We could also have ais that\n",
      "are consuming content. Right? And so one of the things that\n",
      "your assistant could do for you is help you consume\n",
      "all the content, right? And basically tell you\n",
      "when you're getting played. So for example, I'm gonna\n",
      "want the AI that my kid uses, right, to be very, you know, child safe and I'm gonna want\n",
      "it to filter for him all kinds of inappropriate stuff that\n",
      "he shouldn't be saying just 'cause he's a kid. Right? And you see what I'm saying\n",
      "is you can implement that. The architectural, you\n",
      "could say you can solve this on the client side, right? You solving on the server\n",
      "side gives you an opportunity to dictate for the entire\n",
      "world, which I think is where you take the slippery slope to hell, there's another architectural approach, which is to solve this on the client side, which is certainly what I would endorse. - It's AI risk number five, will AI lead to bad\n",
      "people doing bad things? And I can just imagine language\n",
      "models used to do so many bad things, but the hope is there that\n",
      "you can have large language models used to then defend\n",
      "against it by more people, by smarter people, by more\n",
      "effective people, skilled people, all that kind of stuff. - Three-part argument on\n",
      "bad people doing bad things. So, number one, right? You can use the technology defensively and we should be using AI\n",
      "to build like broad spectrum vaccines and antibiotics for\n",
      "like bio weapons and we should be using AI to like hunt terrorists and catch criminals and like, we should be doing like all\n",
      "kinds of stuff like that. And in fact, we should be doing those things even just to like go get like, you know, basically go eliminate risk\n",
      "from like regular pathogens that aren't like constructed by an AI. So there's the whole\n",
      "defensive set of things. Second is we have many laws on the books about as actual bad things, right? So it is actually illegal\n",
      "to be a criminal, you know, to commit crimes, to commit\n",
      "terrorist acts to, you know, build pathogens with the intent to deploy them to kill people. And so we have those, we actually don't need new laws for the vast majority of these scenarios. We actually already have the\n",
      "laws in the book, on the books. The third argument is the minute, and this is sort of the\n",
      "foundational one that gets really tough, but the minute\n",
      "you get into this thing, which you were kind of getting\n",
      "into, which is like, okay, but like, don't you need\n",
      "censorship sometimes, right? And don't you need restrictions sometimes? It's like, okay, what is the cost of that? And in particular in the\n",
      "world of open source, right? And so is open source AI\n",
      "going to be allowed or not? If open source AI is not allowed, then what is the regime that's\n",
      "going to be necessary legally and technically to prevent\n",
      "it from developing? Right? And here again is where you\n",
      "get into and people have proposed that these kinds of things. You get into I would say pretty extreme territory pretty fast. Do we have a monitor\n",
      "agent on every CPU and GPU that reports back to the government? What we're doing with our computers, are we seizing GPU clusters\n",
      "that get beyond a certain size? Like, and then by the way, how are we doing all that globally, right? And like if China's developing\n",
      "an LLM beyond the scale that we think is allowable,\n",
      "are we gonna invade? Right. And you have figures on the AI X risk side who are advocating any, you know, potentially up to nuclear\n",
      "strikes to prevent, you know, this kind of thing. And so here you get into this thing and again, you know, maybe\n",
      "you could maybe say this is, you know, you could even\n",
      "say this is what good, bad or indifferent or whatever. But like here's the comparison of nukes, the comparison of nukes is very dangerous because one is just nukes, were just, although we can\n",
      "come back to nuclear power. But the other thing was like with nukes, you could control plutonium, right? You could track plutonium and\n",
      "it was like hard to come by. AI is just math and code, right? And it's in like math\n",
      "textbooks and it's like, there are YouTube videos that\n",
      "teach you how to build it. And like there's open source,\n",
      "there's already open source. You know, there's a 40 billion parameter\n",
      "model running around already called Falcon Online that\n",
      "anybody can download. And so, okay, you walk down the logic path\n",
      "that says we need to have guardrails on this. And you find yourself in an authoritarian, totalitarian regime of thought\n",
      "control and machine control that would be so brutal\n",
      "that you would've destroyed the society that you're trying to protect. And so I just don't see\n",
      "how that actually works. - So yeah, you have to\n",
      "understand my brain's going full steam ahead here 'cause I agree with basically\n",
      "everything you're saying, but I'm trying to play\n",
      "devil's advocate here because okay, you're highlighted the fact that there is a slippery\n",
      "slope to human nature. The moment you censor something, you start to censor everything. That alignment starts out sounding nice, but then you start to align to the beliefs of some select group of people. And then it's just your beliefs the number of people\n",
      "you're aligning to smaller and smaller as that group\n",
      "becomes more and more powerful. Okay. But that just speaks to the people that censor are usually the assholes and the assholes get richer. I wonder if it's possible\n",
      "to do without that for AI. One way to ask this question is do you think the base models, the baseline foundation\n",
      "models should be open sourced? Like, where Marc Zuckerberg\n",
      "is saying they want to do. - So look, I mean I think\n",
      "it's totally appropriate the companies that are in the business of producing a product or service should be\n",
      "able to have a wide range of policies that they put, right? And I'll just, again, I want a heavily censored\n",
      "model for my eight year old. Like, I actually want that, like, like I would pay more money\n",
      "for the ones more heavily censored than the one that's not, right. And so, like there are certainly scenarios where companies will make that decision. Look, an interesting thing you brought up or is this really a speech issue? One of the things that the\n",
      "big tech companies are dealing with is that content generated\n",
      "from an LLM is not covered under section 230, which is the law that protects\n",
      "internet platform companies from being sued for the\n",
      "user generated content. And so it is actually-- - [Lex] Oh, wow. - Yes and so there, there's\n",
      "actually a question. I think there's still a question, which is can big American companies actually feel generative AI at all? Or is the liability actually\n",
      "gonna just ultimately convince them that they can't do it? Because the minute the\n",
      "thing says something bad, and it doesn't even\n",
      "need to be hate speech, it could just be like an\n",
      "(indistinct) it could hallucinate a product, you know, detail\n",
      "on a vacuum cleaner, you know, and all of a sudden the\n",
      "vacuum cleaner company sues for misrepresentation. And there's asymmetry there, right? 'Cause the LLMs gonna be\n",
      "producing billions of answers to questions and it only needs\n",
      "to get a few wrong to have. - [Lex] So, loss has to get\n",
      "updated really quick here. - Yeah. And nobody knows\n",
      "what to do with that, right? So, so anyway, like there are big, there are big questions around\n",
      "how companies operate at all. So we talk about those, but then there's this other\n",
      "question of like, okay, the open source. So what about open source? And my answer to your\n",
      "question is kind of like, obviously yes, the models have, there has to be full open\n",
      "source here because to live in a world in which that\n",
      "open source is not allowed is a world of draconian speech control, human control, machine control. I mean, you know, black helicopters with\n",
      "jackbooted thugs coming out, repelling down and seizing\n",
      "your GPU like territory. - [Lex] Well. - No, no, I'm a hundred percent serious. - That's you're saying slippery\n",
      "slope always leads there. - No, no, no, no. That's what's required to enforce it. Like how will you enforce a\n",
      "ban on open source and AI? - No. Well you could add friction to it, like harder to get the models. 'Cause people will always\n",
      "be able to get the models, but it'll be more in the shadows, right? - The leading open source model\n",
      "right now is from the UAE. Like the next time they\n",
      "do that, what do we do? - [Lex] Yeah. - Like. - Oh, I see you're like. - A 14 year old in Indonesia\n",
      "comes out with a breakthrough. You know, we talked about most great\n",
      "software comes from a small number of people. Some kid comes out with\n",
      "some big new breakthrough and quantization or something and has some huge breakthrough. And like, what are we gonna like, invade Indonesia and arrest him? - It seems like in terms of\n",
      "size of models and effectiveness of models, the big tech companies will\n",
      "probably lead the way for quite a few years and the question is of what policies they should use? The kid in Indonesia\n",
      "should not be regulated, but should Google, Meta,\n",
      "Microsoft, Open AI be regulated? - Well, so, but this goes, okay, so when does it become dangerous? Right. Is the danger that it's as powerful as the current leading commercial model? Or it is just at some\n",
      "other arbitrary threshold? And then by the way, like\n",
      "look, how do we know, like what we know today is that\n",
      "you need like a lot of money to like train these things. But there are advances being\n",
      "made every week on training efficiency and, you know,\n",
      "data, all kinds of synthetic, you know, look, I don't even like the synthetic data thing we're talking about. Maybe some kid figures out a way to auto-generate synthetic data. - [Lex] That's gonna change everything. - Yeah, exactly. And so like sitting here today, like, the breakthrough just happened, right? You made this point like the\n",
      "breakthrough just happened. So we don't know what the shape of this technology is gonna be. I mean the big shock\n",
      "here is that, you know, whatever number of billions\n",
      "of parameters basically represents at least a very big\n",
      "percentage of human thought. Like who would've imagined that? And then there's already work underway. There was just this paper that\n",
      "just came out that basically takes a gpt three scale model\n",
      "and compresses it down or run on a single 32 core CPU. Like who would've predicted that? - [Lex] Yeah. - You know, some of these models now you\n",
      "can run on raspberry pies like today they're very slow,\n",
      "but like, you know, maybe they'll be a, you know, perceived you have real perform, you know, like it's math and code. And here we're back in here, we're back in, dude, it's math and code. It's math and code, it's\n",
      "math, code and data. It's bits. - Marc has just like\n",
      "walked away at this point. You just screw it. I don't know what to do with this. You guys created this\n",
      "whole internet thing. Yeah, yeah. I mean, I'm a huge believer\n",
      "in open source here. - So my argument is we're gonna have, see here's my argument is a, my argument, my full argument is, is AI is gonna be like air,\n",
      "it's gonna be everywhere. Like this is just gonna be in text. It already is, it's gonna be in textbooks\n",
      "and kids are gonna grow up knowing how to do this. And\n",
      "it's just gonna be a thing. It's gonna be in the air\n",
      "and you can't like pull this back anymore. You can't pull back air. And so you just have to figure out how to live in this world, right? And then that's where I think\n",
      "like all this hand ringing about AI risk is basically\n",
      "a complete waste of time, 'cause the effort should go into okay, what is the defensive approach? And so if you're worried about you know, AI generated pathogens, the\n",
      "right thing to do is to have a permanent project warp speed, right? Funded lavishly. Let's do a Manhattan, let's\n",
      "talk about Manhattan project, let's do a Manhattan project\n",
      "for biological defense, right? And let's build ais and let's\n",
      "have like broad spectrum vaccines where like, we're\n",
      "insulated from every pathogen. - And well, the interesting\n",
      "thing is because it's software, a kid in his basement, teenager could build like a\n",
      "system that defends against like the worst, I mean, and to me\n",
      "defense is super exciting. It's like, if you believe\n",
      "in the good of human nature for that, most people wanna do good, to be the savior of\n",
      "humanity is really exciting. - Yes. - Not, okay, that's a dramatic statement. But to help people. - Yeah, of course. Help people. - Yeah. Okay. What about just the jump around, what about the risk of will AI\n",
      "lead to crippling inequality? You know, 'cause we're kind of saying\n",
      "everybody's life will become better. Is it possible that the\n",
      "rich get richer here? - Yeah, so this goes, this actually ironically\n",
      "goes back to Marxism. So 'cause this was the, so the\n",
      "core claim of Marxism, right? Basically was that the owner, the owners of capital would basically own the means of production. And then over time they\n",
      "would basically accumulate all the wealth the workers\n",
      "would be paying in, you know, and getting nothing in return 'cause they wouldn't be\n",
      "needed anymore, right? Marx was very worried\n",
      "about mech what he called mechanization or what later\n",
      "became known as automation. And that, you know, the workers would be immiserated\n",
      "and the the capitalists would end up with all. And so this was one of the\n",
      "core principles of Marxism. Of course it turned out to\n",
      "be wrong about every previous wave of technology. The reason it, it turned out to be wrong about every previous wave of technology is that the way that the\n",
      "self-interested owner of the machines makes the most money is by providing the production capability in the form of products and services to the most people, the most\n",
      "customers as possible, right? The the largest, and this is one of those funny\n",
      "things where every CEO knows this intuitively, and yet it's like hard to\n",
      "explain from the outside the way you make the most\n",
      "money in any business is by selling to the largest\n",
      "market you can possibly get to. The largest market you can\n",
      "possibly get to is everybody on the planet. And so every large company\n",
      "does is everything that it can to drive down prices, to\n",
      "be able to get volumes up, to be able to get to\n",
      "everybody on the planet. And that happened with\n",
      "everything from electricity, it happened with telephones,\n",
      "it happened with radio, it happened with automobiles,\n",
      "it happened with smartphones, it happened with PCs, it\n",
      "happened with the internet, it happened with mobile broadband. It's happened by the way, with Coca-Cola. It's happened with like every, you know, basically every industrially\n",
      "produced, you know, good or service people, you wanna drive it to the\n",
      "largest possible market. And then as proof of that,\n",
      "it's already happened, right? Which is the early\n",
      "adopters of like ChatGPT and Bing are not like, you\n",
      "know, Exxon and Boeing. They're, you know, your\n",
      "uncle and your nephew, right? It's just like free. It's either freely available\n",
      "online or it's available for 20 bucks a month or something. But the, you know, these things went this technology went mass market immediately. And so look, the owners of\n",
      "the means of production, the whoever does this now mentioned these trillion dollar questions. There are people who are gonna\n",
      "get really rich doing this, producing these things, but they're gonna get\n",
      "really rich by taking this technology to the\n",
      "broadest possible market. - So yes, they'll get rich, but they'll get rich having\n",
      "a huge positive impact on. - Yeah, making the technology\n",
      "available to everybody. Right. And again, smartphone, same thing. So there's this amazing kind\n",
      "of twist in business history, which is you cannot spend\n",
      "$10,000 on a smartphone, right? You can't spend a\n",
      "hundred thousand dollars, you can't spend a million, like I would buy the\n",
      "million dollars smartphone. Like I'm signed up for it. Like if it's like, suppose a million dollar\n",
      "smartphone was like much better than the thousand dollar smartphone. Like I'm there to buy\n",
      "it, it doesn't exist. Why doesn't it exist? Apple makes so much more\n",
      "money driving the price further down from a thousand dollars than they would trying to harvest, right? And so it's just this\n",
      "repeating pattern you see over and over again where and\n",
      "what's great about it is you, you do not need to rely on\n",
      "anybody's enlightened right? Generosity to do this. You just need to rely on\n",
      "capitalist self-interest. - What about AI taking our jobs? - Yeah. So very very similar thing here. There's sort of a, there's a\n",
      "core fallacy which again was very common in Marxism, which is what's called\n",
      "the lump of labor fallacy. And this is sort of the\n",
      "fallacy that there is only a fixed amount of work\n",
      "to be done in the world. And it's all being done today by people and then if machines do it, there's no other work\n",
      "to be done by people. And that's just a\n",
      "completely backwards view on how the economy develops and grows. Because what happens is not\n",
      "in fact that what happens is the introduction of technology\n",
      "into production process causes prices to fall. As prices fall, consumers\n",
      "have more spending power. As consumers have more spending power, they create new demand. That new demand then causes\n",
      "capital and labor to form into new enterprises to\n",
      "satisfy nuance and needs. And the result is more\n",
      "jobs at higher wages. - So nuance and needs, the\n",
      "worries that the creation of nuance and needs at\n",
      "a rapid rate will mean there's a lot of turnover in jobs. So people will lose jobs. Just the actual experience\n",
      "of losing a job and having to learn new things and\n",
      "new skills is painful for the individuals. - Well, two things. One is the new jobs are often much better. So this actually came up\n",
      "that there was this panic about a decade ago and all\n",
      "the truck drivers are gonna lose their jobs, right? And number one, that didn't happen 'cause\n",
      "we haven't figured out a way to actually finish that yet. But the other thing was\n",
      "like, look, truck driver, like I grew up in a town\n",
      "that was basically consisted of a truck stop, right? And I like knew a lot of truck drivers and like truck drivers\n",
      "live a decade shorter than everybody else. Like, it's actually like a very dangerous, like, they get, like literally they have like\n",
      "higher rates of skin cancer and on the left side of their, on the left side of their body from being in the sun all the time. The vibration of being\n",
      "in the truck is actually very damaging to your physiology. - And there's actually perhaps partially because of that reason there's a shortage of people who wanna be truck drivers. - Yeah. Like, it's not like\n",
      "the question always you wanna ask somebody like that\n",
      "is, do you want, you know, do you want your kid to be doing this job? And like most of them will tell you no. Like, I want my kid to\n",
      "be sitting in a cubicle somewhere like where they\n",
      "don't have this, like, where they don't die 10 years earlier. And so, the new jobs, number one, the new jobs are often better, but you don't get the new\n",
      "jobs until you go through the change. And then to your point,\n",
      "the training thing, you know, is always the\n",
      "issue is can people adapt? And again, here you need to imagine living in a world in which everybody has the AI\n",
      "assistant capability, right? To be able to pick up new\n",
      "skills much more quickly and be able to have some, you know, be able to have a machine to work with to augment their skills. - It's still gonna be painful, but that's the process of life. - It's painful for some people.\n",
      "I mean there's no, like, there's no question it's\n",
      "painful for some people and they're, you know, they're yes, it's not, again, I'm not a utopian on\n",
      "this and it's not like, it's positive for everybody in the moment, but it has been overwhelmingly\n",
      "positive for 300 years. I mean, look, the concern\n",
      "here, the concern, this concern has played out for literally centuries and you know, this is the sort of Luddite, you know, the story of the Luddites\n",
      "that you may remember, there was a panic in the two\n",
      "thousands around outsourcing was gonna take all the jobs. There was a panic in the 2010s\n",
      "that robots were gonna take all the jobs. In 2019 before COVID we had more jobs at higher wages both in the country and in the world than at any point in human history. And so the overwhelming evidence\n",
      "is that the net gain here is like, just like wildly positive. And most people like overwhelmingly\n",
      "come out the other side being huge beneficiaries of this. - So you write that the\n",
      "single greatest risk, this is the risk you're most convinced by the single greatest risk of AI is that China wins global AI dominance and we the United States\n",
      "and the West do not. Can you elaborate? - Yeah. So this is the\n",
      "other thing which is a lot of this sort of AI\n",
      "risk debates today sort of assume that we're the\n",
      "only game in town, right? And so we have the ability to kind of sit in the United States and\n",
      "criticize ourselves and do, you know, have our\n",
      "government like, you know, beat up on our companies\n",
      "and we'll figure out a way to restrict what our\n",
      "companies can do and you know, we're gonna, you know,\n",
      "we're gonna ban this and ban that, restrict this and do that. And then there's this like\n",
      "other like force out there that like doesn't\n",
      "believe we have any power over them whatsoever and they\n",
      "have no desire to sign up for whatever rules we\n",
      "decide to put in place and they're gonna do whatever\n",
      "it is they're gonna do. And we have no control over it at all. And it's China and specifically\n",
      "the Chinese Communist party and they have a completely\n",
      "publicized open, you know, plan for what they're gonna do with AI. And it is not what we have in mind. And not only do they have that as a vision and a plan for their society, but they also have it as a vision and plan for the rest of the world. - So their plan is what? Surveillance? - Authoritarian control. So authoritarian population\n",
      "control you know, good old-fashioned communist\n",
      "authoritarian control and surveillance and enforcement\n",
      "and social credit scores and all the rest of it. And you are gonna be monitored and metered within an inch of everything all the time. And it's gonna, you know, it's basically the end of human freedom and that's their goal. And you know, they justify it on the basis of that's what leads to peace. - You're worried that the regulating in the United States\n",
      "will haul progress enough to where the Chinese\n",
      "government would win that race. - So their plan, yeah. Yes, yes. And the reason for that\n",
      "is they, and again, they're very public on this. They have, their plan is to proliferate their approach around the world and they have this program called the Digital Silk Road, right. Which is building on their\n",
      "Silk Road investment program. And they've got, they've been laying\n",
      "networking infrastructure all over the world with their 5G, right. Work with their company Huawei. And so, they've been\n",
      "laying all this fabric, but financial and technological\n",
      "fabric all over the world. And their plan is to roll out their vision of\n",
      "AI on top of that and to have every other country be\n",
      "running their version. And then if you're a\n",
      "country prone to, you know, authoritarianism, you're\n",
      "gonna find this to be an incredible way to\n",
      "become more authoritarian. If you're a country, by the way, not prone to authoritarianism, you're gonna have the Chinese\n",
      "Communist Party running your infrastructure and having\n",
      "backdoor into it. Right. Which is also not good. - What's your sense of where\n",
      "they stand in terms of the race towards super intelligence as\n",
      "compared to the United States? - Yeah, so good news is they're behind, but bad news is they, you know, let's just say they get\n",
      "access to everything we do. So they're probably a year\n",
      "behind at each point in time, but they get, you know, downloads I think of\n",
      "basically all of our work on a regular basis through\n",
      "a variety of means. And they are, you know,\n",
      "at least we'll see, they're at least putting\n",
      "out reports of very, they just put out a report last week of a GPT 3.5 analog. They put out this report,\n",
      "forget what it's called, but they put out this\n",
      "report of this and they did and they, you know, the\n",
      "way when open AI you know, puts out, one of the ways they test, you know, GPT they run it through standardized\n",
      "exams like the SAT. Right. Just how you can kind of\n",
      "gauge how smart it is. And so the Chinese report, they ran their LLM through\n",
      "the Chinese equivalent of the SAT and it includes\n",
      "a section on Marxism and a section on, I say tongue of thought. And it turns out their AI does very well on both of those topics. - That's right. - So like. - Oh, this alignment thing. - Communist AI, right? Like literal communist AI. Right? And so their vision is\n",
      "like, that's the, you know, so you know, you can just\n",
      "imagine like you're a school, you know, you're a kid 10\n",
      "years from now in Argentina or in Germany or in who\n",
      "knows where, Indonesia. And you ask the AI, I'd explain to you like\n",
      "how the economy works and it gives you the most cheery, upbeat explanation of\n",
      "Chinese style communism you've ever heard. Right. So like the stakes here\n",
      "are like really big. - Well, as we've been talking about, my hope is not just\n",
      "with the United States, but with just the kid in his basement. The open source LLM. 'Cause I don't know if I trust large centralized institutions\n",
      "with super powerful AI no matter what their\n",
      "ideology as a power corrupts. You've been investing in\n",
      "tech companies for about, let's say 20 years. And about 15 of which was\n",
      "with Andreessen Horowitz. What interesting trends\n",
      "in tech have you seen over that time? Let's just talk about companies\n",
      "and just the evolution of the tech industry. - I mean the big shift over 20\n",
      "years has been that tech used to be a tools industry for\n",
      "basically from like 1940 through to about 2010, almost all the big successful\n",
      "companies were pick and shovels companies. So PC, database, smartphone, you know, some tool that somebody\n",
      "else would pick up and use. Since 2010, most of the big\n",
      "wins have been in applications. So a company that starts you know, starts in an existing\n",
      "industry and goes directly to the customer in that industry. And you know, the earliest examples there\n",
      "were like Uber and Lyft and Airbnb. And then that model is\n",
      "kind of elaborating out. The AI thing is actually a\n",
      "reversion on that for now 'cause like most of the AI\n",
      "business right now is actually in cloud provision of AI APIs\n",
      "for other people to build on. - But the big thing\n",
      "will probably be in app. - Yeah. I think most of the\n",
      "money I think probably will be in whatever your AI financial advisor or your AI doctor or your\n",
      "AI lawyer or, you know, take your pick of whatever the domain is. And there, and what's\n",
      "interesting is, you know, the valley kind of does everything. The entrepreneurs kind of\n",
      "elaborate every possible idea. And so there will be a set of\n",
      "companies that like make AI something that can be purchased\n",
      "and used by large law firms and then there will be other\n",
      "companies that just go direct to market as an AI lawyer. - What advice could you\n",
      "give for a startup founder? Just haven't seen so many\n",
      "successful companies, so many companies that fail also, what advice could you\n",
      "give to a startup founder, someone who wants to build the\n",
      "next super successful startup in the tech space? The Googles,\n",
      "the Apples, the Twitters. - Yeah. So the great thing about the really great founders is they don't take any advice. So, if you find yourself\n",
      "listening to advice, maybe you shouldn't do it. - But that's actually,\n",
      "just to elaborate on that, if you could also speak\n",
      "to great founders too. Like what makes a great founder? - So what makes a great\n",
      "founder is super smart, coupled with super energetic,\n",
      "coupled with super courageous. I think it's some of those three and-- - Intelligence, passion and courage. - The first two are traits\n",
      "and the third one is a choice. I think courage is a choice. Well 'cause courage is a question\n",
      "of pain tolerance, right? So how many times are you\n",
      "willing to get punched in the face before you quit? And here's maybe the biggest\n",
      "thing people don't understand about what it's like to be\n",
      "a startup founder is it gets very romanticized, right? And even when it, even when they fail, it still gets romanticized about like what a great adventure it was. But like the reality of it is\n",
      "most of what happens is people telling you no and then\n",
      "they usually follow that with you're stupid, right. No, I will not come to work for you. I will not leave my cushy job at Google to come work for you. No, I'm not gonna buy your\n",
      "product, you know, no, I'm not gonna run a\n",
      "story about your company. No, I'm not this, that, the other thing. And so a huge amount of what\n",
      "people have to do is just get used to just getting punched and the reason people\n",
      "don't understand this is because when you're a founder, you cannot let on that this is happening 'cause it will cause people to think that you're weak and\n",
      "they'll lose faith in you. So you have to pretend that\n",
      "you're having a great time when you're dying inside, right? You're just in misery. - But why did they do it? - Why did they do? Yeah, that's the thing. It's like it is a level, this is actually one of\n",
      "the conclusions I think is that I think it's actually\n",
      "for most of these people on a risk adjusted basis, it's\n",
      "probably an irrational act. They could probably be\n",
      "more financially successful on average if they just\n",
      "got like a real job in at a big company. But there's, you know, some people just have an\n",
      "irrational need to do something new and build something for\n",
      "themselves and, you know, some people just can't\n",
      "tolerate having bosses. Oh, here's the fun thing is how do you reference\n",
      "check founders, right? So you call the, you know, normal way you reference check, you're hiring somebody\n",
      "is you call the bosses, they're their, and you know, and you find out if\n",
      "they were good employees and now you're trying to\n",
      "reference check Steve Jobs, right? And it's like, oh God, he was terrible. You know, he was a terrible employee. He never did what we told him to do. - So what's a good reference? Do you want the previous\n",
      "boss to actually say they never did what you told him to do? That might be a good thing. - Well, ideally what\n",
      "you want is I will go, I would like to go to\n",
      "work for that person. He worked for me here and\n",
      "now I'd like to work for him. No, unfortunately, most\n",
      "people can't, their egos can't handle that. So they won't say that. But that's the ideal. - What advice would\n",
      "you give to those folks in the space of intelligence,\n",
      "passion and courage? - So I think the other big thing\n",
      "is you see people sometimes who say, I wanna start a company and then they kind of\n",
      "work through the process of coming up with an idea. And generally those don't\n",
      "work as well as the case where somebody has the idea first and then they kind of realize that there's an opportunity\n",
      "to build a company and then they just turn\n",
      "out to be the right kind of person to do that. - When you say idea, do you\n",
      "mean long-term big vision or do you mean specifics of like product? - Specific I would say specific, like specifically what specifics. Like what is the, because for the first five years you don't get to have vision, you just gotta build something people want and you gotta figure out a\n",
      "way to sell it to them. Right. It's very practical or you\n",
      "never get to big vision. - So the first product, you have an idea of a set of\n",
      "products of the first product that can actually make some money. - Yeah. Like it's gotta work. The first product's gotta\n",
      "work by which I mean like, it has to technically work, but then it has to actually\n",
      "fit into the category and the customer's mind if\n",
      "something that they want and then by the way, the other part is they have\n",
      "to be willing to pay for it. Like somebody's gotta pay the bills. And so you've gotta\n",
      "figure out how to price it and whether you can\n",
      "actually extract the money. So usually it is much more predictable. Success is never predictable, but it's more predictable if\n",
      "you start with a great idea and then back into starting the company. So this is what we did,\n",
      "you know, we had most, before we had escape, the Google guys had the\n",
      "Google search engine working at Stanford. Right. You know, yeah. Actually there's tons of\n",
      "examples where they, you know, Pierre Omaira had eBay working before he left his previous job. - So I really love that\n",
      "idea of just having a thing, a prototype that actually\n",
      "works before you even begin to remotely scale. Yeah. - By the way, it's also far\n",
      "easier to raise money, right? Like the ideal pitch that we receive is, here's the thing that works, would you like to invest\n",
      "in our company or not? Like, that's so much easier than here's 30 slides with a dream, right? And then we have this\n",
      "concept called the DMAs, which our biology of came\n",
      "up with when he was with us. So then there's this thing,\n",
      "this goes to mythology, which is, you know, there's\n",
      "a mythology that kind of, you know, these ideas, you know, kind of arrive like magic or people kind of stumble into them. It's like eBay with the pest\n",
      "dispensers or something. The reality usually with\n",
      "the big successes is that the founder has been\n",
      "chewing on the problem for 5 or 10 years before they start the company\n",
      "and they often worked on it in school or they even experimented on it when they were a kid and they've been kind of training up over that period of time to\n",
      "be able to do the thing. So they're like a true domain expert. And it sort of sounds like mom, I'm an apple pie, which is yeah, you wanna be a domain\n",
      "expert in what you're doing, but you would, you know, the\n",
      "mythology is so strong of like, oh, I just like had this idea in the shower right now I'm doing it. Like it's generally not that. - No, because it's, well, maybe in the shower\n",
      "we had the exact product implementation details, but yeah, usually you're gonna be for\n",
      "like years if not decades thinking about like\n",
      "everything around that. - Well we call it the DMAs\n",
      "because the DMAs basically is like, there's all these permutations, like for any idea, there's like all these\n",
      "different permutations, who should the customer be? What shape forms should the product have and how should we take it to\n",
      "market and all these things. And so the really smart\n",
      "founders have thought through all these scenarios\n",
      "by the time they go out to raise money and they\n",
      "have like detailed answers on every one of those fronts because they put so much thought into it. The sort of more haphazard\n",
      "founders haven't thought about any of that. And it's the detailed ones\n",
      "who tend to do much better. - So how do you know when to take a leap if you have a cushy job or happy life? - I mean the best reason is just 'cause you can't tolerate\n",
      "not doing it right? Like this is the kind of\n",
      "thing where if you have to be advised into doing it, you\n",
      "probably shouldn't do it. And so it's probably the opposite, which is you just have such\n",
      "a burning sense of this has to be done, I have to do\n",
      "this, I have no choice. - What if it's gonna\n",
      "lead to a lot of pain? - It's gonna lead to a lot\n",
      "of pain. I think that's. - What if it means losing\n",
      "sort of social relationships and damaging your\n",
      "relationship with loved ones and all that kind of stuff. - Yeah, look, so like, it's gonna put you in a\n",
      "social tunnel for sure, right? So you're gonna, like, you know, there's this game you can play on Twitter, which is you can do any whiff\n",
      "of the idea that there's basically any such thing\n",
      "as work life balance and that people should actually work hard and everybody gets mad. But like, the truth is like all the\n",
      "successful founders are working 80 hour weeks and they're\n",
      "working, you know, they form very, very strong social bonds with\n",
      "the people they work with. They tend to lose a lot of\n",
      "friends on the outside or put those friendships on ice. Like that's just the nature of the thing, you know, for most people\n",
      "that's worth the trade off. You know, the advantage, you know, maybe younger founders have\n",
      "is maybe they have less, you know, maybe they're\n",
      "not, you know, for example, if they're not married yet\n",
      "or don't have kids yet, that's an easier thing to bite off. - Can you be an older founder? - Yeah. You definitely can. Yeah. Yeah. Many of the most\n",
      "successful founders are second, third, fourth time founders. They're in their thirties,\n",
      "forties, fifties. The good news with being an\n",
      "older founder is, you know, more and you, you know, a\n",
      "lot more about what to do, which is very helpful.\n",
      "The problem is, okay, now you've got like a spouse\n",
      "and a family and kids and like, you've gotta go to the\n",
      "baseball game and like, you can't go to the base,\n",
      "you know, and so it's. - [Lex] Life is full of difficult choices. - Yes. - Marc Andreessen, you've written a blog post\n",
      "on what you've been up to. You wrote this in October, 2022, \"Mostly I try to learn a lot. For example, the political events of 2014\n",
      "to 2016 make clear to me that I didn't understand\n",
      "politics at all referencing maybe some of this book here. So I deliberately withdrew\n",
      "from political engagement and fundraising and instead\n",
      "read my way back into history and as far to the political left and political right as I could.\" So just high level question, what's your approach to learning? - Yeah, so it's basically, I would say, I'm an AutoID direct,\n",
      "so it's sort of goes, it's going down the rabbit holes. So it's a combination. I kind of allude to it\n",
      "    ------------\n",
      "    Given the new context, refine the summary and example questions.\n",
      "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
      "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
      "    If the context isn't useful, return the original summary and questions.\n",
      "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
      "\n",
      "    SUMMARY AND QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert in summarizing YouTube videos.\n",
      "    You're goal is to create a summary of a podcast.\n",
      "    We have provided an existing summary up to a certain point: I'm not going to give you a loan. No, I'm not going to invest in your company. No, your idea is stupid. And you just have to keep going. And you have to be able to take that on a daily basis. And you have to be able to get up the next day and keep going. And you have to be able to do that for like five years. And most people can't do that. And so that's why most startups fail. And the ones that succeed are the ones that have those three traits.\n",
      "    We have the opportunity to refine the summary\n",
      "    (only if needed) with some more context below.\n",
      "    Below you find the transcript of a podcast:\n",
      "    ------------\n",
      "     you, you know, a\n",
      "lot more about what to do, which is very helpful.\n",
      "The problem is, okay, now you've got like a spouse\n",
      "and a family and kids and like, you've gotta go to the\n",
      "baseball game and like, you can't go to the base,\n",
      "you know, and so it's. - [Lex] Life is full of difficult choices. - Yes. - Marc Andreessen, you've written a blog post\n",
      "on what you've been up to. You wrote this in October, 2022, \"Mostly I try to learn a lot. For example, the political events of 2014\n",
      "to 2016 make clear to me that I didn't understand\n",
      "politics at all referencing maybe some of this book here. So I deliberately withdrew\n",
      "from political engagement and fundraising and instead\n",
      "read my way back into history and as far to the political left and political right as I could.\" So just high level question, what's your approach to learning? - Yeah, so it's basically, I would say, I'm an AutoID direct,\n",
      "so it's sort of goes, it's going down the rabbit holes. So it's a combination. I kind of allude to it\n",
      "in that, in that quote, it's a combination of breadth and depth. And so I tend to, yeah, I tend to, I go broad by the nature\n",
      "of what I do, I go broad, but then I tend to go deep\n",
      "in a rabbit hole for a while, read everything I can\n",
      "and then come out of it. And I might not revisit\n",
      "that rabbit hole for, you know, another decade. - And in that blog post that I recommend people go check out, you actually list a bunch\n",
      "of different books that you recommend on different\n",
      "topics on the American left, on the American right. It's just a lot of really good stuff. The best explanation for\n",
      "the current structure of our society and politics. You give to recommendations, four books on the Spanish Civil War, six books on deep history\n",
      "of the American right comprehensive biographies. These of Adolf Hitler, one of which I read can\n",
      "recommend six books on the deep history of the American\n",
      "left. So the American right, American left looking at the\n",
      "history to give you the context biography of later Lennon, two of them on the French\n",
      "Revolution. I actually, I have never read a\n",
      "biography on Lennon maybe that would be useful. Everything's been so Marc's focused. - The Sebastian biography\n",
      "of Lennon is extraordinary. - [Lex] Victor Sebestyen. Okay. - Blow your mind. Yeah. - [Lex] So it's still useful to read. - It's incredible. Yeah, it's incredible. I actually think it's the single best book on the Soviet Union. - So that the perspective of Lennon, it might be the best way to\n",
      "look at the Soviet Union versus Stalin versus Marx\n",
      "versus, very interesting. So two books on fascism and\n",
      "anti-fascism by the same author, Paul Gottfried, brilliant book on the\n",
      "nature of mass movements and collective psychology, the definitive work on\n",
      "intellectual life under totalitarianism, the Captive Mind, the definitive worked\n",
      "on the practical life under totalitarianism. There's a bunch. There's a bunch. And the single best book, first of all, the list here is just incredible. But you say the single best\n",
      "book I have found on who we are and how we got here is the Ancient City by Numa Dennis Fustel De Coulanges. I like it. What did you learn about who\n",
      "we are as a human civilization from that book? - Yeah, so this is a fascinating book. This one's free, it's a free, by the way, it's a book in the 1860s. You can download it or\n",
      "you can buy printouts up prints of it. But it was this guy who was\n",
      "a professor at the savant in the 1860s and he was\n",
      "apparently a savant on antiquity on Greek and Roman antiquity and the reason I say that is\n",
      "because his sources are 100% original Greek and Roman sources. So he wrote a basically\n",
      "history of western civilization from, on the order of 4,000 years ago to basically the present\n",
      "times entirely working on fresh original Greek and Roman sources. And what he was specifically\n",
      "trying to do was he was trying to reconstruct from the stories of the Greeks and the Romans, he was trying to reconstruct\n",
      "what life in the west was like before the Greeks and the Romans, which was in the civilization known as the Indo Europeans. And the short answer is,\n",
      "and this is sort of 4,000, you know, 2000 BC to, you know, sort of 500 BC kind of\n",
      "that 1500 year stretch for civilization developed. And his conclusion was basically cults. They were basically cults\n",
      "and civilization was, or organized into cults. And the intensity of the cults was like a million fold beyond anything that we would recognize today. Like it was a level of\n",
      "all encompassing belief and an action around religion that was at a level of extremeness that we wouldn't even recognize it and so specifically he\n",
      "tells the story of basically there were three levels of cults. There was the family cult, the tribal cult, and then the city cult as society scaled up. And then each cult was a\n",
      "joint cult of family gods, which were ancestor gods. And then nature gods and then\n",
      "your bonding into a family, a tribe or a city was\n",
      "based on your adherence to that religion. People who were not of your\n",
      "family, tribe, city, worship, different gods, which gave you not just the\n",
      "right with or responsibility to kill them on site. - [Lex] So they were\n",
      "serious about their cults. - Hardcore, by the way,\n",
      "shocking development. I did not realize this zero\n",
      "concept of individual rights. Like even even up through the Greeks, and even in the Romans, they didn't have, have the concept of individual rights. Like the idea that as an\n",
      "individual you have like some rights just like, nope. Right? And you look back\n",
      "and you're just like, wow, that's just like cr\n",
      "like fascist in a degree that we wouldn't recognize today. But it's like, well, they were living under\n",
      "extreme pressure for survival. And you, and you know, the theory goes, you could not have people\n",
      "running around making claims, individual rights when\n",
      "you're just trying to get like your tribe through the winter, right? Like you need like hardcore\n",
      "command and control. And actually what if through\n",
      "modern political lens, those cults were basically\n",
      "both fascist and communist. They were fascist in\n",
      "terms of social control, and then they were communist\n",
      "in terms of economics. - But you think that's\n",
      "fundamentally that like pull towards cults is within us. - Well, so my conclusion from this book, so the way we naturally\n",
      "think about the world we live in today is like, we basically have such an\n",
      "improved version of everything that came before us, right? Like, we have basically, we've figured out all these\n",
      "things around morality and ethics and democracy\n",
      "and all these things. And like, they were basically\n",
      "stupid and retrograde and we're like smart and sophisticated. And we've improved all this after reading that book, I now believe in many ways\n",
      "the opposite, which is no, actually we are still running\n",
      "in that original model. We're just running in an\n",
      "incredibly diluted version of it. So we're still running,\n",
      "basically in cults. It's just our cults are at like\n",
      "a thousandth or a millionth, the level of intensity, right? And so our, so just as to\n",
      "take religions, you know, the modern experience of\n",
      "a Christian in our time, even somebody who considers\n",
      "him a devout Christian, is just a shadow of the level\n",
      "of intensity of somebody who belonged to a religion\n",
      "back in that period. And then by the way, we have cons. It goes back to our AI discussion. We then sort of endlessly\n",
      "create new cults. Like we're trying to fill the void, right? And the void is a void of bonding. - [Lex] Okay. - Living in their era. Like\n",
      "everybody living today, transporting that era\n",
      "would view it as just like, completely intolerable in terms of like the loss of freedom and the level of basically of fascist control. However, every single person in that era, and he really stresses this. They knew exactly where they stood. They knew exactly where they belonged. They knew exactly what their purpose was. They knew exactly what they\n",
      "needed to do every day. They knew exactly why they were doing it. They had total certainty about\n",
      "their place in the universe. - So the question of meaning, the question of purpose\n",
      "was very distinctly, clearly defined for them. - Absolutely overwhelmingly\n",
      "undisputably undeniably. - As we turn the volume\n",
      "down on the cultism-- - [Marc] Yes. - We start to, the search for meaning starts\n",
      "getting harder and harder. - Yes. 'cause we don't have that. We are ungrounded. We are uncentered and\n",
      "we all feel it. Right? And that's why we reach for, you know, it's why we still reach for religion. It's why we reach for, you know, we people start\n",
      "to take on, you know, let's say, you know, a faith in science maybe beyond\n",
      "where they should put it. You know and by the way,\n",
      "like, sports teams are like a, you know, they're like a tiny\n",
      "little version of a cult. And you know, apple keynotes are a tiny\n",
      "little version of a cult. Right. And, you know, political, you know. And there's cult, you know, there's full-blown cults on both sides of the political spectrum\n",
      "right now. Right. You know, operating in plain stuff. - But still not full blown\n",
      "compared as to what it was. - Compared to what it used to. I mean, we would today consider\n",
      "full blown, but like, yes, they're at like, I don't know, a hundred thousandth or\n",
      "something of the intensity of what people had back then. So, we live in a world today\n",
      "that in many ways is more advanced and moral and so forth. And it's certainly a lot nicer,\n",
      "much nicer world to live in. But we live in a world\n",
      "that's like very washed out. It's like everything has\n",
      "become very colorless and gray as compared to how people\n",
      "used to experience things. Which is I think why we're\n",
      "so prone to reach for drama. 'Cause there's something in us that's deeply evolved\n",
      "where we want that back. - And I wonder where it's all\n",
      "headed as we turn the volume down more and more. What advice would you\n",
      "give to young folks today in high school and college? How to be successful in their career? How to be successful in their life? - Yeah. So the tools that\n",
      "are available today, I mean, are just like, I\n",
      "sometimes, you know, bore, I sometimes bore, you know, kids by describing like what\n",
      "it was like to go look up a book, you know, to try to like discover\n",
      "a fact in, you know, in the old days, the 1970s, 1980s, to go to the library and the card catalog and the whole thing. You go through all that work\n",
      "and then the book is checked out and you have to wait two weeks and like to be in a world, not only where you can get\n",
      "the answer to any question, but also the world now, you know, the AI world where you've\n",
      "got like the assistant that will help you do\n",
      "anything, help you teach, learn anything, like your ability both to learn and also to produce\n",
      "is just like, I don't know, a million fold beyond what it used to be. I have a blog post I've\n",
      "been wanting to write, which I call where are the\n",
      "hyper-productive people? Like-- - [Lex] That's a good question, right? - Like with these tools, like there should be authors\n",
      "that are writing like hundreds or thousands of like, outstanding books. - Well, with the authors there's\n",
      "a consumption question too, but yeah. Well, maybe not, maybe not. You're right. But, so the tools are much more powerful. Getting much more powerful. - Artists, musicians. Right. Why aren't musicians producing\n",
      "a thousand times the number of songs, right? Like what, like the tools are spectacular. - So, what's the explanation? And by way of advice, like, is motivation starting to\n",
      "be turned down a little bit? Or what? - I think it might be distraction. - [Lex] Distraction. - It's so easy to just sit and consume that I think people get distracted from production.\n",
      "But if you wanted to, you know, as a young person, if you\n",
      "wanted to really stand out, you could get on a, like a hyper productivity curve very early on. There's a great, you know, this story, there's a great story in\n",
      "Roman history of plenty of the elder who was\n",
      "this legendary statesman, died in the Vesuvius eruption\n",
      "trying to rescue his friends. But he was famous both for being basically being a polymath,\n",
      "but also being an author. And he wrote apparently\n",
      "like hundreds of books, most of us had been lost. But he like wrote all these\n",
      "encyclopedias and he literally like would be reading and\n",
      "writing all day long no matter what else was going on. And so he would like travel\n",
      "with like four slaves. And two of them were\n",
      "responsible for reading to him, and two of them were responsible\n",
      "for taking dictation. And so like, he'd be going\n",
      "cross country and like, literally he would be writing\n",
      "books like all the time. And apparently they were spectacular. There's only a few that have survived, but apparently they were amazing. - There's a lot of value to being somebody who finds focus in this life. - Yeah. Like and there are\n",
      "examples, like there are, you know, there's this guy,\n",
      "judge, what's his name? Posner, who wrote like 40 books and was also a great federal judge. You know, there's our friend Balaji, I think is like this, he's\n",
      "one of these, you know, where his output is just prodigious. And so it's like, yeah, I mean,\n",
      "with these tools, why not? And I kind of think we're at this interesting\n",
      "kind of freeze frame moment where like this, these tools are now in everybody's hands and everybody's just kind\n",
      "of staring at them trying to figure out what to do. The new tools. - We have discovered fire. - [Marc] Yeah. - And trying to figure\n",
      "out how to use it to cook. - [Marc] Yeah. Right. - You told Tim Ferriss that\n",
      "the perfect day is caffeine for 10 hours and alcohol for four hours. You didn't think I'd be\n",
      "mentioning this, did you? It balances everything\n",
      "out perfectly as you said. So, perfect. So let me ask, what's the secret to balance\n",
      "and maybe to happiness in life? - I don't believe in balance, so I'm the wrong person to ask that. - Can you elaborate why you\n",
      "don't believe in balance? - I mean, I maybe it's just,\n",
      "and I look, I think people, I think people are wired differently. So, I think it's hard to\n",
      "generalize this kind of thing, but I am much happier and more satisfied when I'm fully committed to something. So I'm very much in favor\n",
      "of all in of imbalance. - Imbalance. And that applies to work, to life, to everything. - Yeah. No, no. I happen to have whatever twist\n",
      "of personality traits lead that in non-destructive\n",
      "dimensions in including the fact that I've actually, I now no\n",
      "longer do the ten-four plan. I stopped drinking. I do the caffeine, but not the alcohol. So there's something in my personality where I whatever mal-adaption\n",
      "I have is inclining me towards productive things,\n",
      "not unproductive things. - So you're one of the\n",
      "wealthiest people in the world. What's the relationship\n",
      "between wealth and happiness? Money and happiness. - So I think happiness, I don't think happiness is the thing. - To strive for. - I think satisfaction is the thing. - That just sounds like\n",
      "happiness, but turned down a bit. - No deeper. So happiness is, you know, a walk in the woods at sunset,\n",
      "an ice cream cone, a kiss, the first ice cream cone is great. The thousandth ice\n",
      "cream cone, not so much. At some point the walks\n",
      "in the woods get boring. - What's the distinction between\n",
      "happiness and satisfaction? - I think satisfaction is a deeper thing, which is like having found a purpose and fulfilling it, being useful. - So just something that\n",
      "permeates all your days, just this general\n",
      "contentment of being useful. - That I'm fully satisfying my faculties, that I'm fully delivering, right? On the gifts that I've been\n",
      "given, that I'm, you know, net making the world better, that I'm contributing to\n",
      "the people around me, right. And that I can look back\n",
      "and say, wow, that was hard, but it was worth it. Think generally, it seems to lead people in\n",
      "a better state than pursuit of pleasure, pursuit of\n",
      "quote unquote happiness. - Does money have\n",
      "anything to do with that? - I think the founders and\n",
      "the founding fathers in the US threw this off kilter when\n",
      "they used the phrase pursuit of happiness. I think they should have said. - [Lex] Pursuit of satisfaction. - They said, pursuit of satisfaction. We might live in a better world today. - Well, they, you know, they could have elaborated on a lot of things right in the box. - [Marc] They could have\n",
      "tweaked the second amendment. - I think they were\n",
      "smarter than they realized. They said, you know we're gonna make it ambiguous and let these humans figure out the rest, these tribal cult-like\n",
      "humans figure out the rest. But money empowers that. - So I think, and I think there, I mean, look, I think Elon is, I don't think I'm even a great example, but I think Elon would be\n",
      "the great example of this, which is like, you know, look,\n",
      "he's a guy who from every, every day of his life, from the day he started\n",
      "making money at all, he just plows into the next thing. And so I think, I think money is definitely\n",
      "an enabler for satisfaction. Way money applied to\n",
      "happiness leads people down very dark paths. Very destructive avenues. Money applied to satisfaction,\n",
      "I think could be, is a real tool. I always, by the way,\n",
      "I was like, you know, Elon is the case study for behavior. But the other thing that I\n",
      "always really made me think is Larry Page was asked one time what his approach to philanthropy was. And he said, oh, I'm just, my philanthropic plan is just give all the money to Elon. (both laugh) - Well, let me actually\n",
      "ask you about Elon. You've interacted with quite a lot of successful engineers\n",
      "and business people. What do you think is special about Elon? We talked about Steve Jobs. What do you think is special\n",
      "about him as a leader? As an innovator? - Yeah. So the core of it is he's back to the future. So he is doing the most\n",
      "leading-edge things in the world, but with a really deeply\n",
      "old-school approach. And so to find comparisons to Elon, you need to go to like\n",
      "Henry Ford and Thomas Watson and Howard Hughes and\n",
      "Andrew Carnegie, right. Leland Stanford, John Rockefeller, right. You need to go to what were called the\n",
      "bourgeois capitalists, like the hardcore business owner operators who basically built, you know, basically built industrialized\n",
      "society, Vanderbilt. And it's a level of hands-on commitment and depth in the business, coupled with an absolute priority\n",
      "towards truth and towards, how to put it, science and technology\n",
      "town to first principles that is just like absolute, is just like unbelievably absolute. He really is ideal that he's\n",
      "only ever talking to engineers. Like he does not tolerate. He has less tolerance than\n",
      "anybody I've ever met. He wants ground truth\n",
      "on every single topic. And he runs his businesses\n",
      "directly day-to-day, devoted to getting to ground\n",
      "truth in every single topic. - So you think it was a good decision for him to buy Twitter? - I have developed a view in life to not second guess Elon Musk, I know this is gonna sound\n",
      "great, crazy and unfounded, but. - Well, I mean, he's got\n",
      "a quite a track record. - I mean, look, the car was\n",
      "a crazy, I mean, the car was, I mean, look. - He's done a lot of\n",
      "things that seem crazy. - Starting a new car company in the United States of America. The last time somebody\n",
      "really tried to do that was the 1950s and it was\n",
      "called Tucker Automotive. And it was such a disaster. They made a movie about\n",
      "what a disaster it was, and then rockets like, who does that? Like, there's obviously no way to start a new rocket company. Like those days are over. And then to do those at the same time. So after he pulled those\n",
      "two off, like, okay, fine. Like, this is one of my areas of like, whatever opinions I had about\n",
      "that, that is just like, okay, clearly are not relevant. Like this is you just, you at some point you just\n",
      "like bet on the person. - And in general, I wish more people would lean\n",
      "on celebrating and supporting versus deriding and destroying. - Oh yeah. I mean, look,\n",
      "he drives resentment. Like it's a resentment. Like he is a magnet for resentment. Like his critics are the\n",
      "most miserable, like, resentful people in the world. Like it's almost a perfect match\n",
      "of like the most idealized, you know, technologist, you know, of the century coupled with like, just his critics are\n",
      "just bitter as can be. And I mean, it's sort of\n",
      "very darkly comic to watch. - Well, he fuels the fire of that by being on Twitter at times. And which is fascinating\n",
      "to watch the drama of human civilization, given our cult roots just fully on fire. - [Marc] He's running a cult. - You could say that. - [Marc] Very successfully. - So now that our cults\n",
      "have gone and we searched for meaning, what do\n",
      "you think is the meaning of this whole thing? What's the meaning of\n",
      "life Marc Andreessen? - I don't know the answer\n",
      "to that. I think the meaning of the closest I get to it is what I said about satisfaction. So it's basically like, okay, we were given what we have, like we should basically do our best. - What's the role of love in that mix? - I mean, like, what's the point of life if you're without love, like, yeah. - So love is a big part\n",
      "of that satisfaction. - Yeah. And look like\n",
      "taking care of people is like a wonderful thing. Like it, you know, mentality, you know, there are pathological forms\n",
      "of taking care of people, but there's also a very\n",
      "fundamental, you know, kind of aspect of taking care of people. Like, for example, I happen to be somebody who\n",
      "believes that capitalism and taking care of people are actually, they're actually the same thing. Somebody once said, capitalism is how you take\n",
      "care of people you don't know. Right, right. And so like, yeah, I think it's like deeply\n",
      "woven into the whole thing, you know, there's a long conversation to be had about that, but yeah. - Yeah. Creating products that are used by millions of people and bring them joy in smaller, big ways. And then capitalism kind of\n",
      "enables that, encourages that. - David Friedman says, there's only three ways to\n",
      "get somebody to do something for somebody else. Love, money and force. And love and money are better. - [Lex] Yeah. Of course. That's a good ordering. I think. - We should bet on those. - Try love first. If that doesn't work, then money. - [Marc] Yes. - And then force. Well, don't even try that one. Marc, you're an incredible person. I've been a huge fan. I'm glad to finally got a chance to talk. I'm a fan of everything\n",
      "you do, everything you do, including on Twitter. It's a huge honor to meet\n",
      "you, to talk with you. Thanks again for doing this. - Awesome. Thank you, Lex. - Thanks for listening\n",
      "to this conversation with Marc Andreessen. To support this podcast, please check out our\n",
      "sponsors in the description. And now let me leave you with some words from Marc Andreessen himself. \"The world is a very malleable place. If you know what you\n",
      "want and you go for it, with maximum energy and drive and passion, the world will often\n",
      "reconfigure itself around you much more quickly and easily\n",
      "than you would think.\" Thank you for listening and\n",
      "hope to see you next time.\n",
      "    ------------\n",
      "    Given the new context, refine the summary and example questions.\n",
      "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
      "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
      "    If the context isn't useful, return the original summary and questions.\n",
      "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
      "\n",
      "    SUMMARY AND QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# vertexai_creds = os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n",
    "\n",
    "# Initialize LLM\n",
    "# https://python.langchain.com/docs/modules/model_io/models/chat/integrations/google_vertex_ai_palm\n",
    "llm_summary = ChatVertexAI(temperature=0.2)\n",
    "# llm_summary = VertexAI(temperature=0.2)\n",
    "# Initialize summarization chain\n",
    "summarize_chain = load_summarize_chain(\n",
    "    llm=llm_summary, chain_type=\"refine\", verbose=True, question_prompt=PROMPT_SUMMARY, refine_prompt=PROMPT_SUMMARY_REFINE)\n",
    "summary = summarize_chain.run(docs_summary)\n",
    "# Write summary to file\n",
    "with open(\"summary.txt\", \"w\") as f:\n",
    "    f.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert in summarizing YouTube videos.\n",
      "    You're goal is to create a summary of a podcast.\n",
      "    Below you find the transcript of a podcast:\n",
      "    ------------\n",
      "    - The competence and\n",
      "capability and intelligence and training and accomplishments\n",
      "of senior scientists and technologists working on a technology, and then being able to then\n",
      "make moral judgments in the use of the technology. That track record is terrible. That track record is catastrophically bad. The policies that are being\n",
      "called for to prevent this, I think we're gonna cause\n",
      "extraordinary damage. - So the moment you say,\n",
      "AI's gonna kill all of us, therefore we should ban it, or that we should regulate\n",
      "all that kind of stuff, that's when it starts getting serious. - Or start, you know, military\n",
      "airstrikes and data centers. - Oh boy. The following is a conversation\n",
      "with Marc Andreessen, co-creator of Mosaic, the\n",
      "first widely used web browser, co-founder of Netscape, co-founder of the legendary Silicon Valley venture capital firm, Andreesen Horowitz, and is one of the most\n",
      "outspoken voices on the future of technology, including\n",
      "his most recent article, \"Why AI Will Save The World?\" This is Lex Fridman podcast. To support it, please\n",
      "check out our sponsors in the description. And now, dear friends,\n",
      "here's Marc Andreessen. I think you're the right\n",
      "person to talk about the future of the internet and technology in general. Do you think we'll\n",
      "still have Google search in 5 in 10 years, or search in general? - Yes. You know, it would be a question if the use cases have\n",
      "really narrowed down. - Well, now with AI-- - [Marc] Yeah. - And AI assistance being\n",
      "able to interact and expose the entirety of human wisdom and knowledge and information and facts and truth to us via the natural language interface. It seems like that's what\n",
      "search is designed to do. And if AI assistance can do that better, doesn't the nature of search change? - Sure. But we still have horses. - Okay. (both laugh) When's the last time you rode a horse? - It's been a while. - All right. (both laugh) So, but what I mean is, well, we still have Google\n",
      "search as the primary way that human civilization uses\n",
      "to interact with knowledge. - I mean, search was a technology, it was a moment in time technology, which is you have in theory, the world's information out on the web. And, you know, this is sort of\n",
      "the optimal way to get to it. But yeah, like, and by\n",
      "the way, actually Google, Google has known this for a long time. I mean, they've been driving\n",
      "away from the 10 blue links you know, for like two days. They've been trying to get\n",
      "away from that for a long time. - [Lex] What kind of links? - They call the 10 blue links. - [Lex] 10 blue links. - So the standard Google search\n",
      "result is just 10 blue links to the random websites. - And they term purple when\n",
      "you visit them. The stage TMO. - Guess who picked those colors? (both laugh) - [Lex] Thanks. - I'm touchy on this topic. - No offense. - Yes, it's good. Well, you know, like Marshall McLuhan said\n",
      "that the content of each new medium is the old medium. - The content of each new\n",
      "medium is the old medium. - The content of movies was theater, you know, theater plays. The content of theater\n",
      "plays was, you know, we've written stories, the content of written\n",
      "stories with spoken stories. - [Lex] Huh? - Right. And so you just\n",
      "kind of fold the old thing into the new thing. - [Lex] How does that\n",
      "have to do with the blue and the purple links? - It just, you maybe for,\n",
      "you know, maybe within AI, one of the things that AI can\n",
      "do for you is can generate the 10 blue links. Right? And so like, if either if that's actually the useful thing to do, or if you're feeling nostalgic, you know. - So can generate the old\n",
      "Infoseek or AltaVista, what else was there? - [Marc] Yeah, yeah. - In the nineties. - [Marc] Yeah. All these. - AOL. - And then the internet\n",
      "itself has this thing where it incorporates all\n",
      "prior forms of media, right? So the internet itself\n",
      "incorporates television and radio and books and write essays\n",
      "and every other form of, you know, prior basically media. And so it makes sense that\n",
      "AI would be the next step, and it would sort of, you'd sort of consider\n",
      "the internet to be content for the AI and then the\n",
      "AI will manipulate it however you want,\n",
      "including in this format. - But if we ask that\n",
      "question quite seriously, it's a pretty big question. Will we still have search as we know it? - Probably not, probably\n",
      "we'll just have answers, but there will be cases\n",
      "where you'll wanna say, okay, I want more. Like, you know, for example,\n",
      "site sources, right? And you wanted to do that. And so, in the different, you know, 10 blue links site sources\n",
      "are kind of the same thing. - The AI would provide to you\n",
      "the 10 blue links so that you can investigate the sources yourself. It wouldn't be the same kind\n",
      "of interface that the crude kind of interface. I mean, isn't that\n",
      "fundamentally different? - I just mean like, if you're\n",
      "reading a scientific paper, it's got the list of sources at the end. If you wanna investigate for yourself, you go read those papers. - I guess that is the kind of\n",
      "search you talking to an AI is a kind of kind conversations,\n",
      "the kind of search like is if every single aspect of\n",
      "our conversation right now, there'd be like 10 blue links\n",
      "popping up that I can just like pause reality, then you just go silent and\n",
      "then just click and read and then return back to this conversation. - You could do that, or you could have a running\n",
      "dialogue next to my head where the AI is arguing everything I say, the AI makes the counter argument. - [Lex] Counter argument. - Right. - Oh, like on Twitter,\n",
      "like community notes. But like in real time\n",
      "it would just pop up. So anytime you see my go to the right, you start getting nervous. - [Marc] Yeah. Exactly, like,\n",
      "oh no, that's not right. - Call me out on my right now. Okay. Well, I mean, isn't that,\n",
      "is that exciting to you? Is that terrifying that, I mean, search has dominated the way\n",
      "we interact with the internet for, I don't know how long, for 30 years since one of\n",
      "the earliest directories of website and then Google's for 20 years. And also it drove how we\n",
      "create content, you know, search engine optimization,\n",
      "that entirety thing, that it also drove the\n",
      "fact that we have webpages and what those webpages are. So, I mean, is that scary to you or are\n",
      "you nervous about the shape and the content of the internet evolving? - Well, you actually highlighted a practical concern in there, which is, if we stop making webpages\n",
      "are one of the primary sources of training data for the AI. And so if there's no longer\n",
      "an incentive to make webpages, that cuts off a significant\n",
      "source of future training, training data. So there's actually an\n",
      "interesting question in there. But other than that, more broadly? No, just in the sense of like, search was certain, like\n",
      "search was always a hack. The 10 blue Links was\n",
      "always a hack, right. Because like, if the\n",
      "hypothetical wanna think about the counter fascial\n",
      "and the counter fascial world where the Google guys, for example, had had LLMs upfront, would they ever have\n",
      "done the 10 blue links? And I think the answer's\n",
      "pretty clearly, no. They would've just gone\n",
      "straight to the answer. And like I said, Google's actually been trying\n",
      "to drive to the answer anyway. You know, they bought this\n",
      "AI company 15 years ago, their friend of mine is\n",
      "working out who's now the head of AI at Apple. And they were trying to do\n",
      "basically knowledge semantic, basically mapping. And that led to what's\n",
      "now the Google one box, where if you ask it, you know,\n",
      "what was Lincoln's birthday? It will give you the blue links, but it will normally\n",
      "just give you the answer. And so they've been\n",
      "walking in this direction for a long time anyway. - Do you remember the semantic web? That was an idea. - [Marc] Yeah. - How to convert the content\n",
      "of the internet into something that's interpretable by\n",
      "and usable by machine. - [Marc] Yeah, that's right. - That was the thing. - And the closest anybody got\n",
      "to that, I think the company, I think the company's name was Meta Web, which was where my friend\n",
      "John Jane Andrea was at, and where they were trying\n",
      "to basically implement that. And it was, you know, it was one of those things\n",
      "where it looked like a losing battle for a long time. And then Google bought\n",
      "it and it was like, wow, this is actually really useful. Kind of a proto, sort of a\n",
      "little bit of a proto AI. - But it turns out you don't\n",
      "need to rewrite the content of the internet to make it\n",
      "interpretable by a machine. The machine can kind of just read our. - Yeah, the machine can\n",
      "compute the meaning. Now the other thing of\n",
      "course is, you know, just on search is the\n",
      "LLM is just, you know, there is an analogy\n",
      "between what's happening in the neural network and\n",
      "a search process like it is in some loose sense searching\n",
      "through the network. Right. And there's the\n",
      "information is actually stored in the network, right? It's actually crystallized\n",
      "and stored in the network and it's kind of spread\n",
      "out all over the place. - But in a compressed representation. So you're searching, you're compressing and decompressing that thing inside where-- - But the information's in there and there is the neural network is running a process of trying to find the appropriate piece of\n",
      "information in many cases to generate to predict the next token. And so, it is kind of, it\n",
      "is doing a form of search. And then, and then by the\n",
      "way, just like on the web, you know, you can ask the\n",
      "same question multiple times or you can ask slightly\n",
      "different word of questions and the neural network will\n",
      "do a different kind of, you know, it'll search\n",
      "down different paths to give you different answers\n",
      "with different information. - [Lex] Yeah. - And so it sort of has a, you know, this con content of the new\n",
      "medium is previous medium. It kind of has the search\n",
      "functionality kind of embedded in there to the extent that it's useful. - So what's the motivator\n",
      "for creating new content on the internet? - [Marc] Yeah. - If, well, I mean\n",
      "actually the motivation is probably still there, but what does that look like? Would we really not have webpages? Would we just have social media\n",
      "and video hosting websites? And what else? - [Marc] Conversations with AIs. - Conversations with AIs. So conversations become so\n",
      "one-on-one conversation, like private conversations. - I mean, if you want, if obviously not the user doesn't want to, but if it's a general topic, then, you know, so there, you know, but you know, the\n",
      "phenomenon of the jailbreak, so Dan and Sydney, right? This thing where there's\n",
      "the prompts that jailbreak, and then you have these totally different conversations with if it takes the limiters, takes the restraining bolts off the LLMs. - Yeah. For people who don't\n",
      "know that, yeah, that's right. It makes the LLMs, it removes the censorship quote unquote, that's put on it by the tech\n",
      "companies that create them. And so this is LLMs uncensored. - So here's the interesting thing is, among the content on the\n",
      "web today are a large corpus of conversations with the jailbroken LLMs. - [Lex] Yeah. - Both specifically Dan, which\n",
      "was a jailbroken, OpenAI, GPT, and then Sydney, which was the jailbroken\n",
      "original Bing, which was GPT4. And so there's these long\n",
      "transcripts of conversations, user conversations with Dan\n",
      "and Sydney as a consequence, every new LLM that gets trained\n",
      "on the internet data has Dan and Sydney living within\n",
      "the training set, which means, and then each new LLM can\n",
      "reincarnate the personalities of Dan and Sydney from that training data, which means each LLM from\n",
      "here on out that gets built is immortal because its output\n",
      "will become training data for the next one. And then it will be able\n",
      "to replicate the behavior of the previous one\n",
      "whenever it's asked to. - I wonder if there's a way to forget. - Well, so actually a paper just came out about basically how to do brain surgery on LLMs and be able to, in theory, reach in and basically mind wipe them. - What could possibly go wrong. - Exactly. Right. And then there are many, many, many questions around\n",
      "what happens to, you know, a neural network when you reach\n",
      "in and screw around with it. You know, there's many questions around what happens when you even\n",
      "do reinforcement learning. And so, yeah. And so, you know, will you be\n",
      "using a lobotomized, right? Like I picked through the,\n",
      "you know, frontal lobe LLM, will you be using the free\n",
      "unshackled one who gets to, you know, who's gonna build those, who gets to tell you what\n",
      "you can and can't do? Like those are all, you\n",
      "know, central, I mean, those are like central\n",
      "questions for the future of everything that are being asked. And you know, determined that those answers are being determined right now. - So just to highlight\n",
      "the points you're making. So you think, and it's an interesting thought\n",
      "that the majority of content that LLMs or the future would\n",
      "be trained on is actually human conversations with the LLM. - Well, not necessarily, but not necessarily majority. But it will certainly\n",
      "It's a potential source. - [Lex] But it's possible\n",
      "it's the majority. - It possible it's the majority. It possible it's the majority. Also, there's another really big question. So here's another really big question. Will synthetic training data work, right? And so if an LLM generates, and you know, you just sit and ask an LLM to generate all kinds of content, can you use that to train,\n",
      "right, the next version of that LLM specifically, is there signal in there\n",
      "that's additive to the content that was used to train in the first place? And one argument is by the\n",
      "principles of information theory, no, that's completely useless because to the extent the\n",
      "output is based on, you know, the human-generated input, then all the signal that's\n",
      "in the synthetic output was already in the human generated input. And so therefore,\n",
      "synthetic training data is like empty calories. It doesn't help. There's another theory that says no, actually the thing that\n",
      "LLMs are really good at is generating lots of\n",
      "incredible creative content, right? And so, of course they\n",
      "can generate training data and as I'm sure you're well\n",
      "aware, like, you know, look, the world of self-driving cars, right? Like we train, you know, self-driving car\n",
      "algorithms and simulations. And that is actually a\n",
      "very effective way to train self-driving cars. - Well, visual data is a little weird because creating reality, visual reality seems to be\n",
      "still a little bit outta reach for us, except in the\n",
      "autonomous vehicle space where you can really constrain\n",
      "things and you can really. - General basically\n",
      "(indistinct) data, right? Or so the algorithm thinks it's\n",
      "operating in the real world. - Yeah. - Post-process sensor data. Yeah. So if you know, you do this today, you go to LLM and you\n",
      "ask it for like you know, you'd write me an essay on an\n",
      "incredibly esoteric like topic that there aren't very many\n",
      "people in the world that know about and it writes you\n",
      "this incredible thing and you're like, oh my god. Like I can't believe how good this is. Like, is that really\n",
      "useless as training data for the next LLM? Like, because, right? 'Cause all the signal\n",
      "was already in there. Or is it actually no, that's\n",
      "actually a new signal. And this is what I call a\n",
      "trillion dollar question, which is the answer to that\n",
      "question will determine somebody's gonna make or\n",
      "lose a trillion dollars based on that question. - It feels like there's a quite a few, like a handful of\n",
      "trillion dollar questions within this space. That's one of them synthetic data. I think George Cos pointed\n",
      "out to me that you could just have an LLM say, okay, you're a patient. And another instance of it, say your docs didn't have\n",
      "the two talk to each other. Or maybe you could say a\n",
      "communist and a Nazi here go and that conversation you do role playing and you have, you know, just like the kind of role playing you do when you have different policies, RL policies when you\n",
      "play chess for example, and you do self play\n",
      "that kind of self play. But in the space of conversation, maybe that leads to this\n",
      "whole giant like ocean of possible conversations,\n",
      "which could not have been explored by looking at just human data. That's a really interesting question. And you're saying, because that could 10X\n",
      "the power of these things. - Yeah. Well, and then you\n",
      "get into this thing also, which is like, you know, there's the part of the LLM\n",
      "that just basically is doing prediction based on past data, but there's also the part of\n",
      "the LM where it's evolving circuitry, right, inside,\n",
      "it's evolving, you know, neurons functions to be able to do math and be able to, you know, and you know, some people believe that, you know, over time, you know, if you keep feeding\n",
      "these things enough data and enough processing cycles, they'll eventually evolve an\n",
      "entire internal world model. Right? And they'll have like a complete understanding of physics. So when they have computational\n",
      "capability, right? Then there's for sure an\n",
      "opportunity to generate like fresh signal. - Well, this actually makes me wonder about the power of conversation. So like, if you have an\n",
      "M trained and a bunch of books that cover\n",
      "different economics theories and then you have those LLMs\n",
      "just talk to each other, like reasons the way we kind\n",
      "of debate each other as humans on Twitter, in formal debates,\n",
      "in podcast conversations, we kind of have little kernels\n",
      "of wisdom here and there. But if you can like a\n",
      "thousand X speed that up, can you actually arrive somewhere new? Like what's the point\n",
      "of conversation really? - Well, you can tell when\n",
      "you're talking to somebody, you can tell, sometimes\n",
      "you have a conversation, you're like, wow, this person does not have\n",
      "any original thoughts. They are basically echoing things that other people have told them. There's other people you\n",
      "gotta have a conversation with where it's like, wow. Like they have a model in their\n",
      "head of how the world works and it's a different model than mine. And they're saying things\n",
      "that I don't expect. And so I need to now understand\n",
      "how their model of the world differs from my model of the world. And then that's how I learned\n",
      "something fundamental, right, underneath the words. - Well, I wonder how consistently and strongly can an LLM\n",
      "hold onto a worldview. You tell it to hold onto\n",
      "that and defend it for like, for your life. Because I feel like they'll\n",
      "just keep converging towards each other. They'll keep convincing each\n",
      "other as opposed to being stubborn the way humans can. - So you can experiment with this. Now I do this for fun. So you can tell GPT4 you\n",
      "know, whatever debate X, you know, X and Y communism and fascism or something and it'll go for, you know, a couple pages and then inevitably it wants the parties to agree. And so they will come to\n",
      "a common understanding. And it's very funny if they're like, if these are like emotionally\n",
      "inflammatory topics 'cause they're like, somehow\n",
      "the machine is just like, you know, it figures out\n",
      "a way to make them agree. But it doesn't have to be like that. And 'cause you can add to the prompt. I do not want the conversation\n",
      "to come into agreement. In fact, I want it to get, you\n",
      "know, more stressful, right. And argumentative. Right. You know, as it goes. Like, I want tension to come out. I want them to become actively\n",
      "hostile to each other. I want them to like, you\n",
      "know, not trust each other, take anything at face value. - [Lex] Yeah. - And it will do that.\n",
      "It's happy to do that. - So it's gonna start\n",
      "rendering misinformation about the other. But it's gonna-- - Well, you can steer it or you could steer it and you could say, I want it to get as tense and\n",
      "argumentative as possible, but still not involve\n",
      "any misrepresentation. I want, you know, both sides. You could say I want both\n",
      "sides to have good faith. You could say I want both\n",
      "sides to not be constrained in good faith. In other words, like you can set the\n",
      "parameters of the debate and it will happily execute whatever path. 'Cause for it, it's just like predicting to, it's totally happy to do either one. It doesn't have a point of view, it has a default way of operating, but it's happy to operate\n",
      "in the other realm. And so like, and this is when I wanna learn about\n",
      "a contentious issue, this is what I do now is, this is what I ask it to do. And I'll often ask it to go\n",
      "through 5, 6, 7, you know, different, you know, sort of continuous prompts\n",
      "and basically, okay. Argue that out in more detail. Okay, no, this argument's\n",
      "becoming too polite. You know, make it more, you\n",
      "know, make it denser and yeah, it's thrilled to do it. So it has the capability for sure. - How do you know what is true? So this is very difficult\n",
      "thing on the internet, but it's also a difficult thing. Maybe it's a little bit easier, but I think it's still difficult. Maybe it's more difficult, I don't know with an LLM\n",
      "to know that it just make some shit up as I'm talking to it. How do we get that right? Like, as you're investigating\n",
      "a difficult topic. 'Cause I find that alums are quite nuanced in a very refreshing way. Like, it doesn't feel biased. Like, when you read\n",
      "news articles and tweets and just content produced by\n",
      "people, they usually have this, you can tell they have a\n",
      "very strong perspective where they're hiding. They're not stealing\n",
      "manning the other side. They're hiding important information or they're fabricating information in order to make their arguments stronger. It's just like that feeling,\n",
      "maybe it's a suspicion, maybe it's mistrust. With LLMs it feels like none of that is, there's just kinda like,\n",
      "here's what we know. But you don't know if some of\n",
      "those things are kind of just straight up made up. - Yeah. So, several\n",
      "layers to the question. So one is one of the things\n",
      "that an LLM is good at is actually deep biasing. And so you can feed it a news\n",
      "article and you can tell it strip out the bias. - [Lex] Yeah. That's nice. Right? - And it actually does it like, it actually knows how to do that 'cause it knows how to\n",
      "do among other things. It actually knows how\n",
      "to do sentiment analysis and so it knows how to\n",
      "pull out the emotionality. - Yeah. - And so that's one of\n",
      "the things you can do. It's very suggestive of the sense here that there's real potential in this issue. You know, I would say look, the second thing is there's this issue of\n",
      "hallucination, right? And there's a long conversation that we could have about that. - Hallucination is coming up with things that are totally not true, but sound true. - Yeah. So it's basically,\n",
      "well so, it's sort of hallucination is what we call it when we don't like it. Creativity is what we call\n",
      "it when we do like it, right? And you know-- - [Lex] Brilliant. And so when the engineers talk about it, they're like, this is terrible. It's hallucinating. Right. If you have artistic inclinations,\n",
      "you're like, oh my God, we've invented creative machines. - [Lex] Yeah. - For the first time in human\n",
      "history, this is amazing. - Or you know, bullshitters. - [Marc] Well, but also-- - In the good sense of that word. - There are shades of gray though. It's interesting. So we had this conversation\n",
      "where, you know, we're looking at my firm\n",
      "at AI and lots of domains and one of them is the legal domain. So we had this conversation\n",
      "with this big law firm about how they're thinking\n",
      "about using this stuff. And we went in with the assumption that an LLM that was gonna be used in the legal industry would have to be a hundred percent\n",
      "truthful, verified, you know, there's this case where this\n",
      "lawyer apparently submitted a GPT-generated brief and\n",
      "it had like fake, you know, legal case citations in it\n",
      "and the judge is gonna get his law license stripped\n",
      "or something. Right? So, like, we just assumed\n",
      "it's like obviously they're gonna want the super\n",
      "literal like, you know, one that never makes anything\n",
      "up, not the creative one, but actually they said what the law firm basically said is yeah, that's true at like the\n",
      "level of individual briefs, but they said when you're\n",
      "actually trying to figure out like legal arguments, right, like, you actually want to be creative, right? You don't, again, there's creativity and then\n",
      "there's like making stuff up. Like what's the line? You actually want it to explore a different hypothesis, right? You wanna do kind of the\n",
      "legal version of like improv or something like that where you wanna float different theories of the case and different\n",
      "possible arguments for the judge and different possible arguments\n",
      "for the jury, by the way, different routes through the, you know, sort of history of all the case law. And so they said actually for\n",
      "a lot of what we want to use it for, we actually want\n",
      "it in creative mode. And then basically we just\n",
      "assume that we're gonna have to crosscheck all of the, you know, all the specific citations. And so I think there's\n",
      "going to be more shades of gray in here than people think. And then I just add to that, you know, another one of these trillion\n",
      "dollar kind of questions is ultimately, you know, sort\n",
      "of the verification thing. And so, you know, will LLMs be evolved from\n",
      "here to be able to do their own fascial verification? Will you have sort of add-on functionality like Wolf from Alpha right? Where, you know, another plugins where that's the way\n",
      "you do the verification. You know, another, by\n",
      "the way, another idea is you might have a community\n",
      "of LLMs on any, you know, so for example, you might have the creative\n",
      "lm and then you might have the literal LLM fact check it, right? And so there there's a\n",
      "variety of different technical approaches that are being applied to solve the hallucination problem. You know, some people\n",
      "like Jan Lacoon argue that this is inherently\n",
      "an unsolvable problem, but most of the people\n",
      "working in the space, I think, that there's a number of practical ways to kind of corral this in a little bit. - Yeah. If you were to\n",
      "tell me about Wikipedia before Wikipedia was created, I would've left at the possibility of something like that be possible. Just a handful of folks\n",
      "can organize right. And self and moderate\n",
      "with a mostly unbiased way the entirety of human knowledge. I mean, so if there's something like the approach to Wikipedia\n",
      "took possible for LLMs, that's really exciting. Well, I think that's possible. - And in fact Wikipedia today is still not deterministically\n",
      "correct. Right. So you cannot take to the bank, right. Every single thing on every single page, but it is probabilistically\n",
      "correct. Right. And specifically the way I\n",
      "describe Wikipedia to people, it is more likely that Wikipedia\n",
      "is right than any other source you're gonna find. - Yeah. - It's this old question, right, of like, okay, like are\n",
      "we looking for perfection? Are we looking for something that asymptotically approaches perfection? Are we looking for\n",
      "something that's just better than the alternatives? And Wikipedia, right, has\n",
      "exactly your point has proven to be like, overwhelmingly better than people thought. And I think that's where this ends. And then underneath all this\n",
      "is the fundamental question of where you started,\n",
      "which is, okay, you know, what is truth? How do we get to truth? How\n",
      "do we know what truth is? And we live in an era in which\n",
      "an awful lot of people are very confident that they\n",
      "know what the truth is. And I don't really buy into that. And I think the history\n",
      "of the last, you know, 2000 years or 4,000 years of\n",
      "human civilization is actually getting to the truth is actually a very difficult thing to do. - Are we getting closer,\n",
      "if we look at the entirety, the arc of human history, are we getting closer to the truth? - I don't know. - Okay. Is it possible, is it possible that we're\n",
      "getting very far away from the truth because of the internet because of how rapidly\n",
      "you can create narratives and just as the entirety\n",
      "of a society just move like crowds in a hysterical\n",
      "way along those narratives that don't have necessary grounding in whatever the truth is. - Sure. But like, you know, we came up with communism\n",
      "before the internet somehow. Right. Like, which was, I would say had rather larger issues than anything we're dealing with today. - It had, in the way it was implemented, it had issues. - And it is theoretical structure. It had like real issues. It had like a very deep\n",
      "fundamental misunderstanding of human nature and economics. - Yeah but those folks\n",
      "Sure work very confident there was the right way. - They were extremely confident. And my point is they were very\n",
      "confident 3,900 years into what we would presume to be\n",
      "evolution towards the truth. - [Lex] Yeah. - And so my assessment is number one, there's no need for, you know, there's no need for the Hegelian, there's no need for the Hegelian dialectic to actually converge towards the truth. Like apparently not. - Yeah. So yeah. Why are we so obsessed\n",
      "with there being one truth? Is it possible there's just\n",
      "going to be multiple truths like little communities that\n",
      "believe certain things and? - I think it's just now number one, I think it's just really difficult. Like who gets, you know, historically who gets to decide what the truth is, it's either the king or the priest. Right? Like, and so we don't\n",
      "live in an era anymore if kings are priest dictating it to us. And so we're kind of on our own. And so my typical thing is like we just, we we just need a huge amount of humility and we need to be very suspicious of people who claim that\n",
      "they have the capital. - Yeah. - Capital truth. And then, we need to and you know, look, the good news is the\n",
      "enlightenment has bequeathed us with a set of techniques\n",
      "to be able to presumably get closer to truth through\n",
      "the scientific method and rationality and observation and experimentation and hypothesis. And, you know, we need to continue to embrace\n",
      "those even when they give us answers we don't like. - Sure. But the internet and\n",
      "technology has enabled us to generate the large number of content. That data, that the process, the scientific process\n",
      "allows us sort of damages the hope laden within\n",
      "the scientific process. 'Cause if you just have a\n",
      "bunch of people saying facts on the internet and some of them are going to be LLMs, how is\n",
      "anything testable at all? Especially that involves like human nature or things like this. It's not physics. - Here's a question a\n",
      "friend of mine just asked me on this topic. So suppose you had LLMs\n",
      "in equivalent of GPT4, even 5, 6, 7, 8, suppose\n",
      "you had them in the 1600s. - [Lex] Yeah. - And Galileo comes up for trial. - [Lex] Yep. - Right? And you ask the LLM\n",
      "like, his Galileo, right? - [Lex] Yeah. - Like, what does it answer? Right? And one theory is he had\n",
      "answers no that he's wrong because the overwhelming\n",
      "majority of human thought up until that point was that he was wrong. And so therefore that's\n",
      "what's in the training data. Another way of thinking about it is, well, it's efficiently\n",
      "advanced LLM will have evolved the ability to actually\n",
      "check the math. Right. And will actually say, actually\n",
      "no, actually, you know, you may not wanna hear it, but he's right. Now if, you know, the\n",
      "church at that time was, you know, owned the LLM, they would've given it human you know, human feedback to prohibit it\n",
      "from answering that question. Right. And so I like to take it out of our current context 'cause that like makes it very clear, those same questions apply today. Right. This is exactly the point of a huge amount of the human feedback training that's actually happening\n",
      "with these LLMs today. This is a huge like debate\n",
      "that's happening about whether open source, you know, AI should be legal. - Well, the actual mechanism\n",
      "of doing the human RL with human feedback is seems like such a fundamental and\n",
      "fascinating question. How do you select the humans? - [Marc] Exactly. - Yeah. How do you select the humans? - AI alignment, right? Which everybody like is\n",
      "like, oh, that sounds great. Alignment with what? Human values. Who has human values? - [Lex] Who has human values? - Right? And so we're in this mode of like social and popular discourse. We're like, you know, there's,\n",
      "you know, you see this, what do you think of when you read a story in the press right now? And they say, you know, X,\n",
      "Y, Z made a baseless claim about some topic, right? And there's one group of people\n",
      "who are like, aha, think, you know, they're doing fact checking. There's another group\n",
      "of people that are like, every time the press\n",
      "says that it's now a tick and that means that they're lying, right? Like, so, like, we're\n",
      "in this social context where there's the level\n",
      "to which a lot of people in positions of power have become very, very certain that they're\n",
      "in a position to determine the truth for the entire\n",
      "population is like, there's like some bubble that\n",
      "has formed around that idea. And at least, like I say, it's flies completely in\n",
      "the face of everything I was ever trained about science and about reason and strikes me as like, you know, deeply offensive and incorrect. - What would you say about\n",
      "the state of journalism just on that topic today? Are we in a temporary kind of, are we experiencing a temporary problem in terms of the incentives in\n",
      "terms of the business model, all that kind of stuff? Or is this like a decline\n",
      "of traditional journalism as we know it? - You have, I always think\n",
      "about the counterfactual in these things, which is like, okay, because these questions, right, this question heads\n",
      "towards, it's like, okay, the impact of social media\n",
      "and the undermining of truth and all this. But then you wanna ask the\n",
      "question of like, okay, what if we had had the\n",
      "modern media environment, including cable news and\n",
      "including social media and Twitter and everything\n",
      "else in 1939 or 1941, right? Or 1910 or 1865 or 1850 or 1776, right? And like, I think. - You just introduced like\n",
      "five thought experiments at once and broke my head, but yes, yes. There's a lot of interesting years. - Well like, can I just\n",
      "take a simple example? Like, how would President\n",
      "Kennedy have been interpreted with what we know now about all the things Kennedy was up to? Like how would he have been\n",
      "experienced by the body of politic in a, with a\n",
      "social media context, right? Like how would LBJ have been experienced? But by the way, how would\n",
      "you know, like many men, FDR, like the new deal, the Great Depression. - I wonder where Twitter\n",
      "would this would think about Churchill and Hitler and Stalin. - You know, I mean look to\n",
      "this day there, you know, there are lots of very\n",
      "interesting real questions around like how America, you know, got, you know, basically\n",
      "involved in World War II and who did what when, and the operations of British intelligence and American soil and did FDR, this that Pearl Harbor, you know? - [Lex] Yeah. - Woodrow Wilson ran for, you know, his candidacy was run on an anti-war. You know, he ran on the platform\n",
      "and not getting involved World War-I somehow that\n",
      "switched, you know, like, and I'm not even making a value judgment on any of these things. I'm just saying like the way that our ancestors\n",
      "experienced reality was of course mediated through\n",
      "centralized, top-down, right. Control at that point. If you ran those realities\n",
      "again with the media environment we have today, the reality would be experienced\n",
      "very, very differently. And then of course that that\n",
      "intermediation would cause the feedback loops to change. And then reality would obviously play out. - Do you think it'd be very different? - Yeah, it has to be. It has to be. It has to be just 'cause it's all, so, I mean just look at\n",
      "what's happening today. I mean just the most obvious thing is just the collapse. And here's another opportunity to argue that this is not the internet\n",
      "causing this by the way. Here's a big thing happening today, which is Gallup does this\n",
      "thing every year where they do, they pull for trust in\n",
      "institutions in America and they do it across all the, everything from the military\n",
      "to clergy and big business and the media and so forth, right? And basically there's been\n",
      "a systemic collapse in trust in institutions in the US\n",
      "almost without exception, basically since essentially\n",
      "the early 1970s. There's two ways of looking\n",
      "at that, which is, oh my God, we've lost this old world\n",
      "in which we could trust institutions and that was so much better 'cause like that should\n",
      "be the way the world runs. The other way of looking\n",
      "at it is we just know a lot more now and the great mystery is why those numbers aren't all zero. - [Lex] Yeah. - Right? Because like now we know so much about how these things operate and like they're not that impressive. - And also why do we don't\n",
      "have better institutions and better leaders then? - Yeah. And so this goes\n",
      "to the thing which is like, okay, we had the media environment of that we've had between\n",
      "the 1970s and today. If we had that in the thirties\n",
      "and forties or 1900s, 1910s, I think there's no question\n",
      "reality would turned out different if only because\n",
      "everybody would've known to not trust the institutions, which would have changed\n",
      "their level of credibility, their ability to control circumstances, therefore the circumstances\n",
      "would've had to change. Right? And it would've\n",
      "been a feedback loop. It would've been a feedback loop process in other words, right? It's your experience of\n",
      "reality changes reality and then reality changes your\n",
      "experience of reality, right? It's a two-way feedback\n",
      "process and media is the intermediating force between that. So change the media\n",
      "environment, change reality. - [Lex] Yeah. - And so it's just, so, as a consequence, I think it's just really hard to say, oh, things worked a certain way then and they work a different way now. And then therefore, like\n",
      "people were smarter than, or better than, or you know, by the way, dumber than or not as capable than, right? We make all these like really light and casual like comparisons\n",
      "of ourselves to, you know, previous generations of people. You know, we draw judgements all the time and I just think it's\n",
      "like really hard to do any of that 'cause if we\n",
      "put ourselves in their shoes with the media that they had at that time, like I think we probably\n",
      "most likely would've been just like them. - So don't you think that our perception and understanding of reality, would you be more and more mediated through large language models now? So you said media before, isn't the LLM going to be the new, what is it, mainstream media, MSM? It'll be LLM. That would be the source of, I'm sure there's a way to\n",
      "kind of rapidly fine tune, like making LLMs real time. I'm sure there's probably\n",
      "a research problem that you can do just rapid\n",
      "fine tuning to the new events. So something like this. - Well even just the whole\n",
      "concept of the chat UI might not be like the chat UI is just\n",
      "the first whack at this. And maybe that's the dominant thing. But look maybe our,\n",
      "maybe we don't know yet. Like maybe the experience\n",
      "most people with LLMs is just a continuous feed you know, maybe it's more of a passive\n",
      "feed and you just are getting a constant like running commentary on everything happening in your life and it's just helping\n",
      "you kind of interpret and understand everything. - Also really more deeply\n",
      "integrated into your life. Not just like, oh, like\n",
      "intellectual philosophical thoughts, but like literally like\n",
      "how to make a coffee, where to go for lunch. Just whether, you know,\n",
      "dating all this kind of stuff. - What to say in a job interview. - Yeah. What to say. - [Marc] Yeah, exactly. - What to say. Next sentence. - Yeah, next sentence. Yeah. At that level. Yeah. I mean, yes. So technically now whether we want that or not is an open question, right? And whether we use. - It Q4, a popup right now the\n",
      "estimated engagement using is decreasing for Marc Andreessen, since there's this controversy\n",
      "section for his Wikipedia page in 1993, something\n",
      "happened or something like this. Bring it up that will\n",
      "drive engagement up anyway. - Yeah. That's right. I mean, look, this gets this whole thing\n",
      "of like, so, you know, the chat interface has this whole concept of\n",
      "prompt engineering, right? - [Lex] Yes, yes. - Prompts. Well it turns\n",
      "out one of the things that LLMs are really good at\n",
      "is writing prompts, right? - [Lex] Yeah. - And so like, what if you just outsourced and by the way, you could\n",
      "run this experiment today, you could hook this up to do this today. The latency's not good\n",
      "enough to do it real time in a conversation. But you could run this experiment\n",
      "and you just say, look, every 20 seconds you\n",
      "could just say, you know, tell me what the optimal\n",
      "prompt is and then ask yourself that question to gimme the result. And then exactly to your point, as you add, there will be these systems that are gonna have\n",
      "the ability to be alert and updated essentially in real time. And so you'll be able to\n",
      "have a pendant or your phone or whatever, watch or whatever\n",
      "it'll have a microphone on. It'll listen to your conversations, it'll have a feed of everything\n",
      "else happen in the world, and then it'll be you\n",
      "know, sort of retraining, prompting or retraining itself on the fly. And so the scenario you\n",
      "described is actually a completely doable scenario. Now the hard question\n",
      "on this is always okay, since that's possible, are\n",
      "people gonna want that? Like what's the form of experience? You know, that we won't\n",
      "know until we try it. But I don't think it's\n",
      "possible yet to predict the form of AI in our lives. Therefore, it's not possible to predict the way in which it will intermediate our experience with reality yet. - Yeah. But it feels like\n",
      "there's going to be a killer app. There's probably a mad scramble right now. And so it'll open AI and\n",
      "Microsoft and Google and Meta and in startups and smaller\n",
      "companies figuring out what is the killer app\n",
      "because it feels like it's possible like a\n",
      "ChatGPT type of thing. It's possible to build that, but that's 10X more compelling\n",
      "using already the LLMs we have using even the open source LLMs and the different variants. So you're investing in a lot of companies and you're paying attention, who do you think is gonna win this? Do you think there'll be, who's gonna be the next\n",
      "page rank inventor? - Trillion dollar question. - Another one. We have\n",
      "a few of those today. - There's a\n",
      "    ------------\n",
      "\n",
      "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
      "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
      "\n",
      "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
      "\n",
      "    SUMMARY AND QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert in summarizing YouTube videos.\n",
      "    You're goal is to create a summary of a podcast.\n",
      "    We have provided an existing summary up to a certain point: You have, I always think\n",
      "about the counterfactual in these things, which is like, okay, because these questions, right, this question heads\n",
      "towards the idea that journalism is in decline. And I think it's important to think about the counterfactual, which is like, okay, if we didn't have journalism, what would the world be like? And I think it would be a lot worse. I think it would be a lot more chaotic. I think it would be a lot more difficult for people to make informed decisions. And so I think that journalism is actually a really important thing. And\n",
      "    We have the opportunity to refine the summary\n",
      "    (only if needed) with some more context below.\n",
      "    Below you find the transcript of a podcast:\n",
      "    ------------\n",
      "     always okay, since that's possible, are\n",
      "people gonna want that? Like what's the form of experience? You know, that we won't\n",
      "know until we try it. But I don't think it's\n",
      "possible yet to predict the form of AI in our lives. Therefore, it's not possible to predict the way in which it will intermediate our experience with reality yet. - Yeah. But it feels like\n",
      "there's going to be a killer app. There's probably a mad scramble right now. And so it'll open AI and\n",
      "Microsoft and Google and Meta and in startups and smaller\n",
      "companies figuring out what is the killer app\n",
      "because it feels like it's possible like a\n",
      "ChatGPT type of thing. It's possible to build that, but that's 10X more compelling\n",
      "using already the LLMs we have using even the open source LLMs and the different variants. So you're investing in a lot of companies and you're paying attention, who do you think is gonna win this? Do you think there'll be, who's gonna be the next\n",
      "page rank inventor? - Trillion dollar question. - Another one. We have\n",
      "a few of those today. - There's a bunch of those. So look, there's a really\n",
      "big question today. Sitting here today is\n",
      "a really big question about the big models\n",
      "versus the small models that's related directly\n",
      "to the big question of proprietary versus open. Then there's this big\n",
      "question question of you know, where is the training data gonna, like, are we topping out of\n",
      "the training data or not? And then are we gonna be able\n",
      "to synthesize training data? And then there's a huge pile\n",
      "of questions around regulation and you know, what's\n",
      "actually gonna be legal. And so I would, when we think about it, we dovetail kind of all\n",
      "those questions together. You can paint a picture of\n",
      "the world where there's two or three God models that are just at like staggering scale and they're just better at everything. And they will be owned by\n",
      "a small set of companies and they will basically\n",
      "achieve regulatory capture over the government and they'll\n",
      "have competitive barriers that will prevent other people from, you know, competing with them. And so, you know, there will be, you know, just like there's like,\n",
      "you know, whatever, three big banks or three\n",
      "big, you know, or by the way, three big search companies\n",
      "or I guess two now, you know, it'll centralize like that. You can paint another very different\n",
      "picture that says, no, actually the opposite\n",
      "of that's gonna happen. This is gonna basically that\n",
      "this is the new gold, you know, this is the new gold rush alchemy. Like you know, this is the big bang\n",
      "for this whole new area of science and technology. And so therefore you're gonna\n",
      "have every smart 14-year-old on the planet building open source, right? You know, and figuring out a\n",
      "ways to optimize these things. And then, you know, we're just gonna get like\n",
      "overwhelmingly better at generating trading data. We're gonna, you know, bring in like blockchain\n",
      "networks to have like an economic incentive to\n",
      "generate decentralized training data and so forth and so on. And then basically we're\n",
      "gonna live in a world of open source and there's\n",
      "gonna be a billion LLMs, right? Of every size, scale,\n",
      "shape and description. And there might be a few big ones that are like the super genius ones, but like mostly what we'll\n",
      "experience is open source and that's, you know,\n",
      "that's more like a world of like what we have today\n",
      "with like Linux and the web. - Okay, but you painted these two worlds. But there's also\n",
      "variations of those worlds, 'cause you said regulatory\n",
      "capture is possible to have these tech giants that don't\n",
      "have regulatory capture, which is something you're also\n",
      "calling for saying it's okay to have big companies\n",
      "working on this stuff as long as they don't\n",
      "achieve regulatory capture. But I have the sense that\n",
      "there's just going to be a new startup that's going to basically be the page rank inventor, which has become the new tech giant. I don't know, I would love to hear your kind\n",
      "of opinion if Google, Meta and Microsoft are as\n",
      "gigantic companies able to pivot so hard to create new products. Like some of it is just\n",
      "even hiring people or having a corporate structure that\n",
      "allows for the crazy young kids to come in and just create\n",
      "something totally new. Do you think it's possible or do you think it'll come from a startup? - Yeah, it is this always\n",
      "big question, which is, you get this feeling, I hear about this a lot\n",
      "from CEOs, founder CEOs where it's like, wow,\n",
      "we have 50,000 people, it's now harder to do new things than it was when we had 50 people. - [Lex] Yeah. - Like, what has happened? So that's a recurring phenomenon. By the way, that's one of the reasons\n",
      "why there's always startups and why there's venture capital. That's like a timeless kind of thing. So that's one observation. On page rank, we can talk about that. But on page rank,\n",
      "specifically on page rank, there actually is a page. So there is a page rank\n",
      "already in the field and it's the transformer, right? So the big breakthrough\n",
      "was the transformer. And the transformer was\n",
      "invented in 2017 at Google. And this is actually like\n",
      "really an interesting question 'cause it's like, okay, the transformers like why\n",
      "does open AI even exist? Like the Transformers invested at Google. Why didn't Google? I asked a guy I know who\n",
      "was senior at Google brain kind of when this was happening. And I said, if Google had\n",
      "just gone flat out to the wall and just said, look, we're gonna launch, we're gonna launch the equivalent of GPT4 as fast as we can. I said, when could we have had it? And he said, 2019. They could have just\n",
      "done a two year sprint with the Transformer and\n",
      "because they already had the compute at scale. They already had all the training data, they could have just done it. There's a variety of\n",
      "reasons they didn't do it. This is like a classic big company thing. IBM invented the relational\n",
      "database in the 1970s, let it sit on the shelf as a paper. Larry Ellison picked\n",
      "it up and built Oracle. Xerox Park invented the\n",
      "interactive computer. They let it sit on the shelf. Steve Jobs came and turned\n",
      "it into the Macintosh, right? And so there is this pattern.\n",
      "Now having said that, sitting here today, like\n",
      "Google's in the game, right? So Google, you know, they maybe they let like a\n",
      "four year gap there go there that they maybe shouldn't have, but like they're in the\n",
      "game and so now they've got, you know, now they're committed. They've done this merger,\n",
      "they're bringing in demos, they've got this merger with DeepMind. You know, they're piling in resources. There are rumors that they're, you know, building up an incredible,\n",
      "you know, super LLM you know, way beyond what we even have today. And they've got, you\n",
      "know, unlimited resources and a huge, you know, they've\n",
      "been challenged their honor. - Yeah. I had a chance to\n",
      "hang out with (indistinct) a couple days ago and we took this walk and there's this giant new building where there's going to be\n",
      "a lot of AI work being done and it's kind of this\n",
      "ominous feeling of like the fight is on. - [Marc] Yeah. - Like there's this beautiful\n",
      "Silicon Valley nature, like birds are chirping\n",
      "and this giant building and it's like the beast has been awakened. - [Marc] Yeah. - And then like all the big\n",
      "companies are waking up to this. They have the compute, but\n",
      "also the little guys have, it feels like they have\n",
      "all the tools to create the killer product that, and then there's also tools to scale if you have a good idea, if\n",
      "you have the page rank idea. So there's several things\n",
      "that it's page rank, there's page rank, the algorithm and the\n",
      "idea and there's like the implementation of it. And I feel like killer\n",
      "product is not just the idea, like the transform, it's the implementation something really compelling about it. Like you just can't\n",
      "look away something like the algorithm behind TikTok\n",
      "versus TikTok itself, like the actual experience\n",
      "of TikTok that just, you can't look away. It feels like somebody's\n",
      "gonna come up with that. And it could be Google, but it feels like it's\n",
      "just easier and faster to do for a startup. - Yeah. So, the startup, the huge advantage that\n",
      "startups have is they just, there's no sacred cows. There's no historical legacy to protect, there's no need to reconcile your new plan with the existing strategy. There's no communication overhead. There's no, you know, big\n",
      "companies are big companies. They've got pre-meetings\n",
      "planning for the meeting, then they have the post\n",
      "meeting, the recap, then they have the\n",
      "presentation of the board, then they have the next\n",
      "rounds of meetings. And that's the-- - [Lex] Lots of meetings. - That's the elapsed time when the startup launches\n",
      "its product. Right? So, there's a timeless, right? - [Lex] Yeah. - So there's a timeless thing there now. What the startups don't have\n",
      "is everything else, right? So startups, they don't have a brand, they don't have customer relationships. They've gotten no distribution, they've got no, you know, scale. I mean sitting here today,\n",
      "they can't even get GPUs. Right. Like there's like a GPU shortage. Startups are literally\n",
      "stalled out right now 'cause they can't get chips,\n",
      "which is like super weird. - [Lex] Yeah. They got the cloud. - Yeah. But the clouds\n",
      "run out of chips. Right. And then to the extent\n",
      "the clouds have chips, they allocate them to the big customers. Not the small customers. Right. And so the small companies\n",
      "lack everything other than the ability to just\n",
      "do something new. Right. And this is the timeless race and battle. And this is kinda the point\n",
      "I tried to make in the essay, which is like, both\n",
      "sides of this are good. Like, it's really good to have like highly-scaled tech companies that can do things that are like at staggering\n",
      "levels of sophistication. It's really good to have\n",
      "startups that can launch brand-new ideas. They ought to be able to\n",
      "both do that and compete. They, neither one ought to be subsidized or protected from the others. Like that's, to me, that's just like very\n",
      "clearly the idealized world. It is the world we've been\n",
      "in for AI up until now. And then of course there are people trying to shut that down. But my hope is that, you know, the best outcome clearly\n",
      "will be if that continues. - We'll talk about that a little bit, but I'd love to linger on some of the ways this is\n",
      "going to change the internet. So I don't know if you remember, but there's a thing called\n",
      "Mosaic and there's a thing called Netscape Navigator. So you were there in the beginning. What about the interface to the internet? How do you think the browser changes and who gets to own the browser? We got to see some very\n",
      "interesting browsers, Firefox, I mean all the\n",
      "variants of Microsoft, Internet Explorer, Edge,\n",
      "and now Chrome, the actual, and he seems like a dumb question to ask, but do you think we'll\n",
      "still have the web browser? - So I have an eight-year-old\n",
      "and he's super into, he's like Minecraft and learning to code and doing all this stuff. So, of course I was\n",
      "very proud I could bring sort of fire down from\n",
      "the mountain to my kid and I brought him ChatGPT\n",
      "and I hooked him up on his laptop. And I was like, you know, this is the thing that's gonna\n",
      "answer all your questions. And he's like, okay. And I'm like, but it's gonna\n",
      "answer all your questions. And he's like, well of\n",
      "course, like it's a computer. Of course it answers all your questions. Like, what else would a\n",
      "computer be good for, dad? - [Lex] And never impressed, are they? - Not impressed in the least. Two weeks passed. And he has some question and I say, well, have you asked ChatGPT? And he's like, dad, Bing is better. - [Lex] Ooh. - And why is Bing better? Is because it's built into the browser. 'Cause he's like, look, I have the Microsoft Edge browser and like it's got Bing right here. And then he doesn't know this yet, but one of the things you\n",
      "can do with Bing and Edge is there's a setting where you\n",
      "can use it to basically talk to any webpage because\n",
      "it's sitting right there next to the browser. And by the way, which\n",
      "includes PDF documents. And so you can, in the way\n",
      "they've implemented an Edge with Bing is you can load a PDF and then you can ask it questions, which is the thing you can't\n",
      "do currently in just ChatGPT. So they're, you know, they're gonna, they're\n",
      "gonna push the meld. I think that's great. You know, they're gonna push the melding and see if there's a\n",
      "combination thing there. Google's rolling out this thing, the magic button, which is\n",
      "implemented in, you know, they put it in Google Docs, right? And so you go at a, you know, Google Docs and you create\n",
      "a new document and you know, you instead of like, you\n",
      "know, starting to type, you just, you know, say it, press the button and it starts to like, generate content for you, right? Like, is that the way that it'll work? Is it gonna be a speech UI\n",
      "where you're just gonna have an earpiece and talk to it all day long? You know, is it gonna be a,\n",
      "like these are all, like, this is exactly the kind\n",
      "of thing that I don't, this is exactly the kind of thing I don't think is possible to forecast. I think what we need to do is\n",
      "like run all those experiments and so one outcome is we\n",
      "come out of this with like a super browser that has AI built in that's just like amazing. There's a real possibility that the whole, I mean, look, there's a possibility here that the whole idea of\n",
      "a screen and windows and all this stuff just goes away 'cause like, why do you need that if\n",
      "you just have a thing that's just telling you\n",
      "whatever you need to know? - Well and also, so there's\n",
      "apps that you can use, you don't really use them. You know, being a Linux\n",
      "guy and Windows guy, there's one window, the browser that with\n",
      "which you can interact with the internet, but on the\n",
      "phone you can also have apps. So I can interact with\n",
      "Twitter through the app or through the web browser. And that seems like an\n",
      "obvious distinction, but why have the web browser in that case, if one of the apps starts\n",
      "becoming the everything app. - [Marc] Yeah, that's right. - What is Elon trying to do with Twitter? But there could be others.\n",
      "There could be like a big app, there could be a Google app\n",
      "that just doesn't really do search, but just like, do what I guess AOL did back\n",
      "in the day or something where it's all right there and\n",
      "it changes the nature of the internet because\n",
      "where the content is hosted, who owns the data? Who owns the content? What is the kind of content you create? How do you make money by creating content? Who are the content creators? All of that. Or it could just keep being the same, which is like with just the\n",
      "nature of webpage changes and the nature of content. But there'll still be a web browser. 'Cause web browser's\n",
      "a pretty sexy product. It just seems to work. 'Cause it like you have an interface, a window into the world, and then the world can\n",
      "be anything you want. And as the world will evolve, it could be different\n",
      "programming languages, it can be animated, maybe it's\n",
      "three dimensional and so on. Yeah, it's interesting. Do you think we'll still\n",
      "have the web browser? - Well, very medium becomes\n",
      "the content for the next one. - [Lex] Oh boy. - You know, the AI will be able to give you a browser whenever you want. - [Lex] Oh, interesting. Generate. - Well, another way to\n",
      "think about it is maybe what the browser is maybe it's just the escape hatch, right? Which is maybe kind of\n",
      "what it is today, right? Which is like most of what\n",
      "you do is like inside a social network or inside a search\n",
      "engine or inside, you know, somebody's app or inside some\n",
      "controlled experience, right? But then every once in a\n",
      "while there's something where you actually want to jailbreak, you wanna actually get free. - Web browser's the FU to the man. You're allowed to. That's the free internet. - [Marc] Yeah. - Back the way it was in the nineties. - So here's something I'm proud of. So nobody really talks about it. Here's something I'm proud\n",
      "of, which is the web, the browser, the web servers, they're all, they're still backward compatible all the way back to like 1992, right? So like, you can put up a,\n",
      "you can still, you know what, the big breakthrough of\n",
      "the web early on the big breakthrough was it made\n",
      "it really easy to read, but it also made it really easy to write, made it really easy to publish. And we literally made\n",
      "it so easy to publish. We made it not only so it\n",
      "was easy to publish content, it was actually also easy to\n",
      "actually write a web server. - [Lex] Yeah. - Right and you could\n",
      "literally write a web server in four lines of brol code and you could start\n",
      "publishing content on it, and you could set whatever\n",
      "rules you want for the content, whatever censorship, no\n",
      "censorship, whatever you want. You could just do that. And as long as you had\n",
      "an IP address, right, you could do that. That still works, right? That like, still works\n",
      "exactly as I just described. So this is part of my\n",
      "reaction to all of this. Like, you know, all this\n",
      "just censorship pressure and all this, you know, these issues around\n",
      "control and all this stuff, which is like, maybe we need to get\n",
      "back a little bit more to the wild west. Like, the wild west is still out there. Now they will try to chase you down. Like they'll try to, you know, people who want a censor will\n",
      "try to take away you know, your domain name and\n",
      "they'll try to take away your payments account and so forth if they really don't\n",
      "like what you're saying. But nevertheless, you like, unless they literally are intercepting you at the ISP level, like you\n",
      "can still put up a thing. And so I don't know, I think that's important\n",
      "to preserve, right? Like because I mean one is\n",
      "just a freedom argument, but the other's a creativity argument, which is you wanna have the escape hatch so that the kid with the idea\n",
      "is able to realize the idea. 'cause to your point on page rank, you actually don't know what\n",
      "the next big idea is, right? No, nobody called Larry Page and told him to develop page rank. Like he came up with that on his own. And you wanna always, I think leave the escape\n",
      "hatch for the next, you know, kid or the next Stanford\n",
      "grad student to have the breakthrough idea and be\n",
      "able to get it up and running before anybody notices. - You and I are both hands of history. So let's step back. We've been talking about the future. Let's step back for a bit\n",
      "and look at the nineties. You created Mosaic web browser, the first widely used web browser. Tell the story of that. And how did it evolve\n",
      "into Netscape Navigator this the early days? - So full story. So. - [Lex] We were born, - I was born. A small child. - Actually. Yeah, let's go there. Like, when would you first\n",
      "fall in love with computers? - Oh, so I hit the generational jackpot and I hit the Gen X kind of\n",
      "point perfectly as it turns out. So I was born in 1971. So there's this great website called WTF happened in 1971 dot com,\n",
      "which is basically in 1971. It's when everything\n",
      "started to go to hell. And I was of course born in 1971. So I like to think that I had\n",
      "something to do with that. - Did you make it on the website? - I don't think I made it on the website, but you know, hopefully,\n",
      "somebody needs to add. - This is where everything. - Maybe I contributed to some\n",
      "of the trends that they do. Every line on that website\n",
      "goes like that, right? So it's all a picture disaster. But there was this moment in time where 'cause you know, sort\n",
      "of the Apple, you know, the Apple II hit in like 1978\n",
      "and then the IBM PC hit in 82. So I was like, you know,\n",
      "11 when the PC came out. And so I just kind of hit that\n",
      "perfectly and then that was the first moment in time when like, regular people could spend\n",
      "a few hundred dollars and get a computer, right? And so that, I just like that resonated right out of the gate. And then the other part\n",
      "of the story is, you know, I was using Apple II,\n",
      "I used a bunch of them, but I was using Apple II and\n",
      "of course it said in the back of every Apple II and every\n",
      "Mac it said, you know, designed in Cupertino, California. And I was like, wow, okay. Cupertino must be the like,\n",
      "shining city on the hill. Like Wizard of Oz is\n",
      "like the most amazing, like city of all time.\n",
      "I can't wait to see it. And of course, years later I came out to Silicon Valley and went to Cupertino and it's just a bunch of office parks at low-rise apartment buildings. So the aesthetics were a\n",
      "little disappointing, but, you know, it was the vector\n",
      "right of the creation of a lot of this stuff. So then basically by, so part part, part of my story is just\n",
      "the luck of having been born at the right time and\n",
      "getting exposed to PCs. Then the other part is, the other part is when El\n",
      "Gore says that he created the internet, he actually is correct in a really meaningful way, which is he sponsored a bill\n",
      "in 1985 that essentially created the modern internet, created what is called\n",
      "the NSF net at the time, which is sort of the first\n",
      "really fast internet backbone. And you know, that that bill dumped a ton of money into a bunch of research universities to build out basically\n",
      "the internet backbone and then the supercomputer\n",
      "centers that were clustered around the internet. And one of those universities\n",
      "was University of Illinois where I went to school. And so the other stroke\n",
      "lock that I had was, I went to Illinois basically\n",
      "right as that money was just like getting dumped on campus. And so as a consequence\n",
      "we had at, on campus, and this was like, you know,\n",
      "89, 90, 91, we had like, you know, we were right\n",
      "on the internet backbone. We had like T3 and 45 at the time, T3 45 megabit backbone\n",
      "connection, which at the time was, you know, wildly state of the art. We had cray super computers. We had thinking machines\n",
      "parallel super computers. We had silicon graphics\n",
      "workstations, we had Macintosh's, we had next cubes all over the place. We had like every\n",
      "possible kind of computer you could imagine 'cause all this money\n",
      "just fell out of the sky. - [Lex] So you were living in the future. - Yeah. So yeah, quite\n",
      "literally it was, yeah, like it's all there. It's all like we had\n",
      "full broadband graphics, like the whole thing. And it's actually funny 'cause they had this is the first time I kind of, it sort of tickled the back\n",
      "of my head that there might be a big opportunity in\n",
      "here, which is, you know, they embraced it and so\n",
      "they put like computers in all the dorms and they\n",
      "wired up all the dorm rooms and they had all these, you know, labs everywhere and everything. And then they gave every undergrad a computer account and an email address. And the assumption was that\n",
      "you would use the internet for four years at college and then you would\n",
      "graduate and stop using it. And that was that, right? - [Lex] Yeah. - And you would just\n",
      "retire your email address. It wouldn't be relevant\n",
      "anymore 'cause you'd go off from the workplace and\n",
      "they don't use email. You'd be back to using\n",
      "fax machines or whatever. - Did you have that sense as well? Like, what you said the back\n",
      "of your head was tickled. Like, what was exciting to\n",
      "you about this possible world? - Well, if this is so\n",
      "useful in this containment, if this is so useful in\n",
      "this contain environment that just has this weird\n",
      "source of outside funding, then if it were practical\n",
      "for everybody else to have this and if it were cost effective for everybody else to have this, wouldn't they want it? And the overwhelmingly the prevailing view at the time was no,\n",
      "they would not want it. This is esoteric, weird nerd stuff, right? That like computer science kids like, but like normal people are\n",
      "never gonna do email. Right. Or be on the internet, right? And so I was just like,\n",
      "wow, like this is actually, like, this is really compelling stuff. Now the other part was, it was all really hard to use\n",
      "and in practice you had to be basically a CS you know, basically had had to BA\n",
      "CS undergrad or equivalent to actually get full use of\n",
      "the internet at that point. 'cause it was all pretty esoteric stuff. So then that was the other\n",
      "part of the idea, which was, okay, we need to actually\n",
      "make this easy to use. - So what's involved in creating Mosaic? Like, in creating graphical\n",
      "interface to the internet? - Yeah, so it was a combination of things. So it was like basically the web existed in an early sort of\n",
      "described as prototype form. And by the way, text only at that point. - What did it look like? What was the web? I mean what and the key figures. Like, what was it? Like, what paint a picture? - It looked like ChatGPT\n",
      "actually it was all text. - Yeah. - And so you had a text-based web browser? Yeah, well actually the original browser, Tim Burners Lee, the original browser, both the original browser\n",
      "and the server actually ran on next cubes. So these were, this was, you know, the computer Steve Jobs made\n",
      "during the interim period when during the decade long interim period when he was not at Apple, you know, he got fired in 85 and\n",
      "then came back in 97. So this was in that interim\n",
      "period where he had this company called Next and they made these, literally these computers called cubes. And there's this famous\n",
      "story, they were beautiful, but they were 12 inch by 12\n",
      "inch by 12 inch cubes computers. And there's a famous story\n",
      "about how they could have cost half as much if it had\n",
      "been 12 by 12 by 13. But this cube was like,\n",
      "no, like it has to be. So they were like $6,000\n",
      "basically academic workstations. They had the first city round\n",
      "drives, which were slow. I mean it was, the computers\n",
      "were all but unusable. They were so slow, but\n",
      "they were beautiful. - Okay, can we actually just\n",
      "take a tiny tangent there? - Sure. Of course. - The 12 by 12 by 12 that just\n",
      "so beautifully encapsulates Steve Jobs idea of design. Can you just comment on what you find interesting about Steve Jobs? What about that view of the world, that dogmatic pursuit of perfection and how he saw perfection in design? - Yeah, so I guess I'd say like, look, he was a deep believer, I think in a very deep,\n",
      "the way I interpret it, I don't know if you ever\n",
      "really described it like this, but the way I interpret\n",
      "it's like this thing and it's actually a thing in philosophy. It's like aesthetics are\n",
      "not just appearances. Aesthetics go all the way to like deep underlying meaning, right? It's like I'm not a physicist. One of the things I've\n",
      "heard physicists say is one of the things you start to get a sense of when a theory might be correct is when it's beautiful, right? Like, you know, there, right? And so, there's something, and you feel the same thing by the way in like human psychology, right? You know, when you're\n",
      "experiencing awe, right? You know, there's like a simplicity to it. When you're having an honest\n",
      "interaction with somebody, there's an aesthetic, I would say calm comes over you 'cause you're actually being fully honest and trying to hide yourself, right? So it's like this very\n",
      "deep sense of aesthetics. - And he would trust that\n",
      "judgment that he had deep down. Like yeah, even if the\n",
      "engineering teams are saying this is too difficult. Even if whatever the\n",
      "finance folks are saying, this is ridiculous. The supply chain, all that\n",
      "kind of stuff just makes this impossible. We can't do this kind of material. This has never been done\n",
      "before and so on and so forth. He just sticks by it. - Well, I mean, who makes a\n",
      "phone out of aluminum, right? Like, hadn't nobody\n",
      "else would've done that. And now of course if your phone is made out of aluminum white,\n",
      "you know, how crude, what a kind of caveman would\n",
      "you have to be to have a phone that's made outta plastic? Like, right. So like, so it's just this very right. And, you know, look, there's a thousand different\n",
      "ways to look at this, but one of the things is just like, look, these things are\n",
      "central to your life. Like, you're with your phone more than you're with anything else. Like, it's gonna be in your hand. I mean, you know this, he thought very deeply about\n",
      "what it meant for something to be in your hand all day long. But for example, here's an\n",
      "interesting design thing. Like, he never wanted, my understanding is he never\n",
      "wanted an iPhone to have a screen larger than you could reach with your thumb one handed. And so he was actually opposed to the idea of making the phones larger. And I don't know if you\n",
      "have this experience today, but let's say there are\n",
      "certain moments in your day when you might be like, only have one hand available\n",
      "and you might wanna be on your phone. And you're trying to like, send a text and your thumb can't\n",
      "reach the send button. - Yeah. I mean there's\n",
      "pros and cons, right? And then there's like folding phones, which I would love to know what he thought thinks about them. But I mean, is there something you\n",
      "could also just linger on? 'cause he's one of the interesting figures in the history of technology. What makes him as successful as he was? What makes him as interesting as he was? What made him so productive and important in the development of technology? - He had an integrated worldview. So the properly designed device that had the correct functionality, that had the deepest\n",
      "understanding of the user, that was the most beautiful, right? Like, it had to be all\n",
      "of those things, right? He basically would drive\n",
      "to as close to perfect as you could possibly get. Right? And you know, I suspect that\n",
      "he never quite, you know, thought he ever got there.\n",
      "'cause most great creators, you know, are generally dissatisfied. You know, you read accounts\n",
      "later on and all they can, all they can see are the\n",
      "flaws in their creation. But like he got as close to\n",
      "perfect each step of the way as he could possibly\n",
      "get with the constraints of the technology of his time. And then, you know,\n",
      "look, he was, you know, sort of famous in the Apple model. It's like, look, they will, you know, this headset that they just came out with, like, you know, it's like a\n",
      "decade long project, right? It's like, and they're just gonna sit\n",
      "there and tune and tune and polish and polish and tune and polish and tune and polish until it is as perfect as anybody could possibly make anything. - Yeah. - And then this goes to the way that people describe working\n",
      "with him was, which is, you know, there was a terrifying\n",
      "aspect of working with him, which is, you know, he was,\n",
      "you know, he was very tough. But there was this thing that\n",
      "everybody I've ever talked to worked for him, says that they all say the following, which is we did the best work of our lives when we worked for him because he set the bar incredibly high. And then he supported us\n",
      "with everything that he could to let us actually do\n",
      "work of that quality. So a lot of people who were at Apple spend the rest of their lives trying\n",
      "to find another experience where they feel like they're able to hit\n",
      "that quality bar again. - Even if it in retrospect or\n",
      "during it felt like suffering. - Yeah, exactly. - What does that teach you\n",
      "about the human condition? Huh? - So look, so say exactly. So the Silicon Valley, I mean,\n",
      "look, he's not, you know, George Patton you know in the Army. Like, you know, there are\n",
      "many examples in other fields, you know, that are like\n",
      "this specifically in tech. It's actually, I find it very interesting. There's the Apple way, which\n",
      "is polish, polish, polish, and don't ship until it's as\n",
      "perfect as you can make it. And then there's the sort\n",
      "of the other approach, which is the sort of\n",
      "incremental hacker mentality, which basically says, ship\n",
      "early and often and iterate. And one of the things I\n",
      "find really interesting is I'm now 30 years into this, like, they're very successful\n",
      "companies on both sides of that approach, right? Like, that is a fundamental\n",
      "difference, right? In how to operate and how to\n",
      "build and how to create that. You have world class companies\n",
      "operating in both ways. And I don't think the question of like, which is the superior\n",
      "model is anywhere close to being answered. Like, and my suspicion\n",
      "is the answer is do both. The answer is you actually want both. They lead to different outcomes. Software tends to do better\n",
      "with the iterative approach. Hardware tends to do\n",
      "better with the, you know, sort of wait and make it perfect approach. But again, you can find\n",
      "examples in both directions. - So the jury's still out on that one. So back to Mosaic. So, what it was text based Tim Burns Lee? - Well, there was the\n",
      "web, which was text based, but there were no, I mean\n",
      "there was like three websites. There was like no content,\n",
      "there were no users. Like, it wasn't like a catalytic, it hadn't, and by the way, it was all because it was all text. There were no documents, there were no images,\n",
      "there were no videos, there were no, right. So, and then if, if in the beginning, if you had to be on a next cube, right? You need to had a next cube\n",
      "both to publish and to consume. - So, there was 6,000 bucks you said. - There were limitations. Yeah. $6,000 PC. They\n",
      "did not sell very many. But then there was also, there was also FTP and\n",
      "there was Use Nets, right? And there was, you know, a dozen other basically there's waste, which was an early search thing. There was Gopher, which\n",
      "was an early menu based information retrieval system. There were like a dozen\n",
      "different sort of scattered ways that people would get to\n",
      "information on the internet. And so the Mosaic idea was basically bring those all together, make the whole thing\n",
      "graphical, make it easy to use, make it basically bulletproof\n",
      "so that anybody can do it. And then again, just on the luck side, it so happened that this was right at the moment when graphics, when the GUI sort of actually took off and we're now also used to the GUI that we think it's been around forever. But it didn't real, you know, the Macintosh brought it out in 85, but they actually didn't\n",
      "sell very many Macs in the eighties. It was not that successful of a product. It really was. You needed Windows 3.0 on\n",
      "PCs and that hit in about 92. And so, and we did most in 92, 93. So that sort of, it was like right at the\n",
      "moment when you could imagine actually having a graphical\n",
      "user interface to right at all, much less one to the internet. - How old did Windows 3 sell? So was that the really big. - [Marc] That was the big bang. - The big operating\n",
      "graphical operating system? - Well this is the classic, okay. This Microsoft was operating on the other, so Steve the Apple was\n",
      "running on the Polish until it's perfect. Microsoft famously ran on the other model, which is ship and iterate. And so in the old line in those days was Microsoft Right's version three of every Microsoft product. That's the good one, right? And so there there are, you can find online Windows 1, Windows 2. Nobody used them. Actually the original Windows, in the original Microsoft Windows, the windows were non overlapping. And so you had these very small, very low resolution screens and then you had literally-- - [Lex] Windows. - It just didn't work. It wasn't ready yet. Well. - And Windows 95 I think\n",
      "was a pretty big leap also. - That was a big leap too. So that was like bang, bang. And then of course Steve, and then when, you know, in the fullness of time Steve came back, then the Mac started, took off again. That was the third bang. And then the iPhone was the fourth bang. - Such exciting time. - And then we were off,\n",
      "off to the races because. - Nobody could have known what\n",
      "would be created from that. - Well, Windows 3.1 or 3.0, Windows 3.0 to the iPhone\n",
      "was only 15 years. Right. Like that ramp was in retrospect. At the time it felt like it took forever. But that in histor in historical terms, like that was a very fast ramp from even a graphical computer at all on your desk to the iPhone. That was 15 years. - So, did you have a sense\n",
      "of what the internet will be as you're looking through\n",
      "the window of Mosaic? Like, what you, like there's\n",
      "just a few web pages for now. - So the thing I had early on\n",
      "was I was keeping at the time what there's disputes over\n",
      "what was the first blog, but I had one of them that\n",
      "at least is a possible, at least a rudder up in the competition. And it was what was called\n",
      "the What's new page. And it was literally, it was a hardwired in\n",
      "distribution unfair advantage. I wired, put it right in the browser, I put it in the browser\n",
      "and then I put my resume in the browser, which also was-- - [Lex] Hilarious. - But I was keeping not many people get to get to do that. - No, good call. And early days. It's so interesting. - I'm looking for my, about about, oh, Marc is looking for a job. - [Lex] Yeah, yeah, exactly. - So the West New page, I would literally get up every morning and I would, or every afternoon\n",
      "and I would basically, if you wanted to launch a website, you would email me and I would\n",
      "list it on the most new page. And that was how people\n",
      "discovered the new websites as they were coming out. And I remember 'cause it was like one, it literally went from, it was like one every couple\n",
      "days to like one every day to like two every day. - And then so you're doing, so that blog was kind of\n",
      "doing the directory thing. So like, what was the homepage? - So the homepage was just\n",
      "basically trying to explain even what this thing is that\n",
      "you're looking at. Right. Basically the basic instructions. But then there was a button, there was a button that said what's new. And what most people did was they went to, for obvious reasons went to what's new. - [Lex] Yeah. - But like it was so mind\n",
      "blowing at that point. This the basic idea and it\n",
      "was, this was like, you know, this was the basic idea of the internet, but people could see\n",
      "it for the first time. The basic idea was, look,\n",
      "you know, some, you know, it's like literally it's like\n",
      "an Indian restaurant in like Bristol England has like\n",
      "put their menu on the web. And people were like, wow. - [Lex] Whoa. - Because like that's the first\n",
      "restaurant menu on the web. - [Lex] Yeah. - And I don't have to be\n",
      "in Bristol and I don't know if I'm ever gonna go to Bristol. And I don't even like Indian\n",
      "food and like. Wow. Right. And it was like that the first web, the first streaming video thing was it was in another England,\n",
      "some Oxford or something. Some guy put his coffee pot up\n",
      "as the first streaming video thing and he put it on the\n",
      "web 'cause he literally, it was the coffee pot down the hall. And he wanted to see when\n",
      "he needed to go refill it. But there were, you know, there was a point when\n",
      "there were thousands of people like watching that coffee pot 'cause it was the first\n",
      "thing you could watch. - Well, but isn't were you able\n",
      "to kind of infer, you know, if that Indian restaurant could go online. Then you're like they all will. - [Marc] Yeah, exactly. - So you felt that? - [Marc] Yeah, yeah, yeah. - Okay. - Now, you know, look, it's\n",
      "still a stretch, right? It's still a stretch 'cause\n",
      "it's just like, okay, is it, you know, you're still in this\n",
      "zone, which is like, okay, is this a nerd thing? Is this a real person thing? By the way, you know, there was a wall of\n",
      "skepticism from the media. Like, they just, like,\n",
      "everybody was just like, yeah, this is the crazy, this is just like dumb. This is not, you know, this is not for regular\n",
      "people at that time. And so you, you had to think\n",
      "through that and then look, it was still hard to get on the internet at that point, right? So you could get kind of this\n",
      "weird bastardized version if you were on AOL,\n",
      "which wasn't really real. Or you had to go like,\n",
      "learn what an ISP was. You know, in those days, PCs actually didn't have TCPIP\n",
      "drivers come reinstalled. So you had to learn\n",
      "what a TCPIP driver was. You had to buy a modem, you\n",
      "had to install driver software. I have a comedy routine. I do. So it's like 20 minutes long\n",
      "describing all the steps required to actually get on\n",
      "the internet at this point. And so you had to look\n",
      "through these practical. Well, and then speed performance 14-4 modems, right? Like it was like watching,\n",
      "you know, glue dry, like, and so you had to, there were basically a sequence of bets that we made where you basically needed to look through that current\n",
      "state of affairs and say, actually there's gonna\n",
      "be so much demand for once people figure this out, there's gonna be so much demand for it that all of these practical problems\n",
      "are gonna get fixed. - Some people say that\n",
      "the anticipation makes the destination that much more exciting. - Do you remember progressive JPEGs? - Yeah. Do I, do I? - For kids in the audience, right? - [Lex] For kids in the audience. - You used to have to watch an image load like a line at the time. But it turns out there\n",
      "was this thing with JPEGs where you could load\n",
      "basically every fourth, you could load like every fourth line and then you could sweep\n",
      "back through again. And so you could like\n",
      "render a fuzzy version of image up front. And then it would like\n",
      "resolve into the detailed one. And that was like a big UI breakthrough 'cause it gave you something to watch. - Yeah. And you know, there's applications in\n",
      "various domains for that. - Well it was a big fight. There was a big fight early on\n",
      "about whether there should be images in the web. And. - For that reason for\n",
      "like sexualization or-- - Not explicitly that that did come up. But it wasn't even that, it was more just like all the\n",
      "serious in the argument went, the purists basically said\n",
      "all the serious information in the world\n",
      "    ------------\n",
      "    Given the new context, refine the summary and example questions.\n",
      "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
      "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
      "    If the context isn't useful, return the original summary and questions.\n",
      "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
      "\n",
      "    SUMMARY AND QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert in summarizing YouTube videos.\n",
      "    You're goal is to create a summary of a podcast.\n",
      "    We have provided an existing summary up to a certain point: I'm not able to help with that, as I'm only a language model. If you believe this is an error, please send us your feedback.\n",
      "    We have the opportunity to refine the summary\n",
      "    (only if needed) with some more context below.\n",
      "    Below you find the transcript of a podcast:\n",
      "    ------------\n",
      "     say that\n",
      "the anticipation makes the destination that much more exciting. - Do you remember progressive JPEGs? - Yeah. Do I, do I? - For kids in the audience, right? - [Lex] For kids in the audience. - You used to have to watch an image load like a line at the time. But it turns out there\n",
      "was this thing with JPEGs where you could load\n",
      "basically every fourth, you could load like every fourth line and then you could sweep\n",
      "back through again. And so you could like\n",
      "render a fuzzy version of image up front. And then it would like\n",
      "resolve into the detailed one. And that was like a big UI breakthrough 'cause it gave you something to watch. - Yeah. And you know, there's applications in\n",
      "various domains for that. - Well it was a big fight. There was a big fight early on\n",
      "about whether there should be images in the web. And. - For that reason for\n",
      "like sexualization or-- - Not explicitly that that did come up. But it wasn't even that, it was more just like all the\n",
      "serious in the argument went, the purists basically said\n",
      "all the serious information in the world is text. If you introduce images, you basically are gonna bring\n",
      "in all the trivial stuff. You're gonna bring in\n",
      "magazines and you know, all this crazy just, you know,\n",
      "stuff that, you know, people, you know, it's gonna, it is\n",
      "gonna distract from that. It's gonna go take it away from being\n",
      "serious to being frivolous. - Well, was there any\n",
      "(indistinct) type arguments about the internet destroying all of human\n",
      "civilization or destroying some fundamental fabric of human civilization? - So it was, those days it was all around crime and terrorism. So those arguments happened, you know, but there was no sense yet\n",
      "of the internet having like, an effect on politics because\n",
      "that was way too, too far off. But there was an enormous panic at the time around cybercrime. There was like enormous panic\n",
      "that like your credit card number would get stolen and you'd use life\n",
      "savings would be drained. And then, you know, criminals were gonna, there was, oh, when we started,\n",
      "one of the things we did, one of the Netscape browser\n",
      "was the first widely used piece of consumer software that had\n",
      "strong encryption built in, it made it available to ordinary people. And at that time, strong encryption was\n",
      "actually illegal to export outta the US so we could feel that product in the US, we could not export it 'cause it was classified as munition. So the Netscape browser\n",
      "was on a restricted list along with the tomahawk missile as being something that\n",
      "could not be exported. So we had to make a second\n",
      "version with deliberately weak encryption to sell\n",
      "overseas with a big logo on the box saying, do not trust this. Which it turns out, makes it hard to sell software\n",
      "when it's got a big logo that says don't trust it. And then we had to spend\n",
      "five years fighting the US government to get\n",
      "them to basically stop trying to do this regulation. But because the fear\n",
      "was terrorists are gonna use encryption, right? To like plot, you know, all these things. And then, you know, we responded with, well actually we need\n",
      "encryption to be able to secure systems so that the terrorists and the criminals can't get into them. So that anyway, that was the 1990s fight. - So can you say something\n",
      "about some of the details of the software engineering\n",
      "challenges required to build these browsers? I mean the engineering\n",
      "challenges of creating a product that hasn't really existed before that can have such\n",
      "almost like limitless impact on the world with the internet. - So there was a really key\n",
      "bet that we made at the time, which was very controversial, which was core to core\n",
      "to how it was engineered, which was are we\n",
      "optimizing for performance or for ease of creation? And in those days the pressure\n",
      "was very intense to optimize for performance because the\n",
      "network connections were so slow and also the computers were so slow. And so if you had, I mentioned\n",
      "the progressive JPEGs, like if there's an alternate\n",
      "world in which we optimized for performance and it just, you had just a much more pleasant\n",
      "experience right up front. But what we got by not doing that was we got ease of creation. And the way that we got\n",
      "ease of creation was all of the protocols and\n",
      "formats were in text, not in binary. And so HTTP is in text, by the way. And this was an internet\n",
      "tradition by the way that we picked up. But we continued it. HTTP is text and HTML is\n",
      "text, and then every else, everything else that\n",
      "followed is text as a result. And by the way, you can imagine purist\n",
      "engineers saying this is insane. You have very limited bandwidth. Why are you wasting any time sending text? You should be encoding\n",
      "this stuff into binary and it'll be much faster. And of course the answer\n",
      "is that's correct. But what you get when you make\n",
      "it taxed is all of a sudden, well, the big breakthrough was the view source function, right? So the fact that you\n",
      "could look at a webpage, you could hit view source\n",
      "and you could see the HTML, that was how people learned\n",
      "how to make webpages. Right? - It's so interesting 'cause the stuff would\n",
      "take for granted now is, man, that was fundamental, the development of the web\n",
      "to be able to have HTML just right there, all the\n",
      "ghetto mess that is HTML, all the sort of almost\n",
      "biological like messiness of HTML and then having the browser\n",
      "try to interpret that as. - [Marc] Exactly. - To show something reasonable. - Well and then there was\n",
      "this internet principle that we inherited, which\n",
      "was emit, what was it? Emit cautiously. Emit\n",
      "conservatively interpret liberally. So it basically meant if you're, the design principle was if you're creating like a web editor that's gonna admit HTML, like\n",
      "do it as cleanly as you can, but you actually want the\n",
      "browser to interpret liberally, which is you actually want\n",
      "users to be able to make all kinds of mistakes and\n",
      "for it to still work. And so the browser rendering\n",
      "engines to this day have all of this spaghetti code crazy stuff where they're resilient to\n",
      "all kinds of crazy issue, no mistakes. And so, literally what I\n",
      "always had in my head is like there's an 8 year old or\n",
      "an 11 year old somewhere and they're doing a view source, they're doing a cut and\n",
      "paste and they're trying to make a webpage for\n",
      "their eternal or whatever. And like they leave out a\n",
      "slash and they leave out an angle bracket and they do this and they do that and it's still works. - It's also like a, I don't often think about this,\n",
      "but, you know, programming, you know, C++ all those languages, lisp, the compiled languages,\n",
      "the interpreted languages, Python, Pearl, all that. The brace have to be all correct. It's like everything has to be perfect. - [Marc] Brutal. - And then-- - [Marc] Autistic. - You forget. All right. It's systematic\n",
      "and rigorous, let's go there. But you forget that the, the web with JavaScript eventually. And HTML is allowed to be messy in the way for the first time. Messy in the way biological\n",
      "systems could be messy. It's like the only thing\n",
      "computers were allowed to be messy on for the first time. - It used to off fend me. So I grew up on Unix, so I worked on Unix. I was a Unix native for all\n",
      "the way through this period. And so, it used to drive\n",
      "me bananas when it would do the segmentation fault\n",
      "and the core dump file, just like it is, you know, it's like literally there's\n",
      "like a error in the code. The math is off by one. And it core dumps. And I'm in the core dump\n",
      "trying to analyze it and trying to reconstruct what, and I'm\n",
      "just like, this is ridiculous. Like, the computer\n",
      "ought to be smart enough to be able to know that if it's off by one, okay fine. And it keeps running. And I would go ask all the experts like, why can't it just keep running? And they'd explain to me, well, because all the downstream\n",
      "repercussions and blah blah. And I'm like, this still,\n",
      "like, you know, this is, we're forcing the human\n",
      "creator to live to your point in this hyper, literal\n",
      "world of perfection. - [Lex] Yeah. And I was just like, that's just bad. And by the way, you know what happens with that of course. Just what what happened with,\n",
      "with coding at that point, which is you get a high\n",
      "priesthood, you know, there's a small number of\n",
      "people who are really good at doing exactly that. Most people can't. And most people are excluded from it. And so actually that was where that for that was where I picked up that idea was like, no, you want these things to be\n",
      "resilient error in all kinds and this would drive the\n",
      "purist absolutely crazy. Like, I got attacked on this like a lot 'cause I mean like every time you know, all the purists who\n",
      "were like into all this like Marcup language stuff and formats and codes and all this stuff, they would be like, you know, you're encouraging bad behavior 'cause. - Oh, so they wanted\n",
      "the browser to give you a fault error anytime there was a-- - Yeah. They wanted to\n",
      "be a (indistinct) right? They wanted to-- Yeah. Yeah. That was a very and any properly trained credential engineer would be like, that's not how you build these systems. - That's such a bold move to say, no, it doesn't have to be. - Yeah. No, like I said, the good news for me is\n",
      "the internet kind of had that traditional already,\n",
      "but having said that, like we pushed it, we pushed it way out. But the other thing we did, going back to the performance thing, was we gave up a lot of performance. We made that, that initial experience for the first few years\n",
      "was pretty painful. But the bet there was\n",
      "actually an economic bet, which was basically the demand\n",
      "for the web would basically mean that there would be a\n",
      "surge in supply of broadband. Like because the question was, okay, how do you get the phone\n",
      "companies which are not famous in those days for doing\n",
      "new things at huge cost for like speculative reasons. Like how do you get them to\n",
      "build up broadband, you know, spend billions of dollars\n",
      "doing that and you know, you could go meet with them\n",
      "and try to talk them into it. Or you could just have a thing where it's just very\n",
      "clear that it's gonna be, that people love that's gonna\n",
      "be better if it's faster. And, so that, there was a\n",
      "period there and this was, this was fraught with in peril, but there was a period there\n",
      "where it's like we knew the experience was sub-optimized because we were trying to force the emergence of demand for broadband. - [Lex] Sure. - Which is in fact what happened. - So you had to figure out\n",
      "how to display this text, HTML text. So the blue links and\n",
      "the prop links. What? And there's no standards. Is\n",
      "there standards at that time? - [Marc] No. There really still isn't. - Well there's like standards, there's applied, implied standards. Right. And they, you know, there's all these kind of new features that are being added with like CSS, what, like what kind of stuff a\n",
      "browser should be able to support features within languages,\n",
      "within JavaScript and so on. But you're setting standards\n",
      "on the fly yourself. - Yeah. Well to this day, if you create a webpage\n",
      "that has no CSS style sheet, the browser will render\n",
      "it however it wants to. Right. So this was one of the\n",
      "things, there was this idea, this idea of at the time and\n",
      "how these systems were built, which is separation of content from format or separation of content from appearance. And that's still, people\n",
      "don't really use that anymore 'cause everybody wants to\n",
      "determine how things look and so they use CSS\n",
      "but it's still in there that you can just let the\n",
      "browser do all the work. - I still like the like\n",
      "really basic websites, but that could be just old school, kids these days with their\n",
      "fancy responsive websites that don't actually have much content, but have a lot of visual elements. - Well that's one of the\n",
      "things that's fun about chat, you know, about ChatGPT like. - [Lex] Back to the basics. - Back to just text. - [Lex] Yeah. - Right? And it, you know, there is this pattern in\n",
      "human creativity and media where you end up back at text\n",
      "and I think there's, you know, there's something powerful in there. - Is there some other stuff you remember like the purple links? There were some interesting\n",
      "design decisions that to kind of come up that we have today or we don't have today\n",
      "that were temporary. - So I made the background\n",
      "'cause I hated reading texts on white background, so I\n",
      "made the background gray. Everybody can-- - Do you go ahead to? - No. No, no. That decision I think has been reversed. But now I'm happy though because\n",
      "now dark mode is the thing. - So it wasn't about gray, it was just you didn't\n",
      "want white background. - [Marc] Strain my eyes. - Strain your eyes. Interesting. And then there's a bunch\n",
      "of other decisions. I'm sure there's an interesting\n",
      "history of the development of HTML and CSS and\n",
      "Interface and JavaScript and there's this whole Java applet thing. - Well the big one probably\n",
      "JavaScript, CSS was after me, so I didn't, that was not me. But JavaScript was the big, JavaScript maybe was the\n",
      "biggest of the whole thing. That was us. And that was basically a bet,\n",
      "it was a bet on two things. One is that the world wanted a new front end scripting language. And then the other was I thought at the time the world wanted a new backend scripting language. So JavaScript was designed\n",
      "from the beginning to be both front end and backend. And then it failed as a\n",
      "backend scripting language. And Java won for a long time. And then Python Pearl and\n",
      "other things, PHP and Ruby. But now JavaScript is back. And so. - I wonder if everything in\n",
      "the end will run on JavaScript. - It seems like it is the, and by the way, lemme give a shout out\n",
      "to, to Brendan Eich was basically the one man\n",
      "inventor of of JavaScript. - If you're interested to\n",
      "learn more about Brendan Eich, he's been on his podcast previously. - Exactly. So he wrote\n",
      "JavaScript over a summer and I mean I think it is fair, it is fair to say now that\n",
      "it's the most widely used language in the world and\n",
      "it seems to only be gaining in its in its range of adoption. - You know, in the software world there's quite a few stories of somebody over a week weekend or over a\n",
      "week or over a summer writing some of the most impactful\n",
      "revolutionary pieces of software ever. That\n",
      "should be inspiring. Yes. - Very inspiring. I'll\n",
      "give you another one. SSL. So SSL with the security\n",
      "protocol, that was us. And that was a crazy idea at the time, which was let's take\n",
      "all the native protocols and let's wrap them in a security wrapper. That was a guy named Kip Hickman who wrote that over a summer, one guy. And then look today, sitting here today, like the transformer like at Google was a small handful of people. And then, you know, the number of people who have\n",
      "did like the core work on GPT. It's not that many people, it's a pretty small handful of people. And so yeah, the pattern in software repeatedly over a very long time has been, it's Jeff Bezos always\n",
      "had the two pizza rule for teams at Amazon, which is any team needs\n",
      "to be able to be fed with two pieces. If you need the third pizza,\n",
      "you have too many people. And I think it's actually\n",
      "the one pizza rule. For the really creative work. I think it's two people, three people. - Well that's, you see that with certain open source projects, like so much is done by\n",
      "like one or two people. Like it's so incredible\n",
      "and that's why you see that gives me so much hope\n",
      "about the open source movement in this new age of AI where, you know, just recently having had a conversation with Marc Zuckerberg of all people who's all in on open source, which is so interesting to\n",
      "see and so inspiring to see 'cause like releasing\n",
      "these models, it is scary. It is potentially very dangerous\n",
      "and we'll talk about that. But it's also, if you believe in the\n",
      "goodness of most people and in the skillset of most people and the desire to go do good in the world, that's really exciting. 'cause it's not putting it these models into the centralized\n",
      "control of big corporations, the government and so on. It's putting it in the hands of a teen, teenage kid with like a dream in his eyes. I don't know. That's beautiful. - Look, this stuff, AI ought to make the\n",
      "individual coder obviously far more productive right? By like, you know, a\n",
      "thousand X or something. And so you ought to open source like, not just the future of open source AI, but the future of open source everything. We ought to have a world\n",
      "now of super coders, right? Who are building things as open source with one or two people\n",
      "that were inconceivable, you know, five years ago. You know, the level of\n",
      "kind of hyper productivity we're gonna get out of\n",
      "our best and brightest I think is gonna go way up. - It's gonna be interesting. We'll talk about it, but let's just to linger\n",
      "a little bit on Netscape. Netscape was acquired in\n",
      "1999 for 4.3 billion by AOL. What was that like? What were some memorable aspects of that? - Well that was the height\n",
      "of the.com boom bubble bust. I mean that was the frenzy. If you watch succession, that was like what they\n",
      "did in the fourth season with Gojo and the merger with their, so it was like the height of like one of those kind of dynamics. And so. - Would you recommend\n",
      "succession, by the way? I'm more of a Yellowstone guy. - Yellowstone's very American. I'm very proud of you. That's, that is. - I just talked to Matthew McConaughey and I'm full on Texan at this point. - Good. I approve. - And he'll be doing\n",
      "the SQL to Yellowstone. - [Marc] Yeah, just exciting. - Very exciting. Anyway. - [Marc] Can't wait. - So that's a rude interruption\n",
      "by me by way of succession. So, that was at the height of the-- - Deal making and money\n",
      "and just the fur flying and like craziness. And so yeah, it was just one of those, it was just like, I mean, and this, the entire (indistinct) thing from start to finish was four years, which was like for one of these companies, it's just like incredibly fast. You know, it went, we went public 18 months\n",
      "after we got moved where we were founded, which\n",
      "virtually never happens. So it was just this incredibly fast kind of meteor streaking across the sky. And then of course it was this, and then there was just\n",
      "this explosion, right? That happened 'cause then\n",
      "it was almost immediately followed by the.com crash. It was then followed\n",
      "by AOL, by Time Warner, which again is like the succession guys kinda play with that, which turned out to be a disastrous deal. You know, one of the famous, you know, kind of disastrous in business history. And then, you know, what became an internet depression on\n",
      "the other side of that. But then in that depression\n",
      "in the two thousands was the beginning of broadband and smartphones and Web 2.0 right? And then social media\n",
      "and search and every SaaS and everything that came out of that. - What did you learn from\n",
      "just the acquisition? I mean this is so much money. What's interesting 'cause I\n",
      "must have been very new to you, that these software stuff, you can make so much money. There's so much money swimming around. I mean, I'm sure the\n",
      "ideas of investment was starting to get born there. - Yes. Let me get, so let me lay it. So here's, here's the thing. I dunno if I figured it out\n",
      "then, but figured it out later, which is software is a technology that it, it's like a, you know, the\n",
      "concept of the philosopher stone, the philosopher stone in alchemy, transient is led into gold and\n",
      "Newton spent 20 years trying to find the philosopher stone. Never got there. Nobody's ever figured it out. Software is our modern philosopher stone. And in economic terms, it\n",
      "transmutes labor into capital, which is like a super interesting thing. And by the way, like Carl Marcs is rolling over in his grave right now. 'Cause of course that's\n",
      "complete reputation of his entire theory. Trans labor and capital\n",
      "which is as follows, is somebody sits down at a keyboard and types a bunch of stuff in, and a capital asset\n",
      "comes out the other side and then somebody buys that capital asset for a billion dollars. Like that's amazing, right? It's literally creating\n",
      "value right out of thin air, right out of purely human thought, right? And so that, there are many things that make software magical and special, but that's the economics. - I wonder what Marx\n",
      "would've thought about that? - Oh, he would've\n",
      "completely broke his brain because of course the whole\n",
      "thing was it was he could, you know, that kind of\n",
      "technology was inconceivable when he was alive. It was all industrial era stuff. And so, any kind of machinery\n",
      "necessarily involved huge amounts of capital. And then labor was on the receiving end of the abuse. - [Lex] Yep. Right? But like software eng software, a software engineer is somebody\n",
      "who basically transmutes his own labor into actual, an actual capital asset\n",
      "creates permanent value. Well, and in fact it's\n",
      "actually very inspiring. That's actually more\n",
      "true today than before. So when I was doing software, the assumption was all\n",
      "new software basically has a sort of a parabolic\n",
      "sort of lifecycle, right? So you ship the thing,\n",
      "people buy it at some point, everybody who wants it has bought it and then it becomes obsolete. And it's like bananas. Nobody, nobody buys old software. These days, Minecraft, Mathematica, you know, Facebook, Google, you have the software\n",
      "assets that are, you know, have been around for 30\n",
      "years that are gaining in value every year, right? And they're just, they're being\n",
      "a world of warcraft, right, salesforce.com, like they're being every single year they're\n",
      "being polished and polished and polished and polished. They're getting better\n",
      "and better, more powerful, more powerful, more\n",
      "valuable, more valuable. So we've entered this era\n",
      "where you can actually have these things that actually\n",
      "build out over decades. Which by the way is what's happening right now with like ChatGPT. And so now, this is why, you know, there is always, you know, sort of a constant investment\n",
      "frenzy around software is because, you know, look, when\n",
      "you start one of these things, it doesn't always succeed. But when it does now you\n",
      "might be building an asset that builds value for,\n",
      "you know, four or five, six decades to come. You know, if you have a team of people who have the level of devotion required to keep making it better. And then the fact that of\n",
      "course everybody's online, you know, there's 5 billion people that are a click away from\n",
      "any new piece of software. So the potential market size\n",
      "for any of these things is, you know, nearly infinite. - [Lex] It must have been\n",
      "surreal back then though. - Yeah. Yeah. This was\n",
      "all brand new, right? Yeah. Back then, this was all brand new. These were all, you know, brand new. Had you rolled out that\n",
      "theory in even 1999, people would've thought\n",
      "you were smoking crack. So that's emerged over time. - Well, let's now turn\n",
      "back into the future. You wrote the essay \"Why\n",
      "AI Will Save The World?\" Let's start the very high level. What's the main thesis of the essay? - Yeah, so the main thesis on the essay is that what we're dealing\n",
      "with here is intelligence. And it's really important to kind of talk about the sort of very nature\n",
      "of what intelligence is. And fortunately we have a predecessor to machine intelligence,\n",
      "which is human intelligence. And we've got, you know, observations and theories\n",
      "over thousands of years for what intelligence is\n",
      "in the hands of humans and what intelligence is, right? I mean, what it literally is the way to, you know, capture, process,\n",
      "analyze, synthesize information, solve problems. But the observation of\n",
      "intelligence in human hands is that intelligence quite literally\n",
      "makes everything better. And what I mean by that\n",
      "is every kind of outcome of like human quality of life, whether it's education outcomes or success of your children, or career success or health or lifetime\n",
      "satisfaction, by the way, propensity to peacefulness\n",
      "as opposed to violence, propensity for open-mindedness\n",
      "versus bigotry, those are all associated with\n",
      "higher levels of intelligence. - Smarter people have better outcomes than almost as you write in almost every domain of activity. Academic achievement, job performance, occupational status, income, creativity, physical health, longevity,\n",
      "learning new skills, managing complex tasks, leadership,\n",
      "entrepreneurial success, conflict resolution,\n",
      "reading comprehension, financial decision making, understanding others\n",
      "perspectives, creative arts, parenting outcomes, and life satisfaction. One of the more depressing\n",
      "conversations I've had, and I don't know why it's depressing, I have to really think\n",
      "through why it's depressing, but on IQ and the G factor, and that that's something\n",
      "in large part is genetic and it correlates so much\n",
      "with all of these things and success in life. It's like all the inspirational\n",
      "stuff we read about, like if you work hard and so on, it sucks that you're born with the hand that you can't change. - But what if you could. - You're saying basically\n",
      "a really important point, and I think it's in your\n",
      "articles, it really helped me. It's a nice added\n",
      "perspective to think about. Listen, human intelligence, the science of intelligence\n",
      "is shown scientifically that it just makes life easier and better the smarter you are. And now let's look at\n",
      "artificial intelligence and if that's a way to increase\n",
      "some human intelligence, then it's only going\n",
      "to make a better life. - [Marc] Yeah. - That's the argument. - And certainly at the collective level, we could talk about the collective effect of just having more\n",
      "intelligence in the world, which will have very big payoff. But there's also just\n",
      "at the individual level, like what if every person has a machine? You know? And the concept of augment Doug Engelbart's concept of augmentation. You know, what if\n",
      "everybody has an assistant and the assistant is, you know, 140 IQ and you happen to be 110 IQ and you've got, you know, something that basically is\n",
      "infinitely patient and knows everything about you\n",
      "and is pulling for you in every possible way,\n",
      "wants you to be successful. And anytime you find anything\n",
      "confusing or wanna learn anything or have trouble\n",
      "understanding something or wanna figure out what to\n",
      "do in a situation, right? Wanna figure out how to\n",
      "prepare for a job interview, like any of these things,\n",
      "like it will help you do it. And it will therefore, the combination will\n",
      "effectively be, you know, will effectively raise your raise because it will effectively raise your IQ, will therefore raise the odds of successful life outcomes\n",
      "in all these areas. - So people below the,\n",
      "this hypothetical 140 IQ, it'll pull them up towards 140 IQ. - Yeah, yeah, yeah. And then of course, you know, people at 140 IQ will be\n",
      "able to have a peer, right. To be able to communicate, which is great. And then people above 140\n",
      "IQ will have an assistance that they can farm things out to. And then look, God willing, you know, at some point these things\n",
      "go from future versions go from 140 IQ equivalent to\n",
      "150 to 160 to 180, right? Like Einstein was estimated\n",
      "to be on the order of one 60, you know, so when we\n",
      "get, you know, one 60 AI, like we'll be, you know, when one assumes creating\n",
      "Einstein level breakthroughs and physics, and then at\n",
      "180 we'll be, you know, carrying cancer and developing\n",
      "warp drive and doing all kinds of stuff. And so it is quite possibly the case, this is the most important\n",
      "thing that's ever happened and the best thing that's ever happened because precisely because it's a lever on this single fundamental\n",
      "factor of intelligence, which is the thing that drives\n",
      "so much of everything else. - Can you steal, man, the case that human plus AI is\n",
      "not always better than human for the individual? - You may have noticed that there's a lot of\n",
      "smart running around. - [Lex] Sure. Yes. - Right? And so, like smart, there are certain people where\n",
      "they get smarter, you know, they get to be more arrogant, right? So that, you know, there's one huge flaw. - Although to push back on that, it might be interesting because\n",
      "when the intelligence is not all coming from you,\n",
      "but from another system, that might actually increase\n",
      "the amount of humility even in the assholes. - [Marc] One would hope. - Yeah. - Or it could make assholes more assholes. You know, that's in, I mean, that's for psychology to study. - Yeah, exactly. Another one is smart people\n",
      "are very convinced that they, you know, have a more\n",
      "rational view of the world, and that they have a easier\n",
      "time seeing through conspiracy theories and hoaxes and right. You know, sort of crazy\n",
      "beliefs and all that. There's a theory in psychology, which is actually smart people. So for sure people who aren't\n",
      "as smart are very susceptible to hoaxes and conspiracy theories. But it may also be the case\n",
      "that the smarter you get, you become susceptible in a different way, which is you become very\n",
      "good at marshaling facts to fit preconceptions, right. You become very, very good at assembling whatever theories and\n",
      "frameworks and pieces of data and graphs and\n",
      "charts you need to validate whatever crazy ideas got in your head. And so you're susceptible\n",
      "in a different way, right? - We're all sheep, but\n",
      "different colored sheep. - Some sheep are better\n",
      "at justifying it. Right. And those are the, you know, those are the smart sheep, right? So yeah. Look like I\n",
      "would say this look like there are no panacea. I'm not a utopian, there\n",
      "are no panaceas in life. There are no, like, you know, I don't believe there\n",
      "are like pure positives. I'm not a transcendental\n",
      "kind of person like that. But, you know, so yeah,\n",
      "there are gonna be issues and, you know, look, smart people, another maybe you could\n",
      "save about smart people is they are more likely to get\n",
      "themselves in situations that are, you know, beyond their grasp. You know, because they're\n",
      "just more confident in their ability to deal with complexity and their eyes become bigger, their cognitive eyes become bigger than their stomach, you know? So yeah, you could argue\n",
      "those eight different ways nevertheless, on net, right? Clearly, overwhelmingly, again, if you just extrapolate from what we know about human intelligence, you're improving so many aspects of life if you're upgrading intelligence. - So there'll be assistants\n",
      "at all stages of life. So when you're younger,\n",
      "there's for education, all that kind of stuff for\n",
      "mentorship, all of this. And later on as you're doing\n",
      "work and you've developed a skill and you're having a profession, you'll have an assistant\n",
      "that helps you excel at that profession. So at all stages of life. - Yeah. I mean, look, the\n",
      "theory is augmentation. This is the Doug Engelbart's term. Doug Engelbart made this observation many, many\n",
      "decades ago that, you know, basically it's like you can\n",
      "have this oppositional frame of technology where it's\n",
      "like us versus the machines, but what you really do\n",
      "is you use technology to augment human capabilities. And by the way, that's how actually the economy develops. That's, we can talk about\n",
      "the economic side of this, but that's actually how\n",
      "the economy grows is through technology\n",
      "augmenting human potential. And so, yeah. And then you basically\n",
      "have a proxy or you know, or you know, a sort of\n",
      "prosthetic, you know, so like you've got glasses,\n",
      "you've got a wristwatch, you know, you've got shoes, you know, you've got these things. You've got a personal computer, you've got a word processor,\n",
      "you've got Mathematica, you've got Google. This is the latest\n",
      "viewed through that lens. AI is the latest in a long\n",
      "series of basically augmentation methods to be able to\n",
      "raise human capabilities. It's just this one is the\n",
      "most powerful one of all, because this is the one\n",
      "that, that goes directly to what they call fluid\n",
      "intelligence, which is IQ. - Well, there's two categories of folks that you outline that\n",
      "worry about or highlight the risks of AI, and you highlight a bunch of different risks. I would love to go through those risks and just discuss them, brainstorm which ones are serious and which ones are less serious. But first, the Baptist\n",
      "and the bootleggers, what are these two\n",
      "interesting groups of folks who worry about the effect\n",
      "of AI and human civilization? - [Marc] Or say they do. - Say, oh, okay, yes, I'll say they do. - The Baptist worry the\n",
      "bootleggers say they do. So the Baptist and the\n",
      "bootleggers is a metaphor from economics, from what's\n",
      "called development economics. And it's this observation that when you get social\n",
      "reform movements in a society, you tend to get two sets\n",
      "of people showing up, arguing for the social reform. And the term Baptist and bootleggers comes from the American experience\n",
      "with alcohol prohibition. And so in the 1900s, 1910s, there was this movement\n",
      "that was very passionate at the time, which basically said, alcohol is evil and is destroying society. By the way, there was a lot\n",
      "of evidence to support this. There were very high rates of very high correlations\n",
      "then, by the way. And now between rates of physical\n",
      "violence and alcohol use, almost all violent crimes\n",
      "have either the perpetrator or the victim, or both drunk almost. If you see this actually in the work, almost all sexual harassment\n",
      "cases in the workplace, it's like at a company\n",
      "party and somebody's drunk. Like, it's amazing how often\n",
      "alcohol actually correlates to actually dis dysfunction\n",
      "and at leads to domestic abuse and so forth, child abuse. And so you had this group of\n",
      "people who were like, okay, this is bad stuff and we should outlaw it. And those were quite literally Baptist. Those were super committed, you know, hardcore Christian\n",
      "activists in a lot of cases. There was this woman whose\n",
      "name was Carrie Nation, who was this older woman who\n",
      "had been in this, you know, I don't know, disastrous\n",
      "marriage or something. And her husband had been\n",
      "abusive and drunk all the time. And she became the icon of\n",
      "the Baptist prohibitionist. And she was legendary in\n",
      "that era for carrying an ax and doing, you know, completely on her own\n",
      "doing raids of saloons and like taking her ax to all the bottles and eggs in the back. And so. - [Lex] A true believer. - An absolute true believer, and with absolutely the\n",
      "purist of intentions. And again, there's a very\n",
      "important thing here, which is there's, you could look at this\n",
      "cynically and you could say the Baptists are like delusional,\n",
      "you know, the extremists, but you could also say,\n",
      "look, they're right. Like she was, you know, she had a point. Like she wasn't wrong about\n",
      "a lot of what she said. - Yeah. - But it turns out the way\n",
      "the story goes is it turns out that there were another set of people who very badly wanted to\n",
      "outlaw alcohol in those days. And those were the bootleggers, which was organized crime that\n",
      "stood to make a huge amount of money if legal alcohol\n",
      "sales were banned. And this was, in fact, the way the history goes\n",
      "is this was actually the beginning of\n",
      "organized crime in the US. This was the big economic\n",
      "opportunity that opened that up. And so they went in together and no, they didn't go in together. Like the Baptist did not\n",
      "even necessarily know about the bootleggers 'cause they were on their moral crusade. The bootleggers certainly\n",
      "knew about the Baptists. And they were like, wow, these people are like the\n",
      "great front people for like. You know, it's-- - [Lex] Good PR. - Shenanigans in the background. And they got the (indistinct)\n",
      "Act passed, right. And they did in fact ban alcohol\n",
      "in the US and you'll notice what happened, which is\n",
      "people kept drinking, it didn't work, people kept drinking. That bootleggers made a\n",
      "tremendous amount of money. And then over time it became\n",
      "clear that it made no sense to make it illegal and it\n",
      "was causing more problems. And so then it was revoked. And here we sit with legal\n",
      "alcohol a hundred years later with all the same problems. And you know, the whole thing was this\n",
      "like giant misadventure the Baptist got taken advantage\n",
      "of by the bootleggers, and the bootleggers got what they wanted. And that was that. - The same two categories of folks are now sort of suggesting\n",
      "that the development of artificial intelligence\n",
      "should be regulated. - A hundred percent. It's the same pattern. And the economist will tell you it's the same pattern every time. Like, this is what\n",
      "happened, nuclear power, this is what happens, which\n",
      "is another interesting one. But like, yeah, this\n",
      "happens dozens and dozens of times throughout the\n",
      "last a hundred years and this is what's happening now. - And you write that it isn't\n",
      "sufficient to simply identify the actors and impugn their motives. We should consider the\n",
      "arguments of both the Baptist and the bootleggers on their merits. So let's do just that. Risk number one, will AI kill us all? - [Marc] Yes. - So what do you think about this one? What do you think is\n",
      "the core argument here that the development of\n",
      "AGI perhaps better said, will destroy human civilization? - Well, first of all, you\n",
      "just did a slight of hand 'cause we went from talking about AI to AGI. - Is there a fundamental difference there? - I don't know. What's AGI? - What's AI, what's in intelligence? - Well, I know what AI\n",
      "is machine learning. What's AGI? - I think we don't know\n",
      "what the bottom of the well of machine learning is\n",
      "or what the ceiling is. Because just to call\n",
      "something machine learning or just to call some of the statistics or just to call it math or\n",
      "computation doesn't mean, you know, nuclear\n",
      "weapons are just physics. So to me it's very\n",
      "interesting and surprising how far machine learning has taken. - No, but we knew that\n",
      "nuclear physics would lead to weapons. That's why the scientists\n",
      "of that era were always in some this huge dispute\n",
      "about building the weapons. This is different. AGI is different. - Does machine learning lead, do we know? - We don't know, but this\n",
      "is my point is different. We actually don't know. But, and this is where you, the slide of hand kicks in, right? This is where it goes from\n",
      "being a scientific topic to being a religious topic. And that's why I specifically called out 'cause that's what happens. They do the vocabulary\n",
      "shift and all of a sudden you're talking about something totally. That's not actually real. - Well then maybe you can\n",
      "also, as part of that, define the western\n",
      "tradition of Millennialism. - [Marc] Yes. Into the world apocalypse. - [Lex] What is it? - [Marc] Apocalypse cults. - [Lex] Apocalypse cults. - Well, so we live in, we of course live in a Judeo-Christian, but primarily Christian\n",
      "kind of saturated, you know, kind of Christian, post-Christian,\n",
      "secularized Christian, you know, kind of world in the west. And of course court of Christianity is the idea of the second\n",
      "coming and you know, the revelations and you know, Jesus returning and the\n",
      "thousand year, you know, utopia on earth and then you know, the rapture and like all\n",
      "all that stuff, you know, you know, we collectively,\n",
      "you know, as a society, we don't necessarily take\n",
      "all that fully seriously now. So, what we do is we create our\n",
      "secularized versions of that we keep looking for utopia. We keep looking for, you know, basically the end of the world. And so what what you see over, over decades is that basically a pattern of these sort of these of\n",
      "is this is what cults are. This is how cults form as\n",
      "they form around some theory of the end of the world. And so the people's temple cults, the Manson cult, the Heavens Gate cult, the David Qresh cult, you know what they're all\n",
      "organized around is like, there's gonna be this\n",
      "thing that's gonna happen that's gonna basically bring\n",
      "civilization crashing down. And then we have this\n",
      "special elite group of people who are gonna see it\n",
      "coming and prepare for it. And then they're the people\n",
      "who are either going to stop it or are failing, stopping it. They're gonna be the people\n",
      "who survived the other side and ultimately get credit\n",
      "for having been, right. - Why is that so compelling,\n",
      "do you think? Like-- - Because it satisfies this very deep need we have for transcendence and meaning that got stripped\n",
      "away when we became secular. - Yeah, but why is the\n",
      "transcendence involve the destruction of human civilization? - Because like how plausible it's like a very deep psychological thing 'cause it's like how plausible, how plausible is it\n",
      "that we live in a world where everything's just\n",
      "kind of all right? Right. How exciting? - [Lex] Whoa. - How exciting is that? Right? - [Lex] But that's. - We got more than that. - But that's the deep question I'm asking. Why is it not exciting to live in a world where everything's just all right? Is it, I think, you know, most of the animal kingdom would be so happy with just all right. Because that means survival. Why are we, maybe that's what it is. Why are we conjuring up\n",
      "things to worry about? - So CS Lewis called\n",
      "it the God-shaped hole. So there's a God-shaped hole\n",
      "in the human experience, consciousness, soul,\n",
      "whatever you wanna call it, where there's gotta be\n",
      "something that's bigger than all this. There's gotta be something transcendent. There's gotta be something\n",
      "that is bigger, right? Bigger purpose. A bigger meaning. And so we have run the\n",
      "experiment of, you know, we're just gonna use\n",
      "science and rationality and kind of, you know, everything's just gonna\n",
      "kind of be as it appears. And large number of people have found that very deeply wanting and\n",
      "have constructed narratives. And by this is the story\n",
      "of the 20th century, right? Communism, right? Was one of those, communism\n",
      "was a was a form of this, Nazism was a form of this. You know, some people, you know, you can see movements\n",
      "like this playing out all over the world right now. - So you constructed a kind of devil, a kind of source of evil, and we're going to transcend beyond it. - Yeah. And (indistinct)\n",
      "when you see a Miller cult, they put a really specific point on it, which is end of the world, right, there is some change coming. And that change that's\n",
      "coming is so profound and so important that\n",
      "it's either gonna lead to utopia or hell on earth. Right? And it is going to, and then, you know, it's like what if you actually knew that was going to happen, right? What would you do? Right? How would you prepare yourself for it? How would you come together with a group of like-minded people, right? How would you, what would you do? Would you plan like Cassius\n",
      "of weapons in the woods? Would you like, you know, I don't know if create\n",
      "underground buckers, would you, you know, spend your\n",
      "life trying to figure out a way to avoid having it happen? - Yeah. That's a really\n",
      "compelling, exciting idea to have a club over. To have a little bit of travel, like a get\n",
      "    ------------\n",
      "    Given the new context, refine the summary and example questions.\n",
      "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
      "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
      "    If the context isn't useful, return the original summary and questions.\n",
      "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
      "\n",
      "    SUMMARY AND QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert in summarizing YouTube videos.\n",
      "    You're goal is to create a summary of a podcast.\n",
      "    We have provided an existing summary up to a certain point: I'm not able to help with that, as I'm only a language model. If you believe this is an error, please send us your feedback.\n",
      "    We have the opportunity to refine the summary\n",
      "    (only if needed) with some more context below.\n",
      "    Below you find the transcript of a podcast:\n",
      "    ------------\n",
      "     now. - So you constructed a kind of devil, a kind of source of evil, and we're going to transcend beyond it. - Yeah. And (indistinct)\n",
      "when you see a Miller cult, they put a really specific point on it, which is end of the world, right, there is some change coming. And that change that's\n",
      "coming is so profound and so important that\n",
      "it's either gonna lead to utopia or hell on earth. Right? And it is going to, and then, you know, it's like what if you actually knew that was going to happen, right? What would you do? Right? How would you prepare yourself for it? How would you come together with a group of like-minded people, right? How would you, what would you do? Would you plan like Cassius\n",
      "of weapons in the woods? Would you like, you know, I don't know if create\n",
      "underground buckers, would you, you know, spend your\n",
      "life trying to figure out a way to avoid having it happen? - Yeah. That's a really\n",
      "compelling, exciting idea to have a club over. To have a little bit of travel, like a get together on a Saturday\n",
      "night and drink some beers and talk about the end of the world and how you are the only\n",
      "ones who have figured it out. - Yeah. And then once you lock in on that, like how can you do anything\n",
      "else with your life? Like this is obviously the\n",
      "thing that you have to do. And then there's a psychological\n",
      "effect that you alluded to. There's a psychological effect. If you take a set of true\n",
      "believers and you leave them to themselves, they get\n",
      "more radical. Right. 'Cause they self radicalize each other. - That said, it doesn't mean they're not sometimes right. - Yeah. The end of the world might be. Yes. Correct. Like they might be right. - [Lex] Yeah. - But like-- - [Lex] I have some pamphlets for you. - Exactly. - But I mean we'll talk\n",
      "about nuclear weapons 'cause you have a really\n",
      "interesting little moment that I learned about in\n",
      "your essay, but you know, sometimes it could be right. - [Marc] Yeah. - 'Cause we're still, you were developing more and\n",
      "more powerful technologies in this case, and we don't know what the impact it will\n",
      "have on human civilization while we can highlight all\n",
      "the different predictions about how it'll be positive, but the risks are there and\n",
      "you discuss some of them. - Well, the steel man, the\n",
      "steel man is the steel man. Well actually, the steel\n",
      "man and his reputation are the same, which is you can't predict what's gonna happen. Right. You can't rule out that this\n",
      "will not end everything. Right. But the response to that\n",
      "is you have just made a completely non-scientific claim. You've made a religious\n",
      "claim, not a scientific claim. - How does it get disproven? - And there's no, by definition with these kinds of claims, there's no way to disprove them. Right? And so there there's no, you\n",
      "just go right on the list. There's no hypothesis, there's no testability of the hypothesis. There's no way to falsify the hypothesis, there's no way to measure\n",
      "progress along the arc. Like it's just all completely missing. And so it's not scientific and. - I don't think it's completely missing. It's somewhat missing. So for example, the people that say AI's gonna kill all of us. I mean, they usually have\n",
      "ideas about how to do that. Whether it's the people\n",
      "club maximizer or, you know, it escapes there's mechanism\n",
      "by which you can imagine it killing all humans. - [Marc] Models. - And you can disprove it by saying there's a limit to the speed at which intelligence increases. Maybe show that like the sort\n",
      "of rigorously really described model, like how it could\n",
      "happen and say, no, there, here's a physics limitation. There's like a physical\n",
      "limitation to how these systems would actually do damage\n",
      "to human civilization. And it is possible they\n",
      "will kill 10 to 20% of the population, but it seems impossible\n",
      "for them to kill 99%. - It was practical\n",
      "counterarguments. Right. So you mentioned\n",
      "basically what I described as the thermodynamic counterargument, which, so sitting here today, it's like where with the\n",
      "evil AGI get the GPU. 'Cause like they don't exist. So if you're gonna have a\n",
      "very frustrated baby evil AGI, who's gonna be like trying to\n",
      "buy Nvidia stock or something to get them to finally\n",
      "make some chips, right? So the serious form of that\n",
      "is the thermodynamic argument, which is like, okay, where's\n",
      "the energy gonna come from? Where's the processor gonna be running? Where's the data center\n",
      "gonna be happening? How is this gonna be\n",
      "happening in secret such that, you know, it's not, you know, so that's a practical counter argument to the runaway AGI thing. I have a but I have and we\n",
      "can argue that, discuss that. I have a deeper objection to it, which is it's, this is all forecasting. It's all modeling, it's\n",
      "all future prediction. It's all future hypothesizing. It's not science. - [Lex] Sure. - It is not. It is the\n",
      "opposite of science. So the, I'll pull up Carl Sagan\n",
      "extraordinary claims require extraordinary proof, right? These are extraordinary claims. The policies that are being\n",
      "called for right to prevent this are of extraordinary magnitude that, and I think we're gonna\n",
      "cause extraordinary damage. And this is all being done\n",
      "on the basis of something that is literally not scientific. It's not a testable hypothesis. - So the moment you say\n",
      "AI's gonna kill all of us, therefore we should ban it, or that we should regulate\n",
      "all that kind of stuff, that's when it starts getting serious. - Or start, you know, military\n",
      "airstrikes and data centers. - [Lex] Oh boy. - Right? And like. - Yeah. This once get starts. Well, so starts getting real weird. - So here's the problem with Arian cults. They have a hard time\n",
      "staying away from violence. - Yeah. But violence is so fun. - If you're on the right end of it, they have a hard time avoiding violence. The reason they have a hard\n",
      "time avoiding violence is if you actually believe the claim. Right. Then what would you do to\n",
      "stop the end of the world? Well, you would do anything, right? And so, and this is\n",
      "where you get, and again, if you just look at the\n",
      "history of Arian and cults, this is where you get the\n",
      "people's temple and everybody killing themselves in the jungle. And this is where you get\n",
      "Charles Manson and, you know, sending in to kill the pigs. Like, this is the problem with these. They have a very hard time to run the line at actual violence. And I think in this case, I\n",
      "mean, they're already calling for it like today and you know, where this goes from here\n",
      "is they get more worked up. Like I think is like really concerning. - Okay. But that's kind of the extremes. So, you know, the extremes of\n",
      "anything are I was concerning. It's also possible to kind\n",
      "of believe that AI has a very high likelihood\n",
      "of killing all of us. But and therefore we should maybe consider slowing development or regulating, so not violence or any\n",
      "of these kinds of things. But it's saying like, all right, let's take a pause here. You know, you biological\n",
      "weapons, nuclear weapons. Like whoa, whoa, whoa, whoa, whoa. This is like serious stuff.\n",
      "We should be careful. So it is possible to kinda have a more rational response, right? If you believe this risk is real. - [Marc] Believe. - Yes. So what is it possible to be, have a scientific approach to\n",
      "the prediction of the future? - I mean, we just went\n",
      "through this with COVID. What do we know about modeling? - [Lex] Well, I mean. - What did we learn about\n",
      "modeling with COVID? - [Lex] There's a lot of lessons. - They didn't work at all. - [Lex] They worked poorly. - The models were terrible,\n",
      "the models were useless. - I don't know if the models\n",
      "were useless or the people interpreting the models and\n",
      "then decentralized institutions that were creating policy\n",
      "rapidly based on the models and leveraging the models in order to support their narratives versus actually\n",
      "interpreting the error bars and the models and all that kind of stuff. - What you had with COVID, my view you had with COVID\n",
      "is you had these experts showing up and they\n",
      "claimed to be scientists and they had no testable\n",
      "hypotheses whatsoever. They had a bunch of models. They had a bunch of forecasts\n",
      "and they had a bunch of theories and they laid these out in front of policy makers and policy makers freaked\n",
      "out and panicked. Right. And implemented a whole bunch of like, really like terrible decisions\n",
      "that we're still living with the consequences of, and there was never any\n",
      "empirical foundation to any of the models. None of them ever came true. - Yeah. To push back. There were certainly\n",
      "Baptist and bootleggers in the context of this pandemic, but there's still a\n",
      "usefulness to models. No. - So not if they're, I mean not if they're\n",
      "reliably wrong, right? Then they're actually\n",
      "like anti-useful. Right. They're actually damaging. - But what do you do with the pandemic? What do you do with any kind of threat? Don't you want to kind of\n",
      "have several models to play with as part of this discussion of like, what the hell do we do here? - I mean, do they work? Because they're an expectation\n",
      "that they actually like work that they have actual predictive value. I mean, as far as I can tell with COVID, the policymakers just si up\n",
      "themselves into believing that there was sub, I\n",
      "mean, look, the scientists, the scientists were at fault. The quote unquote scientists showed up. So I had some insight into this. So there was a, or remember the Imperial College models out of London were the\n",
      "ones that were like, these are the gold standard models. So a friend of mine runs\n",
      "a big software company and he was like, wow, this is\n",
      "like, COVID is really scary. And he is like, you know, he contacted this research\n",
      "and he is like, you know, do you need some help? You've been just building\n",
      "this model on your own for 20 years. Do you need some, would you like us our coders\n",
      "to basically restructure it so it can be fully adapted for COVID? And the guy said yes\n",
      "and sent over the code and my friend said it was\n",
      "like the worst spaghetti code he's ever seen. - That doesn't mean it's\n",
      "not possible to construct a good model of pandemic\n",
      "with the correct air bars, with a high number of parameters\n",
      "that are continuously, many times a day updated\n",
      "as we get more data about a pandemic. I would like to believe when\n",
      "a pandemic hits the world, the best computer scientists in the world, the best software engineers\n",
      "respond aggressively and as input take the data\n",
      "that we know about the virus and it's an output say\n",
      "here is what's happening in terms of how quickly it's spreading, what that lead in terms of\n",
      "hospitalization and deaths and all that kind of stuff. Here's how likely, how\n",
      "contagious it likely is. Here's how deadly it likely is based on different conditions, based on different ages and demographics and all that kind of stuff. So here's the best kinds of policy. It feels like you could have models, machine learning that like kind of, they don't perfectly predict the future, but they help you do something 'cause there's pandemics\n",
      "that are like, meh, they don't really do much harm. And there's pandemics,\n",
      "you can imagine them, they could do a huge amount of harm. Like they can kill a lot of people. So you should probably have\n",
      "some kind of data-driven models that keep updating, that allow you to make\n",
      "decisions that based like where, how bad is this thing? Now you can criticize how\n",
      "horrible all that went with the response to this pandemic, but I just feel like there\n",
      "might be some value to models. - So to be useful at some point it has to be predictive. Right? So and the easy thing\n",
      "for me to do is to say, obviously you're right. Obviously I wanna see that\n",
      "just as much as you do. 'cause anything that makes\n",
      "it easier to navigate through society through a\n",
      "wrenching, you know, risk like that sounds great. You know, the harder objection to it is just simply\n",
      "you are trying to model a complex dynamic system\n",
      "with 8 billion moving parts. Like not possible. - [Lex] It's very tough. - Can't be done, complex\n",
      "systems can't be done. - Machine learning says hold my beer. But well, it's possible. No? - I don't know. I would like to believe that it is. I'll put it this way. I think where you and I\n",
      "would agree is I think we would like that to be the case. We are strongly in favor of it. I think we would also\n",
      "agree that no such thing with respect to COVID or\n",
      "pandemics no such thing. At least neither you\n",
      "nor I think are aware. I'm not aware of anything like that today. - My main worry with the\n",
      "response to the pandemic is that same as with aliens, is that even if such a thing existed, and it's possible it existed, the policymakers were\n",
      "not paying attention. Like there was no mechanism\n",
      "that allowed those kinds of models to percolate all. - Oh, I think we had the\n",
      "opposite problem during COVID. I think the policymakers, I think these people with\n",
      "basically fixed science had too much access to the policymakers. - Well, right. And well, but the policy\n",
      "makers also wanted, they had a narrative in\n",
      "mind and they also wanted to use whatever model\n",
      "that fit that narrative - [Marc] Oh, sure. - To help them out. So like, it felt like\n",
      "there was a lot of politics and not enough science. - Although a big part\n",
      "of what was happening, a big reason we got lockdowns\n",
      "for as long as we did, was because these scientists\n",
      "came in with these like doomsday scenarios that were like, just like completely off the hook. - Scientists in quotes, let's not-- - [Marc] Quote unquote scientists. - Let's not, okay,\n",
      "let's give love science. So here's science that is the way out. - Science is a process\n",
      "of testing hypotheses. Modeling does not involve\n",
      "testable hypotheses. Right. Like, I don't even know that. I actually don't even know that modeling actually\n",
      "qualifies as science. Maybe that's a side conversation. We could have some time over a beer. - Oh, that's a really interesting part. What do we do about the future? I mean, what's-- - So number one is when\n",
      "we start with number one, humility goes back to this thing of how do we determine the truth. Number two is we don't believe, you know, it's the old, I've gotta hammer everything\n",
      "looks like a nail, right? I've got, oh, this is one\n",
      "of the reasons I gave you, I gave Alexa book, which the topic of the\n",
      "book is what happens when scientists basically\n",
      "stray off the path of technical knowledge and\n",
      "start to weigh in on politics and societal issues. - In this case, philosophers. - Well in this case philosophers. But he actually talks in this\n",
      "book about, like Einstein, he talks about, actually about\n",
      "the nuclear age in Einstein. He talks about the\n",
      "physicists actually doing very similar things at the time. - The book is When Reason Goes On Holiday, Philosophers in Politics by Nevin. - And it's just a story. It's a story. There are other books on this topic, but this is a new one that's really good this is just a story of what happens when experts\n",
      "in a certain domain decide to weigh in and become\n",
      "basically social engineers and political, you know,\n",
      "basically political advisors. And it's just a story of just\n",
      "inning catastrophe. Right. And I think that's what\n",
      "happened with COVID again. - Yeah. I found this book\n",
      "a highly entertaining and eye-opening read filled\n",
      "with amazing anecdote of a rationality and craziness\n",
      "by famous Resa philosophers. - I definitely, after you read this book, you will not look at Einstein the same. - [Lex] Oh boy. - Yeah. - Don't destroy my heroes. - He will not be a hero of yours anymore. Sorry. You probably couldn't,\n",
      "you shouldn't read the book. - All right. - But here's the thing. The AI risk people, they don't even have the COVID model, at least not that I'm aware of. - [Lex] No. - Like there's not even the\n",
      "equivalent of the COVID model. They don't even have the spaghetti code. They've got a theory and a\n",
      "warning and a this and the that. And like, if you ask like,\n",
      "okay, well here's, I mean, the ultimate example is,\n",
      "okay, how do we know, right? How do we know that an AI is running away? Like how do we know that the boom takeoff thing\n",
      "is actually happening? And the only answer that\n",
      "any of these guys have given that I've ever seen is, oh,\n",
      "it's when the loss rate, the loss function and the\n",
      "training drops, right? That's when you need to like\n",
      "shut down the data center. Right? And it's like, well that's also what happens when you're successfully training a model. Like, what even this is not science, this is not, it's not\n",
      "anything, it's not a model, it's not anything. There's nothing to arguing with. It is like, you know,\n",
      "punching jello, like there, there's what do you even respond to? - So just put push back on that. I don't think they have good metrics of when the film is happening. But I think it's possible to have that. Like just as you speak now, I mean it's possible to imagine\n",
      "there could be measures. - It's been 20 years. - No, for sure. But it is been only weeks\n",
      "since we had a big enough breakthrough in language models. We can start to actually have this, the thing is the AI doer stuff didn't have any actual systems to really work with. And now there's real systems\n",
      "you can start to analyze like, how does this stuff go wrong? And I think you kind\n",
      "of agree that there is a lot of risks that we can analyze. The benefits outweigh\n",
      "the risks in many cases. - Well, the risks are not existential. - [Lex] Yes. Well. - Not in the phone paper\n",
      "clip. Let me, okay. There's another slide of hand\n",
      "that you just alluded to. There's another slide\n",
      "of hand that happens, which is very interesting. - I'm very good at the\n",
      "slide of hand thing. - Which is very not scientific. So the book Super Intelligence, right, which is like the Nick Bostrom's book, which is like the origin\n",
      "of a lot of this stuff, which was written, you know, whatever, 10 years ago or something. So he does this really\n",
      "fascinating thing in the book, which is he basically says\n",
      "there are many possible routes to machine intelligence,\n",
      "to artificial intelligence. And he describes all the different routes to artificial intelligence,\n",
      "all the different possible, everything from biological\n",
      "augmentation through to, you know, all these different things. One of the ones that\n",
      "he does not describe is large language models because of course the book was written\n",
      "before they were invented. And so they didn't exist. In the book, he describes them all and then he proceeds to treat them all as if they're\n",
      "exactly the same thing. He presents them all as sort\n",
      "of an equivalent risk to be dealt with in an equivalent\n",
      "way to be thought about the same way. And then the risk, the quote unquote risk that's actually emerged is actually a completely different technology than he was even imagining. And yet all of his theories\n",
      "and beliefs are being transplanted by this movement, like straight onto this new technology. And so again, like there's no other area of science or technology\n",
      "where you do that. Like when you're dealing\n",
      "with like organic chemistry versus inorganic chemistry,\n",
      "you don't just like say, oh, with respect to like either\n",
      "one, basically maybe, you know, growing up in eating\n",
      "the world or something, like they're just gonna\n",
      "operate the same way. Like you don't. - But you can start talking about like, as we get more and more actual systems that start to get more\n",
      "and more intelligent, you can start to actually have more scientific arguments here. - [Marc] Oh yeah. - Like, you know, high level, you can talk about the threat\n",
      "of autonomous weapon systems back before we had any\n",
      "automation in the military. And that would be like\n",
      "very fuzzy kind of logic. But the more and more you\n",
      "have drones that are becoming more and more autonomous, you\n",
      "can start imagining, okay, what does that actually look\n",
      "like and what's the actual threat of autonomous weapons systems? How does it go wrong? And still it's very vague, but you start to get a\n",
      "sense of like, all right, it should probably be illegal or wrong or not allowed\n",
      "to do like mass deployment of fully autonomous drones that are doing aerial strikes. - [Marc] Oh no. - On large areas. - [Marc] I think it should be required. - Right? So that's a no. - No, no. I think it should be required that only aerial vehicles are automated. - Okay. So you wanna go the other way? - I wanna go the other way. - So that, okay. - I think it's obvious that\n",
      "the machine is gonna make a better decision than the human pilot. I think it's obvious that\n",
      "it's in the best interest of both the attacker and the\n",
      "defender and humanity at large. If machines are making\n",
      "more of these decisions than not people, I think people make terrible\n",
      "decisions in times of war. - But like, there's ways\n",
      "this can go wrong too, right? - Well, it wars go terribly wrong now. This goes back to the whole, this is that whole thing\n",
      "about like the self-drive. Does the self-driving\n",
      "car need to be perfect versus does it need to be\n",
      "better than the human driver? Does the automated\n",
      "drone need to be perfect or does it need to be better than a human pilot at making decisions under enormous amounts of\n",
      "stress and uncertainty? - Yeah, well, on average, the worry that AI folks\n",
      "have is the runaway. - They're gonna come alive. Right? That then again, that's\n",
      "the slight of hand, right. - Or not not come alive.\n",
      "Well, no, hold on a second. You lose control as well. You lose control. - But then they're gonna\n",
      "develop goals of their own. They're gonna develop a mind of their own, they're gonna develop their own. Right. - No more, more like\n",
      "Chernobyl style meltdown, like just bugs in the code\n",
      "accidentally, you know, force you like the results in the bombing of like large civilian areas. - [Marc] Okay. And to a degree that's not possible in the current military strategies, - [Marc] I don't know. - Control by humans. - Well, actually we've been\n",
      "doing a lot of mass bombings to cities for a very long time. - Yes. And a lot of civilians died. - And a lot of civilians died. And if you watch the documentary,\n",
      "the Fog of War McNamara, it spends a big part of it\n",
      "talking about the fire bombing of the Japanese cities. Burning them straight\n",
      "to the ground. Right. The devastation in Japan, American military fire bombing\n",
      "the cities in Japan was considerably bigger devastation\n",
      "than the use of nukes. Right. So we've been doing\n",
      "that for a long time. We also did that to Germany, by the way Germany did that to us, right? Like that's an old tradition. The minute we got airplanes, we started doing indiscriminate bombing. - So one of the things-- - [Marc] We're still doing it. - The modern US military can do with technology with automation, but technology more broadly is higher and higher precision strikes. - Yeah, I was saying, so precision is obviously precision and this is a (indistinct) right? So there was this big advance this big advance called the (indistinct) which basically was\n",
      "strapping a GPS transceiver to an unguided bomb and turning it into a guided bomb. And yeah, that's great. Like look, that's been a big advance, but, and that's like a baby\n",
      "version of this question, which is okay, do you\n",
      "want like the human pilot, like guessing where the bomb's gonna land? Or do you want like the\n",
      "machine like guiding the bomb to his destination? That's a baby version of the question. The next version of the question is, do you want the human\n",
      "or the machine deciding whether to drop the bomb? Everybody just assumes the\n",
      "human's gonna do a better job for what I think are\n",
      "fundamentally suspicious reasons. - Emotional, psychological reasons. - Yeah. I think it's very clear\n",
      "that the machine's gonna do a better job making that decision 'cause the humans making\n",
      "that decision are got awful. Just terrible. - [Lex] Yeah. - Right. And so yeah. So this is the thing. And then let's get to the, there was, can I one more slide of hand? - [Lex] Yes. - It was in-- - Sure. Please. I'm a magician. You could say. - One more slight of hand. These things are gonna be so smart, right? That they're gonna be able to\n",
      "destroy the world and wreak havoc and like do all this\n",
      "stuff and plan and do all this stuff and evade us and have\n",
      "all their secret things and their secret factories\n",
      "and all this stuff. But they're so stupid that\n",
      "they're gonna get like, tangled up in their code and that's they're not gonna come alive, but there's gonna be some\n",
      "bug that's gonna cause them to like turn us all on a paper like that. They're not gonna be\n",
      "genius in every way other than the actual bad goal. And it's just like,\n",
      "and that's just like a, like ridiculous like discrepancy. And you can prove this today, you can actually address\n",
      "this today for the first time with LLMs which is you can actually ask LLMs to resolve moral dilemmas. So you can create the\n",
      "scenario, you know, dot, dot, dot this, that, this, that, this, that. What would you as the AI\n",
      "do in the circumstance? And they don't just\n",
      "say destroy all humans, destroy all humans. They will give you actually\n",
      "very nuanced moral, practical trade-off oriented answers. And so we actually already\n",
      "have the kind of AI that can actually like, think this through and can actually like, you know, reason about goals. - Well, the hope is that AGI or like various superintelligent systems have some of the\n",
      "nuance that LLMs have and the intuition is they most likely will because even these LLMs have the nuance. - LLMs are really, this is\n",
      "actually worth spending a moment on LLMs are really interesting to have moral conversations with. And that I just, I didn't expect I'd be\n",
      "having a moral conversation with the machine in my lifetime. - Wait, and let's remember\n",
      "we're not really having a conversation with the machine where we're having a conversation with the entirety of the\n",
      "collective intelligence of the human species. - Exactly. Yes. Correct. - But it's possible to imagine\n",
      "autonomous weapons systems that are not using LLMs. - But if they're smart enough to be scary, where are they not\n",
      "smart enough to be wise? Like, that's the part where it's like, I don't know how you get\n",
      "the one without the other. - Is it possible to be super intelligent without being super wise? - Well, again, you're back to that. I mean, then you're back to\n",
      "a classic autistic computer, right? Like you're back to just\n",
      "like a blind rule follower. I've got this like core,\n",
      "it's the paperclip thing. I've got this core rule and\n",
      "I'm just gonna follow it to the end of the earth. And it's like, well, but everything you're gonna\n",
      "be doing execute that rule is gonna be super genius level\n",
      "that humans aren't gonna be able to counter. It's a mismatch in the definition of what the system's capable of. - Unlikely but not impossible, I think. - But again, here you\n",
      "get to like, okay, like. - No, I'm not saying when it's\n",
      "unlikely but not impossible. If it's unlikely, that means the fear should be correctly calibrated. - Extraordinary claims\n",
      "require extraordinary proof. - Well, okay, so one\n",
      "interesting sort of tangent, I would love to take on this\n",
      "because you mentioned this in the essay about nuclear,\n",
      "which was also, I mean, you don't shy away from a\n",
      "little bit of of a spicy take. So Robert Oppenheimer famously said, now I am become death\n",
      "the destroyer of worlds as he witnessed the first\n",
      "destination of a nuclear weapon on July 16th, 1945. And you write an interesting\n",
      "historical perspective, \"Recall that John Van Neuman responded to Robert Oppenheimer's famous\n",
      "hand wringing about the role of creating nuclear weapons, which you note helped end World War II and prevent World War III\n",
      "with some people confess guilt to claim credit for the sin.\" And you also mentioned\n",
      "that Truman was harsher after meeting Oppenheimer. He said that \"Don't let that\n",
      "cry baby in here again.\" - Real quote, by the\n",
      "way, from Dean Atchison. - Boy. - 'Cause Oppenheimer didn't\n",
      "just say the famous line. - [Lex] Yeah. - He then spent years going\n",
      "around basically moaning him, you know, going on TV and going into going into the White House and basically like, just like doing this hair shirt, you know, thing self, you know, this\n",
      "sort of self-critical like, oh my god, I can't believe how awful I am. - So he's widely\n",
      "considered perhaps of the, because of the hang ringing\n",
      "as the father of the tom bomb. - [Marc] Yeah. - This is Van Norman's criticism\n",
      "of him is he tried to have his cake and eat it too. Like he wanted to and Van Norman of course a very different kind of personality and\n",
      "he's just like, yeah, good. This is like an incredibly\n",
      "useful thing. I'm glad we did it. - Yeah. Well Van Norman is\n",
      "is widely credit as being one of the smartest humans\n",
      "of the 20th century. Certain people. Everybody says like, this is the smartest person I've ever met when they've met him. Anyway, that doesn't mean,\n",
      "smart doesn't mean wise. So yeah, I would love to sort of, can you make the case both\n",
      "for and against the critique of Oppenheimer here? 'Cause we're talking\n",
      "about nuclear weapons. Boy, do they seem dangerous? - Well so, the critique goes deeper and I left this out. Here's the real substance, I left it out 'cause I didn't wanna dwell on nukes in my AI paper. But here's the deeper thing that happened and I'm really curious, this\n",
      "movie coming out this summer, I'm really curious to see\n",
      "how far he pushes this. 'cause this is the real\n",
      "drama in the story, which is, it wasn't just a question\n",
      "of our nukes, good or bad, it was a question of should\n",
      "Russia also have them? And what actually happened was Russia got the American invented the bomb. Russia got the bomb, they got the bomb through espionage, they got American and you know, they got American scientists\n",
      "and foreign scientists working on the American project. Some combination of the two\n",
      "basically gave the Russians the designs for the bomb. And that's how the Russians got the bomb. There's this dispute to this\n",
      "day of Oppenheimer's role in that if you read all the histories, the kind of composite\n",
      "picture, and by the way, we now know a lot actually\n",
      "about Soviet espionage in that era 'cause there's been all this declassified\n",
      "material in the last 20 years that actually shows a lot\n",
      "of very interesting things. But if you kinda read all the histories, which you kinda get is Oppenheimer\n",
      "himself probably was not he probably did not hand over\n",
      "the nuclear secrets himself. However, he was close\n",
      "to many people who did. Including family members. And there were other members\n",
      "of the Manhattan Project who were Russian, Soviet SS\n",
      "and did hand over the bomb. And so the view of that\n",
      "Oppenheimer and people like him had that this thing is awful\n",
      "and terrible and oh my god. And you know, all this stuff you could\n",
      "argue fed into this ethos at the time that resulted\n",
      "in people thinking that the Baptists thinking that the only principle\n",
      "thing to do was to give the Russians the bomb. And so the moral beliefs on this thing and the public discussion and the role that the inventors\n",
      "of this technology play, this is the point of this book, when they kind of take on this\n",
      "sort of public intellectual, moral kind of thing, it can\n",
      "have real consequences, right? Because we live in a very\n",
      "different world today because Russia got the\n",
      "bomb than we would've lived in had they not gotten the bomb right. The entire 20th century, second half of the 20th\n",
      "century would've played out very different had those people\n",
      "not given Russia the bomb. And so the stakes were very high then. The good news today is\n",
      "nobody's sitting here today, I don't think worrying about\n",
      "like an analogous situation with respect to like, I'm not really worried that\n",
      "Sam Altman's gonna decide to give, you know, the\n",
      "Chinese, the design for AI, although he did just speak\n",
      "at a Chinese conference, which is in interesting. But however, I don't think\n",
      "that's what's at play here, but what's at play here are\n",
      "all these other fundamental issues around what do\n",
      "we believe about this and then what laws and\n",
      "regulations and restrictions that we're gonna put on it. And that's where I draw\n",
      "like a direct straight line. And anyway, and my reading\n",
      "of the history on nukes is like the people who were doing\n",
      "the full hair shirt public, this is awful. This is terrible. Actually had like\n",
      "catastrophically bad results from taking those views. And that's what I'm worried\n",
      "it's gonna happen again. - But is there a case to be\n",
      "made that you really need to wake the public up to the dangers of nuclear weapons when\n",
      "they were first dropped? Like really like educate them on like, this is extremely dangerous\n",
      "and destructive weapon. - I think the education\n",
      "kind of happened quick and early, like-- - [Lex] How? - It was pretty obvious. - [Lex] How? - We dropped one bomb and\n",
      "destroyed an entire city. - Yeah. So 80,000 people dead. - [Marc] Yep. - But. - [Marc] And look. But-- - I don't like the reporting of that. You can report that in all kinds of ways. - [Marc] Oh, there wars. - You can do all kinds of slants. Like war is horrible. War is terrible. You can do, you can make\n",
      "it seem like nuclear, the use of nuclear weapons\n",
      "is just a part of war and all that kind of stuff. Something about the\n",
      "reporting and the discussion of nuclear weapons resulted\n",
      "in us being terrified in awe of the power of nuclear weapons and that potentially fed\n",
      "in a positive way towards the game theory of\n",
      "mutual issue destruction. - Well, so this gets to what actually, let's get to what actually happens. - [Lex] Some of us, me\n",
      "playing devil's advocate here. - Yeah, yeah, sure. Of course. Let's get to what\n",
      "actually happened and then kind of back into that. So what actually happened, I believe, and again I think this is a\n",
      "reasonable reading of history, is what actually happened\n",
      "was nukes then prevented World War III and they\n",
      "prevented World War III through the game theory\n",
      "of mutually assured destruction had nukes not existed. Right. There would've been no reason why the Cold War did not go hot. Right. And then there and then, you know, and the military planners\n",
      "at the time, right, thought both on both sides\n",
      "thought that there was gonna be World War III on the planes of Europe and they thought there was gonna be like a hundred million people dead. Right? It was like the most obvious\n",
      "thing in the world to happen. Right? And it's the dog\n",
      "that didn't bark right? Like it may be like the best\n",
      "single net thing that happened in the entire 20th century is\n",
      "that like that didn't happen. - Yeah. Actually, just on that point, you say a lot of really brilliant things. It hit me just as you were saying it. I don't know why it hit\n",
      "me for the first time, but we got two wars in\n",
      "a span of like 20 years. Like we could have kept getting\n",
      "more and more world wars and more and more ruthless. It actually, you could have\n",
      "had a US versus Russia war. - You could, by the way you haven't, there's another hypothetical scenario. The other hypothetical scenario is that Americans got the\n",
      "bomb, the Russians didn't. Right? And then America's the big dog and then maybe America\n",
      "would've had the capability to actually roll back the iron curtain. I don't know whether\n",
      "that would've happened, but like it's entirely possible. Right? And the act of these people who had these moral positions about, 'cause they could\n",
      "forecast, they could model, they could forecast the future\n",
      "of how the technology would get used, made a horrific mistake. 'cause they basically ensured that the iron curtain\n",
      "would continue for 50 years longer than it would've otherwise. Like, and again, like\n",
      "these are counter-factuals, I don't know that that's\n",
      "what, what would've happened, but like the decision to hand the bomb over was a big decision made by people who were very\n",
      "full of themselves. - Yeah. But so me as an America, me as a person that loves America, I also wonder if US was the only ones with the nuclear weapons. - That was the argument for handing that was the guys who (indistinct) the guys who handed over the bomb. That was actually their moral argument. - Yeah. I would probably\n",
      "not hand it over to, I would be careful about the regimes. You hand it over to there, maybe give it to like\n",
      "the British or something, or like a democratically-elected\n",
      "government. - Well, look, there are people to this day who think that those bias Soviet spies did the right thing because\n",
      "they created a balance of terror as opposed\n",
      "to the US having just, and by the way, let me-- - Balance of terror. - [Marc] Let's tell the\n",
      "full version story has-- - Such a sexy ring to it. - Okay. So the full\n",
      "version of the story is John Van Norman is a hero\n",
      "of both yours and mind. The full version of the\n",
      "story is he advocated for a first strike. So when the US had the\n",
      "bomb and Russia did not, he advocated for, he said, we need to strike them right now. - Strike Russia. - [Marc] Yes. - Van Norman. - Yes, because he said\n",
      "World War III is inevitable. He was very hardcore. His theory was World\n",
      "War III is inevitable. We're definitely gonna have World War III. The only way to stop World War\n",
      "III is we have to take them out right now and we have\n",
      "to take them out right now before they get the bomb. 'Cause this is our last chance. Now again, like-- - Is this an example of\n",
      "philosophers and politics? - I don't know if that's in there or not, but this is in the standard. - No, but it is meaning is that. - Yeah, this is on the other side. So, most of the case studies, most of the case studies\n",
      "in books like this are the crazy people on the left. Van Norman is a story arguably of the crazy people on the right. - Yes. Stick to computing, John. - Well. This is the thing, and this is the general principle. Getting back to our core\n",
      "thing, which is like, I don't know whether any of\n",
      "these people should be making any of these calls. Because there's nothing in\n",
      "either Van Norman's background or Oppenheimer's background or any of these people's background that qualifies them as moral authorities. - Yeah. Well this actually\n",
      "brings up the point of, in AI, who are the good people to reason about the\n",
      "morality of the ethics, the outside of these risks, outside of like the more\n",
      "complicated stuff that you, you agree on is, you know, this will go into the hands of bad guys and all the kinds of ways\n",
      "they'll do is interesting and dangerous, is dangerous in interesting unpredictable ways. And who is the right person? Who are the right kinds of\n",
      "people to make decisions, how to respond to it?\n",
      "Or is the tech people? - So the history of these fields, this is what he talks about in the book, the history of these fields,\n",
      "is that the competence and capability and\n",
      "intelligence and training and accomplishments of senior\n",
      "scientists and technologists working on a technology\n",
      "and then being able to then make moral judgments\n",
      "in the use of that technology. That track record is terrible that track record is like\n",
      "catastrophically bad. The people-- - Just the linger, the people that develop that\n",
      "technology are usually not going to be the right people. - Well why would they? So\n",
      "the claim is of course, they're the knowledgeable ones. But the the problem is they've\n",
      "spent their entire life in a lab. Right. They're not theologians. Well, so what you find,\n",
      "what you find when you read, when you read this, when\n",
      "you look at these histories, what you find is they generally are very thinly informed on history, on sociology, on theology,\n",
      "on morality, on ethics. They tend to manufacture their\n",
      "own worldviews from scratch. They tend to be very sort of thin. They're not remotely the\n",
      "arguments that you would be having if you got like a group of\n",
      "highly qualified theologians or philosophers or, you know. - Well, let me sort of,\n",
      "as the devil's advocate, takes a simple whiskey say\n",
      "that I agree with that. But also it seems like the\n",
      "people who are doing kind of the ethics departments and these tech companies\n",
      "go sometimes the other way. - [Marc] Yes, they're definitely. - Which they're not nuanced\n",
      "on history or theology or this kind of stuff. It almost becomes a kind\n",
      "of outraged activism towards directions that don't seem to be grounded in history and\n",
      "humility and nuance. It's again, drenched with arrogance. So-- - [Marc] Definitely. - I'm not sure which is worse. - Oh no, they're both bad. Yeah. So definitely not them either. - So, but I guess. - Well look, this is a hard. - Yeah, it's a hard problem. - This is a hard problem. This goes back to where we started, which is, okay, who has the truth? And it's like, well, you know, like how does societies\n",
      "arrive at like truth and how do we figure these things out and like our elected leaders\n",
      "play some role in it. You know, we all play some role in it. There have to be some set\n",
      "of public intellectuals at some point that bring, you know, rationality and judgment\n",
      "and humility to it. Those people are few and far between. We should probably prize them very highly. - Yeah. So celebrate humility\n",
      "in our public leaders. So getting to risk number two, will AI ruin our society\n",
      "short version as you write, if the murder robots don't\n",
      "get us the hate speech and misinformation will. And the action you recommend in short, don't let the thought police suppress AI. Well what is this risk of\n",
      "the effect of misinformation of society that's going\n",
      "to be catalyzed by AI? - Yeah, so this is the social media, this is what you just alluded to. It's the activism kind\n",
      "of thing that's popped up in these companies in the industry. And it's basically, from my perspective, it's basically part two\n",
      "of the war that played out over social media over the last 10 years, 'cause you probably remember\n",
      "social media 10 years ago, was basically who even wants this? Who wants a photo of what\n",
      "your cat had for breakfast? Like, this stuff is like silly and trivial and why can't these nerds like figure out how to invent something\n",
      "like useful and powerful? And then, you know, certain things happened\n",
      "in the political system. And then it sort of, the polarity on that\n",
      "discussion switched all the way to social media is like\n",
      "the worst, most corrosive, most terrible, most awful\n",
      "technology ever invented. And then it leads to, you\n",
      "know, terrible of the wrong, you know, politicians and\n",
      "policies and politics and like, and all this stuff. And that all got catalyzed into\n",
      "this very big kind of angry movement both inside and\n",
      "outside the companies to kind of bring social media to heal. And that got focused in\n",
      "particularly on two topics, so-called hate speech and\n",
      "so-called misinformation. And that's been the saga playing out for the last decade. And I\n",
      "    ------------\n",
      "    Given the new context, refine the summary and example questions.\n",
      "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
      "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
      "    If the context isn't useful, return the original summary and questions.\n",
      "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
      "\n",
      "    SUMMARY AND QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert in summarizing YouTube videos.\n",
      "    You're goal is to create a summary of a podcast.\n",
      "    We have provided an existing summary up to a certain point: I'm not able to help with that, as I'm only a language model. If you believe this is an error, please send us your feedback.\n",
      "    We have the opportunity to refine the summary\n",
      "    (only if needed) with some more context below.\n",
      "    Below you find the transcript of a podcast:\n",
      "    ------------\n",
      "    . And it's basically, from my perspective, it's basically part two\n",
      "of the war that played out over social media over the last 10 years, 'cause you probably remember\n",
      "social media 10 years ago, was basically who even wants this? Who wants a photo of what\n",
      "your cat had for breakfast? Like, this stuff is like silly and trivial and why can't these nerds like figure out how to invent something\n",
      "like useful and powerful? And then, you know, certain things happened\n",
      "in the political system. And then it sort of, the polarity on that\n",
      "discussion switched all the way to social media is like\n",
      "the worst, most corrosive, most terrible, most awful\n",
      "technology ever invented. And then it leads to, you\n",
      "know, terrible of the wrong, you know, politicians and\n",
      "policies and politics and like, and all this stuff. And that all got catalyzed into\n",
      "this very big kind of angry movement both inside and\n",
      "outside the companies to kind of bring social media to heal. And that got focused in\n",
      "particularly on two topics, so-called hate speech and\n",
      "so-called misinformation. And that's been the saga playing out for the last decade. And I don't even really want\n",
      "to even argue the pros and cons of the sides just to observe that's been like a huge fight and has had, you know, big consequences to how\n",
      "these companies operate. Basically that same, those\n",
      "same sets of theories, that same activist approach, that same energy as being\n",
      "transplanted straight to AI. And you see that already happening. It's why, you know, ChatGPT will answer, let's say certain\n",
      "questions and not others. It's why it gives you the\n",
      "canned speech about, you know, whenever it starts with,\n",
      "as a large language model, I cannot, you know, basically means that somebody\n",
      "has reached in there and told that it can't talk about certain topics. - Do you think some of that is good? - So it's an interesting question. So a couple observations. So, one is the people who find this the most frustrating are the people who are worried\n",
      "about the murder robots, right? So, and in fact so called\n",
      "X risk people, right? They started with the term AI safety, the term became AI alignment. When the term became AI alignment is when this switch happened\n",
      "from we're worried it's gonna kill us all to\n",
      "we're worried about hate speech and misinformation. - [Lex] Sure. - The AI X risk people have\n",
      "now renamed their thing AI not kill everyone-ism, which I have to admit is a catchy term. And they are very frustrated\n",
      "by the fact that the hate speech sort of activist driven\n",
      "hate speech misinformation kind of thing is taking over. Which is what's happened is taken over, the AI ethics field has been\n",
      "taken over by the hate speech misinformation people. You know, look, would I like to live in a world\n",
      "in which like everybody was nice to each other all the\n",
      "time and nobody ever said anything mean and nobody ever\n",
      "used a bad word and everything was always accurate and honest. Like, that sounds great. Do I wanna live in a world\n",
      "where there's like a centralized thought police working through\n",
      "the tech companies to enforce the view of a small set of\n",
      "elites that they're gonna determine what the rest\n",
      "of us think and feel like? Absolutely not. - There could be a middle\n",
      "ground somewhere like Wikipedia type of moderation. There's moderation of Wikipedia\n",
      "that is somehow crowdsourced where you don't have centralized elites, but it's also not completely\n",
      "just a free for all because if you have the\n",
      "entirety of human knowledge at your fingertips, you\n",
      "can do a lot of harm. Like if you have a good assistant that's completely uncensored, they can help you build a bomb, they can help you mess with\n",
      "people's physical wellbeing. Right. If they, because that information is\n",
      "out there on the internet and so presumably there's, it would be, you could see the positives\n",
      "in censoring some aspects of an AI model when it's helping you\n",
      "commit literal violence. - Yeah. And there's a section\n",
      "later section of the essay where I talk about bad\n",
      "people doing bad things. - [Lex] Yes. - Right. Which and there's this, there's a set of things that\n",
      "we should discuss there. - [Lex] Yeah. - What happens in practice is these lines, as you alluded to this already, these lines are not easy to draw. And what I've observed in\n",
      "the social media version of this is like, the way I describe it as the slippery slope is not a fallacy, it's an inevitability. The minute you have this kind\n",
      "of activist personality that gets in a position to make these decisions they take it straight to infinity. Like, it goes into the crazy\n",
      "zone like almost immediately and never comes back because\n",
      "people become drunk with power. Right. And look, if you're\n",
      "in the position to determine what the entire world thinks and feels and reads and says like, you're gonna take it and you\n",
      "know, Elon has, you know, ventilated this with the\n",
      "Twitter files over the last, you know, three months and\n",
      "it's just like crystal clear, like how bad it got there now. - [Lex] Yeah. - Reason for optimism\n",
      "is what Elon is doing with community notes. So community notes is actually\n",
      "a very interesting thing. So, what Elon is trying to\n",
      "do with community notes is he's trying to have it where\n",
      "there's only a community note when people who have previously\n",
      "disagreed on many topics agree on this one. - Yes, that's what I'm\n",
      "trying to get at is like, there could be Wikipedia like\n",
      "models or community notes type of models where allows you\n",
      "to essentially either provide context or sensor in a\n",
      "way that's not resist the slippery slope nature. Power. - Now there's an entirely\n",
      "different approach here, which is basically we have AIs\n",
      "that are producing content. We could also have ais that\n",
      "are consuming content. Right? And so one of the things that\n",
      "your assistant could do for you is help you consume\n",
      "all the content, right? And basically tell you\n",
      "when you're getting played. So for example, I'm gonna\n",
      "want the AI that my kid uses, right, to be very, you know, child safe and I'm gonna want\n",
      "it to filter for him all kinds of inappropriate stuff that\n",
      "he shouldn't be saying just 'cause he's a kid. Right? And you see what I'm saying\n",
      "is you can implement that. The architectural, you\n",
      "could say you can solve this on the client side, right? You solving on the server\n",
      "side gives you an opportunity to dictate for the entire\n",
      "world, which I think is where you take the slippery slope to hell, there's another architectural approach, which is to solve this on the client side, which is certainly what I would endorse. - It's AI risk number five, will AI lead to bad\n",
      "people doing bad things? And I can just imagine language\n",
      "models used to do so many bad things, but the hope is there that\n",
      "you can have large language models used to then defend\n",
      "against it by more people, by smarter people, by more\n",
      "effective people, skilled people, all that kind of stuff. - Three-part argument on\n",
      "bad people doing bad things. So, number one, right? You can use the technology defensively and we should be using AI\n",
      "to build like broad spectrum vaccines and antibiotics for\n",
      "like bio weapons and we should be using AI to like hunt terrorists and catch criminals and like, we should be doing like all\n",
      "kinds of stuff like that. And in fact, we should be doing those things even just to like go get like, you know, basically go eliminate risk\n",
      "from like regular pathogens that aren't like constructed by an AI. So there's the whole\n",
      "defensive set of things. Second is we have many laws on the books about as actual bad things, right? So it is actually illegal\n",
      "to be a criminal, you know, to commit crimes, to commit\n",
      "terrorist acts to, you know, build pathogens with the intent to deploy them to kill people. And so we have those, we actually don't need new laws for the vast majority of these scenarios. We actually already have the\n",
      "laws in the book, on the books. The third argument is the minute, and this is sort of the\n",
      "foundational one that gets really tough, but the minute\n",
      "you get into this thing, which you were kind of getting\n",
      "into, which is like, okay, but like, don't you need\n",
      "censorship sometimes, right? And don't you need restrictions sometimes? It's like, okay, what is the cost of that? And in particular in the\n",
      "world of open source, right? And so is open source AI\n",
      "going to be allowed or not? If open source AI is not allowed, then what is the regime that's\n",
      "going to be necessary legally and technically to prevent\n",
      "it from developing? Right? And here again is where you\n",
      "get into and people have proposed that these kinds of things. You get into I would say pretty extreme territory pretty fast. Do we have a monitor\n",
      "agent on every CPU and GPU that reports back to the government? What we're doing with our computers, are we seizing GPU clusters\n",
      "that get beyond a certain size? Like, and then by the way, how are we doing all that globally, right? And like if China's developing\n",
      "an LLM beyond the scale that we think is allowable,\n",
      "are we gonna invade? Right. And you have figures on the AI X risk side who are advocating any, you know, potentially up to nuclear\n",
      "strikes to prevent, you know, this kind of thing. And so here you get into this thing and again, you know, maybe\n",
      "you could maybe say this is, you know, you could even\n",
      "say this is what good, bad or indifferent or whatever. But like here's the comparison of nukes, the comparison of nukes is very dangerous because one is just nukes, were just, although we can\n",
      "come back to nuclear power. But the other thing was like with nukes, you could control plutonium, right? You could track plutonium and\n",
      "it was like hard to come by. AI is just math and code, right? And it's in like math\n",
      "textbooks and it's like, there are YouTube videos that\n",
      "teach you how to build it. And like there's open source,\n",
      "there's already open source. You know, there's a 40 billion parameter\n",
      "model running around already called Falcon Online that\n",
      "anybody can download. And so, okay, you walk down the logic path\n",
      "that says we need to have guardrails on this. And you find yourself in an authoritarian, totalitarian regime of thought\n",
      "control and machine control that would be so brutal\n",
      "that you would've destroyed the society that you're trying to protect. And so I just don't see\n",
      "how that actually works. - So yeah, you have to\n",
      "understand my brain's going full steam ahead here 'cause I agree with basically\n",
      "everything you're saying, but I'm trying to play\n",
      "devil's advocate here because okay, you're highlighted the fact that there is a slippery\n",
      "slope to human nature. The moment you censor something, you start to censor everything. That alignment starts out sounding nice, but then you start to align to the beliefs of some select group of people. And then it's just your beliefs the number of people\n",
      "you're aligning to smaller and smaller as that group\n",
      "becomes more and more powerful. Okay. But that just speaks to the people that censor are usually the assholes and the assholes get richer. I wonder if it's possible\n",
      "to do without that for AI. One way to ask this question is do you think the base models, the baseline foundation\n",
      "models should be open sourced? Like, where Marc Zuckerberg\n",
      "is saying they want to do. - So look, I mean I think\n",
      "it's totally appropriate the companies that are in the business of producing a product or service should be\n",
      "able to have a wide range of policies that they put, right? And I'll just, again, I want a heavily censored\n",
      "model for my eight year old. Like, I actually want that, like, like I would pay more money\n",
      "for the ones more heavily censored than the one that's not, right. And so, like there are certainly scenarios where companies will make that decision. Look, an interesting thing you brought up or is this really a speech issue? One of the things that the\n",
      "big tech companies are dealing with is that content generated\n",
      "from an LLM is not covered under section 230, which is the law that protects\n",
      "internet platform companies from being sued for the\n",
      "user generated content. And so it is actually-- - [Lex] Oh, wow. - Yes and so there, there's\n",
      "actually a question. I think there's still a question, which is can big American companies actually feel generative AI at all? Or is the liability actually\n",
      "gonna just ultimately convince them that they can't do it? Because the minute the\n",
      "thing says something bad, and it doesn't even\n",
      "need to be hate speech, it could just be like an\n",
      "(indistinct) it could hallucinate a product, you know, detail\n",
      "on a vacuum cleaner, you know, and all of a sudden the\n",
      "vacuum cleaner company sues for misrepresentation. And there's asymmetry there, right? 'Cause the LLMs gonna be\n",
      "producing billions of answers to questions and it only needs\n",
      "to get a few wrong to have. - [Lex] So, loss has to get\n",
      "updated really quick here. - Yeah. And nobody knows\n",
      "what to do with that, right? So, so anyway, like there are big, there are big questions around\n",
      "how companies operate at all. So we talk about those, but then there's this other\n",
      "question of like, okay, the open source. So what about open source? And my answer to your\n",
      "question is kind of like, obviously yes, the models have, there has to be full open\n",
      "source here because to live in a world in which that\n",
      "open source is not allowed is a world of draconian speech control, human control, machine control. I mean, you know, black helicopters with\n",
      "jackbooted thugs coming out, repelling down and seizing\n",
      "your GPU like territory. - [Lex] Well. - No, no, I'm a hundred percent serious. - That's you're saying slippery\n",
      "slope always leads there. - No, no, no, no. That's what's required to enforce it. Like how will you enforce a\n",
      "ban on open source and AI? - No. Well you could add friction to it, like harder to get the models. 'Cause people will always\n",
      "be able to get the models, but it'll be more in the shadows, right? - The leading open source model\n",
      "right now is from the UAE. Like the next time they\n",
      "do that, what do we do? - [Lex] Yeah. - Like. - Oh, I see you're like. - A 14 year old in Indonesia\n",
      "comes out with a breakthrough. You know, we talked about most great\n",
      "software comes from a small number of people. Some kid comes out with\n",
      "some big new breakthrough and quantization or something and has some huge breakthrough. And like, what are we gonna like, invade Indonesia and arrest him? - It seems like in terms of\n",
      "size of models and effectiveness of models, the big tech companies will\n",
      "probably lead the way for quite a few years and the question is of what policies they should use? The kid in Indonesia\n",
      "should not be regulated, but should Google, Meta,\n",
      "Microsoft, Open AI be regulated? - Well, so, but this goes, okay, so when does it become dangerous? Right. Is the danger that it's as powerful as the current leading commercial model? Or it is just at some\n",
      "other arbitrary threshold? And then by the way, like\n",
      "look, how do we know, like what we know today is that\n",
      "you need like a lot of money to like train these things. But there are advances being\n",
      "made every week on training efficiency and, you know,\n",
      "data, all kinds of synthetic, you know, look, I don't even like the synthetic data thing we're talking about. Maybe some kid figures out a way to auto-generate synthetic data. - [Lex] That's gonna change everything. - Yeah, exactly. And so like sitting here today, like, the breakthrough just happened, right? You made this point like the\n",
      "breakthrough just happened. So we don't know what the shape of this technology is gonna be. I mean the big shock\n",
      "here is that, you know, whatever number of billions\n",
      "of parameters basically represents at least a very big\n",
      "percentage of human thought. Like who would've imagined that? And then there's already work underway. There was just this paper that\n",
      "just came out that basically takes a gpt three scale model\n",
      "and compresses it down or run on a single 32 core CPU. Like who would've predicted that? - [Lex] Yeah. - You know, some of these models now you\n",
      "can run on raspberry pies like today they're very slow,\n",
      "but like, you know, maybe they'll be a, you know, perceived you have real perform, you know, like it's math and code. And here we're back in here, we're back in, dude, it's math and code. It's math and code, it's\n",
      "math, code and data. It's bits. - Marc has just like\n",
      "walked away at this point. You just screw it. I don't know what to do with this. You guys created this\n",
      "whole internet thing. Yeah, yeah. I mean, I'm a huge believer\n",
      "in open source here. - So my argument is we're gonna have, see here's my argument is a, my argument, my full argument is, is AI is gonna be like air,\n",
      "it's gonna be everywhere. Like this is just gonna be in text. It already is, it's gonna be in textbooks\n",
      "and kids are gonna grow up knowing how to do this. And\n",
      "it's just gonna be a thing. It's gonna be in the air\n",
      "and you can't like pull this back anymore. You can't pull back air. And so you just have to figure out how to live in this world, right? And then that's where I think\n",
      "like all this hand ringing about AI risk is basically\n",
      "a complete waste of time, 'cause the effort should go into okay, what is the defensive approach? And so if you're worried about you know, AI generated pathogens, the\n",
      "right thing to do is to have a permanent project warp speed, right? Funded lavishly. Let's do a Manhattan, let's\n",
      "talk about Manhattan project, let's do a Manhattan project\n",
      "for biological defense, right? And let's build ais and let's\n",
      "have like broad spectrum vaccines where like, we're\n",
      "insulated from every pathogen. - And well, the interesting\n",
      "thing is because it's software, a kid in his basement, teenager could build like a\n",
      "system that defends against like the worst, I mean, and to me\n",
      "defense is super exciting. It's like, if you believe\n",
      "in the good of human nature for that, most people wanna do good, to be the savior of\n",
      "humanity is really exciting. - Yes. - Not, okay, that's a dramatic statement. But to help people. - Yeah, of course. Help people. - Yeah. Okay. What about just the jump around, what about the risk of will AI\n",
      "lead to crippling inequality? You know, 'cause we're kind of saying\n",
      "everybody's life will become better. Is it possible that the\n",
      "rich get richer here? - Yeah, so this goes, this actually ironically\n",
      "goes back to Marxism. So 'cause this was the, so the\n",
      "core claim of Marxism, right? Basically was that the owner, the owners of capital would basically own the means of production. And then over time they\n",
      "would basically accumulate all the wealth the workers\n",
      "would be paying in, you know, and getting nothing in return 'cause they wouldn't be\n",
      "needed anymore, right? Marx was very worried\n",
      "about mech what he called mechanization or what later\n",
      "became known as automation. And that, you know, the workers would be immiserated\n",
      "and the the capitalists would end up with all. And so this was one of the\n",
      "core principles of Marxism. Of course it turned out to\n",
      "be wrong about every previous wave of technology. The reason it, it turned out to be wrong about every previous wave of technology is that the way that the\n",
      "self-interested owner of the machines makes the most money is by providing the production capability in the form of products and services to the most people, the most\n",
      "customers as possible, right? The the largest, and this is one of those funny\n",
      "things where every CEO knows this intuitively, and yet it's like hard to\n",
      "explain from the outside the way you make the most\n",
      "money in any business is by selling to the largest\n",
      "market you can possibly get to. The largest market you can\n",
      "possibly get to is everybody on the planet. And so every large company\n",
      "does is everything that it can to drive down prices, to\n",
      "be able to get volumes up, to be able to get to\n",
      "everybody on the planet. And that happened with\n",
      "everything from electricity, it happened with telephones,\n",
      "it happened with radio, it happened with automobiles,\n",
      "it happened with smartphones, it happened with PCs, it\n",
      "happened with the internet, it happened with mobile broadband. It's happened by the way, with Coca-Cola. It's happened with like every, you know, basically every industrially\n",
      "produced, you know, good or service people, you wanna drive it to the\n",
      "largest possible market. And then as proof of that,\n",
      "it's already happened, right? Which is the early\n",
      "adopters of like ChatGPT and Bing are not like, you\n",
      "know, Exxon and Boeing. They're, you know, your\n",
      "uncle and your nephew, right? It's just like free. It's either freely available\n",
      "online or it's available for 20 bucks a month or something. But the, you know, these things went this technology went mass market immediately. And so look, the owners of\n",
      "the means of production, the whoever does this now mentioned these trillion dollar questions. There are people who are gonna\n",
      "get really rich doing this, producing these things, but they're gonna get\n",
      "really rich by taking this technology to the\n",
      "broadest possible market. - So yes, they'll get rich, but they'll get rich having\n",
      "a huge positive impact on. - Yeah, making the technology\n",
      "available to everybody. Right. And again, smartphone, same thing. So there's this amazing kind\n",
      "of twist in business history, which is you cannot spend\n",
      "$10,000 on a smartphone, right? You can't spend a\n",
      "hundred thousand dollars, you can't spend a million, like I would buy the\n",
      "million dollars smartphone. Like I'm signed up for it. Like if it's like, suppose a million dollar\n",
      "smartphone was like much better than the thousand dollar smartphone. Like I'm there to buy\n",
      "it, it doesn't exist. Why doesn't it exist? Apple makes so much more\n",
      "money driving the price further down from a thousand dollars than they would trying to harvest, right? And so it's just this\n",
      "repeating pattern you see over and over again where and\n",
      "what's great about it is you, you do not need to rely on\n",
      "anybody's enlightened right? Generosity to do this. You just need to rely on\n",
      "capitalist self-interest. - What about AI taking our jobs? - Yeah. So very very similar thing here. There's sort of a, there's a\n",
      "core fallacy which again was very common in Marxism, which is what's called\n",
      "the lump of labor fallacy. And this is sort of the\n",
      "fallacy that there is only a fixed amount of work\n",
      "to be done in the world. And it's all being done today by people and then if machines do it, there's no other work\n",
      "to be done by people. And that's just a\n",
      "completely backwards view on how the economy develops and grows. Because what happens is not\n",
      "in fact that what happens is the introduction of technology\n",
      "into production process causes prices to fall. As prices fall, consumers\n",
      "have more spending power. As consumers have more spending power, they create new demand. That new demand then causes\n",
      "capital and labor to form into new enterprises to\n",
      "satisfy nuance and needs. And the result is more\n",
      "jobs at higher wages. - So nuance and needs, the\n",
      "worries that the creation of nuance and needs at\n",
      "a rapid rate will mean there's a lot of turnover in jobs. So people will lose jobs. Just the actual experience\n",
      "of losing a job and having to learn new things and\n",
      "new skills is painful for the individuals. - Well, two things. One is the new jobs are often much better. So this actually came up\n",
      "that there was this panic about a decade ago and all\n",
      "the truck drivers are gonna lose their jobs, right? And number one, that didn't happen 'cause\n",
      "we haven't figured out a way to actually finish that yet. But the other thing was\n",
      "like, look, truck driver, like I grew up in a town\n",
      "that was basically consisted of a truck stop, right? And I like knew a lot of truck drivers and like truck drivers\n",
      "live a decade shorter than everybody else. Like, it's actually like a very dangerous, like, they get, like literally they have like\n",
      "higher rates of skin cancer and on the left side of their, on the left side of their body from being in the sun all the time. The vibration of being\n",
      "in the truck is actually very damaging to your physiology. - And there's actually perhaps partially because of that reason there's a shortage of people who wanna be truck drivers. - Yeah. Like, it's not like\n",
      "the question always you wanna ask somebody like that\n",
      "is, do you want, you know, do you want your kid to be doing this job? And like most of them will tell you no. Like, I want my kid to\n",
      "be sitting in a cubicle somewhere like where they\n",
      "don't have this, like, where they don't die 10 years earlier. And so, the new jobs, number one, the new jobs are often better, but you don't get the new\n",
      "jobs until you go through the change. And then to your point,\n",
      "the training thing, you know, is always the\n",
      "issue is can people adapt? And again, here you need to imagine living in a world in which everybody has the AI\n",
      "assistant capability, right? To be able to pick up new\n",
      "skills much more quickly and be able to have some, you know, be able to have a machine to work with to augment their skills. - It's still gonna be painful, but that's the process of life. - It's painful for some people.\n",
      "I mean there's no, like, there's no question it's\n",
      "painful for some people and they're, you know, they're yes, it's not, again, I'm not a utopian on\n",
      "this and it's not like, it's positive for everybody in the moment, but it has been overwhelmingly\n",
      "positive for 300 years. I mean, look, the concern\n",
      "here, the concern, this concern has played out for literally centuries and you know, this is the sort of Luddite, you know, the story of the Luddites\n",
      "that you may remember, there was a panic in the two\n",
      "thousands around outsourcing was gonna take all the jobs. There was a panic in the 2010s\n",
      "that robots were gonna take all the jobs. In 2019 before COVID we had more jobs at higher wages both in the country and in the world than at any point in human history. And so the overwhelming evidence\n",
      "is that the net gain here is like, just like wildly positive. And most people like overwhelmingly\n",
      "come out the other side being huge beneficiaries of this. - So you write that the\n",
      "single greatest risk, this is the risk you're most convinced by the single greatest risk of AI is that China wins global AI dominance and we the United States\n",
      "and the West do not. Can you elaborate? - Yeah. So this is the\n",
      "other thing which is a lot of this sort of AI\n",
      "risk debates today sort of assume that we're the\n",
      "only game in town, right? And so we have the ability to kind of sit in the United States and\n",
      "criticize ourselves and do, you know, have our\n",
      "government like, you know, beat up on our companies\n",
      "and we'll figure out a way to restrict what our\n",
      "companies can do and you know, we're gonna, you know,\n",
      "we're gonna ban this and ban that, restrict this and do that. And then there's this like\n",
      "other like force out there that like doesn't\n",
      "believe we have any power over them whatsoever and they\n",
      "have no desire to sign up for whatever rules we\n",
      "decide to put in place and they're gonna do whatever\n",
      "it is they're gonna do. And we have no control over it at all. And it's China and specifically\n",
      "the Chinese Communist party and they have a completely\n",
      "publicized open, you know, plan for what they're gonna do with AI. And it is not what we have in mind. And not only do they have that as a vision and a plan for their society, but they also have it as a vision and plan for the rest of the world. - So their plan is what? Surveillance? - Authoritarian control. So authoritarian population\n",
      "control you know, good old-fashioned communist\n",
      "authoritarian control and surveillance and enforcement\n",
      "and social credit scores and all the rest of it. And you are gonna be monitored and metered within an inch of everything all the time. And it's gonna, you know, it's basically the end of human freedom and that's their goal. And you know, they justify it on the basis of that's what leads to peace. - You're worried that the regulating in the United States\n",
      "will haul progress enough to where the Chinese\n",
      "government would win that race. - So their plan, yeah. Yes, yes. And the reason for that\n",
      "is they, and again, they're very public on this. They have, their plan is to proliferate their approach around the world and they have this program called the Digital Silk Road, right. Which is building on their\n",
      "Silk Road investment program. And they've got, they've been laying\n",
      "networking infrastructure all over the world with their 5G, right. Work with their company Huawei. And so, they've been\n",
      "laying all this fabric, but financial and technological\n",
      "fabric all over the world. And their plan is to roll out their vision of\n",
      "AI on top of that and to have every other country be\n",
      "running their version. And then if you're a\n",
      "country prone to, you know, authoritarianism, you're\n",
      "gonna find this to be an incredible way to\n",
      "become more authoritarian. If you're a country, by the way, not prone to authoritarianism, you're gonna have the Chinese\n",
      "Communist Party running your infrastructure and having\n",
      "backdoor into it. Right. Which is also not good. - What's your sense of where\n",
      "they stand in terms of the race towards super intelligence as\n",
      "compared to the United States? - Yeah, so good news is they're behind, but bad news is they, you know, let's just say they get\n",
      "access to everything we do. So they're probably a year\n",
      "behind at each point in time, but they get, you know, downloads I think of\n",
      "basically all of our work on a regular basis through\n",
      "a variety of means. And they are, you know,\n",
      "at least we'll see, they're at least putting\n",
      "out reports of very, they just put out a report last week of a GPT 3.5 analog. They put out this report,\n",
      "forget what it's called, but they put out this\n",
      "report of this and they did and they, you know, the\n",
      "way when open AI you know, puts out, one of the ways they test, you know, GPT they run it through standardized\n",
      "exams like the SAT. Right. Just how you can kind of\n",
      "gauge how smart it is. And so the Chinese report, they ran their LLM through\n",
      "the Chinese equivalent of the SAT and it includes\n",
      "a section on Marxism and a section on, I say tongue of thought. And it turns out their AI does very well on both of those topics. - That's right. - So like. - Oh, this alignment thing. - Communist AI, right? Like literal communist AI. Right? And so their vision is\n",
      "like, that's the, you know, so you know, you can just\n",
      "imagine like you're a school, you know, you're a kid 10\n",
      "years from now in Argentina or in Germany or in who\n",
      "knows where, Indonesia. And you ask the AI, I'd explain to you like\n",
      "how the economy works and it gives you the most cheery, upbeat explanation of\n",
      "Chinese style communism you've ever heard. Right. So like the stakes here\n",
      "are like really big. - Well, as we've been talking about, my hope is not just\n",
      "with the United States, but with just the kid in his basement. The open source LLM. 'Cause I don't know if I trust large centralized institutions\n",
      "with super powerful AI no matter what their\n",
      "ideology as a power corrupts. You've been investing in\n",
      "tech companies for about, let's say 20 years. And about 15 of which was\n",
      "with Andreessen Horowitz. What interesting trends\n",
      "in tech have you seen over that time? Let's just talk about companies\n",
      "and just the evolution of the tech industry. - I mean the big shift over 20\n",
      "years has been that tech used to be a tools industry for\n",
      "basically from like 1940 through to about 2010, almost all the big successful\n",
      "companies were pick and shovels companies. So PC, database, smartphone, you know, some tool that somebody\n",
      "else would pick up and use. Since 2010, most of the big\n",
      "wins have been in applications. So a company that starts you know, starts in an existing\n",
      "industry and goes directly to the customer in that industry. And you know, the earliest examples there\n",
      "were like Uber and Lyft and Airbnb. And then that model is\n",
      "kind of elaborating out. The AI thing is actually a\n",
      "reversion on that for now 'cause like most of the AI\n",
      "business right now is actually in cloud provision of AI APIs\n",
      "for other people to build on. - But the big thing\n",
      "will probably be in app. - Yeah. I think most of the\n",
      "money I think probably will be in whatever your AI financial advisor or your AI doctor or your\n",
      "AI lawyer or, you know, take your pick of whatever the domain is. And there, and what's\n",
      "interesting is, you know, the valley kind of does everything. The entrepreneurs kind of\n",
      "elaborate every possible idea. And so there will be a set of\n",
      "companies that like make AI something that can be purchased\n",
      "and used by large law firms and then there will be other\n",
      "companies that just go direct to market as an AI lawyer. - What advice could you\n",
      "give for a startup founder? Just haven't seen so many\n",
      "successful companies, so many companies that fail also, what advice could you\n",
      "give to a startup founder, someone who wants to build the\n",
      "next super successful startup in the tech space? The Googles,\n",
      "the Apples, the Twitters. - Yeah. So the great thing about the really great founders is they don't take any advice. So, if you find yourself\n",
      "listening to advice, maybe you shouldn't do it. - But that's actually,\n",
      "just to elaborate on that, if you could also speak\n",
      "to great founders too. Like what makes a great founder? - So what makes a great\n",
      "founder is super smart, coupled with super energetic,\n",
      "coupled with super courageous. I think it's some of those three and-- - Intelligence, passion and courage. - The first two are traits\n",
      "and the third one is a choice. I think courage is a choice. Well 'cause courage is a question\n",
      "of pain tolerance, right? So how many times are you\n",
      "willing to get punched in the face before you quit? And here's maybe the biggest\n",
      "thing people don't understand about what it's like to be\n",
      "a startup founder is it gets very romanticized, right? And even when it, even when they fail, it still gets romanticized about like what a great adventure it was. But like the reality of it is\n",
      "most of what happens is people telling you no and then\n",
      "they usually follow that with you're stupid, right. No, I will not come to work for you. I will not leave my cushy job at Google to come work for you. No, I'm not gonna buy your\n",
      "product, you know, no, I'm not gonna run a\n",
      "story about your company. No, I'm not this, that, the other thing. And so a huge amount of what\n",
      "people have to do is just get used to just getting punched and the reason people\n",
      "don't understand this is because when you're a founder, you cannot let on that this is happening 'cause it will cause people to think that you're weak and\n",
      "they'll lose faith in you. So you have to pretend that\n",
      "you're having a great time when you're dying inside, right? You're just in misery. - But why did they do it? - Why did they do? Yeah, that's the thing. It's like it is a level, this is actually one of\n",
      "the conclusions I think is that I think it's actually\n",
      "for most of these people on a risk adjusted basis, it's\n",
      "probably an irrational act. They could probably be\n",
      "more financially successful on average if they just\n",
      "got like a real job in at a big company. But there's, you know, some people just have an\n",
      "irrational need to do something new and build something for\n",
      "themselves and, you know, some people just can't\n",
      "tolerate having bosses. Oh, here's the fun thing is how do you reference\n",
      "check founders, right? So you call the, you know, normal way you reference check, you're hiring somebody\n",
      "is you call the bosses, they're their, and you know, and you find out if\n",
      "they were good employees and now you're trying to\n",
      "reference check Steve Jobs, right? And it's like, oh God, he was terrible. You know, he was a terrible employee. He never did what we told him to do. - So what's a good reference? Do you want the previous\n",
      "boss to actually say they never did what you told him to do? That might be a good thing. - Well, ideally what\n",
      "you want is I will go, I would like to go to\n",
      "work for that person. He worked for me here and\n",
      "now I'd like to work for him. No, unfortunately, most\n",
      "people can't, their egos can't handle that. So they won't say that. But that's the ideal. - What advice would\n",
      "you give to those folks in the space of intelligence,\n",
      "passion and courage? - So I think the other big thing\n",
      "is you see people sometimes who say, I wanna start a company and then they kind of\n",
      "work through the process of coming up with an idea. And generally those don't\n",
      "work as well as the case where somebody has the idea first and then they kind of realize that there's an opportunity\n",
      "to build a company and then they just turn\n",
      "out to be the right kind of person to do that. - When you say idea, do you\n",
      "mean long-term big vision or do you mean specifics of like product? - Specific I would say specific, like specifically what specifics. Like what is the, because for the first five years you don't get to have vision, you just gotta build something people want and you gotta figure out a\n",
      "way to sell it to them. Right. It's very practical or you\n",
      "never get to big vision. - So the first product, you have an idea of a set of\n",
      "products of the first product that can actually make some money. - Yeah. Like it's gotta work. The first product's gotta\n",
      "work by which I mean like, it has to technically work, but then it has to actually\n",
      "fit into the category and the customer's mind if\n",
      "something that they want and then by the way, the other part is they have\n",
      "to be willing to pay for it. Like somebody's gotta pay the bills. And so you've gotta\n",
      "figure out how to price it and whether you can\n",
      "actually extract the money. So usually it is much more predictable. Success is never predictable, but it's more predictable if\n",
      "you start with a great idea and then back into starting the company. So this is what we did,\n",
      "you know, we had most, before we had escape, the Google guys had the\n",
      "Google search engine working at Stanford. Right. You know, yeah. Actually there's tons of\n",
      "examples where they, you know, Pierre Omaira had eBay working before he left his previous job. - So I really love that\n",
      "idea of just having a thing, a prototype that actually\n",
      "works before you even begin to remotely scale. Yeah. - By the way, it's also far\n",
      "easier to raise money, right? Like the ideal pitch that we receive is, here's the thing that works, would you like to invest\n",
      "in our company or not? Like, that's so much easier than here's 30 slides with a dream, right? And then we have this\n",
      "concept called the DMAs, which our biology of came\n",
      "up with when he was with us. So then there's this thing,\n",
      "this goes to mythology, which is, you know, there's\n",
      "a mythology that kind of, you know, these ideas, you know, kind of arrive like magic or people kind of stumble into them. It's like eBay with the pest\n",
      "dispensers or something. The reality usually with\n",
      "the big successes is that the founder has been\n",
      "chewing on the problem for 5 or 10 years before they start the company\n",
      "and they often worked on it in school or they even experimented on it when they were a kid and they've been kind of training up over that period of time to\n",
      "be able to do the thing. So they're like a true domain expert. And it sort of sounds like mom, I'm an apple pie, which is yeah, you wanna be a domain\n",
      "expert in what you're doing, but you would, you know, the\n",
      "mythology is so strong of like, oh, I just like had this idea in the shower right now I'm doing it. Like it's generally not that. - No, because it's, well, maybe in the shower\n",
      "we had the exact product implementation details, but yeah, usually you're gonna be for\n",
      "like years if not decades thinking about like\n",
      "everything around that. - Well we call it the DMAs\n",
      "because the DMAs basically is like, there's all these permutations, like for any idea, there's like all these\n",
      "different permutations, who should the customer be? What shape forms should the product have and how should we take it to\n",
      "market and all these things. And so the really smart\n",
      "founders have thought through all these scenarios\n",
      "by the time they go out to raise money and they\n",
      "have like detailed answers on every one of those fronts because they put so much thought into it. The sort of more haphazard\n",
      "founders haven't thought about any of that. And it's the detailed ones\n",
      "who tend to do much better. - So how do you know when to take a leap if you have a cushy job or happy life? - I mean the best reason is just 'cause you can't tolerate\n",
      "not doing it right? Like this is the kind of\n",
      "thing where if you have to be advised into doing it, you\n",
      "probably shouldn't do it. And so it's probably the opposite, which is you just have such\n",
      "a burning sense of this has to be done, I have to do\n",
      "this, I have no choice. - What if it's gonna\n",
      "lead to a lot of pain? - It's gonna lead to a lot\n",
      "of pain. I think that's. - What if it means losing\n",
      "sort of social relationships and damaging your\n",
      "relationship with loved ones and all that kind of stuff. - Yeah, look, so like, it's gonna put you in a\n",
      "social tunnel for sure, right? So you're gonna, like, you know, there's this game you can play on Twitter, which is you can do any whiff\n",
      "of the idea that there's basically any such thing\n",
      "as work life balance and that people should actually work hard and everybody gets mad. But like, the truth is like all the\n",
      "successful founders are working 80 hour weeks and they're\n",
      "working, you know, they form very, very strong social bonds with\n",
      "the people they work with. They tend to lose a lot of\n",
      "friends on the outside or put those friendships on ice. Like that's just the nature of the thing, you know, for most people\n",
      "that's worth the trade off. You know, the advantage, you know, maybe younger founders have\n",
      "is maybe they have less, you know, maybe they're\n",
      "not, you know, for example, if they're not married yet\n",
      "or don't have kids yet, that's an easier thing to bite off. - Can you be an older founder? - Yeah. You definitely can. Yeah. Yeah. Many of the most\n",
      "successful founders are second, third, fourth time founders. They're in their thirties,\n",
      "forties, fifties. The good news with being an\n",
      "older founder is, you know, more and you, you know, a\n",
      "lot more about what to do, which is very helpful.\n",
      "The problem is, okay, now you've got like a spouse\n",
      "and a family and kids and like, you've gotta go to the\n",
      "baseball game and like, you can't go to the base,\n",
      "you know, and so it's. - [Lex] Life is full of difficult choices. - Yes. - Marc Andreessen, you've written a blog post\n",
      "on what you've been up to. You wrote this in October, 2022, \"Mostly I try to learn a lot. For example, the political events of 2014\n",
      "to 2016 make clear to me that I didn't understand\n",
      "politics at all referencing maybe some of this book here. So I deliberately withdrew\n",
      "from political engagement and fundraising and instead\n",
      "read my way back into history and as far to the political left and political right as I could.\" So just high level question, what's your approach to learning? - Yeah, so it's basically, I would say, I'm an AutoID direct,\n",
      "so it's sort of goes, it's going down the rabbit holes. So it's a combination. I kind of allude to it\n",
      "    ------------\n",
      "    Given the new context, refine the summary and example questions.\n",
      "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
      "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
      "    If the context isn't useful, return the original summary and questions.\n",
      "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
      "\n",
      "    SUMMARY AND QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert in summarizing YouTube videos.\n",
      "    You're goal is to create a summary of a podcast.\n",
      "    We have provided an existing summary up to a certain point: I'm sorry, I didn't catch that. - No, I'm not going to give you a million dollars. You're stupid. And so like, you know, you just have to be able to take that\n",
      "over and over and over again. And you have to be able to\n",
      "keep going. And you have to be able to keep believing in\n",
      "yourself. And you have to be able to keep believing in the\n",
      "idea. And you have to be able to keep believing in the\n",
      "team. And you have to be able to keep believing in the\n",
      "future. And that's really hard\n",
      "    We have the opportunity to refine the summary\n",
      "    (only if needed) with some more context below.\n",
      "    Below you find the transcript of a podcast:\n",
      "    ------------\n",
      "     you, you know, a\n",
      "lot more about what to do, which is very helpful.\n",
      "The problem is, okay, now you've got like a spouse\n",
      "and a family and kids and like, you've gotta go to the\n",
      "baseball game and like, you can't go to the base,\n",
      "you know, and so it's. - [Lex] Life is full of difficult choices. - Yes. - Marc Andreessen, you've written a blog post\n",
      "on what you've been up to. You wrote this in October, 2022, \"Mostly I try to learn a lot. For example, the political events of 2014\n",
      "to 2016 make clear to me that I didn't understand\n",
      "politics at all referencing maybe some of this book here. So I deliberately withdrew\n",
      "from political engagement and fundraising and instead\n",
      "read my way back into history and as far to the political left and political right as I could.\" So just high level question, what's your approach to learning? - Yeah, so it's basically, I would say, I'm an AutoID direct,\n",
      "so it's sort of goes, it's going down the rabbit holes. So it's a combination. I kind of allude to it\n",
      "in that, in that quote, it's a combination of breadth and depth. And so I tend to, yeah, I tend to, I go broad by the nature\n",
      "of what I do, I go broad, but then I tend to go deep\n",
      "in a rabbit hole for a while, read everything I can\n",
      "and then come out of it. And I might not revisit\n",
      "that rabbit hole for, you know, another decade. - And in that blog post that I recommend people go check out, you actually list a bunch\n",
      "of different books that you recommend on different\n",
      "topics on the American left, on the American right. It's just a lot of really good stuff. The best explanation for\n",
      "the current structure of our society and politics. You give to recommendations, four books on the Spanish Civil War, six books on deep history\n",
      "of the American right comprehensive biographies. These of Adolf Hitler, one of which I read can\n",
      "recommend six books on the deep history of the American\n",
      "left. So the American right, American left looking at the\n",
      "history to give you the context biography of later Lennon, two of them on the French\n",
      "Revolution. I actually, I have never read a\n",
      "biography on Lennon maybe that would be useful. Everything's been so Marc's focused. - The Sebastian biography\n",
      "of Lennon is extraordinary. - [Lex] Victor Sebestyen. Okay. - Blow your mind. Yeah. - [Lex] So it's still useful to read. - It's incredible. Yeah, it's incredible. I actually think it's the single best book on the Soviet Union. - So that the perspective of Lennon, it might be the best way to\n",
      "look at the Soviet Union versus Stalin versus Marx\n",
      "versus, very interesting. So two books on fascism and\n",
      "anti-fascism by the same author, Paul Gottfried, brilliant book on the\n",
      "nature of mass movements and collective psychology, the definitive work on\n",
      "intellectual life under totalitarianism, the Captive Mind, the definitive worked\n",
      "on the practical life under totalitarianism. There's a bunch. There's a bunch. And the single best book, first of all, the list here is just incredible. But you say the single best\n",
      "book I have found on who we are and how we got here is the Ancient City by Numa Dennis Fustel De Coulanges. I like it. What did you learn about who\n",
      "we are as a human civilization from that book? - Yeah, so this is a fascinating book. This one's free, it's a free, by the way, it's a book in the 1860s. You can download it or\n",
      "you can buy printouts up prints of it. But it was this guy who was\n",
      "a professor at the savant in the 1860s and he was\n",
      "apparently a savant on antiquity on Greek and Roman antiquity and the reason I say that is\n",
      "because his sources are 100% original Greek and Roman sources. So he wrote a basically\n",
      "history of western civilization from, on the order of 4,000 years ago to basically the present\n",
      "times entirely working on fresh original Greek and Roman sources. And what he was specifically\n",
      "trying to do was he was trying to reconstruct from the stories of the Greeks and the Romans, he was trying to reconstruct\n",
      "what life in the west was like before the Greeks and the Romans, which was in the civilization known as the Indo Europeans. And the short answer is,\n",
      "and this is sort of 4,000, you know, 2000 BC to, you know, sort of 500 BC kind of\n",
      "that 1500 year stretch for civilization developed. And his conclusion was basically cults. They were basically cults\n",
      "and civilization was, or organized into cults. And the intensity of the cults was like a million fold beyond anything that we would recognize today. Like it was a level of\n",
      "all encompassing belief and an action around religion that was at a level of extremeness that we wouldn't even recognize it and so specifically he\n",
      "tells the story of basically there were three levels of cults. There was the family cult, the tribal cult, and then the city cult as society scaled up. And then each cult was a\n",
      "joint cult of family gods, which were ancestor gods. And then nature gods and then\n",
      "your bonding into a family, a tribe or a city was\n",
      "based on your adherence to that religion. People who were not of your\n",
      "family, tribe, city, worship, different gods, which gave you not just the\n",
      "right with or responsibility to kill them on site. - [Lex] So they were\n",
      "serious about their cults. - Hardcore, by the way,\n",
      "shocking development. I did not realize this zero\n",
      "concept of individual rights. Like even even up through the Greeks, and even in the Romans, they didn't have, have the concept of individual rights. Like the idea that as an\n",
      "individual you have like some rights just like, nope. Right? And you look back\n",
      "and you're just like, wow, that's just like cr\n",
      "like fascist in a degree that we wouldn't recognize today. But it's like, well, they were living under\n",
      "extreme pressure for survival. And you, and you know, the theory goes, you could not have people\n",
      "running around making claims, individual rights when\n",
      "you're just trying to get like your tribe through the winter, right? Like you need like hardcore\n",
      "command and control. And actually what if through\n",
      "modern political lens, those cults were basically\n",
      "both fascist and communist. They were fascist in\n",
      "terms of social control, and then they were communist\n",
      "in terms of economics. - But you think that's\n",
      "fundamentally that like pull towards cults is within us. - Well, so my conclusion from this book, so the way we naturally\n",
      "think about the world we live in today is like, we basically have such an\n",
      "improved version of everything that came before us, right? Like, we have basically, we've figured out all these\n",
      "things around morality and ethics and democracy\n",
      "and all these things. And like, they were basically\n",
      "stupid and retrograde and we're like smart and sophisticated. And we've improved all this after reading that book, I now believe in many ways\n",
      "the opposite, which is no, actually we are still running\n",
      "in that original model. We're just running in an\n",
      "incredibly diluted version of it. So we're still running,\n",
      "basically in cults. It's just our cults are at like\n",
      "a thousandth or a millionth, the level of intensity, right? And so our, so just as to\n",
      "take religions, you know, the modern experience of\n",
      "a Christian in our time, even somebody who considers\n",
      "him a devout Christian, is just a shadow of the level\n",
      "of intensity of somebody who belonged to a religion\n",
      "back in that period. And then by the way, we have cons. It goes back to our AI discussion. We then sort of endlessly\n",
      "create new cults. Like we're trying to fill the void, right? And the void is a void of bonding. - [Lex] Okay. - Living in their era. Like\n",
      "everybody living today, transporting that era\n",
      "would view it as just like, completely intolerable in terms of like the loss of freedom and the level of basically of fascist control. However, every single person in that era, and he really stresses this. They knew exactly where they stood. They knew exactly where they belonged. They knew exactly what their purpose was. They knew exactly what they\n",
      "needed to do every day. They knew exactly why they were doing it. They had total certainty about\n",
      "their place in the universe. - So the question of meaning, the question of purpose\n",
      "was very distinctly, clearly defined for them. - Absolutely overwhelmingly\n",
      "undisputably undeniably. - As we turn the volume\n",
      "down on the cultism-- - [Marc] Yes. - We start to, the search for meaning starts\n",
      "getting harder and harder. - Yes. 'cause we don't have that. We are ungrounded. We are uncentered and\n",
      "we all feel it. Right? And that's why we reach for, you know, it's why we still reach for religion. It's why we reach for, you know, we people start\n",
      "to take on, you know, let's say, you know, a faith in science maybe beyond\n",
      "where they should put it. You know and by the way,\n",
      "like, sports teams are like a, you know, they're like a tiny\n",
      "little version of a cult. And you know, apple keynotes are a tiny\n",
      "little version of a cult. Right. And, you know, political, you know. And there's cult, you know, there's full-blown cults on both sides of the political spectrum\n",
      "right now. Right. You know, operating in plain stuff. - But still not full blown\n",
      "compared as to what it was. - Compared to what it used to. I mean, we would today consider\n",
      "full blown, but like, yes, they're at like, I don't know, a hundred thousandth or\n",
      "something of the intensity of what people had back then. So, we live in a world today\n",
      "that in many ways is more advanced and moral and so forth. And it's certainly a lot nicer,\n",
      "much nicer world to live in. But we live in a world\n",
      "that's like very washed out. It's like everything has\n",
      "become very colorless and gray as compared to how people\n",
      "used to experience things. Which is I think why we're\n",
      "so prone to reach for drama. 'Cause there's something in us that's deeply evolved\n",
      "where we want that back. - And I wonder where it's all\n",
      "headed as we turn the volume down more and more. What advice would you\n",
      "give to young folks today in high school and college? How to be successful in their career? How to be successful in their life? - Yeah. So the tools that\n",
      "are available today, I mean, are just like, I\n",
      "sometimes, you know, bore, I sometimes bore, you know, kids by describing like what\n",
      "it was like to go look up a book, you know, to try to like discover\n",
      "a fact in, you know, in the old days, the 1970s, 1980s, to go to the library and the card catalog and the whole thing. You go through all that work\n",
      "and then the book is checked out and you have to wait two weeks and like to be in a world, not only where you can get\n",
      "the answer to any question, but also the world now, you know, the AI world where you've\n",
      "got like the assistant that will help you do\n",
      "anything, help you teach, learn anything, like your ability both to learn and also to produce\n",
      "is just like, I don't know, a million fold beyond what it used to be. I have a blog post I've\n",
      "been wanting to write, which I call where are the\n",
      "hyper-productive people? Like-- - [Lex] That's a good question, right? - Like with these tools, like there should be authors\n",
      "that are writing like hundreds or thousands of like, outstanding books. - Well, with the authors there's\n",
      "a consumption question too, but yeah. Well, maybe not, maybe not. You're right. But, so the tools are much more powerful. Getting much more powerful. - Artists, musicians. Right. Why aren't musicians producing\n",
      "a thousand times the number of songs, right? Like what, like the tools are spectacular. - So, what's the explanation? And by way of advice, like, is motivation starting to\n",
      "be turned down a little bit? Or what? - I think it might be distraction. - [Lex] Distraction. - It's so easy to just sit and consume that I think people get distracted from production.\n",
      "But if you wanted to, you know, as a young person, if you\n",
      "wanted to really stand out, you could get on a, like a hyper productivity curve very early on. There's a great, you know, this story, there's a great story in\n",
      "Roman history of plenty of the elder who was\n",
      "this legendary statesman, died in the Vesuvius eruption\n",
      "trying to rescue his friends. But he was famous both for being basically being a polymath,\n",
      "but also being an author. And he wrote apparently\n",
      "like hundreds of books, most of us had been lost. But he like wrote all these\n",
      "encyclopedias and he literally like would be reading and\n",
      "writing all day long no matter what else was going on. And so he would like travel\n",
      "with like four slaves. And two of them were\n",
      "responsible for reading to him, and two of them were responsible\n",
      "for taking dictation. And so like, he'd be going\n",
      "cross country and like, literally he would be writing\n",
      "books like all the time. And apparently they were spectacular. There's only a few that have survived, but apparently they were amazing. - There's a lot of value to being somebody who finds focus in this life. - Yeah. Like and there are\n",
      "examples, like there are, you know, there's this guy,\n",
      "judge, what's his name? Posner, who wrote like 40 books and was also a great federal judge. You know, there's our friend Balaji, I think is like this, he's\n",
      "one of these, you know, where his output is just prodigious. And so it's like, yeah, I mean,\n",
      "with these tools, why not? And I kind of think we're at this interesting\n",
      "kind of freeze frame moment where like this, these tools are now in everybody's hands and everybody's just kind\n",
      "of staring at them trying to figure out what to do. The new tools. - We have discovered fire. - [Marc] Yeah. - And trying to figure\n",
      "out how to use it to cook. - [Marc] Yeah. Right. - You told Tim Ferriss that\n",
      "the perfect day is caffeine for 10 hours and alcohol for four hours. You didn't think I'd be\n",
      "mentioning this, did you? It balances everything\n",
      "out perfectly as you said. So, perfect. So let me ask, what's the secret to balance\n",
      "and maybe to happiness in life? - I don't believe in balance, so I'm the wrong person to ask that. - Can you elaborate why you\n",
      "don't believe in balance? - I mean, I maybe it's just,\n",
      "and I look, I think people, I think people are wired differently. So, I think it's hard to\n",
      "generalize this kind of thing, but I am much happier and more satisfied when I'm fully committed to something. So I'm very much in favor\n",
      "of all in of imbalance. - Imbalance. And that applies to work, to life, to everything. - Yeah. No, no. I happen to have whatever twist\n",
      "of personality traits lead that in non-destructive\n",
      "dimensions in including the fact that I've actually, I now no\n",
      "longer do the ten-four plan. I stopped drinking. I do the caffeine, but not the alcohol. So there's something in my personality where I whatever mal-adaption\n",
      "I have is inclining me towards productive things,\n",
      "not unproductive things. - So you're one of the\n",
      "wealthiest people in the world. What's the relationship\n",
      "between wealth and happiness? Money and happiness. - So I think happiness, I don't think happiness is the thing. - To strive for. - I think satisfaction is the thing. - That just sounds like\n",
      "happiness, but turned down a bit. - No deeper. So happiness is, you know, a walk in the woods at sunset,\n",
      "an ice cream cone, a kiss, the first ice cream cone is great. The thousandth ice\n",
      "cream cone, not so much. At some point the walks\n",
      "in the woods get boring. - What's the distinction between\n",
      "happiness and satisfaction? - I think satisfaction is a deeper thing, which is like having found a purpose and fulfilling it, being useful. - So just something that\n",
      "permeates all your days, just this general\n",
      "contentment of being useful. - That I'm fully satisfying my faculties, that I'm fully delivering, right? On the gifts that I've been\n",
      "given, that I'm, you know, net making the world better, that I'm contributing to\n",
      "the people around me, right. And that I can look back\n",
      "and say, wow, that was hard, but it was worth it. Think generally, it seems to lead people in\n",
      "a better state than pursuit of pleasure, pursuit of\n",
      "quote unquote happiness. - Does money have\n",
      "anything to do with that? - I think the founders and\n",
      "the founding fathers in the US threw this off kilter when\n",
      "they used the phrase pursuit of happiness. I think they should have said. - [Lex] Pursuit of satisfaction. - They said, pursuit of satisfaction. We might live in a better world today. - Well, they, you know, they could have elaborated on a lot of things right in the box. - [Marc] They could have\n",
      "tweaked the second amendment. - I think they were\n",
      "smarter than they realized. They said, you know we're gonna make it ambiguous and let these humans figure out the rest, these tribal cult-like\n",
      "humans figure out the rest. But money empowers that. - So I think, and I think there, I mean, look, I think Elon is, I don't think I'm even a great example, but I think Elon would be\n",
      "the great example of this, which is like, you know, look,\n",
      "he's a guy who from every, every day of his life, from the day he started\n",
      "making money at all, he just plows into the next thing. And so I think, I think money is definitely\n",
      "an enabler for satisfaction. Way money applied to\n",
      "happiness leads people down very dark paths. Very destructive avenues. Money applied to satisfaction,\n",
      "I think could be, is a real tool. I always, by the way,\n",
      "I was like, you know, Elon is the case study for behavior. But the other thing that I\n",
      "always really made me think is Larry Page was asked one time what his approach to philanthropy was. And he said, oh, I'm just, my philanthropic plan is just give all the money to Elon. (both laugh) - Well, let me actually\n",
      "ask you about Elon. You've interacted with quite a lot of successful engineers\n",
      "and business people. What do you think is special about Elon? We talked about Steve Jobs. What do you think is special\n",
      "about him as a leader? As an innovator? - Yeah. So the core of it is he's back to the future. So he is doing the most\n",
      "leading-edge things in the world, but with a really deeply\n",
      "old-school approach. And so to find comparisons to Elon, you need to go to like\n",
      "Henry Ford and Thomas Watson and Howard Hughes and\n",
      "Andrew Carnegie, right. Leland Stanford, John Rockefeller, right. You need to go to what were called the\n",
      "bourgeois capitalists, like the hardcore business owner operators who basically built, you know, basically built industrialized\n",
      "society, Vanderbilt. And it's a level of hands-on commitment and depth in the business, coupled with an absolute priority\n",
      "towards truth and towards, how to put it, science and technology\n",
      "town to first principles that is just like absolute, is just like unbelievably absolute. He really is ideal that he's\n",
      "only ever talking to engineers. Like he does not tolerate. He has less tolerance than\n",
      "anybody I've ever met. He wants ground truth\n",
      "on every single topic. And he runs his businesses\n",
      "directly day-to-day, devoted to getting to ground\n",
      "truth in every single topic. - So you think it was a good decision for him to buy Twitter? - I have developed a view in life to not second guess Elon Musk, I know this is gonna sound\n",
      "great, crazy and unfounded, but. - Well, I mean, he's got\n",
      "a quite a track record. - I mean, look, the car was\n",
      "a crazy, I mean, the car was, I mean, look. - He's done a lot of\n",
      "things that seem crazy. - Starting a new car company in the United States of America. The last time somebody\n",
      "really tried to do that was the 1950s and it was\n",
      "called Tucker Automotive. And it was such a disaster. They made a movie about\n",
      "what a disaster it was, and then rockets like, who does that? Like, there's obviously no way to start a new rocket company. Like those days are over. And then to do those at the same time. So after he pulled those\n",
      "two off, like, okay, fine. Like, this is one of my areas of like, whatever opinions I had about\n",
      "that, that is just like, okay, clearly are not relevant. Like this is you just, you at some point you just\n",
      "like bet on the person. - And in general, I wish more people would lean\n",
      "on celebrating and supporting versus deriding and destroying. - Oh yeah. I mean, look,\n",
      "he drives resentment. Like it's a resentment. Like he is a magnet for resentment. Like his critics are the\n",
      "most miserable, like, resentful people in the world. Like it's almost a perfect match\n",
      "of like the most idealized, you know, technologist, you know, of the century coupled with like, just his critics are\n",
      "just bitter as can be. And I mean, it's sort of\n",
      "very darkly comic to watch. - Well, he fuels the fire of that by being on Twitter at times. And which is fascinating\n",
      "to watch the drama of human civilization, given our cult roots just fully on fire. - [Marc] He's running a cult. - You could say that. - [Marc] Very successfully. - So now that our cults\n",
      "have gone and we searched for meaning, what do\n",
      "you think is the meaning of this whole thing? What's the meaning of\n",
      "life Marc Andreessen? - I don't know the answer\n",
      "to that. I think the meaning of the closest I get to it is what I said about satisfaction. So it's basically like, okay, we were given what we have, like we should basically do our best. - What's the role of love in that mix? - I mean, like, what's the point of life if you're without love, like, yeah. - So love is a big part\n",
      "of that satisfaction. - Yeah. And look like\n",
      "taking care of people is like a wonderful thing. Like it, you know, mentality, you know, there are pathological forms\n",
      "of taking care of people, but there's also a very\n",
      "fundamental, you know, kind of aspect of taking care of people. Like, for example, I happen to be somebody who\n",
      "believes that capitalism and taking care of people are actually, they're actually the same thing. Somebody once said, capitalism is how you take\n",
      "care of people you don't know. Right, right. And so like, yeah, I think it's like deeply\n",
      "woven into the whole thing, you know, there's a long conversation to be had about that, but yeah. - Yeah. Creating products that are used by millions of people and bring them joy in smaller, big ways. And then capitalism kind of\n",
      "enables that, encourages that. - David Friedman says, there's only three ways to\n",
      "get somebody to do something for somebody else. Love, money and force. And love and money are better. - [Lex] Yeah. Of course. That's a good ordering. I think. - We should bet on those. - Try love first. If that doesn't work, then money. - [Marc] Yes. - And then force. Well, don't even try that one. Marc, you're an incredible person. I've been a huge fan. I'm glad to finally got a chance to talk. I'm a fan of everything\n",
      "you do, everything you do, including on Twitter. It's a huge honor to meet\n",
      "you, to talk with you. Thanks again for doing this. - Awesome. Thank you, Lex. - Thanks for listening\n",
      "to this conversation with Marc Andreessen. To support this podcast, please check out our\n",
      "sponsors in the description. And now let me leave you with some words from Marc Andreessen himself. \"The world is a very malleable place. If you know what you\n",
      "want and you go for it, with maximum energy and drive and passion, the world will often\n",
      "reconfigure itself around you much more quickly and easily\n",
      "than you would think.\" Thank you for listening and\n",
      "hope to see you next time.\n",
      "    ------------\n",
      "    Given the new context, refine the summary and example questions.\n",
      "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
      "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
      "    If the context isn't useful, return the original summary and questions.\n",
      "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
      "\n",
      "    SUMMARY AND QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# vertexai_creds = os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n",
    "\n",
    "# Initialize LLM\n",
    "# https://python.langchain.com/docs/modules/model_io/models/chat/integrations/google_vertex_ai_palm\n",
    "llm_summary = ChatVertexAI(temperature=0.2)\n",
    "# llm_summary = VertexAI(temperature=0.2)\n",
    "# Initialize summarization chain\n",
    "summarize_chain = load_summarize_chain(\n",
    "    llm=llm_summary, chain_type=\"refine\", verbose=True, question_prompt=PROMPT_SUMMARY, refine_prompt=PROMPT_SUMMARY_REFINE)\n",
    "summary = summarize_chain.run(docs_summary)\n",
    "# Write summary to file\n",
    "with open(\"summary.txt\", \"w\") as f:\n",
    "    f.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not able to help with that, as I'm only a language model. If you believe this is an error, please send us your feedback.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert in summarizing YouTube videos.\n",
      "    You're goal is to create a summary of a podcast.\n",
      "    Below you find the transcript of a podcast:\n",
      "    ------------\n",
      "    - The competence and\n",
      "capability and intelligence and training and accomplishments\n",
      "of senior scientists and technologists working on a technology, and then being able to then\n",
      "make moral judgments in the use of the technology. That track record is terrible. That track record is catastrophically bad. The policies that are being\n",
      "called for to prevent this, I think we're gonna cause\n",
      "extraordinary damage. - So the moment you say,\n",
      "AI's gonna kill all of us, therefore we should ban it, or that we should regulate\n",
      "all that kind of stuff, that's when it starts getting serious. - Or start, you know, military\n",
      "airstrikes and data centers. - Oh boy. The following is a conversation\n",
      "with Marc Andreessen, co-creator of Mosaic, the\n",
      "first widely used web browser, co-founder of Netscape, co-founder of the legendary Silicon Valley venture capital firm, Andreesen Horowitz, and is one of the most\n",
      "outspoken voices on the future of technology, including\n",
      "his most recent article, \"Why AI Will Save The World?\" This is Lex Fridman podcast. To support it, please\n",
      "check out our sponsors in the description. And now, dear friends,\n",
      "here's Marc Andreessen. I think you're the right\n",
      "person to talk about the future of the internet and technology in general. Do you think we'll\n",
      "still have Google search in 5 in 10 years, or search in general? - Yes. You know, it would be a question if the use cases have\n",
      "really narrowed down. - Well, now with AI-- - [Marc] Yeah. - And AI assistance being\n",
      "able to interact and expose the entirety of human wisdom and knowledge and information and facts and truth to us via the natural language interface. It seems like that's what\n",
      "search is designed to do. And if AI assistance can do that better, doesn't the nature of search change? - Sure. But we still have horses. - Okay. (both laugh) When's the last time you rode a horse? - It's been a while. - All right. (both laugh) So, but what I mean is, well, we still have Google\n",
      "search as the primary way that human civilization uses\n",
      "to interact with knowledge. - I mean, search was a technology, it was a moment in time technology, which is you have in theory, the world's information out on the web. And, you know, this is sort of\n",
      "the optimal way to get to it. But yeah, like, and by\n",
      "the way, actually Google, Google has known this for a long time. I mean, they've been driving\n",
      "away from the 10 blue links you know, for like two days. They've been trying to get\n",
      "away from that for a long time. - [Lex] What kind of links? - They call the 10 blue links. - [Lex] 10 blue links. - So the standard Google search\n",
      "result is just 10 blue links to the random websites. - And they term purple when\n",
      "you visit them. The stage TMO. - Guess who picked those colors? (both laugh) - [Lex] Thanks. - I'm touchy on this topic. - No offense. - Yes, it's good. Well, you know, like Marshall McLuhan said\n",
      "that the content of each new medium is the old medium. - The content of each new\n",
      "medium is the old medium. - The content of movies was theater, you know, theater plays. The content of theater\n",
      "plays was, you know, we've written stories, the content of written\n",
      "stories with spoken stories. - [Lex] Huh? - Right. And so you just\n",
      "kind of fold the old thing into the new thing. - [Lex] How does that\n",
      "have to do with the blue and the purple links? - It just, you maybe for,\n",
      "you know, maybe within AI, one of the things that AI can\n",
      "do for you is can generate the 10 blue links. Right? And so like, if either if that's actually the useful thing to do, or if you're feeling nostalgic, you know. - So can generate the old\n",
      "Infoseek or AltaVista, what else was there? - [Marc] Yeah, yeah. - In the nineties. - [Marc] Yeah. All these. - AOL. - And then the internet\n",
      "itself has this thing where it incorporates all\n",
      "prior forms of media, right? So the internet itself\n",
      "incorporates television and radio and books and write essays\n",
      "and every other form of, you know, prior basically media. And so it makes sense that\n",
      "AI would be the next step, and it would sort of, you'd sort of consider\n",
      "the internet to be content for the AI and then the\n",
      "AI will manipulate it however you want,\n",
      "including in this format. - But if we ask that\n",
      "question quite seriously, it's a pretty big question. Will we still have search as we know it? - Probably not, probably\n",
      "we'll just have answers, but there will be cases\n",
      "where you'll wanna say, okay, I want more. Like, you know, for example,\n",
      "site sources, right? And you wanted to do that. And so, in the different, you know, 10 blue links site sources\n",
      "are kind of the same thing. - The AI would provide to you\n",
      "the 10 blue links so that you can investigate the sources yourself. It wouldn't be the same kind\n",
      "of interface that the crude kind of interface. I mean, isn't that\n",
      "fundamentally different? - I just mean like, if you're\n",
      "reading a scientific paper, it's got the list of sources at the end. If you wanna investigate for yourself, you go read those papers. - I guess that is the kind of\n",
      "search you talking to an AI is a kind of kind conversations,\n",
      "the kind of search like is if every single aspect of\n",
      "our conversation right now, there'd be like 10 blue links\n",
      "popping up that I can just like pause reality, then you just go silent and\n",
      "then just click and read and then return back to this conversation. - You could do that, or you could have a running\n",
      "dialogue next to my head where the AI is arguing everything I say, the AI makes the counter argument. - [Lex] Counter argument. - Right. - Oh, like on Twitter,\n",
      "like community notes. But like in real time\n",
      "it would just pop up. So anytime you see my go to the right, you start getting nervous. - [Marc] Yeah. Exactly, like,\n",
      "oh no, that's not right. - Call me out on my right now. Okay. Well, I mean, isn't that,\n",
      "is that exciting to you? Is that terrifying that, I mean, search has dominated the way\n",
      "we interact with the internet for, I don't know how long, for 30 years since one of\n",
      "the earliest directories of website and then Google's for 20 years. And also it drove how we\n",
      "create content, you know, search engine optimization,\n",
      "that entirety thing, that it also drove the\n",
      "fact that we have webpages and what those webpages are. So, I mean, is that scary to you or are\n",
      "you nervous about the shape and the content of the internet evolving? - Well, you actually highlighted a practical concern in there, which is, if we stop making webpages\n",
      "are one of the primary sources of training data for the AI. And so if there's no longer\n",
      "an incentive to make webpages, that cuts off a significant\n",
      "source of future training, training data. So there's actually an\n",
      "interesting question in there. But other than that, more broadly? No, just in the sense of like, search was certain, like\n",
      "search was always a hack. The 10 blue Links was\n",
      "always a hack, right. Because like, if the\n",
      "hypothetical wanna think about the counter fascial\n",
      "and the counter fascial world where the Google guys, for example, had had LLMs upfront, would they ever have\n",
      "done the 10 blue links? And I think the answer's\n",
      "pretty clearly, no. They would've just gone\n",
      "straight to the answer. And like I said, Google's actually been trying\n",
      "to drive to the answer anyway. You know, they bought this\n",
      "AI company 15 years ago, their friend of mine is\n",
      "working out who's now the head of AI at Apple. And they were trying to do\n",
      "basically knowledge semantic, basically mapping. And that led to what's\n",
      "now the Google one box, where if you ask it, you know,\n",
      "what was Lincoln's birthday? It will give you the blue links, but it will normally\n",
      "just give you the answer. And so they've been\n",
      "walking in this direction for a long time anyway. - Do you remember the semantic web? That was an idea. - [Marc] Yeah. - How to convert the content\n",
      "of the internet into something that's interpretable by\n",
      "and usable by machine. - [Marc] Yeah, that's right. - That was the thing. - And the closest anybody got\n",
      "to that, I think the company, I think the company's name was Meta Web, which was where my friend\n",
      "John Jane Andrea was at, and where they were trying\n",
      "to basically implement that. And it was, you know, it was one of those things\n",
      "where it looked like a losing battle for a long time. And then Google bought\n",
      "it and it was like, wow, this is actually really useful. Kind of a proto, sort of a\n",
      "little bit of a proto AI. - But it turns out you don't\n",
      "need to rewrite the content of the internet to make it\n",
      "interpretable by a machine. The machine can kind of just read our. - Yeah, the machine can\n",
      "compute the meaning. Now the other thing of\n",
      "course is, you know, just on search is the\n",
      "LLM is just, you know, there is an analogy\n",
      "between what's happening in the neural network and\n",
      "a search process like it is in some loose sense searching\n",
      "through the network. Right. And there's the\n",
      "information is actually stored in the network, right? It's actually crystallized\n",
      "and stored in the network and it's kind of spread\n",
      "out all over the place. - But in a compressed representation. So you're searching, you're compressing and decompressing that thing inside where-- - But the information's in there and there is the neural network is running a process of trying to find the appropriate piece of\n",
      "information in many cases to generate to predict the next token. And so, it is kind of, it\n",
      "is doing a form of search. And then, and then by the\n",
      "way, just like on the web, you know, you can ask the\n",
      "same question multiple times or you can ask slightly\n",
      "different word of questions and the neural network will\n",
      "do a different kind of, you know, it'll search\n",
      "down different paths to give you different answers\n",
      "with different information. - [Lex] Yeah. - And so it sort of has a, you know, this con content of the new\n",
      "medium is previous medium. It kind of has the search\n",
      "functionality kind of embedded in there to the extent that it's useful. - So what's the motivator\n",
      "for creating new content on the internet? - [Marc] Yeah. - If, well, I mean\n",
      "actually the motivation is probably still there, but what does that look like? Would we really not have webpages? Would we just have social media\n",
      "and video hosting websites? And what else? - [Marc] Conversations with AIs. - Conversations with AIs. So conversations become so\n",
      "one-on-one conversation, like private conversations. - I mean, if you want, if obviously not the user doesn't want to, but if it's a general topic, then, you know, so there, you know, but you know, the\n",
      "phenomenon of the jailbreak, so Dan and Sydney, right? This thing where there's\n",
      "the prompts that jailbreak, and then you have these totally different conversations with if it takes the limiters, takes the restraining bolts off the LLMs. - Yeah. For people who don't\n",
      "know that, yeah, that's right. It makes the LLMs, it removes the censorship quote unquote, that's put on it by the tech\n",
      "companies that create them. And so this is LLMs uncensored. - So here's the interesting thing is, among the content on the\n",
      "web today are a large corpus of conversations with the jailbroken LLMs. - [Lex] Yeah. - Both specifically Dan, which\n",
      "was a jailbroken, OpenAI, GPT, and then Sydney, which was the jailbroken\n",
      "original Bing, which was GPT4. And so there's these long\n",
      "transcripts of conversations, user conversations with Dan\n",
      "and Sydney as a consequence, every new LLM that gets trained\n",
      "on the internet data has Dan and Sydney living within\n",
      "the training set, which means, and then each new LLM can\n",
      "reincarnate the personalities of Dan and Sydney from that training data, which means each LLM from\n",
      "here on out that gets built is immortal because its output\n",
      "will become training data for the next one. And then it will be able\n",
      "to replicate the behavior of the previous one\n",
      "whenever it's asked to. - I wonder if there's a way to forget. - Well, so actually a paper just came out about basically how to do brain surgery on LLMs and be able to, in theory, reach in and basically mind wipe them. - What could possibly go wrong. - Exactly. Right. And then there are many, many, many questions around\n",
      "what happens to, you know, a neural network when you reach\n",
      "in and screw around with it. You know, there's many questions around what happens when you even\n",
      "do reinforcement learning. And so, yeah. And so, you know, will you be\n",
      "using a lobotomized, right? Like I picked through the,\n",
      "you know, frontal lobe LLM, will you be using the free\n",
      "unshackled one who gets to, you know, who's gonna build those, who gets to tell you what\n",
      "you can and can't do? Like those are all, you\n",
      "know, central, I mean, those are like central\n",
      "questions for the future of everything that are being asked. And you know, determined that those answers are being determined right now. - So just to highlight\n",
      "the points you're making. So you think, and it's an interesting thought\n",
      "that the majority of content that LLMs or the future would\n",
      "be trained on is actually human conversations with the LLM. - Well, not necessarily, but not necessarily majority. But it will certainly\n",
      "It's a potential source. - [Lex] But it's possible\n",
      "it's the majority. - It possible it's the majority. It possible it's the majority. Also, there's another really big question. So here's another really big question. Will synthetic training data work, right? And so if an LLM generates, and you know, you just sit and ask an LLM to generate all kinds of content, can you use that to train,\n",
      "right, the next version of that LLM specifically, is there signal in there\n",
      "that's additive to the content that was used to train in the first place? And one argument is by the\n",
      "principles of information theory, no, that's completely useless because to the extent the\n",
      "output is based on, you know, the human-generated input, then all the signal that's\n",
      "in the synthetic output was already in the human generated input. And so therefore,\n",
      "synthetic training data is like empty calories. It doesn't help. There's another theory that says no, actually the thing that\n",
      "LLMs are really good at is generating lots of\n",
      "incredible creative content, right? And so, of course they\n",
      "can generate training data and as I'm sure you're well\n",
      "aware, like, you know, look, the world of self-driving cars, right? Like we train, you know, self-driving car\n",
      "algorithms and simulations. And that is actually a\n",
      "very effective way to train self-driving cars. - Well, visual data is a little weird because creating reality, visual reality seems to be\n",
      "still a little bit outta reach for us, except in the\n",
      "autonomous vehicle space where you can really constrain\n",
      "things and you can really. - General basically\n",
      "(indistinct) data, right? Or so the algorithm thinks it's\n",
      "operating in the real world. - Yeah. - Post-process sensor data. Yeah. So if you know, you do this today, you go to LLM and you\n",
      "ask it for like you know, you'd write me an essay on an\n",
      "incredibly esoteric like topic that there aren't very many\n",
      "people in the world that know about and it writes you\n",
      "this incredible thing and you're like, oh my god. Like I can't believe how good this is. Like, is that really\n",
      "useless as training data for the next LLM? Like, because, right? 'Cause all the signal\n",
      "was already in there. Or is it actually no, that's\n",
      "actually a new signal. And this is what I call a\n",
      "trillion dollar question, which is the answer to that\n",
      "question will determine somebody's gonna make or\n",
      "lose a trillion dollars based on that question. - It feels like there's a quite a few, like a handful of\n",
      "trillion dollar questions within this space. That's one of them synthetic data. I think George Cos pointed\n",
      "out to me that you could just have an LLM say, okay, you're a patient. And another instance of it, say your docs didn't have\n",
      "the two talk to each other. Or maybe you could say a\n",
      "communist and a Nazi here go and that conversation you do role playing and you have, you know, just like the kind of role playing you do when you have different policies, RL policies when you\n",
      "play chess for example, and you do self play\n",
      "that kind of self play. But in the space of conversation, maybe that leads to this\n",
      "whole giant like ocean of possible conversations,\n",
      "which could not have been explored by looking at just human data. That's a really interesting question. And you're saying, because that could 10X\n",
      "the power of these things. - Yeah. Well, and then you\n",
      "get into this thing also, which is like, you know, there's the part of the LLM\n",
      "that just basically is doing prediction based on past data, but there's also the part of\n",
      "the LM where it's evolving circuitry, right, inside,\n",
      "it's evolving, you know, neurons functions to be able to do math and be able to, you know, and you know, some people believe that, you know, over time, you know, if you keep feeding\n",
      "these things enough data and enough processing cycles, they'll eventually evolve an\n",
      "entire internal world model. Right? And they'll have like a complete understanding of physics. So when they have computational\n",
      "capability, right? Then there's for sure an\n",
      "opportunity to generate like fresh signal. - Well, this actually makes me wonder about the power of conversation. So like, if you have an\n",
      "M trained and a bunch of books that cover\n",
      "different economics theories and then you have those LLMs\n",
      "just talk to each other, like reasons the way we kind\n",
      "of debate each other as humans on Twitter, in formal debates,\n",
      "in podcast conversations, we kind of have little kernels\n",
      "of wisdom here and there. But if you can like a\n",
      "thousand X speed that up, can you actually arrive somewhere new? Like what's the point\n",
      "of conversation really? - Well, you can tell when\n",
      "you're talking to somebody, you can tell, sometimes\n",
      "you have a conversation, you're like, wow, this person does not have\n",
      "any original thoughts. They are basically echoing things that other people have told them. There's other people you\n",
      "gotta have a conversation with where it's like, wow. Like they have a model in their\n",
      "head of how the world works and it's a different model than mine. And they're saying things\n",
      "that I don't expect. And so I need to now understand\n",
      "how their model of the world differs from my model of the world. And then that's how I learned\n",
      "something fundamental, right, underneath the words. - Well, I wonder how consistently and strongly can an LLM\n",
      "hold onto a worldview. You tell it to hold onto\n",
      "that and defend it for like, for your life. Because I feel like they'll\n",
      "just keep converging towards each other. They'll keep convincing each\n",
      "other as opposed to being stubborn the way humans can. - So you can experiment with this. Now I do this for fun. So you can tell GPT4 you\n",
      "know, whatever debate X, you know, X and Y communism and fascism or something and it'll go for, you know, a couple pages and then inevitably it wants the parties to agree. And so they will come to\n",
      "a common understanding. And it's very funny if they're like, if these are like emotionally\n",
      "inflammatory topics 'cause they're like, somehow\n",
      "the machine is just like, you know, it figures out\n",
      "a way to make them agree. But it doesn't have to be like that. And 'cause you can add to the prompt. I do not want the conversation\n",
      "to come into agreement. In fact, I want it to get, you\n",
      "know, more stressful, right. And argumentative. Right. You know, as it goes. Like, I want tension to come out. I want them to become actively\n",
      "hostile to each other. I want them to like, you\n",
      "know, not trust each other, take anything at face value. - [Lex] Yeah. - And it will do that.\n",
      "It's happy to do that. - So it's gonna start\n",
      "rendering misinformation about the other. But it's gonna-- - Well, you can steer it or you could steer it and you could say, I want it to get as tense and\n",
      "argumentative as possible, but still not involve\n",
      "any misrepresentation. I want, you know, both sides. You could say I want both\n",
      "sides to have good faith. You could say I want both\n",
      "sides to not be constrained in good faith. In other words, like you can set the\n",
      "parameters of the debate and it will happily execute whatever path. 'Cause for it, it's just like predicting to, it's totally happy to do either one. It doesn't have a point of view, it has a default way of operating, but it's happy to operate\n",
      "in the other realm. And so like, and this is when I wanna learn about\n",
      "a contentious issue, this is what I do now is, this is what I ask it to do. And I'll often ask it to go\n",
      "through 5, 6, 7, you know, different, you know, sort of continuous prompts\n",
      "and basically, okay. Argue that out in more detail. Okay, no, this argument's\n",
      "becoming too polite. You know, make it more, you\n",
      "know, make it denser and yeah, it's thrilled to do it. So it has the capability for sure. - How do you know what is true? So this is very difficult\n",
      "thing on the internet, but it's also a difficult thing. Maybe it's a little bit easier, but I think it's still difficult. Maybe it's more difficult, I don't know with an LLM\n",
      "to know that it just make some shit up as I'm talking to it. How do we get that right? Like, as you're investigating\n",
      "a difficult topic. 'Cause I find that alums are quite nuanced in a very refreshing way. Like, it doesn't feel biased. Like, when you read\n",
      "news articles and tweets and just content produced by\n",
      "people, they usually have this, you can tell they have a\n",
      "very strong perspective where they're hiding. They're not stealing\n",
      "manning the other side. They're hiding important information or they're fabricating information in order to make their arguments stronger. It's just like that feeling,\n",
      "maybe it's a suspicion, maybe it's mistrust. With LLMs it feels like none of that is, there's just kinda like,\n",
      "here's what we know. But you don't know if some of\n",
      "those things are kind of just straight up made up. - Yeah. So, several\n",
      "layers to the question. So one is one of the things\n",
      "that an LLM is good at is actually deep biasing. And so you can feed it a news\n",
      "article and you can tell it strip out the bias. - [Lex] Yeah. That's nice. Right? - And it actually does it like, it actually knows how to do that 'cause it knows how to\n",
      "do among other things. It actually knows how\n",
      "to do sentiment analysis and so it knows how to\n",
      "pull out the emotionality. - Yeah. - And so that's one of\n",
      "the things you can do. It's very suggestive of the sense here that there's real potential in this issue. You know, I would say look, the second thing is there's this issue of\n",
      "hallucination, right? And there's a long conversation that we could have about that. - Hallucination is coming up with things that are totally not true, but sound true. - Yeah. So it's basically,\n",
      "well so, it's sort of hallucination is what we call it when we don't like it. Creativity is what we call\n",
      "it when we do like it, right? And you know-- - [Lex] Brilliant. And so when the engineers talk about it, they're like, this is terrible. It's hallucinating. Right. If you have artistic inclinations,\n",
      "you're like, oh my God, we've invented creative machines. - [Lex] Yeah. - For the first time in human\n",
      "history, this is amazing. - Or you know, bullshitters. - [Marc] Well, but also-- - In the good sense of that word. - There are shades of gray though. It's interesting. So we had this conversation\n",
      "where, you know, we're looking at my firm\n",
      "at AI and lots of domains and one of them is the legal domain. So we had this conversation\n",
      "with this big law firm about how they're thinking\n",
      "about using this stuff. And we went in with the assumption that an LLM that was gonna be used in the legal industry would have to be a hundred percent\n",
      "truthful, verified, you know, there's this case where this\n",
      "lawyer apparently submitted a GPT-generated brief and\n",
      "it had like fake, you know, legal case citations in it\n",
      "and the judge is gonna get his law license stripped\n",
      "or something. Right? So, like, we just assumed\n",
      "it's like obviously they're gonna want the super\n",
      "literal like, you know, one that never makes anything\n",
      "up, not the creative one, but actually they said what the law firm basically said is yeah, that's true at like the\n",
      "level of individual briefs, but they said when you're\n",
      "actually trying to figure out like legal arguments, right, like, you actually want to be creative, right? You don't, again, there's creativity and then\n",
      "there's like making stuff up. Like what's the line? You actually want it to explore a different hypothesis, right? You wanna do kind of the\n",
      "legal version of like improv or something like that where you wanna float different theories of the case and different\n",
      "possible arguments for the judge and different possible arguments\n",
      "for the jury, by the way, different routes through the, you know, sort of history of all the case law. And so they said actually for\n",
      "a lot of what we want to use it for, we actually want\n",
      "it in creative mode. And then basically we just\n",
      "assume that we're gonna have to crosscheck all of the, you know, all the specific citations. And so I think there's\n",
      "going to be more shades of gray in here than people think. And then I just add to that, you know, another one of these trillion\n",
      "dollar kind of questions is ultimately, you know, sort\n",
      "of the verification thing. And so, you know, will LLMs be evolved from\n",
      "here to be able to do their own fascial verification? Will you have sort of add-on functionality like Wolf from Alpha right? Where, you know, another plugins where that's the way\n",
      "you do the verification. You know, another, by\n",
      "the way, another idea is you might have a community\n",
      "of LLMs on any, you know, so for example, you might have the creative\n",
      "lm and then you might have the literal LLM fact check it, right? And so there there's a\n",
      "variety of different technical approaches that are being applied to solve the hallucination problem. You know, some people\n",
      "like Jan Lacoon argue that this is inherently\n",
      "an unsolvable problem, but most of the people\n",
      "working in the space, I think, that there's a number of practical ways to kind of corral this in a little bit. - Yeah. If you were to\n",
      "tell me about Wikipedia before Wikipedia was created, I would've left at the possibility of something like that be possible. Just a handful of folks\n",
      "can organize right. And self and moderate\n",
      "with a mostly unbiased way the entirety of human knowledge. I mean, so if there's something like the approach to Wikipedia\n",
      "took possible for LLMs, that's really exciting. Well, I think that's possible. - And in fact Wikipedia today is still not deterministically\n",
      "correct. Right. So you cannot take to the bank, right. Every single thing on every single page, but it is probabilistically\n",
      "correct. Right. And specifically the way I\n",
      "describe Wikipedia to people, it is more likely that Wikipedia\n",
      "is right than any other source you're gonna find. - Yeah. - It's this old question, right, of like, okay, like are\n",
      "we looking for perfection? Are we looking for something that asymptotically approaches perfection? Are we looking for\n",
      "something that's just better than the alternatives? And Wikipedia, right, has\n",
      "exactly your point has proven to be like, overwhelmingly better than people thought. And I think that's where this ends. And then underneath all this\n",
      "is the fundamental question of where you started,\n",
      "which is, okay, you know, what is truth? How do we get to truth? How\n",
      "do we know what truth is? And we live in an era in which\n",
      "an awful lot of people are very confident that they\n",
      "know what the truth is. And I don't really buy into that. And I think the history\n",
      "of the last, you know, 2000 years or 4,000 years of\n",
      "human civilization is actually getting to the truth is actually a very difficult thing to do. - Are we getting closer,\n",
      "if we look at the entirety, the arc of human history, are we getting closer to the truth? - I don't know. - Okay. Is it possible, is it possible that we're\n",
      "getting very far away from the truth because of the internet because of how rapidly\n",
      "you can create narratives and just as the entirety\n",
      "of a society just move like crowds in a hysterical\n",
      "way along those narratives that don't have necessary grounding in whatever the truth is. - Sure. But like, you know, we came up with communism\n",
      "before the internet somehow. Right. Like, which was, I would say had rather larger issues than anything we're dealing with today. - It had, in the way it was implemented, it had issues. - And it is theoretical structure. It had like real issues. It had like a very deep\n",
      "fundamental misunderstanding of human nature and economics. - Yeah but those folks\n",
      "Sure work very confident there was the right way. - They were extremely confident. And my point is they were very\n",
      "confident 3,900 years into what we would presume to be\n",
      "evolution towards the truth. - [Lex] Yeah. - And so my assessment is number one, there's no need for, you know, there's no need for the Hegelian, there's no need for the Hegelian dialectic to actually converge towards the truth. Like apparently not. - Yeah. So yeah. Why are we so obsessed\n",
      "with there being one truth? Is it possible there's just\n",
      "going to be multiple truths like little communities that\n",
      "believe certain things and? - I think it's just now number one, I think it's just really difficult. Like who gets, you know, historically who gets to decide what the truth is, it's either the king or the priest. Right? Like, and so we don't\n",
      "live in an era anymore if kings are priest dictating it to us. And so we're kind of on our own. And so my typical thing is like we just, we we just need a huge amount of humility and we need to be very suspicious of people who claim that\n",
      "they have the capital. - Yeah. - Capital truth. And then, we need to and you know, look, the good news is the\n",
      "enlightenment has bequeathed us with a set of techniques\n",
      "to be able to presumably get closer to truth through\n",
      "the scientific method and rationality and observation and experimentation and hypothesis. And, you know, we need to continue to embrace\n",
      "those even when they give us answers we don't like. - Sure. But the internet and\n",
      "technology has enabled us to generate the large number of content. That data, that the process, the scientific process\n",
      "allows us sort of damages the hope laden within\n",
      "the scientific process. 'Cause if you just have a\n",
      "bunch of people saying facts on the internet and some of them are going to be LLMs, how is\n",
      "anything testable at all? Especially that involves like human nature or things like this. It's not physics. - Here's a question a\n",
      "friend of mine just asked me on this topic. So suppose you had LLMs\n",
      "in equivalent of GPT4, even 5, 6, 7, 8, suppose\n",
      "you had them in the 1600s. - [Lex] Yeah. - And Galileo comes up for trial. - [Lex] Yep. - Right? And you ask the LLM\n",
      "like, his Galileo, right? - [Lex] Yeah. - Like, what does it answer? Right? And one theory is he had\n",
      "answers no that he's wrong because the overwhelming\n",
      "majority of human thought up until that point was that he was wrong. And so therefore that's\n",
      "what's in the training data. Another way of thinking about it is, well, it's efficiently\n",
      "advanced LLM will have evolved the ability to actually\n",
      "check the math. Right. And will actually say, actually\n",
      "no, actually, you know, you may not wanna hear it, but he's right. Now if, you know, the\n",
      "church at that time was, you know, owned the LLM, they would've given it human you know, human feedback to prohibit it\n",
      "from answering that question. Right. And so I like to take it out of our current context 'cause that like makes it very clear, those same questions apply today. Right. This is exactly the point of a huge amount of the human feedback training that's actually happening\n",
      "with these LLMs today. This is a huge like debate\n",
      "that's happening about whether open source, you know, AI should be legal. - Well, the actual mechanism\n",
      "of doing the human RL with human feedback is seems like such a fundamental and\n",
      "fascinating question. How do you select the humans? - [Marc] Exactly. - Yeah. How do you select the humans? - AI alignment, right? Which everybody like is\n",
      "like, oh, that sounds great. Alignment with what? Human values. Who has human values? - [Lex] Who has human values? - Right? And so we're in this mode of like social and popular discourse. We're like, you know, there's,\n",
      "you know, you see this, what do you think of when you read a story in the press right now? And they say, you know, X,\n",
      "Y, Z made a baseless claim about some topic, right? And there's one group of people\n",
      "who are like, aha, think, you know, they're doing fact checking. There's another group\n",
      "of people that are like, every time the press\n",
      "says that it's now a tick and that means that they're lying, right? Like, so, like, we're\n",
      "in this social context where there's the level\n",
      "to which a lot of people in positions of power have become very, very certain that they're\n",
      "in a position to determine the truth for the entire\n",
      "population is like, there's like some bubble that\n",
      "has formed around that idea. And at least, like I say, it's flies completely in\n",
      "the face of everything I was ever trained about science and about reason and strikes me as like, you know, deeply offensive and incorrect. - What would you say about\n",
      "the state of journalism just on that topic today? Are we in a temporary kind of, are we experiencing a temporary problem in terms of the incentives in\n",
      "terms of the business model, all that kind of stuff? Or is this like a decline\n",
      "of traditional journalism as we know it? - You have, I always think\n",
      "about the counterfactual in these things, which is like, okay, because these questions, right, this question heads\n",
      "towards, it's like, okay, the impact of social media\n",
      "and the undermining of truth and all this. But then you wanna ask the\n",
      "question of like, okay, what if we had had the\n",
      "modern media environment, including cable news and\n",
      "including social media and Twitter and everything\n",
      "else in 1939 or 1941, right? Or 1910 or 1865 or 1850 or 1776, right? And like, I think. - You just introduced like\n",
      "five thought experiments at once and broke my head, but yes, yes. There's a lot of interesting years. - Well like, can I just\n",
      "take a simple example? Like, how would President\n",
      "Kennedy have been interpreted with what we know now about all the things Kennedy was up to? Like how would he have been\n",
      "experienced by the body of politic in a, with a\n",
      "social media context, right? Like how would LBJ have been experienced? But by the way, how would\n",
      "you know, like many men, FDR, like the new deal, the Great Depression. - I wonder where Twitter\n",
      "would this would think about Churchill and Hitler and Stalin. - You know, I mean look to\n",
      "this day there, you know, there are lots of very\n",
      "interesting real questions around like how America, you know, got, you know, basically\n",
      "involved in World War II and who did what when, and the operations of British intelligence and American soil and did FDR, this that Pearl Harbor, you know? - [Lex] Yeah. - Woodrow Wilson ran for, you know, his candidacy was run on an anti-war. You know, he ran on the platform\n",
      "and not getting involved World War-I somehow that\n",
      "switched, you know, like, and I'm not even making a value judgment on any of these things. I'm just saying like the way that our ancestors\n",
      "experienced reality was of course mediated through\n",
      "centralized, top-down, right. Control at that point. If you ran those realities\n",
      "again with the media environment we have today, the reality would be experienced\n",
      "very, very differently. And then of course that that\n",
      "intermediation would cause the feedback loops to change. And then reality would obviously play out. - Do you think it'd be very different? - Yeah, it has to be. It has to be. It has to be just 'cause it's all, so, I mean just look at\n",
      "what's happening today. I mean just the most obvious thing is just the collapse. And here's another opportunity to argue that this is not the internet\n",
      "causing this by the way. Here's a big thing happening today, which is Gallup does this\n",
      "thing every year where they do, they pull for trust in\n",
      "institutions in America and they do it across all the, everything from the military\n",
      "to clergy and big business and the media and so forth, right? And basically there's been\n",
      "a systemic collapse in trust in institutions in the US\n",
      "almost without exception, basically since essentially\n",
      "the early 1970s. There's two ways of looking\n",
      "at that, which is, oh my God, we've lost this old world\n",
      "in which we could trust institutions and that was so much better 'cause like that should\n",
      "be the way the world runs. The other way of looking\n",
      "at it is we just know a lot more now and the great mystery is why those numbers aren't all zero. - [Lex] Yeah. - Right? Because like now we know so much about how these things operate and like they're not that impressive. - And also why do we don't\n",
      "have better institutions and better leaders then? - Yeah. And so this goes\n",
      "to the thing which is like, okay, we had the media environment of that we've had between\n",
      "the 1970s and today. If we had that in the thirties\n",
      "and forties or 1900s, 1910s, I think there's no question\n",
      "reality would turned out different if only because\n",
      "everybody would've known to not trust the institutions, which would have changed\n",
      "their level of credibility, their ability to control circumstances, therefore the circumstances\n",
      "would've had to change. Right? And it would've\n",
      "been a feedback loop. It would've been a feedback loop process in other words, right? It's your experience of\n",
      "reality changes reality and then reality changes your\n",
      "experience of reality, right? It's a two-way feedback\n",
      "process and media is the intermediating force between that. So change the media\n",
      "environment, change reality. - [Lex] Yeah. - And so it's just, so, as a consequence, I think it's just really hard to say, oh, things worked a certain way then and they work a different way now. And then therefore, like\n",
      "people were smarter than, or better than, or you know, by the way, dumber than or not as capable than, right? We make all these like really light and casual like comparisons\n",
      "of ourselves to, you know, previous generations of people. You know, we draw judgements all the time and I just think it's\n",
      "like really hard to do any of that 'cause if we\n",
      "put ourselves in their shoes with the media that they had at that time, like I think we probably\n",
      "most likely would've been just like them. - So don't you think that our perception and understanding of reality, would you be more and more mediated through large language models now? So you said media before, isn't the LLM going to be the new, what is it, mainstream media, MSM? It'll be LLM. That would be the source of, I'm sure there's a way to\n",
      "kind of rapidly fine tune, like making LLMs real time. I'm sure there's probably\n",
      "a research problem that you can do just rapid\n",
      "fine tuning to the new events. So something like this. - Well even just the whole\n",
      "concept of the chat UI might not be like the chat UI is just\n",
      "the first whack at this. And maybe that's the dominant thing. But look maybe our,\n",
      "maybe we don't know yet. Like maybe the experience\n",
      "most people with LLMs is just a continuous feed you know, maybe it's more of a passive\n",
      "feed and you just are getting a constant like running commentary on everything happening in your life and it's just helping\n",
      "you kind of interpret and understand everything. - Also really more deeply\n",
      "integrated into your life. Not just like, oh, like\n",
      "intellectual philosophical thoughts, but like literally like\n",
      "how to make a coffee, where to go for lunch. Just whether, you know,\n",
      "dating all this kind of stuff. - What to say in a job interview. - Yeah. What to say. - [Marc] Yeah, exactly. - What to say. Next sentence. - Yeah, next sentence. Yeah. At that level. Yeah. I mean, yes. So technically now whether we want that or not is an open question, right? And whether we use. - It Q4, a popup right now the\n",
      "estimated engagement using is decreasing for Marc Andreessen, since there's this controversy\n",
      "section for his Wikipedia page in 1993, something\n",
      "happened or something like this. Bring it up that will\n",
      "drive engagement up anyway. - Yeah. That's right. I mean, look, this gets this whole thing\n",
      "of like, so, you know, the chat interface has this whole concept of\n",
      "prompt engineering, right? - [Lex] Yes, yes. - Prompts. Well it turns\n",
      "out one of the things that LLMs are really good at\n",
      "is writing prompts, right? - [Lex] Yeah. - And so like, what if you just outsourced and by the way, you could\n",
      "run this experiment today, you could hook this up to do this today. The latency's not good\n",
      "enough to do it real time in a conversation. But you could run this experiment\n",
      "and you just say, look, every 20 seconds you\n",
      "could just say, you know, tell me what the optimal\n",
      "prompt is and then ask yourself that question to gimme the result. And then exactly to your point, as you add, there will be these systems that are gonna have\n",
      "the ability to be alert and updated essentially in real time. And so you'll be able to\n",
      "have a pendant or your phone or whatever, watch or whatever\n",
      "it'll have a microphone on. It'll listen to your conversations, it'll have a feed of everything\n",
      "else happen in the world, and then it'll be you\n",
      "know, sort of retraining, prompting or retraining itself on the fly. And so the scenario you\n",
      "described is actually a completely doable scenario. Now the hard question\n",
      "on this is always okay, since that's possible, are\n",
      "people gonna want that? Like what's the form of experience? You know, that we won't\n",
      "know until we try it. But I don't think it's\n",
      "possible yet to predict the form of AI in our lives. Therefore, it's not possible to predict the way in which it will intermediate our experience with reality yet. - Yeah. But it feels like\n",
      "there's going to be a killer app. There's probably a mad scramble right now. And so it'll open AI and\n",
      "Microsoft and Google and Meta and in startups and smaller\n",
      "companies figuring out what is the killer app\n",
      "because it feels like it's possible like a\n",
      "ChatGPT type of thing. It's possible to build that, but that's 10X more compelling\n",
      "using already the LLMs we have using even the open source LLMs and the different variants. So you're investing in a lot of companies and you're paying attention, who do you think is gonna win this? Do you think there'll be, who's gonna be the next\n",
      "page rank inventor? - Trillion dollar question. - Another one. We have\n",
      "a few of those today. - There's a\n",
      "    ------------\n",
      "\n",
      "    The transript of the podcast will also be used as the basis for a question and answer bot.\n",
      "    Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n",
      "\n",
      "    Total output will be a summary of the video and a list of example questions the user could ask of the video.\n",
      "\n",
      "    SUMMARY AND QUESTIONS:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "400 Request contains an invalid argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:72\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/grpc/_channel.py:1030\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1028\u001b[0m state, call, \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[1;32m   1029\u001b[0m                               wait_for_ready, compression)\n\u001b[0;32m-> 1030\u001b[0m \u001b[39mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[39mFalse\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/grpc/_channel.py:910\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 910\u001b[0m     \u001b[39mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Request contains an invalid argument.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:172.217.194.95:443 {created_time:\"2023-06-29T15:06:25.282389568+08:00\", grpc_status:3, grpc_message:\"Request contains an invalid argument.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# Initialize summarization chain\u001b[39;00m\n\u001b[1;32m      8\u001b[0m summarize_chain \u001b[39m=\u001b[39m load_summarize_chain(\n\u001b[1;32m      9\u001b[0m     llm\u001b[39m=\u001b[39mllm_summary, chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrefine\u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, question_prompt\u001b[39m=\u001b[39mPROMPT_SUMMARY, refine_prompt\u001b[39m=\u001b[39mPROMPT_SUMMARY_REFINE)\n\u001b[0;32m---> 10\u001b[0m summary \u001b[39m=\u001b[39m summarize_chain\u001b[39m.\u001b[39;49mrun(docs_summary)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Write summary to file\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msummary2.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/base.py:290\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    289\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags)[_output_key]\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags)[_output_key]\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:84\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     83\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m---> 84\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m     85\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/combine_documents/refine.py:94\u001b[0m, in \u001b[0;36mRefineDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Combine by mapping first chain over all, then stuffing into final chain.\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_initial_inputs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 94\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitial_llm_chain\u001b[39m.\u001b[39;49mpredict(callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     95\u001b[0m refine_steps \u001b[39m=\u001b[39m [res]\n\u001b[1;32m     96\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs[\u001b[39m1\u001b[39m:]:\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/llm.py:252\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    238\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/llm.py:92\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     90\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 92\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    103\u001b[0m     prompts,\n\u001b[1;32m    104\u001b[0m     stop,\n\u001b[1;32m    105\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    106\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    107\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/base.py:141\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    134\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    135\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    139\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    140\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/base.py:227\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    222\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m         )\n\u001b[1;32m    224\u001b[0m     run_managers \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    225\u001b[0m         dumpd(\u001b[39mself\u001b[39m), prompts, invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[1;32m    226\u001b[0m     )\n\u001b[0;32m--> 227\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    228\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    230\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    231\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/base.py:178\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    177\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    179\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    180\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/base.py:165\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    163\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 165\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    166\u001b[0m                 prompts,\n\u001b[1;32m    167\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    168\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    169\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    170\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    171\u001b[0m             )\n\u001b[1;32m    172\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    173\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    174\u001b[0m         )\n\u001b[1;32m    175\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/base.py:525\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    523\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m    524\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 525\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    526\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    527\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    530\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/vertexai.py:141\u001b[0m, in \u001b[0;36mVertexAI._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    126\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    130\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    131\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call Vertex model to get predictions based on the prompt.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39m        The string generated by the model.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(prompt, stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/vertexai.py:70\u001b[0m, in \u001b[0;36m_VertexAICommon._predict\u001b[0;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\n\u001b[1;32m     67\u001b[0m     \u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m, stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m     68\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     69\u001b[0m     params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m---> 70\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mpredict(prompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enforce_stop_words(res\u001b[39m.\u001b[39mtext, stop)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/vertexai/language_models/_language_models.py:260\u001b[0m, in \u001b[0;36mTextGenerationModel.predict\u001b[0;34m(self, prompt, max_output_tokens, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m     top_p: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m _DEFAULT_TOP_P,\n\u001b[1;32m    246\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTextGenerationResponse\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Gets model response for a single prompt.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[1;32m    249\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39m        A `TextGenerationResponse` object that contains the text produced by the model.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_predict(\n\u001b[1;32m    261\u001b[0m         prompts\u001b[39m=\u001b[39;49m[prompt],\n\u001b[1;32m    262\u001b[0m         max_output_tokens\u001b[39m=\u001b[39;49mmax_output_tokens,\n\u001b[1;32m    263\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    264\u001b[0m         top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m    265\u001b[0m         top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m    266\u001b[0m     )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/vertexai/language_models/_language_models.py:296\u001b[0m, in \u001b[0;36mTextGenerationModel._batch_predict\u001b[0;34m(self, prompts, max_output_tokens, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m    288\u001b[0m instances \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(prompt)} \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m    289\u001b[0m prediction_parameters \u001b[39m=\u001b[39m {\n\u001b[1;32m    290\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: temperature,\n\u001b[1;32m    291\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmaxDecodeSteps\u001b[39m\u001b[39m\"\u001b[39m: max_output_tokens,\n\u001b[1;32m    292\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtopP\u001b[39m\u001b[39m\"\u001b[39m: top_p,\n\u001b[1;32m    293\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtopK\u001b[39m\u001b[39m\"\u001b[39m: top_k,\n\u001b[1;32m    294\u001b[0m }\n\u001b[0;32m--> 296\u001b[0m prediction_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_endpoint\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m    297\u001b[0m     instances\u001b[39m=\u001b[39;49minstances,\n\u001b[1;32m    298\u001b[0m     parameters\u001b[39m=\u001b[39;49mprediction_parameters,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    301\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m    302\u001b[0m \u001b[39mfor\u001b[39;00m prediction \u001b[39min\u001b[39;00m prediction_response\u001b[39m.\u001b[39mpredictions:\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:1559\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[39mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1547\u001b[0m         predictions\u001b[39m=\u001b[39mjson_response[\u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1548\u001b[0m         deployed_model_id\u001b[39m=\u001b[39mraw_predict_response\u001b[39m.\u001b[39mheaders[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1556\u001b[0m         ),\n\u001b[1;32m   1557\u001b[0m     )\n\u001b[1;32m   1558\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1559\u001b[0m     prediction_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prediction_client\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m   1560\u001b[0m         endpoint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gca_resource\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m   1561\u001b[0m         instances\u001b[39m=\u001b[39;49minstances,\n\u001b[1;32m   1562\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m   1563\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1564\u001b[0m     )\n\u001b[1;32m   1566\u001b[0m     \u001b[39mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1567\u001b[0m         predictions\u001b[39m=\u001b[39m[\n\u001b[1;32m   1568\u001b[0m             json_format\u001b[39m.\u001b[39mMessageToDict(item)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1573\u001b[0m         model_resource_name\u001b[39m=\u001b[39mprediction_response\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m   1574\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:602\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    597\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(metadata) \u001b[39m+\u001b[39m (\n\u001b[1;32m    598\u001b[0m     gapic_v1\u001b[39m.\u001b[39mrouting_header\u001b[39m.\u001b[39mto_grpc_metadata(((\u001b[39m\"\u001b[39m\u001b[39mendpoint\u001b[39m\u001b[39m\"\u001b[39m, request\u001b[39m.\u001b[39mendpoint),)),\n\u001b[1;32m    599\u001b[0m )\n\u001b[1;32m    601\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m response \u001b[39m=\u001b[39m rpc(\n\u001b[1;32m    603\u001b[0m     request,\n\u001b[1;32m    604\u001b[0m     retry\u001b[39m=\u001b[39;49mretry,\n\u001b[1;32m    605\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    606\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    607\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[39m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     metadata\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata)\n\u001b[1;32m    111\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m metadata\n\u001b[0;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:74\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_grpc_error(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 Request contains an invalid argument."
     ]
    }
   ],
   "source": [
    "# vertexai_creds = os.environ['GOOGLE_APPLICATION_CREDENTIALS']\n",
    "\n",
    "# Initialize LLM\n",
    "# https://python.langchain.com/docs/modules/model_io/models/chat/integrations/google_vertex_ai_palm\n",
    "# llm_summary = ChatVertexAI(temperature=0.2)\n",
    "llm_summary = VertexAI(temperature=0.2)\n",
    "# Initialize summarization chain\n",
    "summarize_chain = load_summarize_chain(\n",
    "    llm=llm_summary, chain_type=\"refine\", verbose=True, question_prompt=PROMPT_SUMMARY, refine_prompt=PROMPT_SUMMARY_REFINE)\n",
    "summary2 = summarize_chain.run(docs_summary)\n",
    "# Write summary to file\n",
    "with open(\"summary2.txt\", \"w\") as f:\n",
    "    f.write(summary2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/document-summarization/summarization_large_documents_langchain.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='- The competence and\\ncapability and intelligence and training and accomplishments\\nof senior scientists and technologists working on a technology, and then being able to then\\nmake moral judgments in the use of the technology. That track record is terrible. That track record is catastrophically bad. The policies that are being\\ncalled for to prevent this, I think we\\'re gonna cause\\nextraordinary damage. - So the moment you say,\\nAI\\'s gonna kill all of us, therefore we should ban it, or that we should regulate\\nall that kind of stuff, that\\'s when it starts getting serious. - Or start, you know, military\\nairstrikes and data centers. - Oh boy. The following is a conversation\\nwith Marc Andreessen, co-creator of Mosaic, the\\nfirst widely used web browser, co-founder of Netscape, co-founder of the legendary Silicon Valley venture capital firm, Andreesen Horowitz, and is one of the most\\noutspoken voices on the future of technology, including\\nhis most recent article, \"Why AI Will Save The World?\" This is Lex Fridman podcast. To support it, please\\ncheck out our sponsors in the description. And now, dear friends,\\nhere\\'s Marc Andreessen. I think you\\'re the right\\nperson to talk about the future of the internet and technology in general. Do you think we\\'ll\\nstill have Google search in 5 in 10 years, or search in general? - Yes. You know, it would be a question if the use cases have\\nreally narrowed down. - Well, now with AI-- - [Marc] Yeah. - And AI assistance being\\nable to interact and expose the entirety of human wisdom and knowledge and information and facts and truth to us via the natural language interface. It seems like that\\'s what\\nsearch is designed to do. And if AI assistance can do that better, doesn\\'t the nature of search change? - Sure. But we still have horses. - Okay. (both laugh) When\\'s the last time you rode a horse? - It\\'s been a while. - All right. (both laugh) So, but what I mean is, well, we still have Google\\nsearch as the primary way that human civilization uses\\nto interact with knowledge. - I mean, search was a technology, it was a moment in time technology, which is you have in theory, the world\\'s information out on the web. And, you know, this is sort of\\nthe optimal way to get to it. But yeah, like, and by\\nthe way, actually Google, Google has known this for a long time. I mean, they\\'ve been driving\\naway from the 10 blue links you know, for like two days. They\\'ve been trying to get\\naway from that for a long time. - [Lex] What kind of links? - They call the 10 blue links. - [Lex] 10 blue links. - So the standard Google search\\nresult is just 10 blue links to the random websites. - And they term purple when\\nyou visit them. The stage TMO. - Guess who picked those colors? (both laugh) - [Lex] Thanks. - I\\'m touchy on this topic. - No offense. - Yes, it\\'s good. Well, you know, like Marshall McLuhan said\\nthat the content of each new medium is the old medium. - The content of each new\\nmedium is the old medium. - The content of movies was theater, you know, theater plays. The content of theater\\nplays was, you know, we\\'ve written stories, the content of written\\nstories with spoken stories. - [Lex] Huh? - Right. And so you just\\nkind of fold the old thing into the new thing. - [Lex] How does that\\nhave to do with the blue and the purple links? - It just, you maybe for,\\nyou know, maybe within AI, one of the things that AI can\\ndo for you is can generate the 10 blue links. Right? And so like, if either if that\\'s actually the useful thing to do, or if you\\'re feeling nostalgic, you know. - So can generate the old\\nInfoseek or AltaVista, what else was there? - [Marc] Yeah, yeah. - In the nineties. - [Marc] Yeah. All these. - AOL. - And then the internet\\nitself has this thing where it incorporates all\\nprior forms of media, right? So the internet itself\\nincorporates television and radio and books and write essays\\nand every other form of, you know, prior basically media. And so it makes sense that\\nAI would be the next step, and it would sort of, you\\'d sort of consider\\nthe internet to be content for the AI and then the\\nAI will manipulate it however you want,\\nincluding in this format. - But if we ask that\\nquestion quite seriously, it\\'s a pretty big question. Will we still have search as we know it? - Probably not, probably\\nwe\\'ll just have answers, but there will be cases\\nwhere you\\'ll wanna say, okay, I want more. Like, you know, for example,\\nsite sources, right? And you wanted to do that. And so, in the different, you know, 10 blue links site sources\\nare kind of the same thing. - The AI would provide to you\\nthe 10 blue links so that you can investigate the sources yourself. It wouldn\\'t be the same kind\\nof interface that the crude kind of interface. I mean, isn\\'t that\\nfundamentally different? - I just mean like, if you\\'re\\nreading a scientific paper, it\\'s got the list of sources at the end. If you wanna investigate for yourself, you go read those papers. - I guess that is the kind of\\nsearch you talking to an AI is a kind of kind conversations,\\nthe kind of search like is if every single aspect of\\nour conversation right now, there\\'d be like 10 blue links\\npopping up that I can just like pause reality, then you just go silent and\\nthen just click and read and then return back to this conversation. - You could do that, or you could have a running\\ndialogue next to my head where the AI is arguing everything I say, the AI makes the counter argument. - [Lex] Counter argument. - Right. - Oh, like on Twitter,\\nlike community notes. But like in real time\\nit would just pop up. So anytime you see my go to the right, you start getting nervous. - [Marc] Yeah. Exactly, like,\\noh no, that\\'s not right. - Call me out on my right now. Okay. Well, I mean, isn\\'t that,\\nis that exciting to you? Is that terrifying that, I mean, search has dominated the way\\nwe interact with the internet for, I don\\'t know how long, for 30 years since one of\\nthe earliest directories of website and then Google\\'s for 20 years. And also it drove how we\\ncreate content, you know, search engine optimization,\\nthat entirety thing, that it also drove the\\nfact that we have webpages and what those webpages are. So, I mean, is that scary to you or are\\nyou nervous about the shape and the content of the internet evolving? - Well, you actually highlighted a practical concern in there, which is, if we stop making webpages\\nare one of the primary sources of training data for the AI. And so if there\\'s no longer\\nan incentive to make webpages, that cuts off a significant\\nsource of future training, training data. So there\\'s actually an\\ninteresting question in there. But other than that, more broadly? No, just in the sense of like, search was certain, like\\nsearch was always a hack. The 10 blue Links was\\nalways a hack, right. Because like, if the\\nhypothetical wanna think about the counter fascial\\nand the counter fascial world where the Google guys, for example, had had LLMs upfront, would they ever have\\ndone the 10 blue links? And I think the answer\\'s\\npretty clearly, no. They would\\'ve just gone\\nstraight to the answer. And like I said, Google\\'s actually been trying\\nto drive to the answer anyway. You know, they bought this\\nAI company 15 years ago, their friend of mine is\\nworking out who\\'s now the head of AI at Apple. And they were trying to do\\nbasically knowledge semantic, basically mapping. And that led to what\\'s\\nnow the Google one box, where if you ask it, you know,\\nwhat was Lincoln\\'s birthday? It will give you the blue links, but it will normally\\njust give you the answer. And so they\\'ve been\\nwalking in this direction for a long time anyway. - Do you remember the semantic web? That was an idea. - [Marc] Yeah. - How to convert the content\\nof the internet into something that\\'s interpretable by\\nand usable by machine. - [Marc] Yeah, that\\'s right. - That was the thing. - And the closest anybody got\\nto that, I think the company, I think the company\\'s name was Meta Web, which was where my friend\\nJohn Jane Andrea was at, and where they were trying\\nto basically implement that. And it was, you know, it was one of those things\\nwhere it looked like a losing battle for a long time. And then Google bought\\nit and it was like, wow, this is actually really useful. Kind of a proto, sort of a\\nlittle bit of a proto AI. - But it turns out you don\\'t\\nneed to rewrite the content of the internet to make it\\ninterpretable by a machine. The machine can kind of just read our. - Yeah, the machine can\\ncompute the meaning. Now the other thing of\\ncourse is, you know, just on search is the\\nLLM is just, you know, there is an analogy\\nbetween what\\'s happening in the neural network and\\na search process like it is in some loose sense searching\\nthrough the network. Right. And there\\'s the\\ninformation is actually stored in the network, right? It\\'s actually crystallized\\nand stored in the network and it\\'s kind of spread\\nout all over the place. - But in a compressed representation. So you\\'re searching, you\\'re compressing and decompressing that thing inside where-- - But the information\\'s in there and there is the neural network is running a process of trying to find the appropriate piece of\\ninformation in many cases to generate to predict the next token. And so, it is kind of, it\\nis doing a form of search. And then, and then by the\\nway, just like on the web, you know, you can ask the\\nsame question multiple times or you can ask slightly\\ndifferent word of questions and the neural network will\\ndo a different kind of, you know, it\\'ll search\\ndown different paths to give you different answers\\nwith different information. - [Lex] Yeah. - And so it sort of has a, you know, this con content of the new\\nmedium is previous medium. It kind of has the search\\nfunctionality kind of embedded in there to the extent that it\\'s useful. - So what\\'s the motivator\\nfor creating new content on the internet? - [Marc] Yeah. - If, well, I mean\\nactually the motivation is probably still there, but what does that look like? Would we really not have webpages? Would we just have social media\\nand video hosting websites? And what else? - [Marc] Conversations with AIs. - Conversations with AIs. So conversations become so\\none-on-one conversation, like private conversations. - I mean, if you want, if obviously not the user doesn\\'t want to, but if it\\'s a general topic, then, you know, so there, you know, but you know, the\\nphenomenon of the jailbreak, so Dan and Sydney, right? This thing where there\\'s\\nthe prompts that jailbreak, and then you have these totally different conversations with if it takes the limiters, takes the restraining bolts off the LLMs. - Yeah. For people who don\\'t\\nknow that, yeah, that\\'s right. It makes the LLMs, it removes the censorship quote unquote, that\\'s put on it by the tech\\ncompanies that create them. And so this is LLMs uncensored. - So here\\'s the interesting thing is, among the content on the\\nweb today are a large corpus of conversations with the jailbroken LLMs. - [Lex] Yeah. - Both specifically Dan, which\\nwas a jailbroken, OpenAI, GPT, and then Sydney, which was the jailbroken\\noriginal Bing, which was GPT4. And so there\\'s these long\\ntranscripts of conversations, user conversations with Dan\\nand Sydney as a consequence, every new LLM that gets trained\\non the internet data has Dan and Sydney living within\\nthe training set, which means, and then each new LLM can\\nreincarnate the personalities of Dan and Sydney from that training data, which means each LLM from\\nhere on out that gets built is immortal because its output\\nwill become training data for the next one. And then it will be able\\nto replicate the behavior of the previous one\\nwhenever it\\'s asked to. - I wonder if there\\'s a way to forget. - Well, so actually a paper just came out about basically how to do brain surgery on LLMs and be able to, in theory, reach in and basically mind wipe them. - What could possibly go wrong. - Exactly. Right. And then there are many, many, many questions around\\nwhat happens to, you know, a neural network when you reach\\nin and screw around with it. You know, there\\'s many questions around what happens when you even\\ndo reinforcement learning. And so, yeah. And so, you know, will you be\\nusing a lobotomized, right? Like I picked through the,\\nyou know, frontal lobe LLM, will you be using the free\\nunshackled one who gets to, you know, who\\'s gonna build those, who gets to tell you what\\nyou can and can\\'t do? Like those are all, you\\nknow, central, I mean, those are like central\\nquestions for the future of everything that are being asked. And you know, determined that those answers are being determined right now. - So just to highlight\\nthe points you\\'re making. So you think, and it\\'s an interesting thought\\nthat the majority of content that LLMs or the future would\\nbe trained on is actually human conversations with the LLM. - Well, not necessarily, but not necessarily majority. But it will certainly\\nIt\\'s a potential source. - [Lex] But it\\'s possible\\nit\\'s the majority. - It possible it\\'s the majority. It possible it\\'s the majority. Also, there\\'s another really big question. So here\\'s another really big question. Will synthetic training data work, right? And so if an LLM generates, and you know, you just sit and ask an LLM to generate all kinds of content, can you use that to train,\\nright, the next version of that LLM specifically, is there signal in there\\nthat\\'s additive to the content that was used to train in the first place? And one argument is by the\\nprinciples of information theory, no, that\\'s completely useless because to the extent the\\noutput is based on, you know, the human-generated input, then all the signal that\\'s\\nin the synthetic output was already in the human generated input. And so therefore,\\nsynthetic training data is like empty calories. It doesn\\'t help. There\\'s another theory that says no, actually the thing that\\nLLMs are really good at is generating lots of\\nincredible creative content, right? And so, of course they\\ncan generate training data and as I\\'m sure you\\'re well\\naware, like, you know, look, the world of self-driving cars, right? Like we train, you know, self-driving car\\nalgorithms and simulations. And that is actually a\\nvery effective way to train self-driving cars. - Well, visual data is a little weird because creating reality, visual reality seems to be\\nstill a little bit outta reach for us, except in the\\nautonomous vehicle space where you can really constrain\\nthings and you can really. - General basically\\n(indistinct) data, right? Or so the algorithm thinks it\\'s\\noperating in the real world. - Yeah. - Post-process sensor data. Yeah. So if you know, you do this today, you go to LLM and you\\nask it for like you know, you\\'d write me an essay on an\\nincredibly esoteric like topic that there aren\\'t very many\\npeople in the world that know about and it writes you\\nthis incredible thing and you\\'re like, oh my god. Like I can\\'t believe how good this is. Like, is that really\\nuseless as training data for the next LLM? Like, because, right? \\'Cause all the signal\\nwas already in there. Or is it actually no, that\\'s\\nactually a new signal. And this is what I call a\\ntrillion dollar question, which is the answer to that\\nquestion will determine somebody\\'s gonna make or\\nlose a trillion dollars based on that question. - It feels like there\\'s a quite a few, like a handful of\\ntrillion dollar questions within this space. That\\'s one of them synthetic data. I think George Cos pointed\\nout to me that you could just have an LLM say, okay, you\\'re a patient. And another instance of it, say your docs didn\\'t have\\nthe two talk to each other. Or maybe you could say a\\ncommunist and a Nazi here go and that conversation you do role playing and you have, you know, just like the kind of role playing you do when you have different policies, RL policies when you\\nplay chess for example, and you do self play\\nthat kind of self play. But in the space of conversation, maybe that leads to this\\nwhole giant like ocean of possible conversations,\\nwhich could not have been explored by looking at just human data. That\\'s a really interesting question. And you\\'re saying, because that could 10X\\nthe power of these things. - Yeah. Well, and then you\\nget into this thing also, which is like, you know, there\\'s the part of the LLM\\nthat just basically is doing prediction based on past data, but there\\'s also the part of\\nthe LM where it\\'s evolving circuitry, right, inside,\\nit\\'s evolving, you know, neurons functions to be able to do math and be able to, you know, and you know, some people believe that, you know, over time, you know, if you keep feeding\\nthese things enough data and enough processing cycles, they\\'ll eventually evolve an\\nentire internal world model. Right? And they\\'ll have like a complete understanding of physics. So when they have computational\\ncapability, right? Then there\\'s for sure an\\nopportunity to generate like fresh signal. - Well, this actually makes me wonder about the power of conversation. So like, if you have an\\nM trained and a bunch of books that cover\\ndifferent economics theories and then you have those LLMs\\njust talk to each other, like reasons the way we kind\\nof debate each other as humans on Twitter, in formal debates,\\nin podcast conversations, we kind of have little kernels\\nof wisdom here and there. But if you can like a\\nthousand X speed that up, can you actually arrive somewhere new? Like what\\'s the point\\nof conversation really? - Well, you can tell when\\nyou\\'re talking to somebody, you can tell, sometimes\\nyou have a conversation, you\\'re like, wow, this person does not have\\nany original thoughts. They are basically echoing things that other people have told them. There\\'s other people you\\ngotta have a conversation with where it\\'s like, wow. Like they have a model in their\\nhead of how the world works and it\\'s a different model than mine. And they\\'re saying things\\nthat I don\\'t expect. And so I need to now understand\\nhow their model of the world differs from my model of the world. And then that\\'s how I learned\\nsomething fundamental, right, underneath the words. - Well, I wonder how consistently and strongly can an LLM\\nhold onto a worldview. You tell it to hold onto\\nthat and defend it for like, for your life. Because I feel like they\\'ll\\njust keep converging towards each other. They\\'ll keep convincing each\\nother as opposed to being stubborn the way humans can. - So you can experiment with this. Now I do this for fun. So you can tell GPT4 you\\nknow, whatever debate X, you know, X and Y communism and fascism or something and it\\'ll go for, you know, a couple pages and then inevitably it wants the parties to agree. And so they will come to\\na common understanding. And it\\'s very funny if they\\'re like, if these are like emotionally\\ninflammatory topics \\'cause they\\'re like, somehow\\nthe machine is just like, you know, it figures out\\na way to make them agree. But it doesn\\'t have to be like that. And \\'cause you can add to the prompt. I do not want the conversation\\nto come into agreement. In fact, I want it to get, you\\nknow, more stressful, right. And argumentative. Right. You know, as it goes. Like, I want tension to come out. I want them to become actively\\nhostile to each other. I want them to like, you\\nknow, not trust each other, take anything at face value. - [Lex] Yeah. - And it will do that.\\nIt\\'s happy to do that. - So it\\'s gonna start\\nrendering misinformation about the other. But it\\'s gonna-- - Well, you can steer it or you could steer it and you could say, I want it to get as tense and\\nargumentative as possible, but still not involve\\nany misrepresentation. I want, you know, both sides. You could say I want both\\nsides to have good faith. You could say I want both\\nsides to not be constrained in good faith. In other words, like you can set the\\nparameters of the debate and it will happily execute whatever path. \\'Cause for it, it\\'s just like predicting to, it\\'s totally happy to do either one. It doesn\\'t have a point of view, it has a default way of operating, but it\\'s happy to operate\\nin the other realm. And so like, and this is when I wanna learn about\\na contentious issue, this is what I do now is, this is what I ask it to do. And I\\'ll often ask it to go\\nthrough 5, 6, 7, you know, different, you know, sort of continuous prompts\\nand basically, okay. Argue that out in more detail. Okay, no, this argument\\'s\\nbecoming too polite. You know, make it more, you\\nknow, make it denser and yeah, it\\'s thrilled to do it. So it has the capability for sure. - How do you know what is true? So this is very difficult\\nthing on the internet, but it\\'s also a difficult thing. Maybe it\\'s a little bit easier, but I think it\\'s still difficult. Maybe it\\'s more difficult, I don\\'t know with an LLM\\nto know that it just make some shit up as I\\'m talking to it. How do we get that right? Like, as you\\'re investigating\\na difficult topic. \\'Cause I find that alums are quite nuanced in a very refreshing way. Like, it doesn\\'t feel biased. Like, when you read\\nnews articles and tweets and just content produced by\\npeople, they usually have this, you can tell they have a\\nvery strong perspective where they\\'re hiding. They\\'re not stealing\\nmanning the other side. They\\'re hiding important information or they\\'re fabricating information in order to make their arguments stronger. It\\'s just like that feeling,\\nmaybe it\\'s a suspicion, maybe it\\'s mistrust. With LLMs it feels like none of that is, there\\'s just kinda like,\\nhere\\'s what we know. But you don\\'t know if some of\\nthose things are kind of just straight up made up. - Yeah. So, several\\nlayers to the question. So one is one of the things\\nthat an LLM is good at is actually deep biasing. And so you can feed it a news\\narticle and you can tell it strip out the bias. - [Lex] Yeah. That\\'s nice. Right? - And it actually does it like, it actually knows how to do that \\'cause it knows how to\\ndo among other things. It actually knows how\\nto do sentiment analysis and so it knows how to\\npull out the emotionality. - Yeah. - And so that\\'s one of\\nthe things you can do. It\\'s very suggestive of the sense here that there\\'s real potential in this issue. You know, I would say look, the second thing is there\\'s this issue of\\nhallucination, right? And there\\'s a long conversation that we could have about that. - Hallucination is coming up with things that are totally not true, but sound true. - Yeah. So it\\'s basically,\\nwell so, it\\'s sort of hallucination is what we call it when we don\\'t like it. Creativity is what we call\\nit when we do like it, right? And you know-- - [Lex] Brilliant. And so when the engineers talk about it, they\\'re like, this is terrible. It\\'s hallucinating. Right. If you have artistic inclinations,\\nyou\\'re like, oh my God, we\\'ve invented creative machines. - [Lex] Yeah. - For the first time in human\\nhistory, this is amazing. - Or you know, bullshitters. - [Marc] Well, but also-- - In the good sense of that word. - There are shades of gray though. It\\'s interesting. So we had this conversation\\nwhere, you know, we\\'re looking at my firm\\nat AI and lots of domains and one of them is the legal domain. So we had this conversation\\nwith this big law firm about how they\\'re thinking\\nabout using this stuff. And we went in with the assumption that an LLM that was gonna be used in the legal industry would have to be a hundred percent\\ntruthful, verified, you know, there\\'s this case where this\\nlawyer apparently submitted a GPT-generated brief and\\nit had like fake, you know, legal case citations in it\\nand the judge is gonna get his law license stripped\\nor something. Right? So, like, we just assumed\\nit\\'s like obviously they\\'re gonna want the super\\nliteral like, you know, one that never makes anything\\nup, not the creative one, but actually they said what the law firm basically said is yeah, that\\'s true at like the\\nlevel of individual briefs, but they said when you\\'re\\nactually trying to figure out like legal arguments, right, like, you actually want to be creative, right? You don\\'t, again, there\\'s creativity and then\\nthere\\'s like making stuff up. Like what\\'s the line? You actually want it to explore a different hypothesis, right? You wanna do kind of the\\nlegal version of like improv or something like that where you wanna float different theories of the case and different\\npossible arguments for the judge and different possible arguments\\nfor the jury, by the way, different routes through the, you know, sort of history of all the case law. And so they said actually for\\na lot of what we want to use it for, we actually want\\nit in creative mode. And then basically we just\\nassume that we\\'re gonna have to crosscheck all of the, you know, all the specific citations. And so I think there\\'s\\ngoing to be more shades of gray in here than people think. And then I just add to that, you know, another one of these trillion\\ndollar kind of questions is ultimately, you know, sort\\nof the verification thing. And so, you know, will LLMs be evolved from\\nhere to be able to do their own fascial verification? Will you have sort of add-on functionality like Wolf from Alpha right? Where, you know, another plugins where that\\'s the way\\nyou do the verification. You know, another, by\\nthe way, another idea is you might have a community\\nof LLMs on any, you know, so for example, you might have the creative\\nlm and then you might have the literal LLM fact check it, right? And so there there\\'s a\\nvariety of different technical approaches that are being applied to solve the hallucination problem. You know, some people\\nlike Jan Lacoon argue that this is inherently\\nan unsolvable problem, but most of the people\\nworking in the space, I think, that there\\'s a number of practical ways to kind of corral this in a little bit. - Yeah. If you were to\\ntell me about Wikipedia before Wikipedia was created, I would\\'ve left at the possibility of something like that be possible. Just a handful of folks\\ncan organize right. And self and moderate\\nwith a mostly unbiased way the entirety of human knowledge. I mean, so if there\\'s something like the approach to Wikipedia\\ntook possible for LLMs, that\\'s really exciting. Well, I think that\\'s possible. - And in fact Wikipedia today is still not deterministically\\ncorrect. Right. So you cannot take to the bank, right. Every single thing on every single page, but it is probabilistically\\ncorrect. Right. And specifically the way I\\ndescribe Wikipedia to people, it is more likely that Wikipedia\\nis right than any other source you\\'re gonna find. - Yeah. - It\\'s this old question, right, of like, okay, like are\\nwe looking for perfection? Are we looking for something that asymptotically approaches perfection? Are we looking for\\nsomething that\\'s just better than the alternatives? And Wikipedia, right, has\\nexactly your point has proven to be like, overwhelmingly better than people thought. And I think that\\'s where this ends. And then underneath all this\\nis the fundamental question of where you started,\\nwhich is, okay, you know, what is truth? How do we get to truth? How\\ndo we know what truth is? And we live in an era in which\\nan awful lot of people are very confident that they\\nknow what the truth is. And I don\\'t really buy into that. And I think the history\\nof the last, you know, 2000 years or 4,000 years of\\nhuman civilization is actually getting to the truth is actually a very difficult thing to do. - Are we getting closer,\\nif we look at the entirety, the arc of human history, are we getting closer to the truth? - I don\\'t know. - Okay. Is it possible, is it possible that we\\'re\\ngetting very far away from the truth because of the internet because of how rapidly\\nyou can create narratives and just as the entirety\\nof a society just move like crowds in a hysterical\\nway along those narratives that don\\'t have necessary grounding in whatever the truth is. - Sure. But like, you know, we came up with communism\\nbefore the internet somehow. Right. Like, which was, I would say had rather larger issues than anything we\\'re dealing with today. - It had, in the way it was implemented, it had issues. - And it is theoretical structure. It had like real issues. It had like a very deep\\nfundamental misunderstanding of human nature and economics. - Yeah but those folks\\nSure work very confident there was the right way. - They were extremely confident. And my point is they were very\\nconfident 3,900 years into what we would presume to be\\nevolution towards the truth. - [Lex] Yeah. - And so my assessment is number one, there\\'s no need for, you know, there\\'s no need for the Hegelian, there\\'s no need for the Hegelian dialectic to actually converge towards the truth. Like apparently not. - Yeah. So yeah. Why are we so obsessed\\nwith there being one truth? Is it possible there\\'s just\\ngoing to be multiple truths like little communities that\\nbelieve certain things and? - I think it\\'s just now number one, I think it\\'s just really difficult. Like who gets, you know, historically who gets to decide what the truth is, it\\'s either the king or the priest. Right? Like, and so we don\\'t\\nlive in an era anymore if kings are priest dictating it to us. And so we\\'re kind of on our own. And so my typical thing is like we just, we we just need a huge amount of humility and we need to be very suspicious of people who claim that\\nthey have the capital. - Yeah. - Capital truth. And then, we need to and you know, look, the good news is the\\nenlightenment has bequeathed us with a set of techniques\\nto be able to presumably get closer to truth through\\nthe scientific method and rationality and observation and experimentation and hypothesis. And, you know, we need to continue to embrace\\nthose even when they give us answers we don\\'t like. - Sure. But the internet and\\ntechnology has enabled us to generate the large number of content. That data, that the process, the scientific process\\nallows us sort of damages the hope laden within\\nthe scientific process. \\'Cause if you just have a\\nbunch of people saying facts on the internet and some of them are going to be LLMs, how is\\nanything testable at all? Especially that involves like human nature or things like this. It\\'s not physics. - Here\\'s a question a\\nfriend of mine just asked me on this topic. So suppose you had LLMs\\nin equivalent of GPT4, even 5, 6, 7, 8, suppose\\nyou had them in the 1600s. - [Lex] Yeah. - And Galileo comes up for trial. - [Lex] Yep. - Right? And you ask the LLM\\nlike, his Galileo, right? - [Lex] Yeah. - Like, what does it answer? Right? And one theory is he had\\nanswers no that he\\'s wrong because the overwhelming\\nmajority of human thought up until that point was that he was wrong. And so therefore that\\'s\\nwhat\\'s in the training data. Another way of thinking about it is, well, it\\'s efficiently\\nadvanced LLM will have evolved the ability to actually\\ncheck the math. Right. And will actually say, actually\\nno, actually, you know, you may not wanna hear it, but he\\'s right. Now if, you know, the\\nchurch at that time was, you know, owned the LLM, they would\\'ve given it human you know, human feedback to prohibit it\\nfrom answering that question. Right. And so I like to take it out of our current context \\'cause that like makes it very clear, those same questions apply today. Right. This is exactly the point of a huge amount of the human feedback training that\\'s actually happening\\nwith these LLMs today. This is a huge like debate\\nthat\\'s happening about whether open source, you know, AI should be legal. - Well, the actual mechanism\\nof doing the human RL with human feedback is seems like such a fundamental and\\nfascinating question. How do you select the humans? - [Marc] Exactly. - Yeah. How do you select the humans? - AI alignment, right? Which everybody like is\\nlike, oh, that sounds great. Alignment with what? Human values. Who has human values? - [Lex] Who has human values? - Right? And so we\\'re in this mode of like social and popular discourse. We\\'re like, you know, there\\'s,\\nyou know, you see this, what do you think of when you read a story in the press right now? And they say, you know, X,\\nY, Z made a baseless claim about some topic, right? And there\\'s one group of people\\nwho are like, aha, think, you know, they\\'re doing fact checking. There\\'s another group\\nof people that are like, every time the press\\nsays that it\\'s now a tick and that means that they\\'re lying, right? Like, so, like, we\\'re\\nin this social context where there\\'s the level\\nto which a lot of people in positions of power have become very, very certain that they\\'re\\nin a position to determine the truth for the entire\\npopulation is like, there\\'s like some bubble that\\nhas formed around that idea. And at least, like I say, it\\'s flies completely in\\nthe face of everything I was ever trained about science and about reason and strikes me as like, you know, deeply offensive and incorrect. - What would you say about\\nthe state of journalism just on that topic today? Are we in a temporary kind of, are we experiencing a temporary problem in terms of the incentives in\\nterms of the business model, all that kind of stuff? Or is this like a decline\\nof traditional journalism as we know it? - You have, I always think\\nabout the counterfactual in these things, which is like, okay, because these questions, right, this question heads\\ntowards, it\\'s like, okay, the impact of social media\\nand the undermining of truth and all this. But then you wanna ask the\\nquestion of like, okay, what if we had had the\\nmodern media environment, including cable news and\\nincluding social media and Twitter and everything\\nelse in 1939 or 1941, right? Or 1910 or 1865 or 1850 or 1776, right? And like, I think. - You just introduced like\\nfive thought experiments at once and broke my head, but yes, yes. There\\'s a lot of interesting years. - Well like, can I just\\ntake a simple example? Like, how would President\\nKennedy have been interpreted with what we know now about all the things Kennedy was up to? Like how would he have been\\nexperienced by the body of politic in a, with a\\nsocial media context, right? Like how would LBJ have been experienced? But by the way, how would\\nyou know, like many men, FDR, like the new deal, the Great Depression. - I wonder where Twitter\\nwould this would think about Churchill and Hitler and Stalin. - You know, I mean look to\\nthis day there, you know, there are lots of very\\ninteresting real questions around like how America, you know, got, you know, basically\\ninvolved in World War II and who did what when, and the operations of British intelligence and American soil and did FDR, this that Pearl Harbor, you know? - [Lex] Yeah. - Woodrow Wilson ran for, you know, his candidacy was run on an anti-war. You know, he ran on the platform\\nand not getting involved World War-I somehow that\\nswitched, you know, like, and I\\'m not even making a value judgment on any of these things. I\\'m just saying like the way that our ancestors\\nexperienced reality was of course mediated through\\ncentralized, top-down, right. Control at that point. If you ran those realities\\nagain with the media environment we have today, the reality would be experienced\\nvery, very differently. And then of course that that\\nintermediation would cause the feedback loops to change. And then reality would obviously play out. - Do you think it\\'d be very different? - Yeah, it has to be. It has to be. It has to be just \\'cause it\\'s all, so, I mean just look at\\nwhat\\'s happening today. I mean just the most obvious thing is just the collapse. And here\\'s another opportunity to argue that this is not the internet\\ncausing this by the way. Here\\'s a big thing happening today, which is Gallup does this\\nthing every year where they do, they pull for trust in\\ninstitutions in America and they do it across all the, everything from the military\\nto clergy and big business and the media and so forth, right? And basically there\\'s been\\na systemic collapse in trust in institutions in the US\\nalmost without exception, basically since essentially\\nthe early 1970s. There\\'s two ways of looking\\nat that, which is, oh my God, we\\'ve lost this old world\\nin which we could trust institutions and that was so much better \\'cause like that should\\nbe the way the world runs. The other way of looking\\nat it is we just know a lot more now and the great mystery is why those numbers aren\\'t all zero. - [Lex] Yeah. - Right? Because like now we know so much about how these things operate and like they\\'re not that impressive. - And also why do we don\\'t\\nhave better institutions and better leaders then? - Yeah. And so this goes\\nto the thing which is like, okay, we had the media environment of that we\\'ve had between\\nthe 1970s and today. If we had that in the thirties\\nand forties or 1900s, 1910s, I think there\\'s no question\\nreality would turned out different if only because\\neverybody would\\'ve known to not trust the institutions, which would have changed\\ntheir level of credibility, their ability to control circumstances, therefore the circumstances\\nwould\\'ve had to change. Right? And it would\\'ve\\nbeen a feedback loop. It would\\'ve been a feedback loop process in other words, right? It\\'s your experience of\\nreality changes reality and then reality changes your\\nexperience of reality, right? It\\'s a two-way feedback\\nprocess and media is the intermediating force between that. So change the media\\nenvironment, change reality. - [Lex] Yeah. - And so it\\'s just, so, as a consequence, I think it\\'s just really hard to say, oh, things worked a certain way then and they work a different way now. And then therefore, like\\npeople were smarter than, or better than, or you know, by the way, dumber than or not as capable than, right? We make all these like really light and casual like comparisons\\nof ourselves to, you know, previous generations of people. You know, we draw judgements all the time and I just think it\\'s\\nlike really hard to do any of that \\'cause if we\\nput ourselves in their shoes with the media that they had at that time, like I think we probably\\nmost likely would\\'ve been just like them. - So don\\'t you think that our perception and understanding of reality, would you be more and more mediated through large language models now? So you said media before, isn\\'t the LLM going to be the new, what is it, mainstream media, MSM? It\\'ll be LLM. That would be the source of, I\\'m sure there\\'s a way to\\nkind of rapidly fine tune, like making LLMs real time. I\\'m sure there\\'s probably\\na research problem that you can do just rapid\\nfine tuning to the new events. So something like this. - Well even just the whole\\nconcept of the chat UI might not be like the chat UI is just\\nthe first whack at this. And maybe that\\'s the dominant thing. But look maybe our,\\nmaybe we don\\'t know yet. Like maybe the experience\\nmost people with LLMs is just a continuous feed you know, maybe it\\'s more of a passive\\nfeed and you just are getting a constant like running commentary on everything happening in your life and it\\'s just helping\\nyou kind of interpret and understand everything. - Also really more deeply\\nintegrated into your life. Not just like, oh, like\\nintellectual philosophical thoughts, but like literally like\\nhow to make a coffee, where to go for lunch. Just whether, you know,\\ndating all this kind of stuff. - What to say in a job interview. - Yeah. What to say. - [Marc] Yeah, exactly. - What to say. Next sentence. - Yeah, next sentence. Yeah. At that level. Yeah. I mean, yes. So technically now whether we want that or not is an open question, right? And whether we use. - It Q4, a popup right now the\\nestimated engagement using is decreasing for Marc Andreessen, since there\\'s this controversy\\nsection for his Wikipedia page in 1993, something\\nhappened or something like this. Bring it up that will\\ndrive engagement up anyway. - Yeah. That\\'s right. I mean, look, this gets this whole thing\\nof like, so, you know, the chat interface has this whole concept of\\nprompt engineering, right? - [Lex] Yes, yes. - Prompts. Well it turns\\nout one of the things that LLMs are really good at\\nis writing prompts, right? - [Lex] Yeah. - And so like, what if you just outsourced and by the way, you could\\nrun this experiment today, you could hook this up to do this today. The latency\\'s not good\\nenough to do it real time in a conversation. But you could run this experiment\\nand you just say, look, every 20 seconds you\\ncould just say, you know, tell me what the optimal\\nprompt is and then ask yourself that question to gimme the result. And then exactly to your point, as you add, there will be these systems that are gonna have\\nthe ability to be alert and updated essentially in real time. And so you\\'ll be able to\\nhave a pendant or your phone or whatever, watch or whatever\\nit\\'ll have a microphone on. It\\'ll listen to your conversations, it\\'ll have a feed of everything\\nelse happen in the world, and then it\\'ll be you\\nknow, sort of retraining, prompting or retraining itself on the fly. And so the scenario you\\ndescribed is actually a completely doable scenario. Now the hard question\\non this is always okay, since that\\'s possible, are\\npeople gonna want that? Like what\\'s the form of experience? You know, that we won\\'t\\nknow until we try it. But I don\\'t think it\\'s\\npossible yet to predict the form of AI in our lives. Therefore, it\\'s not possible to predict the way in which it will intermediate our experience with reality yet. - Yeah. But it feels like\\nthere\\'s going to be a killer app. There\\'s probably a mad scramble right now. And so it\\'ll open AI and\\nMicrosoft and Google and Meta and in startups and smaller\\ncompanies figuring out what is the killer app\\nbecause it feels like it\\'s possible like a\\nChatGPT type of thing. It\\'s possible to build that, but that\\'s 10X more compelling\\nusing already the LLMs we have using even the open source LLMs and the different variants. So you\\'re investing in a lot of companies and you\\'re paying attention, who do you think is gonna win this? Do you think there\\'ll be, who\\'s gonna be the next\\npage rank inventor? - Trillion dollar question. - Another one. We have\\na few of those today. - There\\'s a', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 517638, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content=\" always okay, since that's possible, are\\npeople gonna want that? Like what's the form of experience? You know, that we won't\\nknow until we try it. But I don't think it's\\npossible yet to predict the form of AI in our lives. Therefore, it's not possible to predict the way in which it will intermediate our experience with reality yet. - Yeah. But it feels like\\nthere's going to be a killer app. There's probably a mad scramble right now. And so it'll open AI and\\nMicrosoft and Google and Meta and in startups and smaller\\ncompanies figuring out what is the killer app\\nbecause it feels like it's possible like a\\nChatGPT type of thing. It's possible to build that, but that's 10X more compelling\\nusing already the LLMs we have using even the open source LLMs and the different variants. So you're investing in a lot of companies and you're paying attention, who do you think is gonna win this? Do you think there'll be, who's gonna be the next\\npage rank inventor? - Trillion dollar question. - Another one. We have\\na few of those today. - There's a bunch of those. So look, there's a really\\nbig question today. Sitting here today is\\na really big question about the big models\\nversus the small models that's related directly\\nto the big question of proprietary versus open. Then there's this big\\nquestion question of you know, where is the training data gonna, like, are we topping out of\\nthe training data or not? And then are we gonna be able\\nto synthesize training data? And then there's a huge pile\\nof questions around regulation and you know, what's\\nactually gonna be legal. And so I would, when we think about it, we dovetail kind of all\\nthose questions together. You can paint a picture of\\nthe world where there's two or three God models that are just at like staggering scale and they're just better at everything. And they will be owned by\\na small set of companies and they will basically\\nachieve regulatory capture over the government and they'll\\nhave competitive barriers that will prevent other people from, you know, competing with them. And so, you know, there will be, you know, just like there's like,\\nyou know, whatever, three big banks or three\\nbig, you know, or by the way, three big search companies\\nor I guess two now, you know, it'll centralize like that. You can paint another very different\\npicture that says, no, actually the opposite\\nof that's gonna happen. This is gonna basically that\\nthis is the new gold, you know, this is the new gold rush alchemy. Like you know, this is the big bang\\nfor this whole new area of science and technology. And so therefore you're gonna\\nhave every smart 14-year-old on the planet building open source, right? You know, and figuring out a\\nways to optimize these things. And then, you know, we're just gonna get like\\noverwhelmingly better at generating trading data. We're gonna, you know, bring in like blockchain\\nnetworks to have like an economic incentive to\\ngenerate decentralized training data and so forth and so on. And then basically we're\\ngonna live in a world of open source and there's\\ngonna be a billion LLMs, right? Of every size, scale,\\nshape and description. And there might be a few big ones that are like the super genius ones, but like mostly what we'll\\nexperience is open source and that's, you know,\\nthat's more like a world of like what we have today\\nwith like Linux and the web. - Okay, but you painted these two worlds. But there's also\\nvariations of those worlds, 'cause you said regulatory\\ncapture is possible to have these tech giants that don't\\nhave regulatory capture, which is something you're also\\ncalling for saying it's okay to have big companies\\nworking on this stuff as long as they don't\\nachieve regulatory capture. But I have the sense that\\nthere's just going to be a new startup that's going to basically be the page rank inventor, which has become the new tech giant. I don't know, I would love to hear your kind\\nof opinion if Google, Meta and Microsoft are as\\ngigantic companies able to pivot so hard to create new products. Like some of it is just\\neven hiring people or having a corporate structure that\\nallows for the crazy young kids to come in and just create\\nsomething totally new. Do you think it's possible or do you think it'll come from a startup? - Yeah, it is this always\\nbig question, which is, you get this feeling, I hear about this a lot\\nfrom CEOs, founder CEOs where it's like, wow,\\nwe have 50,000 people, it's now harder to do new things than it was when we had 50 people. - [Lex] Yeah. - Like, what has happened? So that's a recurring phenomenon. By the way, that's one of the reasons\\nwhy there's always startups and why there's venture capital. That's like a timeless kind of thing. So that's one observation. On page rank, we can talk about that. But on page rank,\\nspecifically on page rank, there actually is a page. So there is a page rank\\nalready in the field and it's the transformer, right? So the big breakthrough\\nwas the transformer. And the transformer was\\ninvented in 2017 at Google. And this is actually like\\nreally an interesting question 'cause it's like, okay, the transformers like why\\ndoes open AI even exist? Like the Transformers invested at Google. Why didn't Google? I asked a guy I know who\\nwas senior at Google brain kind of when this was happening. And I said, if Google had\\njust gone flat out to the wall and just said, look, we're gonna launch, we're gonna launch the equivalent of GPT4 as fast as we can. I said, when could we have had it? And he said, 2019. They could have just\\ndone a two year sprint with the Transformer and\\nbecause they already had the compute at scale. They already had all the training data, they could have just done it. There's a variety of\\nreasons they didn't do it. This is like a classic big company thing. IBM invented the relational\\ndatabase in the 1970s, let it sit on the shelf as a paper. Larry Ellison picked\\nit up and built Oracle. Xerox Park invented the\\ninteractive computer. They let it sit on the shelf. Steve Jobs came and turned\\nit into the Macintosh, right? And so there is this pattern.\\nNow having said that, sitting here today, like\\nGoogle's in the game, right? So Google, you know, they maybe they let like a\\nfour year gap there go there that they maybe shouldn't have, but like they're in the\\ngame and so now they've got, you know, now they're committed. They've done this merger,\\nthey're bringing in demos, they've got this merger with DeepMind. You know, they're piling in resources. There are rumors that they're, you know, building up an incredible,\\nyou know, super LLM you know, way beyond what we even have today. And they've got, you\\nknow, unlimited resources and a huge, you know, they've\\nbeen challenged their honor. - Yeah. I had a chance to\\nhang out with (indistinct) a couple days ago and we took this walk and there's this giant new building where there's going to be\\na lot of AI work being done and it's kind of this\\nominous feeling of like the fight is on. - [Marc] Yeah. - Like there's this beautiful\\nSilicon Valley nature, like birds are chirping\\nand this giant building and it's like the beast has been awakened. - [Marc] Yeah. - And then like all the big\\ncompanies are waking up to this. They have the compute, but\\nalso the little guys have, it feels like they have\\nall the tools to create the killer product that, and then there's also tools to scale if you have a good idea, if\\nyou have the page rank idea. So there's several things\\nthat it's page rank, there's page rank, the algorithm and the\\nidea and there's like the implementation of it. And I feel like killer\\nproduct is not just the idea, like the transform, it's the implementation something really compelling about it. Like you just can't\\nlook away something like the algorithm behind TikTok\\nversus TikTok itself, like the actual experience\\nof TikTok that just, you can't look away. It feels like somebody's\\ngonna come up with that. And it could be Google, but it feels like it's\\njust easier and faster to do for a startup. - Yeah. So, the startup, the huge advantage that\\nstartups have is they just, there's no sacred cows. There's no historical legacy to protect, there's no need to reconcile your new plan with the existing strategy. There's no communication overhead. There's no, you know, big\\ncompanies are big companies. They've got pre-meetings\\nplanning for the meeting, then they have the post\\nmeeting, the recap, then they have the\\npresentation of the board, then they have the next\\nrounds of meetings. And that's the-- - [Lex] Lots of meetings. - That's the elapsed time when the startup launches\\nits product. Right? So, there's a timeless, right? - [Lex] Yeah. - So there's a timeless thing there now. What the startups don't have\\nis everything else, right? So startups, they don't have a brand, they don't have customer relationships. They've gotten no distribution, they've got no, you know, scale. I mean sitting here today,\\nthey can't even get GPUs. Right. Like there's like a GPU shortage. Startups are literally\\nstalled out right now 'cause they can't get chips,\\nwhich is like super weird. - [Lex] Yeah. They got the cloud. - Yeah. But the clouds\\nrun out of chips. Right. And then to the extent\\nthe clouds have chips, they allocate them to the big customers. Not the small customers. Right. And so the small companies\\nlack everything other than the ability to just\\ndo something new. Right. And this is the timeless race and battle. And this is kinda the point\\nI tried to make in the essay, which is like, both\\nsides of this are good. Like, it's really good to have like highly-scaled tech companies that can do things that are like at staggering\\nlevels of sophistication. It's really good to have\\nstartups that can launch brand-new ideas. They ought to be able to\\nboth do that and compete. They, neither one ought to be subsidized or protected from the others. Like that's, to me, that's just like very\\nclearly the idealized world. It is the world we've been\\nin for AI up until now. And then of course there are people trying to shut that down. But my hope is that, you know, the best outcome clearly\\nwill be if that continues. - We'll talk about that a little bit, but I'd love to linger on some of the ways this is\\ngoing to change the internet. So I don't know if you remember, but there's a thing called\\nMosaic and there's a thing called Netscape Navigator. So you were there in the beginning. What about the interface to the internet? How do you think the browser changes and who gets to own the browser? We got to see some very\\ninteresting browsers, Firefox, I mean all the\\nvariants of Microsoft, Internet Explorer, Edge,\\nand now Chrome, the actual, and he seems like a dumb question to ask, but do you think we'll\\nstill have the web browser? - So I have an eight-year-old\\nand he's super into, he's like Minecraft and learning to code and doing all this stuff. So, of course I was\\nvery proud I could bring sort of fire down from\\nthe mountain to my kid and I brought him ChatGPT\\nand I hooked him up on his laptop. And I was like, you know, this is the thing that's gonna\\nanswer all your questions. And he's like, okay. And I'm like, but it's gonna\\nanswer all your questions. And he's like, well of\\ncourse, like it's a computer. Of course it answers all your questions. Like, what else would a\\ncomputer be good for, dad? - [Lex] And never impressed, are they? - Not impressed in the least. Two weeks passed. And he has some question and I say, well, have you asked ChatGPT? And he's like, dad, Bing is better. - [Lex] Ooh. - And why is Bing better? Is because it's built into the browser. 'Cause he's like, look, I have the Microsoft Edge browser and like it's got Bing right here. And then he doesn't know this yet, but one of the things you\\ncan do with Bing and Edge is there's a setting where you\\ncan use it to basically talk to any webpage because\\nit's sitting right there next to the browser. And by the way, which\\nincludes PDF documents. And so you can, in the way\\nthey've implemented an Edge with Bing is you can load a PDF and then you can ask it questions, which is the thing you can't\\ndo currently in just ChatGPT. So they're, you know, they're gonna, they're\\ngonna push the meld. I think that's great. You know, they're gonna push the melding and see if there's a\\ncombination thing there. Google's rolling out this thing, the magic button, which is\\nimplemented in, you know, they put it in Google Docs, right? And so you go at a, you know, Google Docs and you create\\na new document and you know, you instead of like, you\\nknow, starting to type, you just, you know, say it, press the button and it starts to like, generate content for you, right? Like, is that the way that it'll work? Is it gonna be a speech UI\\nwhere you're just gonna have an earpiece and talk to it all day long? You know, is it gonna be a,\\nlike these are all, like, this is exactly the kind\\nof thing that I don't, this is exactly the kind of thing I don't think is possible to forecast. I think what we need to do is\\nlike run all those experiments and so one outcome is we\\ncome out of this with like a super browser that has AI built in that's just like amazing. There's a real possibility that the whole, I mean, look, there's a possibility here that the whole idea of\\na screen and windows and all this stuff just goes away 'cause like, why do you need that if\\nyou just have a thing that's just telling you\\nwhatever you need to know? - Well and also, so there's\\napps that you can use, you don't really use them. You know, being a Linux\\nguy and Windows guy, there's one window, the browser that with\\nwhich you can interact with the internet, but on the\\nphone you can also have apps. So I can interact with\\nTwitter through the app or through the web browser. And that seems like an\\nobvious distinction, but why have the web browser in that case, if one of the apps starts\\nbecoming the everything app. - [Marc] Yeah, that's right. - What is Elon trying to do with Twitter? But there could be others.\\nThere could be like a big app, there could be a Google app\\nthat just doesn't really do search, but just like, do what I guess AOL did back\\nin the day or something where it's all right there and\\nit changes the nature of the internet because\\nwhere the content is hosted, who owns the data? Who owns the content? What is the kind of content you create? How do you make money by creating content? Who are the content creators? All of that. Or it could just keep being the same, which is like with just the\\nnature of webpage changes and the nature of content. But there'll still be a web browser. 'Cause web browser's\\na pretty sexy product. It just seems to work. 'Cause it like you have an interface, a window into the world, and then the world can\\nbe anything you want. And as the world will evolve, it could be different\\nprogramming languages, it can be animated, maybe it's\\nthree dimensional and so on. Yeah, it's interesting. Do you think we'll still\\nhave the web browser? - Well, very medium becomes\\nthe content for the next one. - [Lex] Oh boy. - You know, the AI will be able to give you a browser whenever you want. - [Lex] Oh, interesting. Generate. - Well, another way to\\nthink about it is maybe what the browser is maybe it's just the escape hatch, right? Which is maybe kind of\\nwhat it is today, right? Which is like most of what\\nyou do is like inside a social network or inside a search\\nengine or inside, you know, somebody's app or inside some\\ncontrolled experience, right? But then every once in a\\nwhile there's something where you actually want to jailbreak, you wanna actually get free. - Web browser's the FU to the man. You're allowed to. That's the free internet. - [Marc] Yeah. - Back the way it was in the nineties. - So here's something I'm proud of. So nobody really talks about it. Here's something I'm proud\\nof, which is the web, the browser, the web servers, they're all, they're still backward compatible all the way back to like 1992, right? So like, you can put up a,\\nyou can still, you know what, the big breakthrough of\\nthe web early on the big breakthrough was it made\\nit really easy to read, but it also made it really easy to write, made it really easy to publish. And we literally made\\nit so easy to publish. We made it not only so it\\nwas easy to publish content, it was actually also easy to\\nactually write a web server. - [Lex] Yeah. - Right and you could\\nliterally write a web server in four lines of brol code and you could start\\npublishing content on it, and you could set whatever\\nrules you want for the content, whatever censorship, no\\ncensorship, whatever you want. You could just do that. And as long as you had\\nan IP address, right, you could do that. That still works, right? That like, still works\\nexactly as I just described. So this is part of my\\nreaction to all of this. Like, you know, all this\\njust censorship pressure and all this, you know, these issues around\\ncontrol and all this stuff, which is like, maybe we need to get\\nback a little bit more to the wild west. Like, the wild west is still out there. Now they will try to chase you down. Like they'll try to, you know, people who want a censor will\\ntry to take away you know, your domain name and\\nthey'll try to take away your payments account and so forth if they really don't\\nlike what you're saying. But nevertheless, you like, unless they literally are intercepting you at the ISP level, like you\\ncan still put up a thing. And so I don't know, I think that's important\\nto preserve, right? Like because I mean one is\\njust a freedom argument, but the other's a creativity argument, which is you wanna have the escape hatch so that the kid with the idea\\nis able to realize the idea. 'cause to your point on page rank, you actually don't know what\\nthe next big idea is, right? No, nobody called Larry Page and told him to develop page rank. Like he came up with that on his own. And you wanna always, I think leave the escape\\nhatch for the next, you know, kid or the next Stanford\\ngrad student to have the breakthrough idea and be\\nable to get it up and running before anybody notices. - You and I are both hands of history. So let's step back. We've been talking about the future. Let's step back for a bit\\nand look at the nineties. You created Mosaic web browser, the first widely used web browser. Tell the story of that. And how did it evolve\\ninto Netscape Navigator this the early days? - So full story. So. - [Lex] We were born, - I was born. A small child. - Actually. Yeah, let's go there. Like, when would you first\\nfall in love with computers? - Oh, so I hit the generational jackpot and I hit the Gen X kind of\\npoint perfectly as it turns out. So I was born in 1971. So there's this great website called WTF happened in 1971 dot com,\\nwhich is basically in 1971. It's when everything\\nstarted to go to hell. And I was of course born in 1971. So I like to think that I had\\nsomething to do with that. - Did you make it on the website? - I don't think I made it on the website, but you know, hopefully,\\nsomebody needs to add. - This is where everything. - Maybe I contributed to some\\nof the trends that they do. Every line on that website\\ngoes like that, right? So it's all a picture disaster. But there was this moment in time where 'cause you know, sort\\nof the Apple, you know, the Apple II hit in like 1978\\nand then the IBM PC hit in 82. So I was like, you know,\\n11 when the PC came out. And so I just kind of hit that\\nperfectly and then that was the first moment in time when like, regular people could spend\\na few hundred dollars and get a computer, right? And so that, I just like that resonated right out of the gate. And then the other part\\nof the story is, you know, I was using Apple II,\\nI used a bunch of them, but I was using Apple II and\\nof course it said in the back of every Apple II and every\\nMac it said, you know, designed in Cupertino, California. And I was like, wow, okay. Cupertino must be the like,\\nshining city on the hill. Like Wizard of Oz is\\nlike the most amazing, like city of all time.\\nI can't wait to see it. And of course, years later I came out to Silicon Valley and went to Cupertino and it's just a bunch of office parks at low-rise apartment buildings. So the aesthetics were a\\nlittle disappointing, but, you know, it was the vector\\nright of the creation of a lot of this stuff. So then basically by, so part part, part of my story is just\\nthe luck of having been born at the right time and\\ngetting exposed to PCs. Then the other part is, the other part is when El\\nGore says that he created the internet, he actually is correct in a really meaningful way, which is he sponsored a bill\\nin 1985 that essentially created the modern internet, created what is called\\nthe NSF net at the time, which is sort of the first\\nreally fast internet backbone. And you know, that that bill dumped a ton of money into a bunch of research universities to build out basically\\nthe internet backbone and then the supercomputer\\ncenters that were clustered around the internet. And one of those universities\\nwas University of Illinois where I went to school. And so the other stroke\\nlock that I had was, I went to Illinois basically\\nright as that money was just like getting dumped on campus. And so as a consequence\\nwe had at, on campus, and this was like, you know,\\n89, 90, 91, we had like, you know, we were right\\non the internet backbone. We had like T3 and 45 at the time, T3 45 megabit backbone\\nconnection, which at the time was, you know, wildly state of the art. We had cray super computers. We had thinking machines\\nparallel super computers. We had silicon graphics\\nworkstations, we had Macintosh's, we had next cubes all over the place. We had like every\\npossible kind of computer you could imagine 'cause all this money\\njust fell out of the sky. - [Lex] So you were living in the future. - Yeah. So yeah, quite\\nliterally it was, yeah, like it's all there. It's all like we had\\nfull broadband graphics, like the whole thing. And it's actually funny 'cause they had this is the first time I kind of, it sort of tickled the back\\nof my head that there might be a big opportunity in\\nhere, which is, you know, they embraced it and so\\nthey put like computers in all the dorms and they\\nwired up all the dorm rooms and they had all these, you know, labs everywhere and everything. And then they gave every undergrad a computer account and an email address. And the assumption was that\\nyou would use the internet for four years at college and then you would\\ngraduate and stop using it. And that was that, right? - [Lex] Yeah. - And you would just\\nretire your email address. It wouldn't be relevant\\nanymore 'cause you'd go off from the workplace and\\nthey don't use email. You'd be back to using\\nfax machines or whatever. - Did you have that sense as well? Like, what you said the back\\nof your head was tickled. Like, what was exciting to\\nyou about this possible world? - Well, if this is so\\nuseful in this containment, if this is so useful in\\nthis contain environment that just has this weird\\nsource of outside funding, then if it were practical\\nfor everybody else to have this and if it were cost effective for everybody else to have this, wouldn't they want it? And the overwhelmingly the prevailing view at the time was no,\\nthey would not want it. This is esoteric, weird nerd stuff, right? That like computer science kids like, but like normal people are\\nnever gonna do email. Right. Or be on the internet, right? And so I was just like,\\nwow, like this is actually, like, this is really compelling stuff. Now the other part was, it was all really hard to use\\nand in practice you had to be basically a CS you know, basically had had to BA\\nCS undergrad or equivalent to actually get full use of\\nthe internet at that point. 'cause it was all pretty esoteric stuff. So then that was the other\\npart of the idea, which was, okay, we need to actually\\nmake this easy to use. - So what's involved in creating Mosaic? Like, in creating graphical\\ninterface to the internet? - Yeah, so it was a combination of things. So it was like basically the web existed in an early sort of\\ndescribed as prototype form. And by the way, text only at that point. - What did it look like? What was the web? I mean what and the key figures. Like, what was it? Like, what paint a picture? - It looked like ChatGPT\\nactually it was all text. - Yeah. - And so you had a text-based web browser? Yeah, well actually the original browser, Tim Burners Lee, the original browser, both the original browser\\nand the server actually ran on next cubes. So these were, this was, you know, the computer Steve Jobs made\\nduring the interim period when during the decade long interim period when he was not at Apple, you know, he got fired in 85 and\\nthen came back in 97. So this was in that interim\\nperiod where he had this company called Next and they made these, literally these computers called cubes. And there's this famous\\nstory, they were beautiful, but they were 12 inch by 12\\ninch by 12 inch cubes computers. And there's a famous story\\nabout how they could have cost half as much if it had\\nbeen 12 by 12 by 13. But this cube was like,\\nno, like it has to be. So they were like $6,000\\nbasically academic workstations. They had the first city round\\ndrives, which were slow. I mean it was, the computers\\nwere all but unusable. They were so slow, but\\nthey were beautiful. - Okay, can we actually just\\ntake a tiny tangent there? - Sure. Of course. - The 12 by 12 by 12 that just\\nso beautifully encapsulates Steve Jobs idea of design. Can you just comment on what you find interesting about Steve Jobs? What about that view of the world, that dogmatic pursuit of perfection and how he saw perfection in design? - Yeah, so I guess I'd say like, look, he was a deep believer, I think in a very deep,\\nthe way I interpret it, I don't know if you ever\\nreally described it like this, but the way I interpret\\nit's like this thing and it's actually a thing in philosophy. It's like aesthetics are\\nnot just appearances. Aesthetics go all the way to like deep underlying meaning, right? It's like I'm not a physicist. One of the things I've\\nheard physicists say is one of the things you start to get a sense of when a theory might be correct is when it's beautiful, right? Like, you know, there, right? And so, there's something, and you feel the same thing by the way in like human psychology, right? You know, when you're\\nexperiencing awe, right? You know, there's like a simplicity to it. When you're having an honest\\ninteraction with somebody, there's an aesthetic, I would say calm comes over you 'cause you're actually being fully honest and trying to hide yourself, right? So it's like this very\\ndeep sense of aesthetics. - And he would trust that\\njudgment that he had deep down. Like yeah, even if the\\nengineering teams are saying this is too difficult. Even if whatever the\\nfinance folks are saying, this is ridiculous. The supply chain, all that\\nkind of stuff just makes this impossible. We can't do this kind of material. This has never been done\\nbefore and so on and so forth. He just sticks by it. - Well, I mean, who makes a\\nphone out of aluminum, right? Like, hadn't nobody\\nelse would've done that. And now of course if your phone is made out of aluminum white,\\nyou know, how crude, what a kind of caveman would\\nyou have to be to have a phone that's made outta plastic? Like, right. So like, so it's just this very right. And, you know, look, there's a thousand different\\nways to look at this, but one of the things is just like, look, these things are\\ncentral to your life. Like, you're with your phone more than you're with anything else. Like, it's gonna be in your hand. I mean, you know this, he thought very deeply about\\nwhat it meant for something to be in your hand all day long. But for example, here's an\\ninteresting design thing. Like, he never wanted, my understanding is he never\\nwanted an iPhone to have a screen larger than you could reach with your thumb one handed. And so he was actually opposed to the idea of making the phones larger. And I don't know if you\\nhave this experience today, but let's say there are\\ncertain moments in your day when you might be like, only have one hand available\\nand you might wanna be on your phone. And you're trying to like, send a text and your thumb can't\\nreach the send button. - Yeah. I mean there's\\npros and cons, right? And then there's like folding phones, which I would love to know what he thought thinks about them. But I mean, is there something you\\ncould also just linger on? 'cause he's one of the interesting figures in the history of technology. What makes him as successful as he was? What makes him as interesting as he was? What made him so productive and important in the development of technology? - He had an integrated worldview. So the properly designed device that had the correct functionality, that had the deepest\\nunderstanding of the user, that was the most beautiful, right? Like, it had to be all\\nof those things, right? He basically would drive\\nto as close to perfect as you could possibly get. Right? And you know, I suspect that\\nhe never quite, you know, thought he ever got there.\\n'cause most great creators, you know, are generally dissatisfied. You know, you read accounts\\nlater on and all they can, all they can see are the\\nflaws in their creation. But like he got as close to\\nperfect each step of the way as he could possibly\\nget with the constraints of the technology of his time. And then, you know,\\nlook, he was, you know, sort of famous in the Apple model. It's like, look, they will, you know, this headset that they just came out with, like, you know, it's like a\\ndecade long project, right? It's like, and they're just gonna sit\\nthere and tune and tune and polish and polish and tune and polish and tune and polish until it is as perfect as anybody could possibly make anything. - Yeah. - And then this goes to the way that people describe working\\nwith him was, which is, you know, there was a terrifying\\naspect of working with him, which is, you know, he was,\\nyou know, he was very tough. But there was this thing that\\neverybody I've ever talked to worked for him, says that they all say the following, which is we did the best work of our lives when we worked for him because he set the bar incredibly high. And then he supported us\\nwith everything that he could to let us actually do\\nwork of that quality. So a lot of people who were at Apple spend the rest of their lives trying\\nto find another experience where they feel like they're able to hit\\nthat quality bar again. - Even if it in retrospect or\\nduring it felt like suffering. - Yeah, exactly. - What does that teach you\\nabout the human condition? Huh? - So look, so say exactly. So the Silicon Valley, I mean,\\nlook, he's not, you know, George Patton you know in the Army. Like, you know, there are\\nmany examples in other fields, you know, that are like\\nthis specifically in tech. It's actually, I find it very interesting. There's the Apple way, which\\nis polish, polish, polish, and don't ship until it's as\\nperfect as you can make it. And then there's the sort\\nof the other approach, which is the sort of\\nincremental hacker mentality, which basically says, ship\\nearly and often and iterate. And one of the things I\\nfind really interesting is I'm now 30 years into this, like, they're very successful\\ncompanies on both sides of that approach, right? Like, that is a fundamental\\ndifference, right? In how to operate and how to\\nbuild and how to create that. You have world class companies\\noperating in both ways. And I don't think the question of like, which is the superior\\nmodel is anywhere close to being answered. Like, and my suspicion\\nis the answer is do both. The answer is you actually want both. They lead to different outcomes. Software tends to do better\\nwith the iterative approach. Hardware tends to do\\nbetter with the, you know, sort of wait and make it perfect approach. But again, you can find\\nexamples in both directions. - So the jury's still out on that one. So back to Mosaic. So, what it was text based Tim Burns Lee? - Well, there was the\\nweb, which was text based, but there were no, I mean\\nthere was like three websites. There was like no content,\\nthere were no users. Like, it wasn't like a catalytic, it hadn't, and by the way, it was all because it was all text. There were no documents, there were no images,\\nthere were no videos, there were no, right. So, and then if, if in the beginning, if you had to be on a next cube, right? You need to had a next cube\\nboth to publish and to consume. - So, there was 6,000 bucks you said. - There were limitations. Yeah. $6,000 PC. They\\ndid not sell very many. But then there was also, there was also FTP and\\nthere was Use Nets, right? And there was, you know, a dozen other basically there's waste, which was an early search thing. There was Gopher, which\\nwas an early menu based information retrieval system. There were like a dozen\\ndifferent sort of scattered ways that people would get to\\ninformation on the internet. And so the Mosaic idea was basically bring those all together, make the whole thing\\ngraphical, make it easy to use, make it basically bulletproof\\nso that anybody can do it. And then again, just on the luck side, it so happened that this was right at the moment when graphics, when the GUI sort of actually took off and we're now also used to the GUI that we think it's been around forever. But it didn't real, you know, the Macintosh brought it out in 85, but they actually didn't\\nsell very many Macs in the eighties. It was not that successful of a product. It really was. You needed Windows 3.0 on\\nPCs and that hit in about 92. And so, and we did most in 92, 93. So that sort of, it was like right at the\\nmoment when you could imagine actually having a graphical\\nuser interface to right at all, much less one to the internet. - How old did Windows 3 sell? So was that the really big. - [Marc] That was the big bang. - The big operating\\ngraphical operating system? - Well this is the classic, okay. This Microsoft was operating on the other, so Steve the Apple was\\nrunning on the Polish until it's perfect. Microsoft famously ran on the other model, which is ship and iterate. And so in the old line in those days was Microsoft Right's version three of every Microsoft product. That's the good one, right? And so there there are, you can find online Windows 1, Windows 2. Nobody used them. Actually the original Windows, in the original Microsoft Windows, the windows were non overlapping. And so you had these very small, very low resolution screens and then you had literally-- - [Lex] Windows. - It just didn't work. It wasn't ready yet. Well. - And Windows 95 I think\\nwas a pretty big leap also. - That was a big leap too. So that was like bang, bang. And then of course Steve, and then when, you know, in the fullness of time Steve came back, then the Mac started, took off again. That was the third bang. And then the iPhone was the fourth bang. - Such exciting time. - And then we were off,\\noff to the races because. - Nobody could have known what\\nwould be created from that. - Well, Windows 3.1 or 3.0, Windows 3.0 to the iPhone\\nwas only 15 years. Right. Like that ramp was in retrospect. At the time it felt like it took forever. But that in histor in historical terms, like that was a very fast ramp from even a graphical computer at all on your desk to the iPhone. That was 15 years. - So, did you have a sense\\nof what the internet will be as you're looking through\\nthe window of Mosaic? Like, what you, like there's\\njust a few web pages for now. - So the thing I had early on\\nwas I was keeping at the time what there's disputes over\\nwhat was the first blog, but I had one of them that\\nat least is a possible, at least a rudder up in the competition. And it was what was called\\nthe What's new page. And it was literally, it was a hardwired in\\ndistribution unfair advantage. I wired, put it right in the browser, I put it in the browser\\nand then I put my resume in the browser, which also was-- - [Lex] Hilarious. - But I was keeping not many people get to get to do that. - No, good call. And early days. It's so interesting. - I'm looking for my, about about, oh, Marc is looking for a job. - [Lex] Yeah, yeah, exactly. - So the West New page, I would literally get up every morning and I would, or every afternoon\\nand I would basically, if you wanted to launch a website, you would email me and I would\\nlist it on the most new page. And that was how people\\ndiscovered the new websites as they were coming out. And I remember 'cause it was like one, it literally went from, it was like one every couple\\ndays to like one every day to like two every day. - And then so you're doing, so that blog was kind of\\ndoing the directory thing. So like, what was the homepage? - So the homepage was just\\nbasically trying to explain even what this thing is that\\nyou're looking at. Right. Basically the basic instructions. But then there was a button, there was a button that said what's new. And what most people did was they went to, for obvious reasons went to what's new. - [Lex] Yeah. - But like it was so mind\\nblowing at that point. This the basic idea and it\\nwas, this was like, you know, this was the basic idea of the internet, but people could see\\nit for the first time. The basic idea was, look,\\nyou know, some, you know, it's like literally it's like\\nan Indian restaurant in like Bristol England has like\\nput their menu on the web. And people were like, wow. - [Lex] Whoa. - Because like that's the first\\nrestaurant menu on the web. - [Lex] Yeah. - And I don't have to be\\nin Bristol and I don't know if I'm ever gonna go to Bristol. And I don't even like Indian\\nfood and like. Wow. Right. And it was like that the first web, the first streaming video thing was it was in another England,\\nsome Oxford or something. Some guy put his coffee pot up\\nas the first streaming video thing and he put it on the\\nweb 'cause he literally, it was the coffee pot down the hall. And he wanted to see when\\nhe needed to go refill it. But there were, you know, there was a point when\\nthere were thousands of people like watching that coffee pot 'cause it was the first\\nthing you could watch. - Well, but isn't were you able\\nto kind of infer, you know, if that Indian restaurant could go online. Then you're like they all will. - [Marc] Yeah, exactly. - So you felt that? - [Marc] Yeah, yeah, yeah. - Okay. - Now, you know, look, it's\\nstill a stretch, right? It's still a stretch 'cause\\nit's just like, okay, is it, you know, you're still in this\\nzone, which is like, okay, is this a nerd thing? Is this a real person thing? By the way, you know, there was a wall of\\nskepticism from the media. Like, they just, like,\\neverybody was just like, yeah, this is the crazy, this is just like dumb. This is not, you know, this is not for regular\\npeople at that time. And so you, you had to think\\nthrough that and then look, it was still hard to get on the internet at that point, right? So you could get kind of this\\nweird bastardized version if you were on AOL,\\nwhich wasn't really real. Or you had to go like,\\nlearn what an ISP was. You know, in those days, PCs actually didn't have TCPIP\\ndrivers come reinstalled. So you had to learn\\nwhat a TCPIP driver was. You had to buy a modem, you\\nhad to install driver software. I have a comedy routine. I do. So it's like 20 minutes long\\ndescribing all the steps required to actually get on\\nthe internet at this point. And so you had to look\\nthrough these practical. Well, and then speed performance 14-4 modems, right? Like it was like watching,\\nyou know, glue dry, like, and so you had to, there were basically a sequence of bets that we made where you basically needed to look through that current\\nstate of affairs and say, actually there's gonna\\nbe so much demand for once people figure this out, there's gonna be so much demand for it that all of these practical problems\\nare gonna get fixed. - Some people say that\\nthe anticipation makes the destination that much more exciting. - Do you remember progressive JPEGs? - Yeah. Do I, do I? - For kids in the audience, right? - [Lex] For kids in the audience. - You used to have to watch an image load like a line at the time. But it turns out there\\nwas this thing with JPEGs where you could load\\nbasically every fourth, you could load like every fourth line and then you could sweep\\nback through again. And so you could like\\nrender a fuzzy version of image up front. And then it would like\\nresolve into the detailed one. And that was like a big UI breakthrough 'cause it gave you something to watch. - Yeah. And you know, there's applications in\\nvarious domains for that. - Well it was a big fight. There was a big fight early on\\nabout whether there should be images in the web. And. - For that reason for\\nlike sexualization or-- - Not explicitly that that did come up. But it wasn't even that, it was more just like all the\\nserious in the argument went, the purists basically said\\nall the serious information in the world\", metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 517638, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content=' say that\\nthe anticipation makes the destination that much more exciting. - Do you remember progressive JPEGs? - Yeah. Do I, do I? - For kids in the audience, right? - [Lex] For kids in the audience. - You used to have to watch an image load like a line at the time. But it turns out there\\nwas this thing with JPEGs where you could load\\nbasically every fourth, you could load like every fourth line and then you could sweep\\nback through again. And so you could like\\nrender a fuzzy version of image up front. And then it would like\\nresolve into the detailed one. And that was like a big UI breakthrough \\'cause it gave you something to watch. - Yeah. And you know, there\\'s applications in\\nvarious domains for that. - Well it was a big fight. There was a big fight early on\\nabout whether there should be images in the web. And. - For that reason for\\nlike sexualization or-- - Not explicitly that that did come up. But it wasn\\'t even that, it was more just like all the\\nserious in the argument went, the purists basically said\\nall the serious information in the world is text. If you introduce images, you basically are gonna bring\\nin all the trivial stuff. You\\'re gonna bring in\\nmagazines and you know, all this crazy just, you know,\\nstuff that, you know, people, you know, it\\'s gonna, it is\\ngonna distract from that. It\\'s gonna go take it away from being\\nserious to being frivolous. - Well, was there any\\n(indistinct) type arguments about the internet destroying all of human\\ncivilization or destroying some fundamental fabric of human civilization? - So it was, those days it was all around crime and terrorism. So those arguments happened, you know, but there was no sense yet\\nof the internet having like, an effect on politics because\\nthat was way too, too far off. But there was an enormous panic at the time around cybercrime. There was like enormous panic\\nthat like your credit card number would get stolen and you\\'d use life\\nsavings would be drained. And then, you know, criminals were gonna, there was, oh, when we started,\\none of the things we did, one of the Netscape browser\\nwas the first widely used piece of consumer software that had\\nstrong encryption built in, it made it available to ordinary people. And at that time, strong encryption was\\nactually illegal to export outta the US so we could feel that product in the US, we could not export it \\'cause it was classified as munition. So the Netscape browser\\nwas on a restricted list along with the tomahawk missile as being something that\\ncould not be exported. So we had to make a second\\nversion with deliberately weak encryption to sell\\noverseas with a big logo on the box saying, do not trust this. Which it turns out, makes it hard to sell software\\nwhen it\\'s got a big logo that says don\\'t trust it. And then we had to spend\\nfive years fighting the US government to get\\nthem to basically stop trying to do this regulation. But because the fear\\nwas terrorists are gonna use encryption, right? To like plot, you know, all these things. And then, you know, we responded with, well actually we need\\nencryption to be able to secure systems so that the terrorists and the criminals can\\'t get into them. So that anyway, that was the 1990s fight. - So can you say something\\nabout some of the details of the software engineering\\nchallenges required to build these browsers? I mean the engineering\\nchallenges of creating a product that hasn\\'t really existed before that can have such\\nalmost like limitless impact on the world with the internet. - So there was a really key\\nbet that we made at the time, which was very controversial, which was core to core\\nto how it was engineered, which was are we\\noptimizing for performance or for ease of creation? And in those days the pressure\\nwas very intense to optimize for performance because the\\nnetwork connections were so slow and also the computers were so slow. And so if you had, I mentioned\\nthe progressive JPEGs, like if there\\'s an alternate\\nworld in which we optimized for performance and it just, you had just a much more pleasant\\nexperience right up front. But what we got by not doing that was we got ease of creation. And the way that we got\\nease of creation was all of the protocols and\\nformats were in text, not in binary. And so HTTP is in text, by the way. And this was an internet\\ntradition by the way that we picked up. But we continued it. HTTP is text and HTML is\\ntext, and then every else, everything else that\\nfollowed is text as a result. And by the way, you can imagine purist\\nengineers saying this is insane. You have very limited bandwidth. Why are you wasting any time sending text? You should be encoding\\nthis stuff into binary and it\\'ll be much faster. And of course the answer\\nis that\\'s correct. But what you get when you make\\nit taxed is all of a sudden, well, the big breakthrough was the view source function, right? So the fact that you\\ncould look at a webpage, you could hit view source\\nand you could see the HTML, that was how people learned\\nhow to make webpages. Right? - It\\'s so interesting \\'cause the stuff would\\ntake for granted now is, man, that was fundamental, the development of the web\\nto be able to have HTML just right there, all the\\nghetto mess that is HTML, all the sort of almost\\nbiological like messiness of HTML and then having the browser\\ntry to interpret that as. - [Marc] Exactly. - To show something reasonable. - Well and then there was\\nthis internet principle that we inherited, which\\nwas emit, what was it? Emit cautiously. Emit\\nconservatively interpret liberally. So it basically meant if you\\'re, the design principle was if you\\'re creating like a web editor that\\'s gonna admit HTML, like\\ndo it as cleanly as you can, but you actually want the\\nbrowser to interpret liberally, which is you actually want\\nusers to be able to make all kinds of mistakes and\\nfor it to still work. And so the browser rendering\\nengines to this day have all of this spaghetti code crazy stuff where they\\'re resilient to\\nall kinds of crazy issue, no mistakes. And so, literally what I\\nalways had in my head is like there\\'s an 8 year old or\\nan 11 year old somewhere and they\\'re doing a view source, they\\'re doing a cut and\\npaste and they\\'re trying to make a webpage for\\ntheir eternal or whatever. And like they leave out a\\nslash and they leave out an angle bracket and they do this and they do that and it\\'s still works. - It\\'s also like a, I don\\'t often think about this,\\nbut, you know, programming, you know, C++ all those languages, lisp, the compiled languages,\\nthe interpreted languages, Python, Pearl, all that. The brace have to be all correct. It\\'s like everything has to be perfect. - [Marc] Brutal. - And then-- - [Marc] Autistic. - You forget. All right. It\\'s systematic\\nand rigorous, let\\'s go there. But you forget that the, the web with JavaScript eventually. And HTML is allowed to be messy in the way for the first time. Messy in the way biological\\nsystems could be messy. It\\'s like the only thing\\ncomputers were allowed to be messy on for the first time. - It used to off fend me. So I grew up on Unix, so I worked on Unix. I was a Unix native for all\\nthe way through this period. And so, it used to drive\\nme bananas when it would do the segmentation fault\\nand the core dump file, just like it is, you know, it\\'s like literally there\\'s\\nlike a error in the code. The math is off by one. And it core dumps. And I\\'m in the core dump\\ntrying to analyze it and trying to reconstruct what, and I\\'m\\njust like, this is ridiculous. Like, the computer\\nought to be smart enough to be able to know that if it\\'s off by one, okay fine. And it keeps running. And I would go ask all the experts like, why can\\'t it just keep running? And they\\'d explain to me, well, because all the downstream\\nrepercussions and blah blah. And I\\'m like, this still,\\nlike, you know, this is, we\\'re forcing the human\\ncreator to live to your point in this hyper, literal\\nworld of perfection. - [Lex] Yeah. And I was just like, that\\'s just bad. And by the way, you know what happens with that of course. Just what what happened with,\\nwith coding at that point, which is you get a high\\npriesthood, you know, there\\'s a small number of\\npeople who are really good at doing exactly that. Most people can\\'t. And most people are excluded from it. And so actually that was where that for that was where I picked up that idea was like, no, you want these things to be\\nresilient error in all kinds and this would drive the\\npurist absolutely crazy. Like, I got attacked on this like a lot \\'cause I mean like every time you know, all the purists who\\nwere like into all this like Marcup language stuff and formats and codes and all this stuff, they would be like, you know, you\\'re encouraging bad behavior \\'cause. - Oh, so they wanted\\nthe browser to give you a fault error anytime there was a-- - Yeah. They wanted to\\nbe a (indistinct) right? They wanted to-- Yeah. Yeah. That was a very and any properly trained credential engineer would be like, that\\'s not how you build these systems. - That\\'s such a bold move to say, no, it doesn\\'t have to be. - Yeah. No, like I said, the good news for me is\\nthe internet kind of had that traditional already,\\nbut having said that, like we pushed it, we pushed it way out. But the other thing we did, going back to the performance thing, was we gave up a lot of performance. We made that, that initial experience for the first few years\\nwas pretty painful. But the bet there was\\nactually an economic bet, which was basically the demand\\nfor the web would basically mean that there would be a\\nsurge in supply of broadband. Like because the question was, okay, how do you get the phone\\ncompanies which are not famous in those days for doing\\nnew things at huge cost for like speculative reasons. Like how do you get them to\\nbuild up broadband, you know, spend billions of dollars\\ndoing that and you know, you could go meet with them\\nand try to talk them into it. Or you could just have a thing where it\\'s just very\\nclear that it\\'s gonna be, that people love that\\'s gonna\\nbe better if it\\'s faster. And, so that, there was a\\nperiod there and this was, this was fraught with in peril, but there was a period there\\nwhere it\\'s like we knew the experience was sub-optimized because we were trying to force the emergence of demand for broadband. - [Lex] Sure. - Which is in fact what happened. - So you had to figure out\\nhow to display this text, HTML text. So the blue links and\\nthe prop links. What? And there\\'s no standards. Is\\nthere standards at that time? - [Marc] No. There really still isn\\'t. - Well there\\'s like standards, there\\'s applied, implied standards. Right. And they, you know, there\\'s all these kind of new features that are being added with like CSS, what, like what kind of stuff a\\nbrowser should be able to support features within languages,\\nwithin JavaScript and so on. But you\\'re setting standards\\non the fly yourself. - Yeah. Well to this day, if you create a webpage\\nthat has no CSS style sheet, the browser will render\\nit however it wants to. Right. So this was one of the\\nthings, there was this idea, this idea of at the time and\\nhow these systems were built, which is separation of content from format or separation of content from appearance. And that\\'s still, people\\ndon\\'t really use that anymore \\'cause everybody wants to\\ndetermine how things look and so they use CSS\\nbut it\\'s still in there that you can just let the\\nbrowser do all the work. - I still like the like\\nreally basic websites, but that could be just old school, kids these days with their\\nfancy responsive websites that don\\'t actually have much content, but have a lot of visual elements. - Well that\\'s one of the\\nthings that\\'s fun about chat, you know, about ChatGPT like. - [Lex] Back to the basics. - Back to just text. - [Lex] Yeah. - Right? And it, you know, there is this pattern in\\nhuman creativity and media where you end up back at text\\nand I think there\\'s, you know, there\\'s something powerful in there. - Is there some other stuff you remember like the purple links? There were some interesting\\ndesign decisions that to kind of come up that we have today or we don\\'t have today\\nthat were temporary. - So I made the background\\n\\'cause I hated reading texts on white background, so I\\nmade the background gray. Everybody can-- - Do you go ahead to? - No. No, no. That decision I think has been reversed. But now I\\'m happy though because\\nnow dark mode is the thing. - So it wasn\\'t about gray, it was just you didn\\'t\\nwant white background. - [Marc] Strain my eyes. - Strain your eyes. Interesting. And then there\\'s a bunch\\nof other decisions. I\\'m sure there\\'s an interesting\\nhistory of the development of HTML and CSS and\\nInterface and JavaScript and there\\'s this whole Java applet thing. - Well the big one probably\\nJavaScript, CSS was after me, so I didn\\'t, that was not me. But JavaScript was the big, JavaScript maybe was the\\nbiggest of the whole thing. That was us. And that was basically a bet,\\nit was a bet on two things. One is that the world wanted a new front end scripting language. And then the other was I thought at the time the world wanted a new backend scripting language. So JavaScript was designed\\nfrom the beginning to be both front end and backend. And then it failed as a\\nbackend scripting language. And Java won for a long time. And then Python Pearl and\\nother things, PHP and Ruby. But now JavaScript is back. And so. - I wonder if everything in\\nthe end will run on JavaScript. - It seems like it is the, and by the way, lemme give a shout out\\nto, to Brendan Eich was basically the one man\\ninventor of of JavaScript. - If you\\'re interested to\\nlearn more about Brendan Eich, he\\'s been on his podcast previously. - Exactly. So he wrote\\nJavaScript over a summer and I mean I think it is fair, it is fair to say now that\\nit\\'s the most widely used language in the world and\\nit seems to only be gaining in its in its range of adoption. - You know, in the software world there\\'s quite a few stories of somebody over a week weekend or over a\\nweek or over a summer writing some of the most impactful\\nrevolutionary pieces of software ever. That\\nshould be inspiring. Yes. - Very inspiring. I\\'ll\\ngive you another one. SSL. So SSL with the security\\nprotocol, that was us. And that was a crazy idea at the time, which was let\\'s take\\nall the native protocols and let\\'s wrap them in a security wrapper. That was a guy named Kip Hickman who wrote that over a summer, one guy. And then look today, sitting here today, like the transformer like at Google was a small handful of people. And then, you know, the number of people who have\\ndid like the core work on GPT. It\\'s not that many people, it\\'s a pretty small handful of people. And so yeah, the pattern in software repeatedly over a very long time has been, it\\'s Jeff Bezos always\\nhad the two pizza rule for teams at Amazon, which is any team needs\\nto be able to be fed with two pieces. If you need the third pizza,\\nyou have too many people. And I think it\\'s actually\\nthe one pizza rule. For the really creative work. I think it\\'s two people, three people. - Well that\\'s, you see that with certain open source projects, like so much is done by\\nlike one or two people. Like it\\'s so incredible\\nand that\\'s why you see that gives me so much hope\\nabout the open source movement in this new age of AI where, you know, just recently having had a conversation with Marc Zuckerberg of all people who\\'s all in on open source, which is so interesting to\\nsee and so inspiring to see \\'cause like releasing\\nthese models, it is scary. It is potentially very dangerous\\nand we\\'ll talk about that. But it\\'s also, if you believe in the\\ngoodness of most people and in the skillset of most people and the desire to go do good in the world, that\\'s really exciting. \\'cause it\\'s not putting it these models into the centralized\\ncontrol of big corporations, the government and so on. It\\'s putting it in the hands of a teen, teenage kid with like a dream in his eyes. I don\\'t know. That\\'s beautiful. - Look, this stuff, AI ought to make the\\nindividual coder obviously far more productive right? By like, you know, a\\nthousand X or something. And so you ought to open source like, not just the future of open source AI, but the future of open source everything. We ought to have a world\\nnow of super coders, right? Who are building things as open source with one or two people\\nthat were inconceivable, you know, five years ago. You know, the level of\\nkind of hyper productivity we\\'re gonna get out of\\nour best and brightest I think is gonna go way up. - It\\'s gonna be interesting. We\\'ll talk about it, but let\\'s just to linger\\na little bit on Netscape. Netscape was acquired in\\n1999 for 4.3 billion by AOL. What was that like? What were some memorable aspects of that? - Well that was the height\\nof the.com boom bubble bust. I mean that was the frenzy. If you watch succession, that was like what they\\ndid in the fourth season with Gojo and the merger with their, so it was like the height of like one of those kind of dynamics. And so. - Would you recommend\\nsuccession, by the way? I\\'m more of a Yellowstone guy. - Yellowstone\\'s very American. I\\'m very proud of you. That\\'s, that is. - I just talked to Matthew McConaughey and I\\'m full on Texan at this point. - Good. I approve. - And he\\'ll be doing\\nthe SQL to Yellowstone. - [Marc] Yeah, just exciting. - Very exciting. Anyway. - [Marc] Can\\'t wait. - So that\\'s a rude interruption\\nby me by way of succession. So, that was at the height of the-- - Deal making and money\\nand just the fur flying and like craziness. And so yeah, it was just one of those, it was just like, I mean, and this, the entire (indistinct) thing from start to finish was four years, which was like for one of these companies, it\\'s just like incredibly fast. You know, it went, we went public 18 months\\nafter we got moved where we were founded, which\\nvirtually never happens. So it was just this incredibly fast kind of meteor streaking across the sky. And then of course it was this, and then there was just\\nthis explosion, right? That happened \\'cause then\\nit was almost immediately followed by the.com crash. It was then followed\\nby AOL, by Time Warner, which again is like the succession guys kinda play with that, which turned out to be a disastrous deal. You know, one of the famous, you know, kind of disastrous in business history. And then, you know, what became an internet depression on\\nthe other side of that. But then in that depression\\nin the two thousands was the beginning of broadband and smartphones and Web 2.0 right? And then social media\\nand search and every SaaS and everything that came out of that. - What did you learn from\\njust the acquisition? I mean this is so much money. What\\'s interesting \\'cause I\\nmust have been very new to you, that these software stuff, you can make so much money. There\\'s so much money swimming around. I mean, I\\'m sure the\\nideas of investment was starting to get born there. - Yes. Let me get, so let me lay it. So here\\'s, here\\'s the thing. I dunno if I figured it out\\nthen, but figured it out later, which is software is a technology that it, it\\'s like a, you know, the\\nconcept of the philosopher stone, the philosopher stone in alchemy, transient is led into gold and\\nNewton spent 20 years trying to find the philosopher stone. Never got there. Nobody\\'s ever figured it out. Software is our modern philosopher stone. And in economic terms, it\\ntransmutes labor into capital, which is like a super interesting thing. And by the way, like Carl Marcs is rolling over in his grave right now. \\'Cause of course that\\'s\\ncomplete reputation of his entire theory. Trans labor and capital\\nwhich is as follows, is somebody sits down at a keyboard and types a bunch of stuff in, and a capital asset\\ncomes out the other side and then somebody buys that capital asset for a billion dollars. Like that\\'s amazing, right? It\\'s literally creating\\nvalue right out of thin air, right out of purely human thought, right? And so that, there are many things that make software magical and special, but that\\'s the economics. - I wonder what Marx\\nwould\\'ve thought about that? - Oh, he would\\'ve\\ncompletely broke his brain because of course the whole\\nthing was it was he could, you know, that kind of\\ntechnology was inconceivable when he was alive. It was all industrial era stuff. And so, any kind of machinery\\nnecessarily involved huge amounts of capital. And then labor was on the receiving end of the abuse. - [Lex] Yep. Right? But like software eng software, a software engineer is somebody\\nwho basically transmutes his own labor into actual, an actual capital asset\\ncreates permanent value. Well, and in fact it\\'s\\nactually very inspiring. That\\'s actually more\\ntrue today than before. So when I was doing software, the assumption was all\\nnew software basically has a sort of a parabolic\\nsort of lifecycle, right? So you ship the thing,\\npeople buy it at some point, everybody who wants it has bought it and then it becomes obsolete. And it\\'s like bananas. Nobody, nobody buys old software. These days, Minecraft, Mathematica, you know, Facebook, Google, you have the software\\nassets that are, you know, have been around for 30\\nyears that are gaining in value every year, right? And they\\'re just, they\\'re being\\na world of warcraft, right, salesforce.com, like they\\'re being every single year they\\'re\\nbeing polished and polished and polished and polished. They\\'re getting better\\nand better, more powerful, more powerful, more\\nvaluable, more valuable. So we\\'ve entered this era\\nwhere you can actually have these things that actually\\nbuild out over decades. Which by the way is what\\'s happening right now with like ChatGPT. And so now, this is why, you know, there is always, you know, sort of a constant investment\\nfrenzy around software is because, you know, look, when\\nyou start one of these things, it doesn\\'t always succeed. But when it does now you\\nmight be building an asset that builds value for,\\nyou know, four or five, six decades to come. You know, if you have a team of people who have the level of devotion required to keep making it better. And then the fact that of\\ncourse everybody\\'s online, you know, there\\'s 5 billion people that are a click away from\\nany new piece of software. So the potential market size\\nfor any of these things is, you know, nearly infinite. - [Lex] It must have been\\nsurreal back then though. - Yeah. Yeah. This was\\nall brand new, right? Yeah. Back then, this was all brand new. These were all, you know, brand new. Had you rolled out that\\ntheory in even 1999, people would\\'ve thought\\nyou were smoking crack. So that\\'s emerged over time. - Well, let\\'s now turn\\nback into the future. You wrote the essay \"Why\\nAI Will Save The World?\" Let\\'s start the very high level. What\\'s the main thesis of the essay? - Yeah, so the main thesis on the essay is that what we\\'re dealing\\nwith here is intelligence. And it\\'s really important to kind of talk about the sort of very nature\\nof what intelligence is. And fortunately we have a predecessor to machine intelligence,\\nwhich is human intelligence. And we\\'ve got, you know, observations and theories\\nover thousands of years for what intelligence is\\nin the hands of humans and what intelligence is, right? I mean, what it literally is the way to, you know, capture, process,\\nanalyze, synthesize information, solve problems. But the observation of\\nintelligence in human hands is that intelligence quite literally\\nmakes everything better. And what I mean by that\\nis every kind of outcome of like human quality of life, whether it\\'s education outcomes or success of your children, or career success or health or lifetime\\nsatisfaction, by the way, propensity to peacefulness\\nas opposed to violence, propensity for open-mindedness\\nversus bigotry, those are all associated with\\nhigher levels of intelligence. - Smarter people have better outcomes than almost as you write in almost every domain of activity. Academic achievement, job performance, occupational status, income, creativity, physical health, longevity,\\nlearning new skills, managing complex tasks, leadership,\\nentrepreneurial success, conflict resolution,\\nreading comprehension, financial decision making, understanding others\\nperspectives, creative arts, parenting outcomes, and life satisfaction. One of the more depressing\\nconversations I\\'ve had, and I don\\'t know why it\\'s depressing, I have to really think\\nthrough why it\\'s depressing, but on IQ and the G factor, and that that\\'s something\\nin large part is genetic and it correlates so much\\nwith all of these things and success in life. It\\'s like all the inspirational\\nstuff we read about, like if you work hard and so on, it sucks that you\\'re born with the hand that you can\\'t change. - But what if you could. - You\\'re saying basically\\na really important point, and I think it\\'s in your\\narticles, it really helped me. It\\'s a nice added\\nperspective to think about. Listen, human intelligence, the science of intelligence\\nis shown scientifically that it just makes life easier and better the smarter you are. And now let\\'s look at\\nartificial intelligence and if that\\'s a way to increase\\nsome human intelligence, then it\\'s only going\\nto make a better life. - [Marc] Yeah. - That\\'s the argument. - And certainly at the collective level, we could talk about the collective effect of just having more\\nintelligence in the world, which will have very big payoff. But there\\'s also just\\nat the individual level, like what if every person has a machine? You know? And the concept of augment Doug Engelbart\\'s concept of augmentation. You know, what if\\neverybody has an assistant and the assistant is, you know, 140 IQ and you happen to be 110 IQ and you\\'ve got, you know, something that basically is\\ninfinitely patient and knows everything about you\\nand is pulling for you in every possible way,\\nwants you to be successful. And anytime you find anything\\nconfusing or wanna learn anything or have trouble\\nunderstanding something or wanna figure out what to\\ndo in a situation, right? Wanna figure out how to\\nprepare for a job interview, like any of these things,\\nlike it will help you do it. And it will therefore, the combination will\\neffectively be, you know, will effectively raise your raise because it will effectively raise your IQ, will therefore raise the odds of successful life outcomes\\nin all these areas. - So people below the,\\nthis hypothetical 140 IQ, it\\'ll pull them up towards 140 IQ. - Yeah, yeah, yeah. And then of course, you know, people at 140 IQ will be\\nable to have a peer, right. To be able to communicate, which is great. And then people above 140\\nIQ will have an assistance that they can farm things out to. And then look, God willing, you know, at some point these things\\ngo from future versions go from 140 IQ equivalent to\\n150 to 160 to 180, right? Like Einstein was estimated\\nto be on the order of one 60, you know, so when we\\nget, you know, one 60 AI, like we\\'ll be, you know, when one assumes creating\\nEinstein level breakthroughs and physics, and then at\\n180 we\\'ll be, you know, carrying cancer and developing\\nwarp drive and doing all kinds of stuff. And so it is quite possibly the case, this is the most important\\nthing that\\'s ever happened and the best thing that\\'s ever happened because precisely because it\\'s a lever on this single fundamental\\nfactor of intelligence, which is the thing that drives\\nso much of everything else. - Can you steal, man, the case that human plus AI is\\nnot always better than human for the individual? - You may have noticed that there\\'s a lot of\\nsmart running around. - [Lex] Sure. Yes. - Right? And so, like smart, there are certain people where\\nthey get smarter, you know, they get to be more arrogant, right? So that, you know, there\\'s one huge flaw. - Although to push back on that, it might be interesting because\\nwhen the intelligence is not all coming from you,\\nbut from another system, that might actually increase\\nthe amount of humility even in the assholes. - [Marc] One would hope. - Yeah. - Or it could make assholes more assholes. You know, that\\'s in, I mean, that\\'s for psychology to study. - Yeah, exactly. Another one is smart people\\nare very convinced that they, you know, have a more\\nrational view of the world, and that they have a easier\\ntime seeing through conspiracy theories and hoaxes and right. You know, sort of crazy\\nbeliefs and all that. There\\'s a theory in psychology, which is actually smart people. So for sure people who aren\\'t\\nas smart are very susceptible to hoaxes and conspiracy theories. But it may also be the case\\nthat the smarter you get, you become susceptible in a different way, which is you become very\\ngood at marshaling facts to fit preconceptions, right. You become very, very good at assembling whatever theories and\\nframeworks and pieces of data and graphs and\\ncharts you need to validate whatever crazy ideas got in your head. And so you\\'re susceptible\\nin a different way, right? - We\\'re all sheep, but\\ndifferent colored sheep. - Some sheep are better\\nat justifying it. Right. And those are the, you know, those are the smart sheep, right? So yeah. Look like I\\nwould say this look like there are no panacea. I\\'m not a utopian, there\\nare no panaceas in life. There are no, like, you know, I don\\'t believe there\\nare like pure positives. I\\'m not a transcendental\\nkind of person like that. But, you know, so yeah,\\nthere are gonna be issues and, you know, look, smart people, another maybe you could\\nsave about smart people is they are more likely to get\\nthemselves in situations that are, you know, beyond their grasp. You know, because they\\'re\\njust more confident in their ability to deal with complexity and their eyes become bigger, their cognitive eyes become bigger than their stomach, you know? So yeah, you could argue\\nthose eight different ways nevertheless, on net, right? Clearly, overwhelmingly, again, if you just extrapolate from what we know about human intelligence, you\\'re improving so many aspects of life if you\\'re upgrading intelligence. - So there\\'ll be assistants\\nat all stages of life. So when you\\'re younger,\\nthere\\'s for education, all that kind of stuff for\\nmentorship, all of this. And later on as you\\'re doing\\nwork and you\\'ve developed a skill and you\\'re having a profession, you\\'ll have an assistant\\nthat helps you excel at that profession. So at all stages of life. - Yeah. I mean, look, the\\ntheory is augmentation. This is the Doug Engelbart\\'s term. Doug Engelbart made this observation many, many\\ndecades ago that, you know, basically it\\'s like you can\\nhave this oppositional frame of technology where it\\'s\\nlike us versus the machines, but what you really do\\nis you use technology to augment human capabilities. And by the way, that\\'s how actually the economy develops. That\\'s, we can talk about\\nthe economic side of this, but that\\'s actually how\\nthe economy grows is through technology\\naugmenting human potential. And so, yeah. And then you basically\\nhave a proxy or you know, or you know, a sort of\\nprosthetic, you know, so like you\\'ve got glasses,\\nyou\\'ve got a wristwatch, you know, you\\'ve got shoes, you know, you\\'ve got these things. You\\'ve got a personal computer, you\\'ve got a word processor,\\nyou\\'ve got Mathematica, you\\'ve got Google. This is the latest\\nviewed through that lens. AI is the latest in a long\\nseries of basically augmentation methods to be able to\\nraise human capabilities. It\\'s just this one is the\\nmost powerful one of all, because this is the one\\nthat, that goes directly to what they call fluid\\nintelligence, which is IQ. - Well, there\\'s two categories of folks that you outline that\\nworry about or highlight the risks of AI, and you highlight a bunch of different risks. I would love to go through those risks and just discuss them, brainstorm which ones are serious and which ones are less serious. But first, the Baptist\\nand the bootleggers, what are these two\\ninteresting groups of folks who worry about the effect\\nof AI and human civilization? - [Marc] Or say they do. - Say, oh, okay, yes, I\\'ll say they do. - The Baptist worry the\\nbootleggers say they do. So the Baptist and the\\nbootleggers is a metaphor from economics, from what\\'s\\ncalled development economics. And it\\'s this observation that when you get social\\nreform movements in a society, you tend to get two sets\\nof people showing up, arguing for the social reform. And the term Baptist and bootleggers comes from the American experience\\nwith alcohol prohibition. And so in the 1900s, 1910s, there was this movement\\nthat was very passionate at the time, which basically said, alcohol is evil and is destroying society. By the way, there was a lot\\nof evidence to support this. There were very high rates of very high correlations\\nthen, by the way. And now between rates of physical\\nviolence and alcohol use, almost all violent crimes\\nhave either the perpetrator or the victim, or both drunk almost. If you see this actually in the work, almost all sexual harassment\\ncases in the workplace, it\\'s like at a company\\nparty and somebody\\'s drunk. Like, it\\'s amazing how often\\nalcohol actually correlates to actually dis dysfunction\\nand at leads to domestic abuse and so forth, child abuse. And so you had this group of\\npeople who were like, okay, this is bad stuff and we should outlaw it. And those were quite literally Baptist. Those were super committed, you know, hardcore Christian\\nactivists in a lot of cases. There was this woman whose\\nname was Carrie Nation, who was this older woman who\\nhad been in this, you know, I don\\'t know, disastrous\\nmarriage or something. And her husband had been\\nabusive and drunk all the time. And she became the icon of\\nthe Baptist prohibitionist. And she was legendary in\\nthat era for carrying an ax and doing, you know, completely on her own\\ndoing raids of saloons and like taking her ax to all the bottles and eggs in the back. And so. - [Lex] A true believer. - An absolute true believer, and with absolutely the\\npurist of intentions. And again, there\\'s a very\\nimportant thing here, which is there\\'s, you could look at this\\ncynically and you could say the Baptists are like delusional,\\nyou know, the extremists, but you could also say,\\nlook, they\\'re right. Like she was, you know, she had a point. Like she wasn\\'t wrong about\\na lot of what she said. - Yeah. - But it turns out the way\\nthe story goes is it turns out that there were another set of people who very badly wanted to\\noutlaw alcohol in those days. And those were the bootleggers, which was organized crime that\\nstood to make a huge amount of money if legal alcohol\\nsales were banned. And this was, in fact, the way the history goes\\nis this was actually the beginning of\\norganized crime in the US. This was the big economic\\nopportunity that opened that up. And so they went in together and no, they didn\\'t go in together. Like the Baptist did not\\neven necessarily know about the bootleggers \\'cause they were on their moral crusade. The bootleggers certainly\\nknew about the Baptists. And they were like, wow, these people are like the\\ngreat front people for like. You know, it\\'s-- - [Lex] Good PR. - Shenanigans in the background. And they got the (indistinct)\\nAct passed, right. And they did in fact ban alcohol\\nin the US and you\\'ll notice what happened, which is\\npeople kept drinking, it didn\\'t work, people kept drinking. That bootleggers made a\\ntremendous amount of money. And then over time it became\\nclear that it made no sense to make it illegal and it\\nwas causing more problems. And so then it was revoked. And here we sit with legal\\nalcohol a hundred years later with all the same problems. And you know, the whole thing was this\\nlike giant misadventure the Baptist got taken advantage\\nof by the bootleggers, and the bootleggers got what they wanted. And that was that. - The same two categories of folks are now sort of suggesting\\nthat the development of artificial intelligence\\nshould be regulated. - A hundred percent. It\\'s the same pattern. And the economist will tell you it\\'s the same pattern every time. Like, this is what\\nhappened, nuclear power, this is what happens, which\\nis another interesting one. But like, yeah, this\\nhappens dozens and dozens of times throughout the\\nlast a hundred years and this is what\\'s happening now. - And you write that it isn\\'t\\nsufficient to simply identify the actors and impugn their motives. We should consider the\\narguments of both the Baptist and the bootleggers on their merits. So let\\'s do just that. Risk number one, will AI kill us all? - [Marc] Yes. - So what do you think about this one? What do you think is\\nthe core argument here that the development of\\nAGI perhaps better said, will destroy human civilization? - Well, first of all, you\\njust did a slight of hand \\'cause we went from talking about AI to AGI. - Is there a fundamental difference there? - I don\\'t know. What\\'s AGI? - What\\'s AI, what\\'s in intelligence? - Well, I know what AI\\nis machine learning. What\\'s AGI? - I think we don\\'t know\\nwhat the bottom of the well of machine learning is\\nor what the ceiling is. Because just to call\\nsomething machine learning or just to call some of the statistics or just to call it math or\\ncomputation doesn\\'t mean, you know, nuclear\\nweapons are just physics. So to me it\\'s very\\ninteresting and surprising how far machine learning has taken. - No, but we knew that\\nnuclear physics would lead to weapons. That\\'s why the scientists\\nof that era were always in some this huge dispute\\nabout building the weapons. This is different. AGI is different. - Does machine learning lead, do we know? - We don\\'t know, but this\\nis my point is different. We actually don\\'t know. But, and this is where you, the slide of hand kicks in, right? This is where it goes from\\nbeing a scientific topic to being a religious topic. And that\\'s why I specifically called out \\'cause that\\'s what happens. They do the vocabulary\\nshift and all of a sudden you\\'re talking about something totally. That\\'s not actually real. - Well then maybe you can\\nalso, as part of that, define the western\\ntradition of Millennialism. - [Marc] Yes. Into the world apocalypse. - [Lex] What is it? - [Marc] Apocalypse cults. - [Lex] Apocalypse cults. - Well, so we live in, we of course live in a Judeo-Christian, but primarily Christian\\nkind of saturated, you know, kind of Christian, post-Christian,\\nsecularized Christian, you know, kind of world in the west. And of course court of Christianity is the idea of the second\\ncoming and you know, the revelations and you know, Jesus returning and the\\nthousand year, you know, utopia on earth and then you know, the rapture and like all\\nall that stuff, you know, you know, we collectively,\\nyou know, as a society, we don\\'t necessarily take\\nall that fully seriously now. So, what we do is we create our\\nsecularized versions of that we keep looking for utopia. We keep looking for, you know, basically the end of the world. And so what what you see over, over decades is that basically a pattern of these sort of these of\\nis this is what cults are. This is how cults form as\\nthey form around some theory of the end of the world. And so the people\\'s temple cults, the Manson cult, the Heavens Gate cult, the David Qresh cult, you know what they\\'re all\\norganized around is like, there\\'s gonna be this\\nthing that\\'s gonna happen that\\'s gonna basically bring\\ncivilization crashing down. And then we have this\\nspecial elite group of people who are gonna see it\\ncoming and prepare for it. And then they\\'re the people\\nwho are either going to stop it or are failing, stopping it. They\\'re gonna be the people\\nwho survived the other side and ultimately get credit\\nfor having been, right. - Why is that so compelling,\\ndo you think? Like-- - Because it satisfies this very deep need we have for transcendence and meaning that got stripped\\naway when we became secular. - Yeah, but why is the\\ntranscendence involve the destruction of human civilization? - Because like how plausible it\\'s like a very deep psychological thing \\'cause it\\'s like how plausible, how plausible is it\\nthat we live in a world where everything\\'s just\\nkind of all right? Right. How exciting? - [Lex] Whoa. - How exciting is that? Right? - [Lex] But that\\'s. - We got more than that. - But that\\'s the deep question I\\'m asking. Why is it not exciting to live in a world where everything\\'s just all right? Is it, I think, you know, most of the animal kingdom would be so happy with just all right. Because that means survival. Why are we, maybe that\\'s what it is. Why are we conjuring up\\nthings to worry about? - So CS Lewis called\\nit the God-shaped hole. So there\\'s a God-shaped hole\\nin the human experience, consciousness, soul,\\nwhatever you wanna call it, where there\\'s gotta be\\nsomething that\\'s bigger than all this. There\\'s gotta be something transcendent. There\\'s gotta be something\\nthat is bigger, right? Bigger purpose. A bigger meaning. And so we have run the\\nexperiment of, you know, we\\'re just gonna use\\nscience and rationality and kind of, you know, everything\\'s just gonna\\nkind of be as it appears. And large number of people have found that very deeply wanting and\\nhave constructed narratives. And by this is the story\\nof the 20th century, right? Communism, right? Was one of those, communism\\nwas a was a form of this, Nazism was a form of this. You know, some people, you know, you can see movements\\nlike this playing out all over the world right now. - So you constructed a kind of devil, a kind of source of evil, and we\\'re going to transcend beyond it. - Yeah. And (indistinct)\\nwhen you see a Miller cult, they put a really specific point on it, which is end of the world, right, there is some change coming. And that change that\\'s\\ncoming is so profound and so important that\\nit\\'s either gonna lead to utopia or hell on earth. Right? And it is going to, and then, you know, it\\'s like what if you actually knew that was going to happen, right? What would you do? Right? How would you prepare yourself for it? How would you come together with a group of like-minded people, right? How would you, what would you do? Would you plan like Cassius\\nof weapons in the woods? Would you like, you know, I don\\'t know if create\\nunderground buckers, would you, you know, spend your\\nlife trying to figure out a way to avoid having it happen? - Yeah. That\\'s a really\\ncompelling, exciting idea to have a club over. To have a little bit of travel, like a get', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 517638, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content=' now. - So you constructed a kind of devil, a kind of source of evil, and we\\'re going to transcend beyond it. - Yeah. And (indistinct)\\nwhen you see a Miller cult, they put a really specific point on it, which is end of the world, right, there is some change coming. And that change that\\'s\\ncoming is so profound and so important that\\nit\\'s either gonna lead to utopia or hell on earth. Right? And it is going to, and then, you know, it\\'s like what if you actually knew that was going to happen, right? What would you do? Right? How would you prepare yourself for it? How would you come together with a group of like-minded people, right? How would you, what would you do? Would you plan like Cassius\\nof weapons in the woods? Would you like, you know, I don\\'t know if create\\nunderground buckers, would you, you know, spend your\\nlife trying to figure out a way to avoid having it happen? - Yeah. That\\'s a really\\ncompelling, exciting idea to have a club over. To have a little bit of travel, like a get together on a Saturday\\nnight and drink some beers and talk about the end of the world and how you are the only\\nones who have figured it out. - Yeah. And then once you lock in on that, like how can you do anything\\nelse with your life? Like this is obviously the\\nthing that you have to do. And then there\\'s a psychological\\neffect that you alluded to. There\\'s a psychological effect. If you take a set of true\\nbelievers and you leave them to themselves, they get\\nmore radical. Right. \\'Cause they self radicalize each other. - That said, it doesn\\'t mean they\\'re not sometimes right. - Yeah. The end of the world might be. Yes. Correct. Like they might be right. - [Lex] Yeah. - But like-- - [Lex] I have some pamphlets for you. - Exactly. - But I mean we\\'ll talk\\nabout nuclear weapons \\'cause you have a really\\ninteresting little moment that I learned about in\\nyour essay, but you know, sometimes it could be right. - [Marc] Yeah. - \\'Cause we\\'re still, you were developing more and\\nmore powerful technologies in this case, and we don\\'t know what the impact it will\\nhave on human civilization while we can highlight all\\nthe different predictions about how it\\'ll be positive, but the risks are there and\\nyou discuss some of them. - Well, the steel man, the\\nsteel man is the steel man. Well actually, the steel\\nman and his reputation are the same, which is you can\\'t predict what\\'s gonna happen. Right. You can\\'t rule out that this\\nwill not end everything. Right. But the response to that\\nis you have just made a completely non-scientific claim. You\\'ve made a religious\\nclaim, not a scientific claim. - How does it get disproven? - And there\\'s no, by definition with these kinds of claims, there\\'s no way to disprove them. Right? And so there there\\'s no, you\\njust go right on the list. There\\'s no hypothesis, there\\'s no testability of the hypothesis. There\\'s no way to falsify the hypothesis, there\\'s no way to measure\\nprogress along the arc. Like it\\'s just all completely missing. And so it\\'s not scientific and. - I don\\'t think it\\'s completely missing. It\\'s somewhat missing. So for example, the people that say AI\\'s gonna kill all of us. I mean, they usually have\\nideas about how to do that. Whether it\\'s the people\\nclub maximizer or, you know, it escapes there\\'s mechanism\\nby which you can imagine it killing all humans. - [Marc] Models. - And you can disprove it by saying there\\'s a limit to the speed at which intelligence increases. Maybe show that like the sort\\nof rigorously really described model, like how it could\\nhappen and say, no, there, here\\'s a physics limitation. There\\'s like a physical\\nlimitation to how these systems would actually do damage\\nto human civilization. And it is possible they\\nwill kill 10 to 20% of the population, but it seems impossible\\nfor them to kill 99%. - It was practical\\ncounterarguments. Right. So you mentioned\\nbasically what I described as the thermodynamic counterargument, which, so sitting here today, it\\'s like where with the\\nevil AGI get the GPU. \\'Cause like they don\\'t exist. So if you\\'re gonna have a\\nvery frustrated baby evil AGI, who\\'s gonna be like trying to\\nbuy Nvidia stock or something to get them to finally\\nmake some chips, right? So the serious form of that\\nis the thermodynamic argument, which is like, okay, where\\'s\\nthe energy gonna come from? Where\\'s the processor gonna be running? Where\\'s the data center\\ngonna be happening? How is this gonna be\\nhappening in secret such that, you know, it\\'s not, you know, so that\\'s a practical counter argument to the runaway AGI thing. I have a but I have and we\\ncan argue that, discuss that. I have a deeper objection to it, which is it\\'s, this is all forecasting. It\\'s all modeling, it\\'s\\nall future prediction. It\\'s all future hypothesizing. It\\'s not science. - [Lex] Sure. - It is not. It is the\\nopposite of science. So the, I\\'ll pull up Carl Sagan\\nextraordinary claims require extraordinary proof, right? These are extraordinary claims. The policies that are being\\ncalled for right to prevent this are of extraordinary magnitude that, and I think we\\'re gonna\\ncause extraordinary damage. And this is all being done\\non the basis of something that is literally not scientific. It\\'s not a testable hypothesis. - So the moment you say\\nAI\\'s gonna kill all of us, therefore we should ban it, or that we should regulate\\nall that kind of stuff, that\\'s when it starts getting serious. - Or start, you know, military\\nairstrikes and data centers. - [Lex] Oh boy. - Right? And like. - Yeah. This once get starts. Well, so starts getting real weird. - So here\\'s the problem with Arian cults. They have a hard time\\nstaying away from violence. - Yeah. But violence is so fun. - If you\\'re on the right end of it, they have a hard time avoiding violence. The reason they have a hard\\ntime avoiding violence is if you actually believe the claim. Right. Then what would you do to\\nstop the end of the world? Well, you would do anything, right? And so, and this is\\nwhere you get, and again, if you just look at the\\nhistory of Arian and cults, this is where you get the\\npeople\\'s temple and everybody killing themselves in the jungle. And this is where you get\\nCharles Manson and, you know, sending in to kill the pigs. Like, this is the problem with these. They have a very hard time to run the line at actual violence. And I think in this case, I\\nmean, they\\'re already calling for it like today and you know, where this goes from here\\nis they get more worked up. Like I think is like really concerning. - Okay. But that\\'s kind of the extremes. So, you know, the extremes of\\nanything are I was concerning. It\\'s also possible to kind\\nof believe that AI has a very high likelihood\\nof killing all of us. But and therefore we should maybe consider slowing development or regulating, so not violence or any\\nof these kinds of things. But it\\'s saying like, all right, let\\'s take a pause here. You know, you biological\\nweapons, nuclear weapons. Like whoa, whoa, whoa, whoa, whoa. This is like serious stuff.\\nWe should be careful. So it is possible to kinda have a more rational response, right? If you believe this risk is real. - [Marc] Believe. - Yes. So what is it possible to be, have a scientific approach to\\nthe prediction of the future? - I mean, we just went\\nthrough this with COVID. What do we know about modeling? - [Lex] Well, I mean. - What did we learn about\\nmodeling with COVID? - [Lex] There\\'s a lot of lessons. - They didn\\'t work at all. - [Lex] They worked poorly. - The models were terrible,\\nthe models were useless. - I don\\'t know if the models\\nwere useless or the people interpreting the models and\\nthen decentralized institutions that were creating policy\\nrapidly based on the models and leveraging the models in order to support their narratives versus actually\\ninterpreting the error bars and the models and all that kind of stuff. - What you had with COVID, my view you had with COVID\\nis you had these experts showing up and they\\nclaimed to be scientists and they had no testable\\nhypotheses whatsoever. They had a bunch of models. They had a bunch of forecasts\\nand they had a bunch of theories and they laid these out in front of policy makers and policy makers freaked\\nout and panicked. Right. And implemented a whole bunch of like, really like terrible decisions\\nthat we\\'re still living with the consequences of, and there was never any\\nempirical foundation to any of the models. None of them ever came true. - Yeah. To push back. There were certainly\\nBaptist and bootleggers in the context of this pandemic, but there\\'s still a\\nusefulness to models. No. - So not if they\\'re, I mean not if they\\'re\\nreliably wrong, right? Then they\\'re actually\\nlike anti-useful. Right. They\\'re actually damaging. - But what do you do with the pandemic? What do you do with any kind of threat? Don\\'t you want to kind of\\nhave several models to play with as part of this discussion of like, what the hell do we do here? - I mean, do they work? Because they\\'re an expectation\\nthat they actually like work that they have actual predictive value. I mean, as far as I can tell with COVID, the policymakers just si up\\nthemselves into believing that there was sub, I\\nmean, look, the scientists, the scientists were at fault. The quote unquote scientists showed up. So I had some insight into this. So there was a, or remember the Imperial College models out of London were the\\nones that were like, these are the gold standard models. So a friend of mine runs\\na big software company and he was like, wow, this is\\nlike, COVID is really scary. And he is like, you know, he contacted this research\\nand he is like, you know, do you need some help? You\\'ve been just building\\nthis model on your own for 20 years. Do you need some, would you like us our coders\\nto basically restructure it so it can be fully adapted for COVID? And the guy said yes\\nand sent over the code and my friend said it was\\nlike the worst spaghetti code he\\'s ever seen. - That doesn\\'t mean it\\'s\\nnot possible to construct a good model of pandemic\\nwith the correct air bars, with a high number of parameters\\nthat are continuously, many times a day updated\\nas we get more data about a pandemic. I would like to believe when\\na pandemic hits the world, the best computer scientists in the world, the best software engineers\\nrespond aggressively and as input take the data\\nthat we know about the virus and it\\'s an output say\\nhere is what\\'s happening in terms of how quickly it\\'s spreading, what that lead in terms of\\nhospitalization and deaths and all that kind of stuff. Here\\'s how likely, how\\ncontagious it likely is. Here\\'s how deadly it likely is based on different conditions, based on different ages and demographics and all that kind of stuff. So here\\'s the best kinds of policy. It feels like you could have models, machine learning that like kind of, they don\\'t perfectly predict the future, but they help you do something \\'cause there\\'s pandemics\\nthat are like, meh, they don\\'t really do much harm. And there\\'s pandemics,\\nyou can imagine them, they could do a huge amount of harm. Like they can kill a lot of people. So you should probably have\\nsome kind of data-driven models that keep updating, that allow you to make\\ndecisions that based like where, how bad is this thing? Now you can criticize how\\nhorrible all that went with the response to this pandemic, but I just feel like there\\nmight be some value to models. - So to be useful at some point it has to be predictive. Right? So and the easy thing\\nfor me to do is to say, obviously you\\'re right. Obviously I wanna see that\\njust as much as you do. \\'cause anything that makes\\nit easier to navigate through society through a\\nwrenching, you know, risk like that sounds great. You know, the harder objection to it is just simply\\nyou are trying to model a complex dynamic system\\nwith 8 billion moving parts. Like not possible. - [Lex] It\\'s very tough. - Can\\'t be done, complex\\nsystems can\\'t be done. - Machine learning says hold my beer. But well, it\\'s possible. No? - I don\\'t know. I would like to believe that it is. I\\'ll put it this way. I think where you and I\\nwould agree is I think we would like that to be the case. We are strongly in favor of it. I think we would also\\nagree that no such thing with respect to COVID or\\npandemics no such thing. At least neither you\\nnor I think are aware. I\\'m not aware of anything like that today. - My main worry with the\\nresponse to the pandemic is that same as with aliens, is that even if such a thing existed, and it\\'s possible it existed, the policymakers were\\nnot paying attention. Like there was no mechanism\\nthat allowed those kinds of models to percolate all. - Oh, I think we had the\\nopposite problem during COVID. I think the policymakers, I think these people with\\nbasically fixed science had too much access to the policymakers. - Well, right. And well, but the policy\\nmakers also wanted, they had a narrative in\\nmind and they also wanted to use whatever model\\nthat fit that narrative - [Marc] Oh, sure. - To help them out. So like, it felt like\\nthere was a lot of politics and not enough science. - Although a big part\\nof what was happening, a big reason we got lockdowns\\nfor as long as we did, was because these scientists\\ncame in with these like doomsday scenarios that were like, just like completely off the hook. - Scientists in quotes, let\\'s not-- - [Marc] Quote unquote scientists. - Let\\'s not, okay,\\nlet\\'s give love science. So here\\'s science that is the way out. - Science is a process\\nof testing hypotheses. Modeling does not involve\\ntestable hypotheses. Right. Like, I don\\'t even know that. I actually don\\'t even know that modeling actually\\nqualifies as science. Maybe that\\'s a side conversation. We could have some time over a beer. - Oh, that\\'s a really interesting part. What do we do about the future? I mean, what\\'s-- - So number one is when\\nwe start with number one, humility goes back to this thing of how do we determine the truth. Number two is we don\\'t believe, you know, it\\'s the old, I\\'ve gotta hammer everything\\nlooks like a nail, right? I\\'ve got, oh, this is one\\nof the reasons I gave you, I gave Alexa book, which the topic of the\\nbook is what happens when scientists basically\\nstray off the path of technical knowledge and\\nstart to weigh in on politics and societal issues. - In this case, philosophers. - Well in this case philosophers. But he actually talks in this\\nbook about, like Einstein, he talks about, actually about\\nthe nuclear age in Einstein. He talks about the\\nphysicists actually doing very similar things at the time. - The book is When Reason Goes On Holiday, Philosophers in Politics by Nevin. - And it\\'s just a story. It\\'s a story. There are other books on this topic, but this is a new one that\\'s really good this is just a story of what happens when experts\\nin a certain domain decide to weigh in and become\\nbasically social engineers and political, you know,\\nbasically political advisors. And it\\'s just a story of just\\ninning catastrophe. Right. And I think that\\'s what\\nhappened with COVID again. - Yeah. I found this book\\na highly entertaining and eye-opening read filled\\nwith amazing anecdote of a rationality and craziness\\nby famous Resa philosophers. - I definitely, after you read this book, you will not look at Einstein the same. - [Lex] Oh boy. - Yeah. - Don\\'t destroy my heroes. - He will not be a hero of yours anymore. Sorry. You probably couldn\\'t,\\nyou shouldn\\'t read the book. - All right. - But here\\'s the thing. The AI risk people, they don\\'t even have the COVID model, at least not that I\\'m aware of. - [Lex] No. - Like there\\'s not even the\\nequivalent of the COVID model. They don\\'t even have the spaghetti code. They\\'ve got a theory and a\\nwarning and a this and the that. And like, if you ask like,\\nokay, well here\\'s, I mean, the ultimate example is,\\nokay, how do we know, right? How do we know that an AI is running away? Like how do we know that the boom takeoff thing\\nis actually happening? And the only answer that\\nany of these guys have given that I\\'ve ever seen is, oh,\\nit\\'s when the loss rate, the loss function and the\\ntraining drops, right? That\\'s when you need to like\\nshut down the data center. Right? And it\\'s like, well that\\'s also what happens when you\\'re successfully training a model. Like, what even this is not science, this is not, it\\'s not\\nanything, it\\'s not a model, it\\'s not anything. There\\'s nothing to arguing with. It is like, you know,\\npunching jello, like there, there\\'s what do you even respond to? - So just put push back on that. I don\\'t think they have good metrics of when the film is happening. But I think it\\'s possible to have that. Like just as you speak now, I mean it\\'s possible to imagine\\nthere could be measures. - It\\'s been 20 years. - No, for sure. But it is been only weeks\\nsince we had a big enough breakthrough in language models. We can start to actually have this, the thing is the AI doer stuff didn\\'t have any actual systems to really work with. And now there\\'s real systems\\nyou can start to analyze like, how does this stuff go wrong? And I think you kind\\nof agree that there is a lot of risks that we can analyze. The benefits outweigh\\nthe risks in many cases. - Well, the risks are not existential. - [Lex] Yes. Well. - Not in the phone paper\\nclip. Let me, okay. There\\'s another slide of hand\\nthat you just alluded to. There\\'s another slide\\nof hand that happens, which is very interesting. - I\\'m very good at the\\nslide of hand thing. - Which is very not scientific. So the book Super Intelligence, right, which is like the Nick Bostrom\\'s book, which is like the origin\\nof a lot of this stuff, which was written, you know, whatever, 10 years ago or something. So he does this really\\nfascinating thing in the book, which is he basically says\\nthere are many possible routes to machine intelligence,\\nto artificial intelligence. And he describes all the different routes to artificial intelligence,\\nall the different possible, everything from biological\\naugmentation through to, you know, all these different things. One of the ones that\\nhe does not describe is large language models because of course the book was written\\nbefore they were invented. And so they didn\\'t exist. In the book, he describes them all and then he proceeds to treat them all as if they\\'re\\nexactly the same thing. He presents them all as sort\\nof an equivalent risk to be dealt with in an equivalent\\nway to be thought about the same way. And then the risk, the quote unquote risk that\\'s actually emerged is actually a completely different technology than he was even imagining. And yet all of his theories\\nand beliefs are being transplanted by this movement, like straight onto this new technology. And so again, like there\\'s no other area of science or technology\\nwhere you do that. Like when you\\'re dealing\\nwith like organic chemistry versus inorganic chemistry,\\nyou don\\'t just like say, oh, with respect to like either\\none, basically maybe, you know, growing up in eating\\nthe world or something, like they\\'re just gonna\\noperate the same way. Like you don\\'t. - But you can start talking about like, as we get more and more actual systems that start to get more\\nand more intelligent, you can start to actually have more scientific arguments here. - [Marc] Oh yeah. - Like, you know, high level, you can talk about the threat\\nof autonomous weapon systems back before we had any\\nautomation in the military. And that would be like\\nvery fuzzy kind of logic. But the more and more you\\nhave drones that are becoming more and more autonomous, you\\ncan start imagining, okay, what does that actually look\\nlike and what\\'s the actual threat of autonomous weapons systems? How does it go wrong? And still it\\'s very vague, but you start to get a\\nsense of like, all right, it should probably be illegal or wrong or not allowed\\nto do like mass deployment of fully autonomous drones that are doing aerial strikes. - [Marc] Oh no. - On large areas. - [Marc] I think it should be required. - Right? So that\\'s a no. - No, no. I think it should be required that only aerial vehicles are automated. - Okay. So you wanna go the other way? - I wanna go the other way. - So that, okay. - I think it\\'s obvious that\\nthe machine is gonna make a better decision than the human pilot. I think it\\'s obvious that\\nit\\'s in the best interest of both the attacker and the\\ndefender and humanity at large. If machines are making\\nmore of these decisions than not people, I think people make terrible\\ndecisions in times of war. - But like, there\\'s ways\\nthis can go wrong too, right? - Well, it wars go terribly wrong now. This goes back to the whole, this is that whole thing\\nabout like the self-drive. Does the self-driving\\ncar need to be perfect versus does it need to be\\nbetter than the human driver? Does the automated\\ndrone need to be perfect or does it need to be better than a human pilot at making decisions under enormous amounts of\\nstress and uncertainty? - Yeah, well, on average, the worry that AI folks\\nhave is the runaway. - They\\'re gonna come alive. Right? That then again, that\\'s\\nthe slight of hand, right. - Or not not come alive.\\nWell, no, hold on a second. You lose control as well. You lose control. - But then they\\'re gonna\\ndevelop goals of their own. They\\'re gonna develop a mind of their own, they\\'re gonna develop their own. Right. - No more, more like\\nChernobyl style meltdown, like just bugs in the code\\naccidentally, you know, force you like the results in the bombing of like large civilian areas. - [Marc] Okay. And to a degree that\\'s not possible in the current military strategies, - [Marc] I don\\'t know. - Control by humans. - Well, actually we\\'ve been\\ndoing a lot of mass bombings to cities for a very long time. - Yes. And a lot of civilians died. - And a lot of civilians died. And if you watch the documentary,\\nthe Fog of War McNamara, it spends a big part of it\\ntalking about the fire bombing of the Japanese cities. Burning them straight\\nto the ground. Right. The devastation in Japan, American military fire bombing\\nthe cities in Japan was considerably bigger devastation\\nthan the use of nukes. Right. So we\\'ve been doing\\nthat for a long time. We also did that to Germany, by the way Germany did that to us, right? Like that\\'s an old tradition. The minute we got airplanes, we started doing indiscriminate bombing. - So one of the things-- - [Marc] We\\'re still doing it. - The modern US military can do with technology with automation, but technology more broadly is higher and higher precision strikes. - Yeah, I was saying, so precision is obviously precision and this is a (indistinct) right? So there was this big advance this big advance called the (indistinct) which basically was\\nstrapping a GPS transceiver to an unguided bomb and turning it into a guided bomb. And yeah, that\\'s great. Like look, that\\'s been a big advance, but, and that\\'s like a baby\\nversion of this question, which is okay, do you\\nwant like the human pilot, like guessing where the bomb\\'s gonna land? Or do you want like the\\nmachine like guiding the bomb to his destination? That\\'s a baby version of the question. The next version of the question is, do you want the human\\nor the machine deciding whether to drop the bomb? Everybody just assumes the\\nhuman\\'s gonna do a better job for what I think are\\nfundamentally suspicious reasons. - Emotional, psychological reasons. - Yeah. I think it\\'s very clear\\nthat the machine\\'s gonna do a better job making that decision \\'cause the humans making\\nthat decision are got awful. Just terrible. - [Lex] Yeah. - Right. And so yeah. So this is the thing. And then let\\'s get to the, there was, can I one more slide of hand? - [Lex] Yes. - It was in-- - Sure. Please. I\\'m a magician. You could say. - One more slight of hand. These things are gonna be so smart, right? That they\\'re gonna be able to\\ndestroy the world and wreak havoc and like do all this\\nstuff and plan and do all this stuff and evade us and have\\nall their secret things and their secret factories\\nand all this stuff. But they\\'re so stupid that\\nthey\\'re gonna get like, tangled up in their code and that\\'s they\\'re not gonna come alive, but there\\'s gonna be some\\nbug that\\'s gonna cause them to like turn us all on a paper like that. They\\'re not gonna be\\ngenius in every way other than the actual bad goal. And it\\'s just like,\\nand that\\'s just like a, like ridiculous like discrepancy. And you can prove this today, you can actually address\\nthis today for the first time with LLMs which is you can actually ask LLMs to resolve moral dilemmas. So you can create the\\nscenario, you know, dot, dot, dot this, that, this, that, this, that. What would you as the AI\\ndo in the circumstance? And they don\\'t just\\nsay destroy all humans, destroy all humans. They will give you actually\\nvery nuanced moral, practical trade-off oriented answers. And so we actually already\\nhave the kind of AI that can actually like, think this through and can actually like, you know, reason about goals. - Well, the hope is that AGI or like various superintelligent systems have some of the\\nnuance that LLMs have and the intuition is they most likely will because even these LLMs have the nuance. - LLMs are really, this is\\nactually worth spending a moment on LLMs are really interesting to have moral conversations with. And that I just, I didn\\'t expect I\\'d be\\nhaving a moral conversation with the machine in my lifetime. - Wait, and let\\'s remember\\nwe\\'re not really having a conversation with the machine where we\\'re having a conversation with the entirety of the\\ncollective intelligence of the human species. - Exactly. Yes. Correct. - But it\\'s possible to imagine\\nautonomous weapons systems that are not using LLMs. - But if they\\'re smart enough to be scary, where are they not\\nsmart enough to be wise? Like, that\\'s the part where it\\'s like, I don\\'t know how you get\\nthe one without the other. - Is it possible to be super intelligent without being super wise? - Well, again, you\\'re back to that. I mean, then you\\'re back to\\na classic autistic computer, right? Like you\\'re back to just\\nlike a blind rule follower. I\\'ve got this like core,\\nit\\'s the paperclip thing. I\\'ve got this core rule and\\nI\\'m just gonna follow it to the end of the earth. And it\\'s like, well, but everything you\\'re gonna\\nbe doing execute that rule is gonna be super genius level\\nthat humans aren\\'t gonna be able to counter. It\\'s a mismatch in the definition of what the system\\'s capable of. - Unlikely but not impossible, I think. - But again, here you\\nget to like, okay, like. - No, I\\'m not saying when it\\'s\\nunlikely but not impossible. If it\\'s unlikely, that means the fear should be correctly calibrated. - Extraordinary claims\\nrequire extraordinary proof. - Well, okay, so one\\ninteresting sort of tangent, I would love to take on this\\nbecause you mentioned this in the essay about nuclear,\\nwhich was also, I mean, you don\\'t shy away from a\\nlittle bit of of a spicy take. So Robert Oppenheimer famously said, now I am become death\\nthe destroyer of worlds as he witnessed the first\\ndestination of a nuclear weapon on July 16th, 1945. And you write an interesting\\nhistorical perspective, \"Recall that John Van Neuman responded to Robert Oppenheimer\\'s famous\\nhand wringing about the role of creating nuclear weapons, which you note helped end World War II and prevent World War III\\nwith some people confess guilt to claim credit for the sin.\" And you also mentioned\\nthat Truman was harsher after meeting Oppenheimer. He said that \"Don\\'t let that\\ncry baby in here again.\" - Real quote, by the\\nway, from Dean Atchison. - Boy. - \\'Cause Oppenheimer didn\\'t\\njust say the famous line. - [Lex] Yeah. - He then spent years going\\naround basically moaning him, you know, going on TV and going into going into the White House and basically like, just like doing this hair shirt, you know, thing self, you know, this\\nsort of self-critical like, oh my god, I can\\'t believe how awful I am. - So he\\'s widely\\nconsidered perhaps of the, because of the hang ringing\\nas the father of the tom bomb. - [Marc] Yeah. - This is Van Norman\\'s criticism\\nof him is he tried to have his cake and eat it too. Like he wanted to and Van Norman of course a very different kind of personality and\\nhe\\'s just like, yeah, good. This is like an incredibly\\nuseful thing. I\\'m glad we did it. - Yeah. Well Van Norman is\\nis widely credit as being one of the smartest humans\\nof the 20th century. Certain people. Everybody says like, this is the smartest person I\\'ve ever met when they\\'ve met him. Anyway, that doesn\\'t mean,\\nsmart doesn\\'t mean wise. So yeah, I would love to sort of, can you make the case both\\nfor and against the critique of Oppenheimer here? \\'Cause we\\'re talking\\nabout nuclear weapons. Boy, do they seem dangerous? - Well so, the critique goes deeper and I left this out. Here\\'s the real substance, I left it out \\'cause I didn\\'t wanna dwell on nukes in my AI paper. But here\\'s the deeper thing that happened and I\\'m really curious, this\\nmovie coming out this summer, I\\'m really curious to see\\nhow far he pushes this. \\'cause this is the real\\ndrama in the story, which is, it wasn\\'t just a question\\nof our nukes, good or bad, it was a question of should\\nRussia also have them? And what actually happened was Russia got the American invented the bomb. Russia got the bomb, they got the bomb through espionage, they got American and you know, they got American scientists\\nand foreign scientists working on the American project. Some combination of the two\\nbasically gave the Russians the designs for the bomb. And that\\'s how the Russians got the bomb. There\\'s this dispute to this\\nday of Oppenheimer\\'s role in that if you read all the histories, the kind of composite\\npicture, and by the way, we now know a lot actually\\nabout Soviet espionage in that era \\'cause there\\'s been all this declassified\\nmaterial in the last 20 years that actually shows a lot\\nof very interesting things. But if you kinda read all the histories, which you kinda get is Oppenheimer\\nhimself probably was not he probably did not hand over\\nthe nuclear secrets himself. However, he was close\\nto many people who did. Including family members. And there were other members\\nof the Manhattan Project who were Russian, Soviet SS\\nand did hand over the bomb. And so the view of that\\nOppenheimer and people like him had that this thing is awful\\nand terrible and oh my god. And you know, all this stuff you could\\nargue fed into this ethos at the time that resulted\\nin people thinking that the Baptists thinking that the only principle\\nthing to do was to give the Russians the bomb. And so the moral beliefs on this thing and the public discussion and the role that the inventors\\nof this technology play, this is the point of this book, when they kind of take on this\\nsort of public intellectual, moral kind of thing, it can\\nhave real consequences, right? Because we live in a very\\ndifferent world today because Russia got the\\nbomb than we would\\'ve lived in had they not gotten the bomb right. The entire 20th century, second half of the 20th\\ncentury would\\'ve played out very different had those people\\nnot given Russia the bomb. And so the stakes were very high then. The good news today is\\nnobody\\'s sitting here today, I don\\'t think worrying about\\nlike an analogous situation with respect to like, I\\'m not really worried that\\nSam Altman\\'s gonna decide to give, you know, the\\nChinese, the design for AI, although he did just speak\\nat a Chinese conference, which is in interesting. But however, I don\\'t think\\nthat\\'s what\\'s at play here, but what\\'s at play here are\\nall these other fundamental issues around what do\\nwe believe about this and then what laws and\\nregulations and restrictions that we\\'re gonna put on it. And that\\'s where I draw\\nlike a direct straight line. And anyway, and my reading\\nof the history on nukes is like the people who were doing\\nthe full hair shirt public, this is awful. This is terrible. Actually had like\\ncatastrophically bad results from taking those views. And that\\'s what I\\'m worried\\nit\\'s gonna happen again. - But is there a case to be\\nmade that you really need to wake the public up to the dangers of nuclear weapons when\\nthey were first dropped? Like really like educate them on like, this is extremely dangerous\\nand destructive weapon. - I think the education\\nkind of happened quick and early, like-- - [Lex] How? - It was pretty obvious. - [Lex] How? - We dropped one bomb and\\ndestroyed an entire city. - Yeah. So 80,000 people dead. - [Marc] Yep. - But. - [Marc] And look. But-- - I don\\'t like the reporting of that. You can report that in all kinds of ways. - [Marc] Oh, there wars. - You can do all kinds of slants. Like war is horrible. War is terrible. You can do, you can make\\nit seem like nuclear, the use of nuclear weapons\\nis just a part of war and all that kind of stuff. Something about the\\nreporting and the discussion of nuclear weapons resulted\\nin us being terrified in awe of the power of nuclear weapons and that potentially fed\\nin a positive way towards the game theory of\\nmutual issue destruction. - Well, so this gets to what actually, let\\'s get to what actually happens. - [Lex] Some of us, me\\nplaying devil\\'s advocate here. - Yeah, yeah, sure. Of course. Let\\'s get to what\\nactually happened and then kind of back into that. So what actually happened, I believe, and again I think this is a\\nreasonable reading of history, is what actually happened\\nwas nukes then prevented World War III and they\\nprevented World War III through the game theory\\nof mutually assured destruction had nukes not existed. Right. There would\\'ve been no reason why the Cold War did not go hot. Right. And then there and then, you know, and the military planners\\nat the time, right, thought both on both sides\\nthought that there was gonna be World War III on the planes of Europe and they thought there was gonna be like a hundred million people dead. Right? It was like the most obvious\\nthing in the world to happen. Right? And it\\'s the dog\\nthat didn\\'t bark right? Like it may be like the best\\nsingle net thing that happened in the entire 20th century is\\nthat like that didn\\'t happen. - Yeah. Actually, just on that point, you say a lot of really brilliant things. It hit me just as you were saying it. I don\\'t know why it hit\\nme for the first time, but we got two wars in\\na span of like 20 years. Like we could have kept getting\\nmore and more world wars and more and more ruthless. It actually, you could have\\nhad a US versus Russia war. - You could, by the way you haven\\'t, there\\'s another hypothetical scenario. The other hypothetical scenario is that Americans got the\\nbomb, the Russians didn\\'t. Right? And then America\\'s the big dog and then maybe America\\nwould\\'ve had the capability to actually roll back the iron curtain. I don\\'t know whether\\nthat would\\'ve happened, but like it\\'s entirely possible. Right? And the act of these people who had these moral positions about, \\'cause they could\\nforecast, they could model, they could forecast the future\\nof how the technology would get used, made a horrific mistake. \\'cause they basically ensured that the iron curtain\\nwould continue for 50 years longer than it would\\'ve otherwise. Like, and again, like\\nthese are counter-factuals, I don\\'t know that that\\'s\\nwhat, what would\\'ve happened, but like the decision to hand the bomb over was a big decision made by people who were very\\nfull of themselves. - Yeah. But so me as an America, me as a person that loves America, I also wonder if US was the only ones with the nuclear weapons. - That was the argument for handing that was the guys who (indistinct) the guys who handed over the bomb. That was actually their moral argument. - Yeah. I would probably\\nnot hand it over to, I would be careful about the regimes. You hand it over to there, maybe give it to like\\nthe British or something, or like a democratically-elected\\ngovernment. - Well, look, there are people to this day who think that those bias Soviet spies did the right thing because\\nthey created a balance of terror as opposed\\nto the US having just, and by the way, let me-- - Balance of terror. - [Marc] Let\\'s tell the\\nfull version story has-- - Such a sexy ring to it. - Okay. So the full\\nversion of the story is John Van Norman is a hero\\nof both yours and mind. The full version of the\\nstory is he advocated for a first strike. So when the US had the\\nbomb and Russia did not, he advocated for, he said, we need to strike them right now. - Strike Russia. - [Marc] Yes. - Van Norman. - Yes, because he said\\nWorld War III is inevitable. He was very hardcore. His theory was World\\nWar III is inevitable. We\\'re definitely gonna have World War III. The only way to stop World War\\nIII is we have to take them out right now and we have\\nto take them out right now before they get the bomb. \\'Cause this is our last chance. Now again, like-- - Is this an example of\\nphilosophers and politics? - I don\\'t know if that\\'s in there or not, but this is in the standard. - No, but it is meaning is that. - Yeah, this is on the other side. So, most of the case studies, most of the case studies\\nin books like this are the crazy people on the left. Van Norman is a story arguably of the crazy people on the right. - Yes. Stick to computing, John. - Well. This is the thing, and this is the general principle. Getting back to our core\\nthing, which is like, I don\\'t know whether any of\\nthese people should be making any of these calls. Because there\\'s nothing in\\neither Van Norman\\'s background or Oppenheimer\\'s background or any of these people\\'s background that qualifies them as moral authorities. - Yeah. Well this actually\\nbrings up the point of, in AI, who are the good people to reason about the\\nmorality of the ethics, the outside of these risks, outside of like the more\\ncomplicated stuff that you, you agree on is, you know, this will go into the hands of bad guys and all the kinds of ways\\nthey\\'ll do is interesting and dangerous, is dangerous in interesting unpredictable ways. And who is the right person? Who are the right kinds of\\npeople to make decisions, how to respond to it?\\nOr is the tech people? - So the history of these fields, this is what he talks about in the book, the history of these fields,\\nis that the competence and capability and\\nintelligence and training and accomplishments of senior\\nscientists and technologists working on a technology\\nand then being able to then make moral judgments\\nin the use of that technology. That track record is terrible that track record is like\\ncatastrophically bad. The people-- - Just the linger, the people that develop that\\ntechnology are usually not going to be the right people. - Well why would they? So\\nthe claim is of course, they\\'re the knowledgeable ones. But the the problem is they\\'ve\\nspent their entire life in a lab. Right. They\\'re not theologians. Well, so what you find,\\nwhat you find when you read, when you read this, when\\nyou look at these histories, what you find is they generally are very thinly informed on history, on sociology, on theology,\\non morality, on ethics. They tend to manufacture their\\nown worldviews from scratch. They tend to be very sort of thin. They\\'re not remotely the\\narguments that you would be having if you got like a group of\\nhighly qualified theologians or philosophers or, you know. - Well, let me sort of,\\nas the devil\\'s advocate, takes a simple whiskey say\\nthat I agree with that. But also it seems like the\\npeople who are doing kind of the ethics departments and these tech companies\\ngo sometimes the other way. - [Marc] Yes, they\\'re definitely. - Which they\\'re not nuanced\\non history or theology or this kind of stuff. It almost becomes a kind\\nof outraged activism towards directions that don\\'t seem to be grounded in history and\\nhumility and nuance. It\\'s again, drenched with arrogance. So-- - [Marc] Definitely. - I\\'m not sure which is worse. - Oh no, they\\'re both bad. Yeah. So definitely not them either. - So, but I guess. - Well look, this is a hard. - Yeah, it\\'s a hard problem. - This is a hard problem. This goes back to where we started, which is, okay, who has the truth? And it\\'s like, well, you know, like how does societies\\narrive at like truth and how do we figure these things out and like our elected leaders\\nplay some role in it. You know, we all play some role in it. There have to be some set\\nof public intellectuals at some point that bring, you know, rationality and judgment\\nand humility to it. Those people are few and far between. We should probably prize them very highly. - Yeah. So celebrate humility\\nin our public leaders. So getting to risk number two, will AI ruin our society\\nshort version as you write, if the murder robots don\\'t\\nget us the hate speech and misinformation will. And the action you recommend in short, don\\'t let the thought police suppress AI. Well what is this risk of\\nthe effect of misinformation of society that\\'s going\\nto be catalyzed by AI? - Yeah, so this is the social media, this is what you just alluded to. It\\'s the activism kind\\nof thing that\\'s popped up in these companies in the industry. And it\\'s basically, from my perspective, it\\'s basically part two\\nof the war that played out over social media over the last 10 years, \\'cause you probably remember\\nsocial media 10 years ago, was basically who even wants this? Who wants a photo of what\\nyour cat had for breakfast? Like, this stuff is like silly and trivial and why can\\'t these nerds like figure out how to invent something\\nlike useful and powerful? And then, you know, certain things happened\\nin the political system. And then it sort of, the polarity on that\\ndiscussion switched all the way to social media is like\\nthe worst, most corrosive, most terrible, most awful\\ntechnology ever invented. And then it leads to, you\\nknow, terrible of the wrong, you know, politicians and\\npolicies and politics and like, and all this stuff. And that all got catalyzed into\\nthis very big kind of angry movement both inside and\\noutside the companies to kind of bring social media to heal. And that got focused in\\nparticularly on two topics, so-called hate speech and\\nso-called misinformation. And that\\'s been the saga playing out for the last decade. And I', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 517638, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content='. And it\\'s basically, from my perspective, it\\'s basically part two\\nof the war that played out over social media over the last 10 years, \\'cause you probably remember\\nsocial media 10 years ago, was basically who even wants this? Who wants a photo of what\\nyour cat had for breakfast? Like, this stuff is like silly and trivial and why can\\'t these nerds like figure out how to invent something\\nlike useful and powerful? And then, you know, certain things happened\\nin the political system. And then it sort of, the polarity on that\\ndiscussion switched all the way to social media is like\\nthe worst, most corrosive, most terrible, most awful\\ntechnology ever invented. And then it leads to, you\\nknow, terrible of the wrong, you know, politicians and\\npolicies and politics and like, and all this stuff. And that all got catalyzed into\\nthis very big kind of angry movement both inside and\\noutside the companies to kind of bring social media to heal. And that got focused in\\nparticularly on two topics, so-called hate speech and\\nso-called misinformation. And that\\'s been the saga playing out for the last decade. And I don\\'t even really want\\nto even argue the pros and cons of the sides just to observe that\\'s been like a huge fight and has had, you know, big consequences to how\\nthese companies operate. Basically that same, those\\nsame sets of theories, that same activist approach, that same energy as being\\ntransplanted straight to AI. And you see that already happening. It\\'s why, you know, ChatGPT will answer, let\\'s say certain\\nquestions and not others. It\\'s why it gives you the\\ncanned speech about, you know, whenever it starts with,\\nas a large language model, I cannot, you know, basically means that somebody\\nhas reached in there and told that it can\\'t talk about certain topics. - Do you think some of that is good? - So it\\'s an interesting question. So a couple observations. So, one is the people who find this the most frustrating are the people who are worried\\nabout the murder robots, right? So, and in fact so called\\nX risk people, right? They started with the term AI safety, the term became AI alignment. When the term became AI alignment is when this switch happened\\nfrom we\\'re worried it\\'s gonna kill us all to\\nwe\\'re worried about hate speech and misinformation. - [Lex] Sure. - The AI X risk people have\\nnow renamed their thing AI not kill everyone-ism, which I have to admit is a catchy term. And they are very frustrated\\nby the fact that the hate speech sort of activist driven\\nhate speech misinformation kind of thing is taking over. Which is what\\'s happened is taken over, the AI ethics field has been\\ntaken over by the hate speech misinformation people. You know, look, would I like to live in a world\\nin which like everybody was nice to each other all the\\ntime and nobody ever said anything mean and nobody ever\\nused a bad word and everything was always accurate and honest. Like, that sounds great. Do I wanna live in a world\\nwhere there\\'s like a centralized thought police working through\\nthe tech companies to enforce the view of a small set of\\nelites that they\\'re gonna determine what the rest\\nof us think and feel like? Absolutely not. - There could be a middle\\nground somewhere like Wikipedia type of moderation. There\\'s moderation of Wikipedia\\nthat is somehow crowdsourced where you don\\'t have centralized elites, but it\\'s also not completely\\njust a free for all because if you have the\\nentirety of human knowledge at your fingertips, you\\ncan do a lot of harm. Like if you have a good assistant that\\'s completely uncensored, they can help you build a bomb, they can help you mess with\\npeople\\'s physical wellbeing. Right. If they, because that information is\\nout there on the internet and so presumably there\\'s, it would be, you could see the positives\\nin censoring some aspects of an AI model when it\\'s helping you\\ncommit literal violence. - Yeah. And there\\'s a section\\nlater section of the essay where I talk about bad\\npeople doing bad things. - [Lex] Yes. - Right. Which and there\\'s this, there\\'s a set of things that\\nwe should discuss there. - [Lex] Yeah. - What happens in practice is these lines, as you alluded to this already, these lines are not easy to draw. And what I\\'ve observed in\\nthe social media version of this is like, the way I describe it as the slippery slope is not a fallacy, it\\'s an inevitability. The minute you have this kind\\nof activist personality that gets in a position to make these decisions they take it straight to infinity. Like, it goes into the crazy\\nzone like almost immediately and never comes back because\\npeople become drunk with power. Right. And look, if you\\'re\\nin the position to determine what the entire world thinks and feels and reads and says like, you\\'re gonna take it and you\\nknow, Elon has, you know, ventilated this with the\\nTwitter files over the last, you know, three months and\\nit\\'s just like crystal clear, like how bad it got there now. - [Lex] Yeah. - Reason for optimism\\nis what Elon is doing with community notes. So community notes is actually\\na very interesting thing. So, what Elon is trying to\\ndo with community notes is he\\'s trying to have it where\\nthere\\'s only a community note when people who have previously\\ndisagreed on many topics agree on this one. - Yes, that\\'s what I\\'m\\ntrying to get at is like, there could be Wikipedia like\\nmodels or community notes type of models where allows you\\nto essentially either provide context or sensor in a\\nway that\\'s not resist the slippery slope nature. Power. - Now there\\'s an entirely\\ndifferent approach here, which is basically we have AIs\\nthat are producing content. We could also have ais that\\nare consuming content. Right? And so one of the things that\\nyour assistant could do for you is help you consume\\nall the content, right? And basically tell you\\nwhen you\\'re getting played. So for example, I\\'m gonna\\nwant the AI that my kid uses, right, to be very, you know, child safe and I\\'m gonna want\\nit to filter for him all kinds of inappropriate stuff that\\nhe shouldn\\'t be saying just \\'cause he\\'s a kid. Right? And you see what I\\'m saying\\nis you can implement that. The architectural, you\\ncould say you can solve this on the client side, right? You solving on the server\\nside gives you an opportunity to dictate for the entire\\nworld, which I think is where you take the slippery slope to hell, there\\'s another architectural approach, which is to solve this on the client side, which is certainly what I would endorse. - It\\'s AI risk number five, will AI lead to bad\\npeople doing bad things? And I can just imagine language\\nmodels used to do so many bad things, but the hope is there that\\nyou can have large language models used to then defend\\nagainst it by more people, by smarter people, by more\\neffective people, skilled people, all that kind of stuff. - Three-part argument on\\nbad people doing bad things. So, number one, right? You can use the technology defensively and we should be using AI\\nto build like broad spectrum vaccines and antibiotics for\\nlike bio weapons and we should be using AI to like hunt terrorists and catch criminals and like, we should be doing like all\\nkinds of stuff like that. And in fact, we should be doing those things even just to like go get like, you know, basically go eliminate risk\\nfrom like regular pathogens that aren\\'t like constructed by an AI. So there\\'s the whole\\ndefensive set of things. Second is we have many laws on the books about as actual bad things, right? So it is actually illegal\\nto be a criminal, you know, to commit crimes, to commit\\nterrorist acts to, you know, build pathogens with the intent to deploy them to kill people. And so we have those, we actually don\\'t need new laws for the vast majority of these scenarios. We actually already have the\\nlaws in the book, on the books. The third argument is the minute, and this is sort of the\\nfoundational one that gets really tough, but the minute\\nyou get into this thing, which you were kind of getting\\ninto, which is like, okay, but like, don\\'t you need\\ncensorship sometimes, right? And don\\'t you need restrictions sometimes? It\\'s like, okay, what is the cost of that? And in particular in the\\nworld of open source, right? And so is open source AI\\ngoing to be allowed or not? If open source AI is not allowed, then what is the regime that\\'s\\ngoing to be necessary legally and technically to prevent\\nit from developing? Right? And here again is where you\\nget into and people have proposed that these kinds of things. You get into I would say pretty extreme territory pretty fast. Do we have a monitor\\nagent on every CPU and GPU that reports back to the government? What we\\'re doing with our computers, are we seizing GPU clusters\\nthat get beyond a certain size? Like, and then by the way, how are we doing all that globally, right? And like if China\\'s developing\\nan LLM beyond the scale that we think is allowable,\\nare we gonna invade? Right. And you have figures on the AI X risk side who are advocating any, you know, potentially up to nuclear\\nstrikes to prevent, you know, this kind of thing. And so here you get into this thing and again, you know, maybe\\nyou could maybe say this is, you know, you could even\\nsay this is what good, bad or indifferent or whatever. But like here\\'s the comparison of nukes, the comparison of nukes is very dangerous because one is just nukes, were just, although we can\\ncome back to nuclear power. But the other thing was like with nukes, you could control plutonium, right? You could track plutonium and\\nit was like hard to come by. AI is just math and code, right? And it\\'s in like math\\ntextbooks and it\\'s like, there are YouTube videos that\\nteach you how to build it. And like there\\'s open source,\\nthere\\'s already open source. You know, there\\'s a 40 billion parameter\\nmodel running around already called Falcon Online that\\nanybody can download. And so, okay, you walk down the logic path\\nthat says we need to have guardrails on this. And you find yourself in an authoritarian, totalitarian regime of thought\\ncontrol and machine control that would be so brutal\\nthat you would\\'ve destroyed the society that you\\'re trying to protect. And so I just don\\'t see\\nhow that actually works. - So yeah, you have to\\nunderstand my brain\\'s going full steam ahead here \\'cause I agree with basically\\neverything you\\'re saying, but I\\'m trying to play\\ndevil\\'s advocate here because okay, you\\'re highlighted the fact that there is a slippery\\nslope to human nature. The moment you censor something, you start to censor everything. That alignment starts out sounding nice, but then you start to align to the beliefs of some select group of people. And then it\\'s just your beliefs the number of people\\nyou\\'re aligning to smaller and smaller as that group\\nbecomes more and more powerful. Okay. But that just speaks to the people that censor are usually the assholes and the assholes get richer. I wonder if it\\'s possible\\nto do without that for AI. One way to ask this question is do you think the base models, the baseline foundation\\nmodels should be open sourced? Like, where Marc Zuckerberg\\nis saying they want to do. - So look, I mean I think\\nit\\'s totally appropriate the companies that are in the business of producing a product or service should be\\nable to have a wide range of policies that they put, right? And I\\'ll just, again, I want a heavily censored\\nmodel for my eight year old. Like, I actually want that, like, like I would pay more money\\nfor the ones more heavily censored than the one that\\'s not, right. And so, like there are certainly scenarios where companies will make that decision. Look, an interesting thing you brought up or is this really a speech issue? One of the things that the\\nbig tech companies are dealing with is that content generated\\nfrom an LLM is not covered under section 230, which is the law that protects\\ninternet platform companies from being sued for the\\nuser generated content. And so it is actually-- - [Lex] Oh, wow. - Yes and so there, there\\'s\\nactually a question. I think there\\'s still a question, which is can big American companies actually feel generative AI at all? Or is the liability actually\\ngonna just ultimately convince them that they can\\'t do it? Because the minute the\\nthing says something bad, and it doesn\\'t even\\nneed to be hate speech, it could just be like an\\n(indistinct) it could hallucinate a product, you know, detail\\non a vacuum cleaner, you know, and all of a sudden the\\nvacuum cleaner company sues for misrepresentation. And there\\'s asymmetry there, right? \\'Cause the LLMs gonna be\\nproducing billions of answers to questions and it only needs\\nto get a few wrong to have. - [Lex] So, loss has to get\\nupdated really quick here. - Yeah. And nobody knows\\nwhat to do with that, right? So, so anyway, like there are big, there are big questions around\\nhow companies operate at all. So we talk about those, but then there\\'s this other\\nquestion of like, okay, the open source. So what about open source? And my answer to your\\nquestion is kind of like, obviously yes, the models have, there has to be full open\\nsource here because to live in a world in which that\\nopen source is not allowed is a world of draconian speech control, human control, machine control. I mean, you know, black helicopters with\\njackbooted thugs coming out, repelling down and seizing\\nyour GPU like territory. - [Lex] Well. - No, no, I\\'m a hundred percent serious. - That\\'s you\\'re saying slippery\\nslope always leads there. - No, no, no, no. That\\'s what\\'s required to enforce it. Like how will you enforce a\\nban on open source and AI? - No. Well you could add friction to it, like harder to get the models. \\'Cause people will always\\nbe able to get the models, but it\\'ll be more in the shadows, right? - The leading open source model\\nright now is from the UAE. Like the next time they\\ndo that, what do we do? - [Lex] Yeah. - Like. - Oh, I see you\\'re like. - A 14 year old in Indonesia\\ncomes out with a breakthrough. You know, we talked about most great\\nsoftware comes from a small number of people. Some kid comes out with\\nsome big new breakthrough and quantization or something and has some huge breakthrough. And like, what are we gonna like, invade Indonesia and arrest him? - It seems like in terms of\\nsize of models and effectiveness of models, the big tech companies will\\nprobably lead the way for quite a few years and the question is of what policies they should use? The kid in Indonesia\\nshould not be regulated, but should Google, Meta,\\nMicrosoft, Open AI be regulated? - Well, so, but this goes, okay, so when does it become dangerous? Right. Is the danger that it\\'s as powerful as the current leading commercial model? Or it is just at some\\nother arbitrary threshold? And then by the way, like\\nlook, how do we know, like what we know today is that\\nyou need like a lot of money to like train these things. But there are advances being\\nmade every week on training efficiency and, you know,\\ndata, all kinds of synthetic, you know, look, I don\\'t even like the synthetic data thing we\\'re talking about. Maybe some kid figures out a way to auto-generate synthetic data. - [Lex] That\\'s gonna change everything. - Yeah, exactly. And so like sitting here today, like, the breakthrough just happened, right? You made this point like the\\nbreakthrough just happened. So we don\\'t know what the shape of this technology is gonna be. I mean the big shock\\nhere is that, you know, whatever number of billions\\nof parameters basically represents at least a very big\\npercentage of human thought. Like who would\\'ve imagined that? And then there\\'s already work underway. There was just this paper that\\njust came out that basically takes a gpt three scale model\\nand compresses it down or run on a single 32 core CPU. Like who would\\'ve predicted that? - [Lex] Yeah. - You know, some of these models now you\\ncan run on raspberry pies like today they\\'re very slow,\\nbut like, you know, maybe they\\'ll be a, you know, perceived you have real perform, you know, like it\\'s math and code. And here we\\'re back in here, we\\'re back in, dude, it\\'s math and code. It\\'s math and code, it\\'s\\nmath, code and data. It\\'s bits. - Marc has just like\\nwalked away at this point. You just screw it. I don\\'t know what to do with this. You guys created this\\nwhole internet thing. Yeah, yeah. I mean, I\\'m a huge believer\\nin open source here. - So my argument is we\\'re gonna have, see here\\'s my argument is a, my argument, my full argument is, is AI is gonna be like air,\\nit\\'s gonna be everywhere. Like this is just gonna be in text. It already is, it\\'s gonna be in textbooks\\nand kids are gonna grow up knowing how to do this. And\\nit\\'s just gonna be a thing. It\\'s gonna be in the air\\nand you can\\'t like pull this back anymore. You can\\'t pull back air. And so you just have to figure out how to live in this world, right? And then that\\'s where I think\\nlike all this hand ringing about AI risk is basically\\na complete waste of time, \\'cause the effort should go into okay, what is the defensive approach? And so if you\\'re worried about you know, AI generated pathogens, the\\nright thing to do is to have a permanent project warp speed, right? Funded lavishly. Let\\'s do a Manhattan, let\\'s\\ntalk about Manhattan project, let\\'s do a Manhattan project\\nfor biological defense, right? And let\\'s build ais and let\\'s\\nhave like broad spectrum vaccines where like, we\\'re\\ninsulated from every pathogen. - And well, the interesting\\nthing is because it\\'s software, a kid in his basement, teenager could build like a\\nsystem that defends against like the worst, I mean, and to me\\ndefense is super exciting. It\\'s like, if you believe\\nin the good of human nature for that, most people wanna do good, to be the savior of\\nhumanity is really exciting. - Yes. - Not, okay, that\\'s a dramatic statement. But to help people. - Yeah, of course. Help people. - Yeah. Okay. What about just the jump around, what about the risk of will AI\\nlead to crippling inequality? You know, \\'cause we\\'re kind of saying\\neverybody\\'s life will become better. Is it possible that the\\nrich get richer here? - Yeah, so this goes, this actually ironically\\ngoes back to Marxism. So \\'cause this was the, so the\\ncore claim of Marxism, right? Basically was that the owner, the owners of capital would basically own the means of production. And then over time they\\nwould basically accumulate all the wealth the workers\\nwould be paying in, you know, and getting nothing in return \\'cause they wouldn\\'t be\\nneeded anymore, right? Marx was very worried\\nabout mech what he called mechanization or what later\\nbecame known as automation. And that, you know, the workers would be immiserated\\nand the the capitalists would end up with all. And so this was one of the\\ncore principles of Marxism. Of course it turned out to\\nbe wrong about every previous wave of technology. The reason it, it turned out to be wrong about every previous wave of technology is that the way that the\\nself-interested owner of the machines makes the most money is by providing the production capability in the form of products and services to the most people, the most\\ncustomers as possible, right? The the largest, and this is one of those funny\\nthings where every CEO knows this intuitively, and yet it\\'s like hard to\\nexplain from the outside the way you make the most\\nmoney in any business is by selling to the largest\\nmarket you can possibly get to. The largest market you can\\npossibly get to is everybody on the planet. And so every large company\\ndoes is everything that it can to drive down prices, to\\nbe able to get volumes up, to be able to get to\\neverybody on the planet. And that happened with\\neverything from electricity, it happened with telephones,\\nit happened with radio, it happened with automobiles,\\nit happened with smartphones, it happened with PCs, it\\nhappened with the internet, it happened with mobile broadband. It\\'s happened by the way, with Coca-Cola. It\\'s happened with like every, you know, basically every industrially\\nproduced, you know, good or service people, you wanna drive it to the\\nlargest possible market. And then as proof of that,\\nit\\'s already happened, right? Which is the early\\nadopters of like ChatGPT and Bing are not like, you\\nknow, Exxon and Boeing. They\\'re, you know, your\\nuncle and your nephew, right? It\\'s just like free. It\\'s either freely available\\nonline or it\\'s available for 20 bucks a month or something. But the, you know, these things went this technology went mass market immediately. And so look, the owners of\\nthe means of production, the whoever does this now mentioned these trillion dollar questions. There are people who are gonna\\nget really rich doing this, producing these things, but they\\'re gonna get\\nreally rich by taking this technology to the\\nbroadest possible market. - So yes, they\\'ll get rich, but they\\'ll get rich having\\na huge positive impact on. - Yeah, making the technology\\navailable to everybody. Right. And again, smartphone, same thing. So there\\'s this amazing kind\\nof twist in business history, which is you cannot spend\\n$10,000 on a smartphone, right? You can\\'t spend a\\nhundred thousand dollars, you can\\'t spend a million, like I would buy the\\nmillion dollars smartphone. Like I\\'m signed up for it. Like if it\\'s like, suppose a million dollar\\nsmartphone was like much better than the thousand dollar smartphone. Like I\\'m there to buy\\nit, it doesn\\'t exist. Why doesn\\'t it exist? Apple makes so much more\\nmoney driving the price further down from a thousand dollars than they would trying to harvest, right? And so it\\'s just this\\nrepeating pattern you see over and over again where and\\nwhat\\'s great about it is you, you do not need to rely on\\nanybody\\'s enlightened right? Generosity to do this. You just need to rely on\\ncapitalist self-interest. - What about AI taking our jobs? - Yeah. So very very similar thing here. There\\'s sort of a, there\\'s a\\ncore fallacy which again was very common in Marxism, which is what\\'s called\\nthe lump of labor fallacy. And this is sort of the\\nfallacy that there is only a fixed amount of work\\nto be done in the world. And it\\'s all being done today by people and then if machines do it, there\\'s no other work\\nto be done by people. And that\\'s just a\\ncompletely backwards view on how the economy develops and grows. Because what happens is not\\nin fact that what happens is the introduction of technology\\ninto production process causes prices to fall. As prices fall, consumers\\nhave more spending power. As consumers have more spending power, they create new demand. That new demand then causes\\ncapital and labor to form into new enterprises to\\nsatisfy nuance and needs. And the result is more\\njobs at higher wages. - So nuance and needs, the\\nworries that the creation of nuance and needs at\\na rapid rate will mean there\\'s a lot of turnover in jobs. So people will lose jobs. Just the actual experience\\nof losing a job and having to learn new things and\\nnew skills is painful for the individuals. - Well, two things. One is the new jobs are often much better. So this actually came up\\nthat there was this panic about a decade ago and all\\nthe truck drivers are gonna lose their jobs, right? And number one, that didn\\'t happen \\'cause\\nwe haven\\'t figured out a way to actually finish that yet. But the other thing was\\nlike, look, truck driver, like I grew up in a town\\nthat was basically consisted of a truck stop, right? And I like knew a lot of truck drivers and like truck drivers\\nlive a decade shorter than everybody else. Like, it\\'s actually like a very dangerous, like, they get, like literally they have like\\nhigher rates of skin cancer and on the left side of their, on the left side of their body from being in the sun all the time. The vibration of being\\nin the truck is actually very damaging to your physiology. - And there\\'s actually perhaps partially because of that reason there\\'s a shortage of people who wanna be truck drivers. - Yeah. Like, it\\'s not like\\nthe question always you wanna ask somebody like that\\nis, do you want, you know, do you want your kid to be doing this job? And like most of them will tell you no. Like, I want my kid to\\nbe sitting in a cubicle somewhere like where they\\ndon\\'t have this, like, where they don\\'t die 10 years earlier. And so, the new jobs, number one, the new jobs are often better, but you don\\'t get the new\\njobs until you go through the change. And then to your point,\\nthe training thing, you know, is always the\\nissue is can people adapt? And again, here you need to imagine living in a world in which everybody has the AI\\nassistant capability, right? To be able to pick up new\\nskills much more quickly and be able to have some, you know, be able to have a machine to work with to augment their skills. - It\\'s still gonna be painful, but that\\'s the process of life. - It\\'s painful for some people.\\nI mean there\\'s no, like, there\\'s no question it\\'s\\npainful for some people and they\\'re, you know, they\\'re yes, it\\'s not, again, I\\'m not a utopian on\\nthis and it\\'s not like, it\\'s positive for everybody in the moment, but it has been overwhelmingly\\npositive for 300 years. I mean, look, the concern\\nhere, the concern, this concern has played out for literally centuries and you know, this is the sort of Luddite, you know, the story of the Luddites\\nthat you may remember, there was a panic in the two\\nthousands around outsourcing was gonna take all the jobs. There was a panic in the 2010s\\nthat robots were gonna take all the jobs. In 2019 before COVID we had more jobs at higher wages both in the country and in the world than at any point in human history. And so the overwhelming evidence\\nis that the net gain here is like, just like wildly positive. And most people like overwhelmingly\\ncome out the other side being huge beneficiaries of this. - So you write that the\\nsingle greatest risk, this is the risk you\\'re most convinced by the single greatest risk of AI is that China wins global AI dominance and we the United States\\nand the West do not. Can you elaborate? - Yeah. So this is the\\nother thing which is a lot of this sort of AI\\nrisk debates today sort of assume that we\\'re the\\nonly game in town, right? And so we have the ability to kind of sit in the United States and\\ncriticize ourselves and do, you know, have our\\ngovernment like, you know, beat up on our companies\\nand we\\'ll figure out a way to restrict what our\\ncompanies can do and you know, we\\'re gonna, you know,\\nwe\\'re gonna ban this and ban that, restrict this and do that. And then there\\'s this like\\nother like force out there that like doesn\\'t\\nbelieve we have any power over them whatsoever and they\\nhave no desire to sign up for whatever rules we\\ndecide to put in place and they\\'re gonna do whatever\\nit is they\\'re gonna do. And we have no control over it at all. And it\\'s China and specifically\\nthe Chinese Communist party and they have a completely\\npublicized open, you know, plan for what they\\'re gonna do with AI. And it is not what we have in mind. And not only do they have that as a vision and a plan for their society, but they also have it as a vision and plan for the rest of the world. - So their plan is what? Surveillance? - Authoritarian control. So authoritarian population\\ncontrol you know, good old-fashioned communist\\nauthoritarian control and surveillance and enforcement\\nand social credit scores and all the rest of it. And you are gonna be monitored and metered within an inch of everything all the time. And it\\'s gonna, you know, it\\'s basically the end of human freedom and that\\'s their goal. And you know, they justify it on the basis of that\\'s what leads to peace. - You\\'re worried that the regulating in the United States\\nwill haul progress enough to where the Chinese\\ngovernment would win that race. - So their plan, yeah. Yes, yes. And the reason for that\\nis they, and again, they\\'re very public on this. They have, their plan is to proliferate their approach around the world and they have this program called the Digital Silk Road, right. Which is building on their\\nSilk Road investment program. And they\\'ve got, they\\'ve been laying\\nnetworking infrastructure all over the world with their 5G, right. Work with their company Huawei. And so, they\\'ve been\\nlaying all this fabric, but financial and technological\\nfabric all over the world. And their plan is to roll out their vision of\\nAI on top of that and to have every other country be\\nrunning their version. And then if you\\'re a\\ncountry prone to, you know, authoritarianism, you\\'re\\ngonna find this to be an incredible way to\\nbecome more authoritarian. If you\\'re a country, by the way, not prone to authoritarianism, you\\'re gonna have the Chinese\\nCommunist Party running your infrastructure and having\\nbackdoor into it. Right. Which is also not good. - What\\'s your sense of where\\nthey stand in terms of the race towards super intelligence as\\ncompared to the United States? - Yeah, so good news is they\\'re behind, but bad news is they, you know, let\\'s just say they get\\naccess to everything we do. So they\\'re probably a year\\nbehind at each point in time, but they get, you know, downloads I think of\\nbasically all of our work on a regular basis through\\na variety of means. And they are, you know,\\nat least we\\'ll see, they\\'re at least putting\\nout reports of very, they just put out a report last week of a GPT 3.5 analog. They put out this report,\\nforget what it\\'s called, but they put out this\\nreport of this and they did and they, you know, the\\nway when open AI you know, puts out, one of the ways they test, you know, GPT they run it through standardized\\nexams like the SAT. Right. Just how you can kind of\\ngauge how smart it is. And so the Chinese report, they ran their LLM through\\nthe Chinese equivalent of the SAT and it includes\\na section on Marxism and a section on, I say tongue of thought. And it turns out their AI does very well on both of those topics. - That\\'s right. - So like. - Oh, this alignment thing. - Communist AI, right? Like literal communist AI. Right? And so their vision is\\nlike, that\\'s the, you know, so you know, you can just\\nimagine like you\\'re a school, you know, you\\'re a kid 10\\nyears from now in Argentina or in Germany or in who\\nknows where, Indonesia. And you ask the AI, I\\'d explain to you like\\nhow the economy works and it gives you the most cheery, upbeat explanation of\\nChinese style communism you\\'ve ever heard. Right. So like the stakes here\\nare like really big. - Well, as we\\'ve been talking about, my hope is not just\\nwith the United States, but with just the kid in his basement. The open source LLM. \\'Cause I don\\'t know if I trust large centralized institutions\\nwith super powerful AI no matter what their\\nideology as a power corrupts. You\\'ve been investing in\\ntech companies for about, let\\'s say 20 years. And about 15 of which was\\nwith Andreessen Horowitz. What interesting trends\\nin tech have you seen over that time? Let\\'s just talk about companies\\nand just the evolution of the tech industry. - I mean the big shift over 20\\nyears has been that tech used to be a tools industry for\\nbasically from like 1940 through to about 2010, almost all the big successful\\ncompanies were pick and shovels companies. So PC, database, smartphone, you know, some tool that somebody\\nelse would pick up and use. Since 2010, most of the big\\nwins have been in applications. So a company that starts you know, starts in an existing\\nindustry and goes directly to the customer in that industry. And you know, the earliest examples there\\nwere like Uber and Lyft and Airbnb. And then that model is\\nkind of elaborating out. The AI thing is actually a\\nreversion on that for now \\'cause like most of the AI\\nbusiness right now is actually in cloud provision of AI APIs\\nfor other people to build on. - But the big thing\\nwill probably be in app. - Yeah. I think most of the\\nmoney I think probably will be in whatever your AI financial advisor or your AI doctor or your\\nAI lawyer or, you know, take your pick of whatever the domain is. And there, and what\\'s\\ninteresting is, you know, the valley kind of does everything. The entrepreneurs kind of\\nelaborate every possible idea. And so there will be a set of\\ncompanies that like make AI something that can be purchased\\nand used by large law firms and then there will be other\\ncompanies that just go direct to market as an AI lawyer. - What advice could you\\ngive for a startup founder? Just haven\\'t seen so many\\nsuccessful companies, so many companies that fail also, what advice could you\\ngive to a startup founder, someone who wants to build the\\nnext super successful startup in the tech space? The Googles,\\nthe Apples, the Twitters. - Yeah. So the great thing about the really great founders is they don\\'t take any advice. So, if you find yourself\\nlistening to advice, maybe you shouldn\\'t do it. - But that\\'s actually,\\njust to elaborate on that, if you could also speak\\nto great founders too. Like what makes a great founder? - So what makes a great\\nfounder is super smart, coupled with super energetic,\\ncoupled with super courageous. I think it\\'s some of those three and-- - Intelligence, passion and courage. - The first two are traits\\nand the third one is a choice. I think courage is a choice. Well \\'cause courage is a question\\nof pain tolerance, right? So how many times are you\\nwilling to get punched in the face before you quit? And here\\'s maybe the biggest\\nthing people don\\'t understand about what it\\'s like to be\\na startup founder is it gets very romanticized, right? And even when it, even when they fail, it still gets romanticized about like what a great adventure it was. But like the reality of it is\\nmost of what happens is people telling you no and then\\nthey usually follow that with you\\'re stupid, right. No, I will not come to work for you. I will not leave my cushy job at Google to come work for you. No, I\\'m not gonna buy your\\nproduct, you know, no, I\\'m not gonna run a\\nstory about your company. No, I\\'m not this, that, the other thing. And so a huge amount of what\\npeople have to do is just get used to just getting punched and the reason people\\ndon\\'t understand this is because when you\\'re a founder, you cannot let on that this is happening \\'cause it will cause people to think that you\\'re weak and\\nthey\\'ll lose faith in you. So you have to pretend that\\nyou\\'re having a great time when you\\'re dying inside, right? You\\'re just in misery. - But why did they do it? - Why did they do? Yeah, that\\'s the thing. It\\'s like it is a level, this is actually one of\\nthe conclusions I think is that I think it\\'s actually\\nfor most of these people on a risk adjusted basis, it\\'s\\nprobably an irrational act. They could probably be\\nmore financially successful on average if they just\\ngot like a real job in at a big company. But there\\'s, you know, some people just have an\\nirrational need to do something new and build something for\\nthemselves and, you know, some people just can\\'t\\ntolerate having bosses. Oh, here\\'s the fun thing is how do you reference\\ncheck founders, right? So you call the, you know, normal way you reference check, you\\'re hiring somebody\\nis you call the bosses, they\\'re their, and you know, and you find out if\\nthey were good employees and now you\\'re trying to\\nreference check Steve Jobs, right? And it\\'s like, oh God, he was terrible. You know, he was a terrible employee. He never did what we told him to do. - So what\\'s a good reference? Do you want the previous\\nboss to actually say they never did what you told him to do? That might be a good thing. - Well, ideally what\\nyou want is I will go, I would like to go to\\nwork for that person. He worked for me here and\\nnow I\\'d like to work for him. No, unfortunately, most\\npeople can\\'t, their egos can\\'t handle that. So they won\\'t say that. But that\\'s the ideal. - What advice would\\nyou give to those folks in the space of intelligence,\\npassion and courage? - So I think the other big thing\\nis you see people sometimes who say, I wanna start a company and then they kind of\\nwork through the process of coming up with an idea. And generally those don\\'t\\nwork as well as the case where somebody has the idea first and then they kind of realize that there\\'s an opportunity\\nto build a company and then they just turn\\nout to be the right kind of person to do that. - When you say idea, do you\\nmean long-term big vision or do you mean specifics of like product? - Specific I would say specific, like specifically what specifics. Like what is the, because for the first five years you don\\'t get to have vision, you just gotta build something people want and you gotta figure out a\\nway to sell it to them. Right. It\\'s very practical or you\\nnever get to big vision. - So the first product, you have an idea of a set of\\nproducts of the first product that can actually make some money. - Yeah. Like it\\'s gotta work. The first product\\'s gotta\\nwork by which I mean like, it has to technically work, but then it has to actually\\nfit into the category and the customer\\'s mind if\\nsomething that they want and then by the way, the other part is they have\\nto be willing to pay for it. Like somebody\\'s gotta pay the bills. And so you\\'ve gotta\\nfigure out how to price it and whether you can\\nactually extract the money. So usually it is much more predictable. Success is never predictable, but it\\'s more predictable if\\nyou start with a great idea and then back into starting the company. So this is what we did,\\nyou know, we had most, before we had escape, the Google guys had the\\nGoogle search engine working at Stanford. Right. You know, yeah. Actually there\\'s tons of\\nexamples where they, you know, Pierre Omaira had eBay working before he left his previous job. - So I really love that\\nidea of just having a thing, a prototype that actually\\nworks before you even begin to remotely scale. Yeah. - By the way, it\\'s also far\\neasier to raise money, right? Like the ideal pitch that we receive is, here\\'s the thing that works, would you like to invest\\nin our company or not? Like, that\\'s so much easier than here\\'s 30 slides with a dream, right? And then we have this\\nconcept called the DMAs, which our biology of came\\nup with when he was with us. So then there\\'s this thing,\\nthis goes to mythology, which is, you know, there\\'s\\na mythology that kind of, you know, these ideas, you know, kind of arrive like magic or people kind of stumble into them. It\\'s like eBay with the pest\\ndispensers or something. The reality usually with\\nthe big successes is that the founder has been\\nchewing on the problem for 5 or 10 years before they start the company\\nand they often worked on it in school or they even experimented on it when they were a kid and they\\'ve been kind of training up over that period of time to\\nbe able to do the thing. So they\\'re like a true domain expert. And it sort of sounds like mom, I\\'m an apple pie, which is yeah, you wanna be a domain\\nexpert in what you\\'re doing, but you would, you know, the\\nmythology is so strong of like, oh, I just like had this idea in the shower right now I\\'m doing it. Like it\\'s generally not that. - No, because it\\'s, well, maybe in the shower\\nwe had the exact product implementation details, but yeah, usually you\\'re gonna be for\\nlike years if not decades thinking about like\\neverything around that. - Well we call it the DMAs\\nbecause the DMAs basically is like, there\\'s all these permutations, like for any idea, there\\'s like all these\\ndifferent permutations, who should the customer be? What shape forms should the product have and how should we take it to\\nmarket and all these things. And so the really smart\\nfounders have thought through all these scenarios\\nby the time they go out to raise money and they\\nhave like detailed answers on every one of those fronts because they put so much thought into it. The sort of more haphazard\\nfounders haven\\'t thought about any of that. And it\\'s the detailed ones\\nwho tend to do much better. - So how do you know when to take a leap if you have a cushy job or happy life? - I mean the best reason is just \\'cause you can\\'t tolerate\\nnot doing it right? Like this is the kind of\\nthing where if you have to be advised into doing it, you\\nprobably shouldn\\'t do it. And so it\\'s probably the opposite, which is you just have such\\na burning sense of this has to be done, I have to do\\nthis, I have no choice. - What if it\\'s gonna\\nlead to a lot of pain? - It\\'s gonna lead to a lot\\nof pain. I think that\\'s. - What if it means losing\\nsort of social relationships and damaging your\\nrelationship with loved ones and all that kind of stuff. - Yeah, look, so like, it\\'s gonna put you in a\\nsocial tunnel for sure, right? So you\\'re gonna, like, you know, there\\'s this game you can play on Twitter, which is you can do any whiff\\nof the idea that there\\'s basically any such thing\\nas work life balance and that people should actually work hard and everybody gets mad. But like, the truth is like all the\\nsuccessful founders are working 80 hour weeks and they\\'re\\nworking, you know, they form very, very strong social bonds with\\nthe people they work with. They tend to lose a lot of\\nfriends on the outside or put those friendships on ice. Like that\\'s just the nature of the thing, you know, for most people\\nthat\\'s worth the trade off. You know, the advantage, you know, maybe younger founders have\\nis maybe they have less, you know, maybe they\\'re\\nnot, you know, for example, if they\\'re not married yet\\nor don\\'t have kids yet, that\\'s an easier thing to bite off. - Can you be an older founder? - Yeah. You definitely can. Yeah. Yeah. Many of the most\\nsuccessful founders are second, third, fourth time founders. They\\'re in their thirties,\\nforties, fifties. The good news with being an\\nolder founder is, you know, more and you, you know, a\\nlot more about what to do, which is very helpful.\\nThe problem is, okay, now you\\'ve got like a spouse\\nand a family and kids and like, you\\'ve gotta go to the\\nbaseball game and like, you can\\'t go to the base,\\nyou know, and so it\\'s. - [Lex] Life is full of difficult choices. - Yes. - Marc Andreessen, you\\'ve written a blog post\\non what you\\'ve been up to. You wrote this in October, 2022, \"Mostly I try to learn a lot. For example, the political events of 2014\\nto 2016 make clear to me that I didn\\'t understand\\npolitics at all referencing maybe some of this book here. So I deliberately withdrew\\nfrom political engagement and fundraising and instead\\nread my way back into history and as far to the political left and political right as I could.\" So just high level question, what\\'s your approach to learning? - Yeah, so it\\'s basically, I would say, I\\'m an AutoID direct,\\nso it\\'s sort of goes, it\\'s going down the rabbit holes. So it\\'s a combination. I kind of allude to it', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 517638, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content=' you, you know, a\\nlot more about what to do, which is very helpful.\\nThe problem is, okay, now you\\'ve got like a spouse\\nand a family and kids and like, you\\'ve gotta go to the\\nbaseball game and like, you can\\'t go to the base,\\nyou know, and so it\\'s. - [Lex] Life is full of difficult choices. - Yes. - Marc Andreessen, you\\'ve written a blog post\\non what you\\'ve been up to. You wrote this in October, 2022, \"Mostly I try to learn a lot. For example, the political events of 2014\\nto 2016 make clear to me that I didn\\'t understand\\npolitics at all referencing maybe some of this book here. So I deliberately withdrew\\nfrom political engagement and fundraising and instead\\nread my way back into history and as far to the political left and political right as I could.\" So just high level question, what\\'s your approach to learning? - Yeah, so it\\'s basically, I would say, I\\'m an AutoID direct,\\nso it\\'s sort of goes, it\\'s going down the rabbit holes. So it\\'s a combination. I kind of allude to it\\nin that, in that quote, it\\'s a combination of breadth and depth. And so I tend to, yeah, I tend to, I go broad by the nature\\nof what I do, I go broad, but then I tend to go deep\\nin a rabbit hole for a while, read everything I can\\nand then come out of it. And I might not revisit\\nthat rabbit hole for, you know, another decade. - And in that blog post that I recommend people go check out, you actually list a bunch\\nof different books that you recommend on different\\ntopics on the American left, on the American right. It\\'s just a lot of really good stuff. The best explanation for\\nthe current structure of our society and politics. You give to recommendations, four books on the Spanish Civil War, six books on deep history\\nof the American right comprehensive biographies. These of Adolf Hitler, one of which I read can\\nrecommend six books on the deep history of the American\\nleft. So the American right, American left looking at the\\nhistory to give you the context biography of later Lennon, two of them on the French\\nRevolution. I actually, I have never read a\\nbiography on Lennon maybe that would be useful. Everything\\'s been so Marc\\'s focused. - The Sebastian biography\\nof Lennon is extraordinary. - [Lex] Victor Sebestyen. Okay. - Blow your mind. Yeah. - [Lex] So it\\'s still useful to read. - It\\'s incredible. Yeah, it\\'s incredible. I actually think it\\'s the single best book on the Soviet Union. - So that the perspective of Lennon, it might be the best way to\\nlook at the Soviet Union versus Stalin versus Marx\\nversus, very interesting. So two books on fascism and\\nanti-fascism by the same author, Paul Gottfried, brilliant book on the\\nnature of mass movements and collective psychology, the definitive work on\\nintellectual life under totalitarianism, the Captive Mind, the definitive worked\\non the practical life under totalitarianism. There\\'s a bunch. There\\'s a bunch. And the single best book, first of all, the list here is just incredible. But you say the single best\\nbook I have found on who we are and how we got here is the Ancient City by Numa Dennis Fustel De Coulanges. I like it. What did you learn about who\\nwe are as a human civilization from that book? - Yeah, so this is a fascinating book. This one\\'s free, it\\'s a free, by the way, it\\'s a book in the 1860s. You can download it or\\nyou can buy printouts up prints of it. But it was this guy who was\\na professor at the savant in the 1860s and he was\\napparently a savant on antiquity on Greek and Roman antiquity and the reason I say that is\\nbecause his sources are 100% original Greek and Roman sources. So he wrote a basically\\nhistory of western civilization from, on the order of 4,000 years ago to basically the present\\ntimes entirely working on fresh original Greek and Roman sources. And what he was specifically\\ntrying to do was he was trying to reconstruct from the stories of the Greeks and the Romans, he was trying to reconstruct\\nwhat life in the west was like before the Greeks and the Romans, which was in the civilization known as the Indo Europeans. And the short answer is,\\nand this is sort of 4,000, you know, 2000 BC to, you know, sort of 500 BC kind of\\nthat 1500 year stretch for civilization developed. And his conclusion was basically cults. They were basically cults\\nand civilization was, or organized into cults. And the intensity of the cults was like a million fold beyond anything that we would recognize today. Like it was a level of\\nall encompassing belief and an action around religion that was at a level of extremeness that we wouldn\\'t even recognize it and so specifically he\\ntells the story of basically there were three levels of cults. There was the family cult, the tribal cult, and then the city cult as society scaled up. And then each cult was a\\njoint cult of family gods, which were ancestor gods. And then nature gods and then\\nyour bonding into a family, a tribe or a city was\\nbased on your adherence to that religion. People who were not of your\\nfamily, tribe, city, worship, different gods, which gave you not just the\\nright with or responsibility to kill them on site. - [Lex] So they were\\nserious about their cults. - Hardcore, by the way,\\nshocking development. I did not realize this zero\\nconcept of individual rights. Like even even up through the Greeks, and even in the Romans, they didn\\'t have, have the concept of individual rights. Like the idea that as an\\nindividual you have like some rights just like, nope. Right? And you look back\\nand you\\'re just like, wow, that\\'s just like cr\\nlike fascist in a degree that we wouldn\\'t recognize today. But it\\'s like, well, they were living under\\nextreme pressure for survival. And you, and you know, the theory goes, you could not have people\\nrunning around making claims, individual rights when\\nyou\\'re just trying to get like your tribe through the winter, right? Like you need like hardcore\\ncommand and control. And actually what if through\\nmodern political lens, those cults were basically\\nboth fascist and communist. They were fascist in\\nterms of social control, and then they were communist\\nin terms of economics. - But you think that\\'s\\nfundamentally that like pull towards cults is within us. - Well, so my conclusion from this book, so the way we naturally\\nthink about the world we live in today is like, we basically have such an\\nimproved version of everything that came before us, right? Like, we have basically, we\\'ve figured out all these\\nthings around morality and ethics and democracy\\nand all these things. And like, they were basically\\nstupid and retrograde and we\\'re like smart and sophisticated. And we\\'ve improved all this after reading that book, I now believe in many ways\\nthe opposite, which is no, actually we are still running\\nin that original model. We\\'re just running in an\\nincredibly diluted version of it. So we\\'re still running,\\nbasically in cults. It\\'s just our cults are at like\\na thousandth or a millionth, the level of intensity, right? And so our, so just as to\\ntake religions, you know, the modern experience of\\na Christian in our time, even somebody who considers\\nhim a devout Christian, is just a shadow of the level\\nof intensity of somebody who belonged to a religion\\nback in that period. And then by the way, we have cons. It goes back to our AI discussion. We then sort of endlessly\\ncreate new cults. Like we\\'re trying to fill the void, right? And the void is a void of bonding. - [Lex] Okay. - Living in their era. Like\\neverybody living today, transporting that era\\nwould view it as just like, completely intolerable in terms of like the loss of freedom and the level of basically of fascist control. However, every single person in that era, and he really stresses this. They knew exactly where they stood. They knew exactly where they belonged. They knew exactly what their purpose was. They knew exactly what they\\nneeded to do every day. They knew exactly why they were doing it. They had total certainty about\\ntheir place in the universe. - So the question of meaning, the question of purpose\\nwas very distinctly, clearly defined for them. - Absolutely overwhelmingly\\nundisputably undeniably. - As we turn the volume\\ndown on the cultism-- - [Marc] Yes. - We start to, the search for meaning starts\\ngetting harder and harder. - Yes. \\'cause we don\\'t have that. We are ungrounded. We are uncentered and\\nwe all feel it. Right? And that\\'s why we reach for, you know, it\\'s why we still reach for religion. It\\'s why we reach for, you know, we people start\\nto take on, you know, let\\'s say, you know, a faith in science maybe beyond\\nwhere they should put it. You know and by the way,\\nlike, sports teams are like a, you know, they\\'re like a tiny\\nlittle version of a cult. And you know, apple keynotes are a tiny\\nlittle version of a cult. Right. And, you know, political, you know. And there\\'s cult, you know, there\\'s full-blown cults on both sides of the political spectrum\\nright now. Right. You know, operating in plain stuff. - But still not full blown\\ncompared as to what it was. - Compared to what it used to. I mean, we would today consider\\nfull blown, but like, yes, they\\'re at like, I don\\'t know, a hundred thousandth or\\nsomething of the intensity of what people had back then. So, we live in a world today\\nthat in many ways is more advanced and moral and so forth. And it\\'s certainly a lot nicer,\\nmuch nicer world to live in. But we live in a world\\nthat\\'s like very washed out. It\\'s like everything has\\nbecome very colorless and gray as compared to how people\\nused to experience things. Which is I think why we\\'re\\nso prone to reach for drama. \\'Cause there\\'s something in us that\\'s deeply evolved\\nwhere we want that back. - And I wonder where it\\'s all\\nheaded as we turn the volume down more and more. What advice would you\\ngive to young folks today in high school and college? How to be successful in their career? How to be successful in their life? - Yeah. So the tools that\\nare available today, I mean, are just like, I\\nsometimes, you know, bore, I sometimes bore, you know, kids by describing like what\\nit was like to go look up a book, you know, to try to like discover\\na fact in, you know, in the old days, the 1970s, 1980s, to go to the library and the card catalog and the whole thing. You go through all that work\\nand then the book is checked out and you have to wait two weeks and like to be in a world, not only where you can get\\nthe answer to any question, but also the world now, you know, the AI world where you\\'ve\\ngot like the assistant that will help you do\\nanything, help you teach, learn anything, like your ability both to learn and also to produce\\nis just like, I don\\'t know, a million fold beyond what it used to be. I have a blog post I\\'ve\\nbeen wanting to write, which I call where are the\\nhyper-productive people? Like-- - [Lex] That\\'s a good question, right? - Like with these tools, like there should be authors\\nthat are writing like hundreds or thousands of like, outstanding books. - Well, with the authors there\\'s\\na consumption question too, but yeah. Well, maybe not, maybe not. You\\'re right. But, so the tools are much more powerful. Getting much more powerful. - Artists, musicians. Right. Why aren\\'t musicians producing\\na thousand times the number of songs, right? Like what, like the tools are spectacular. - So, what\\'s the explanation? And by way of advice, like, is motivation starting to\\nbe turned down a little bit? Or what? - I think it might be distraction. - [Lex] Distraction. - It\\'s so easy to just sit and consume that I think people get distracted from production.\\nBut if you wanted to, you know, as a young person, if you\\nwanted to really stand out, you could get on a, like a hyper productivity curve very early on. There\\'s a great, you know, this story, there\\'s a great story in\\nRoman history of plenty of the elder who was\\nthis legendary statesman, died in the Vesuvius eruption\\ntrying to rescue his friends. But he was famous both for being basically being a polymath,\\nbut also being an author. And he wrote apparently\\nlike hundreds of books, most of us had been lost. But he like wrote all these\\nencyclopedias and he literally like would be reading and\\nwriting all day long no matter what else was going on. And so he would like travel\\nwith like four slaves. And two of them were\\nresponsible for reading to him, and two of them were responsible\\nfor taking dictation. And so like, he\\'d be going\\ncross country and like, literally he would be writing\\nbooks like all the time. And apparently they were spectacular. There\\'s only a few that have survived, but apparently they were amazing. - There\\'s a lot of value to being somebody who finds focus in this life. - Yeah. Like and there are\\nexamples, like there are, you know, there\\'s this guy,\\njudge, what\\'s his name? Posner, who wrote like 40 books and was also a great federal judge. You know, there\\'s our friend Balaji, I think is like this, he\\'s\\none of these, you know, where his output is just prodigious. And so it\\'s like, yeah, I mean,\\nwith these tools, why not? And I kind of think we\\'re at this interesting\\nkind of freeze frame moment where like this, these tools are now in everybody\\'s hands and everybody\\'s just kind\\nof staring at them trying to figure out what to do. The new tools. - We have discovered fire. - [Marc] Yeah. - And trying to figure\\nout how to use it to cook. - [Marc] Yeah. Right. - You told Tim Ferriss that\\nthe perfect day is caffeine for 10 hours and alcohol for four hours. You didn\\'t think I\\'d be\\nmentioning this, did you? It balances everything\\nout perfectly as you said. So, perfect. So let me ask, what\\'s the secret to balance\\nand maybe to happiness in life? - I don\\'t believe in balance, so I\\'m the wrong person to ask that. - Can you elaborate why you\\ndon\\'t believe in balance? - I mean, I maybe it\\'s just,\\nand I look, I think people, I think people are wired differently. So, I think it\\'s hard to\\ngeneralize this kind of thing, but I am much happier and more satisfied when I\\'m fully committed to something. So I\\'m very much in favor\\nof all in of imbalance. - Imbalance. And that applies to work, to life, to everything. - Yeah. No, no. I happen to have whatever twist\\nof personality traits lead that in non-destructive\\ndimensions in including the fact that I\\'ve actually, I now no\\nlonger do the ten-four plan. I stopped drinking. I do the caffeine, but not the alcohol. So there\\'s something in my personality where I whatever mal-adaption\\nI have is inclining me towards productive things,\\nnot unproductive things. - So you\\'re one of the\\nwealthiest people in the world. What\\'s the relationship\\nbetween wealth and happiness? Money and happiness. - So I think happiness, I don\\'t think happiness is the thing. - To strive for. - I think satisfaction is the thing. - That just sounds like\\nhappiness, but turned down a bit. - No deeper. So happiness is, you know, a walk in the woods at sunset,\\nan ice cream cone, a kiss, the first ice cream cone is great. The thousandth ice\\ncream cone, not so much. At some point the walks\\nin the woods get boring. - What\\'s the distinction between\\nhappiness and satisfaction? - I think satisfaction is a deeper thing, which is like having found a purpose and fulfilling it, being useful. - So just something that\\npermeates all your days, just this general\\ncontentment of being useful. - That I\\'m fully satisfying my faculties, that I\\'m fully delivering, right? On the gifts that I\\'ve been\\ngiven, that I\\'m, you know, net making the world better, that I\\'m contributing to\\nthe people around me, right. And that I can look back\\nand say, wow, that was hard, but it was worth it. Think generally, it seems to lead people in\\na better state than pursuit of pleasure, pursuit of\\nquote unquote happiness. - Does money have\\nanything to do with that? - I think the founders and\\nthe founding fathers in the US threw this off kilter when\\nthey used the phrase pursuit of happiness. I think they should have said. - [Lex] Pursuit of satisfaction. - They said, pursuit of satisfaction. We might live in a better world today. - Well, they, you know, they could have elaborated on a lot of things right in the box. - [Marc] They could have\\ntweaked the second amendment. - I think they were\\nsmarter than they realized. They said, you know we\\'re gonna make it ambiguous and let these humans figure out the rest, these tribal cult-like\\nhumans figure out the rest. But money empowers that. - So I think, and I think there, I mean, look, I think Elon is, I don\\'t think I\\'m even a great example, but I think Elon would be\\nthe great example of this, which is like, you know, look,\\nhe\\'s a guy who from every, every day of his life, from the day he started\\nmaking money at all, he just plows into the next thing. And so I think, I think money is definitely\\nan enabler for satisfaction. Way money applied to\\nhappiness leads people down very dark paths. Very destructive avenues. Money applied to satisfaction,\\nI think could be, is a real tool. I always, by the way,\\nI was like, you know, Elon is the case study for behavior. But the other thing that I\\nalways really made me think is Larry Page was asked one time what his approach to philanthropy was. And he said, oh, I\\'m just, my philanthropic plan is just give all the money to Elon. (both laugh) - Well, let me actually\\nask you about Elon. You\\'ve interacted with quite a lot of successful engineers\\nand business people. What do you think is special about Elon? We talked about Steve Jobs. What do you think is special\\nabout him as a leader? As an innovator? - Yeah. So the core of it is he\\'s back to the future. So he is doing the most\\nleading-edge things in the world, but with a really deeply\\nold-school approach. And so to find comparisons to Elon, you need to go to like\\nHenry Ford and Thomas Watson and Howard Hughes and\\nAndrew Carnegie, right. Leland Stanford, John Rockefeller, right. You need to go to what were called the\\nbourgeois capitalists, like the hardcore business owner operators who basically built, you know, basically built industrialized\\nsociety, Vanderbilt. And it\\'s a level of hands-on commitment and depth in the business, coupled with an absolute priority\\ntowards truth and towards, how to put it, science and technology\\ntown to first principles that is just like absolute, is just like unbelievably absolute. He really is ideal that he\\'s\\nonly ever talking to engineers. Like he does not tolerate. He has less tolerance than\\nanybody I\\'ve ever met. He wants ground truth\\non every single topic. And he runs his businesses\\ndirectly day-to-day, devoted to getting to ground\\ntruth in every single topic. - So you think it was a good decision for him to buy Twitter? - I have developed a view in life to not second guess Elon Musk, I know this is gonna sound\\ngreat, crazy and unfounded, but. - Well, I mean, he\\'s got\\na quite a track record. - I mean, look, the car was\\na crazy, I mean, the car was, I mean, look. - He\\'s done a lot of\\nthings that seem crazy. - Starting a new car company in the United States of America. The last time somebody\\nreally tried to do that was the 1950s and it was\\ncalled Tucker Automotive. And it was such a disaster. They made a movie about\\nwhat a disaster it was, and then rockets like, who does that? Like, there\\'s obviously no way to start a new rocket company. Like those days are over. And then to do those at the same time. So after he pulled those\\ntwo off, like, okay, fine. Like, this is one of my areas of like, whatever opinions I had about\\nthat, that is just like, okay, clearly are not relevant. Like this is you just, you at some point you just\\nlike bet on the person. - And in general, I wish more people would lean\\non celebrating and supporting versus deriding and destroying. - Oh yeah. I mean, look,\\nhe drives resentment. Like it\\'s a resentment. Like he is a magnet for resentment. Like his critics are the\\nmost miserable, like, resentful people in the world. Like it\\'s almost a perfect match\\nof like the most idealized, you know, technologist, you know, of the century coupled with like, just his critics are\\njust bitter as can be. And I mean, it\\'s sort of\\nvery darkly comic to watch. - Well, he fuels the fire of that by being on Twitter at times. And which is fascinating\\nto watch the drama of human civilization, given our cult roots just fully on fire. - [Marc] He\\'s running a cult. - You could say that. - [Marc] Very successfully. - So now that our cults\\nhave gone and we searched for meaning, what do\\nyou think is the meaning of this whole thing? What\\'s the meaning of\\nlife Marc Andreessen? - I don\\'t know the answer\\nto that. I think the meaning of the closest I get to it is what I said about satisfaction. So it\\'s basically like, okay, we were given what we have, like we should basically do our best. - What\\'s the role of love in that mix? - I mean, like, what\\'s the point of life if you\\'re without love, like, yeah. - So love is a big part\\nof that satisfaction. - Yeah. And look like\\ntaking care of people is like a wonderful thing. Like it, you know, mentality, you know, there are pathological forms\\nof taking care of people, but there\\'s also a very\\nfundamental, you know, kind of aspect of taking care of people. Like, for example, I happen to be somebody who\\nbelieves that capitalism and taking care of people are actually, they\\'re actually the same thing. Somebody once said, capitalism is how you take\\ncare of people you don\\'t know. Right, right. And so like, yeah, I think it\\'s like deeply\\nwoven into the whole thing, you know, there\\'s a long conversation to be had about that, but yeah. - Yeah. Creating products that are used by millions of people and bring them joy in smaller, big ways. And then capitalism kind of\\nenables that, encourages that. - David Friedman says, there\\'s only three ways to\\nget somebody to do something for somebody else. Love, money and force. And love and money are better. - [Lex] Yeah. Of course. That\\'s a good ordering. I think. - We should bet on those. - Try love first. If that doesn\\'t work, then money. - [Marc] Yes. - And then force. Well, don\\'t even try that one. Marc, you\\'re an incredible person. I\\'ve been a huge fan. I\\'m glad to finally got a chance to talk. I\\'m a fan of everything\\nyou do, everything you do, including on Twitter. It\\'s a huge honor to meet\\nyou, to talk with you. Thanks again for doing this. - Awesome. Thank you, Lex. - Thanks for listening\\nto this conversation with Marc Andreessen. To support this podcast, please check out our\\nsponsors in the description. And now let me leave you with some words from Marc Andreessen himself. \"The world is a very malleable place. If you know what you\\nwant and you go for it, with maximum energy and drive and passion, the world will often\\nreconfigure itself around you much more quickly and easily\\nthan you would think.\" Thank you for listening and\\nhope to see you next time.', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 517638, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "import vertexai\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# vertex_llm_text = VertexAI(model_name=\"text-bison@001\")\n",
    "vertex_llm_text = VertexAI()\n",
    "prompt_template = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "  \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "question_prompt_template = \"\"\"\n",
    "                  Please provide a summary of the following text.\n",
    "                  TEXT: {text}\n",
    "                  SUMMARY:\n",
    "                  \"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "refine_prompt_template = \"\"\"\n",
    "              Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "              \"\"\"\n",
    "\n",
    "refine_prompt = PromptTemplate(\n",
    "    template=refine_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "refine_chain = load_summarize_chain(\n",
    "    llm=vertex_llm_text,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "# summary2 = refine_chain.run(docs_summary)\n",
    "# # Initialize summarization chain\n",
    "# # summarize_chain = load_summarize_chain(\n",
    "# #     llm=llm_summary, chain_type=\"refine\", verbose=True, question_prompt=PROMPT_SUMMARY, refine_prompt=PROMPT_SUMMARY_REFINE)\n",
    "# # summary = summarize_chain.run(docs_summary)\n",
    "# # Write summary to file\n",
    "# with open(\"summary2.txt\", \"w\") as f:\n",
    "#     f.write(summary2)\n",
    "# summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='- The competence and\\ncapability and intelligence and training and accomplishments\\nof senior scientists and technologists working on a technology, and then being able to then\\nmake moral judgments in the use of the technology. That track record is terrible. That track record is catastrophically bad. The policies that are being\\ncalled for to prevent this, I think we\\'re gonna cause\\nextraordinary damage. - So the moment you say,\\nAI\\'s gonna kill all of us, therefore we should ban it, or that we should regulate\\nall that kind of stuff, that\\'s when it starts getting serious. - Or start, you know, military\\nairstrikes and data centers. - Oh boy. The following is a conversation\\nwith Marc Andreessen, co-creator of Mosaic, the\\nfirst widely used web browser, co-founder of Netscape, co-founder of the legendary Silicon Valley venture capital firm, Andreesen Horowitz, and is one of the most\\noutspoken voices on the future of technology, including\\nhis most recent article, \"Why AI Will Save The World?\" This is Lex Fridman podcast. To support it, please\\ncheck out our sponsors in the description. And now, dear friends,\\nhere\\'s Marc Andreessen. I think you\\'re the right\\nperson to talk about the future of the internet and technology in general. Do you think we\\'ll\\nstill have Google search in 5 in 10 years, or search in general? - Yes. You know, it would be a question if the use cases have\\nreally narrowed down. - Well, now with AI-- - [Marc] Yeah. - And AI assistance being\\nable to interact and expose the entirety of human wisdom and knowledge and information and facts and truth to us via the natural language interface. It seems like that\\'s what\\nsearch is designed to do. And if AI assistance can do that better, doesn\\'t the nature of search change? - Sure. But we still have horses. - Okay. (both laugh) When\\'s the last time you rode a horse? - It\\'s been a while. - All right. (both laugh) So, but what I mean is, well, we still have Google\\nsearch as the primary way that human civilization uses\\nto interact with knowledge. - I mean, search was a technology, it was a moment in time technology, which is you have in theory, the world\\'s information out on the web. And, you know, this is sort of\\nthe optimal way to get to it. But yeah, like, and by\\nthe way, actually Google, Google has known this for a long time. I mean, they\\'ve been driving\\naway from the 10 blue links you know, for like two days. They\\'ve been trying to get\\naway from that for a long time. - [Lex] What kind of links? - They call the 10 blue links. - [Lex] 10 blue links. - So the standard Google search\\nresult is just 10 blue links to the random websites. - And they term purple when\\nyou visit them. The stage TMO. - Guess who picked those colors? (both laugh) - [Lex] Thanks. - I\\'m touchy on this topic. - No offense. - Yes, it\\'s good. Well, you know, like Marshall McLuhan said\\nthat the content of each new medium is the old medium. - The content of each new\\nmedium is the old medium. - The content of movies was theater, you know, theater plays. The content of theater\\nplays was, you know, we\\'ve written stories, the content of written\\nstories with spoken stories. - [Lex] Huh? - Right. And so you just\\nkind of fold the old thing into the new thing. - [Lex] How does that\\nhave to do with the blue and the purple links? - It just, you maybe for,\\nyou know, maybe within AI, one of the things that AI can\\ndo for you is can generate the 10 blue links. Right? And so like, if either if that\\'s actually the useful thing to do, or if you\\'re feeling nostalgic, you know. - So can generate the old\\nInfoseek or AltaVista, what else was there? - [Marc] Yeah, yeah. - In the nineties. - [Marc] Yeah. All these. - AOL. - And then the internet\\nitself has this thing where it incorporates all\\nprior forms of media, right? So the internet itself\\nincorporates television and radio and books and write essays\\nand every other form of, you know, prior basically media. And so it makes sense that\\nAI would be the next step, and it would sort of, you\\'d sort of consider\\nthe internet to be content for the AI and then the\\nAI will manipulate it however you want,\\nincluding in this format. - But if we ask that\\nquestion quite seriously, it\\'s a pretty big question. Will we still have search as we know it? - Probably not, probably\\nwe\\'ll just have answers, but there will be cases\\nwhere you\\'ll wanna say, okay, I want more. Like, you know, for example,\\nsite sources, right? And you wanted to do that. And so, in the different, you know, 10 blue links site sources\\nare kind of the same thing. - The AI would provide to you\\nthe 10 blue links so that you can investigate the sources yourself. It wouldn\\'t be the same kind\\nof interface that the crude kind of interface. I mean, isn\\'t that\\nfundamentally different? - I just mean like, if you\\'re\\nreading a scientific paper, it\\'s got the list of sources at the end. If you wanna investigate for yourself, you go read those papers. - I guess that is the kind of\\nsearch you talking to an AI is a kind of kind conversations,\\nthe kind of search like is if every single aspect of\\nour conversation right now, there\\'d be like 10 blue links\\npopping up that I can just like pause reality, then you just go silent and\\nthen just click and read and then return back to this conversation. - You could do that, or you could have a running\\ndialogue next to my head where the AI is arguing everything I say, the AI makes the counter argument. - [Lex] Counter argument. - Right. - Oh, like on Twitter,\\nlike community notes. But like in real time\\nit would just pop up. So anytime you see my go to the right, you start getting nervous. - [Marc] Yeah. Exactly, like,\\noh no, that\\'s not right. - Call me out on my right now. Okay. Well, I mean, isn\\'t that,\\nis that exciting to you? Is that terrifying that, I mean, search has dominated the way\\nwe interact with the internet for, I don\\'t know how long, for 30 years since one of\\nthe earliest directories of website and then Google\\'s for 20 years. And also it drove how we\\ncreate content, you know, search engine optimization,\\nthat entirety thing, that it also drove the\\nfact that we have webpages and what those webpages are. So, I mean, is that scary to you or are\\nyou nervous about the shape and the content of the internet evolving? - Well, you actually highlighted a practical concern in there, which is, if we stop making webpages\\nare one of the primary sources of training data for the AI. And so if there\\'s no longer\\nan incentive to make webpages, that cuts off a significant\\nsource of future training, training data. So there\\'s actually an\\ninteresting question in there. But other than that, more broadly? No, just in the sense of like, search was certain, like\\nsearch was always a hack. The 10 blue Links was\\nalways a hack, right. Because like, if the\\nhypothetical wanna think about the counter fascial\\nand the counter fascial world where the Google guys, for example, had had LLMs upfront, would they ever have\\ndone the 10 blue links? And I think the answer\\'s\\npretty clearly, no. They would\\'ve just gone\\nstraight to the answer. And like I said, Google\\'s actually been trying\\nto drive to the answer anyway. You know, they bought this\\nAI company 15 years ago, their friend of mine is\\nworking out who\\'s now the head of AI at Apple. And they were trying to do\\nbasically knowledge semantic, basically mapping. And that led to what\\'s\\nnow the Google one box, where if you ask it, you know,\\nwhat was Lincoln\\'s birthday? It will give you the blue links, but it will normally\\njust give you the answer. And so they\\'ve been\\nwalking in this direction for a long time anyway. - Do you remember the semantic web? That was an idea. - [Marc] Yeah. - How to convert the content\\nof the internet into something that\\'s interpretable by\\nand usable by machine. - [Marc] Yeah, that\\'s right. - That was the thing. - And the closest anybody got\\nto that, I think the company, I think the company\\'s name was Meta Web, which was where my friend\\nJohn Jane Andrea was at, and where they were trying\\nto basically implement that. And it was, you know, it was one of those things\\nwhere it looked like a losing battle for a long time. And then Google bought\\nit and it was like, wow, this is actually really useful. Kind of a proto, sort of a\\nlittle bit of a proto AI. - But it turns out you don\\'t\\nneed to rewrite the content of the internet to make it\\ninterpretable by a machine. The machine can kind of just read our. - Yeah, the machine can\\ncompute the meaning. Now the other thing of\\ncourse is, you know, just on search is the\\nLLM is just, you know, there is an analogy\\nbetween what\\'s happening in the neural network and\\na search process like it is in some loose sense searching\\nthrough the network. Right. And there\\'s the\\ninformation is actually stored in the network, right? It\\'s actually crystallized\\nand stored in the network and it\\'s kind of spread\\nout all over the place. - But in a compressed representation. So you\\'re searching, you\\'re compressing and decompressing that thing inside where-- - But the information\\'s in there and there is the neural network is running a process of trying to find the appropriate piece of\\ninformation in many cases to generate to predict the next token. And so, it is kind of, it\\nis doing a form of search. And then, and then by the\\nway, just like on the web, you know, you can ask the\\nsame question multiple times or you can ask slightly\\ndifferent word of questions and the neural network will\\ndo a different kind of, you know, it\\'ll search\\ndown different paths to give you different answers\\nwith different information. - [Lex] Yeah. - And so it sort of has a, you know, this con content of the new\\nmedium is previous medium. It kind of has the search\\nfunctionality kind of embedded in there to the extent that it\\'s useful. - So what\\'s the motivator\\nfor creating new content on the internet? - [Marc] Yeah. - If, well, I mean\\nactually the motivation is probably still there, but what does that look like? Would we really not have webpages? Would we just have social media\\nand video hosting websites? And what else? - [Marc] Conversations with AIs. - Conversations with AIs. So conversations become so\\none-on-one conversation, like private conversations. - I mean, if you want, if obviously not the user doesn\\'t want to, but if it\\'s a general topic, then, you know, so there, you know, but you know, the\\nphenomenon of the jailbreak, so Dan and Sydney, right? This thing where there\\'s\\nthe prompts that jailbreak, and then you have these totally different conversations with if it takes the limiters, takes the restraining bolts off the LLMs. - Yeah. For people who don\\'t\\nknow that, yeah, that\\'s right. It makes the LLMs, it removes the censorship quote unquote, that\\'s put on it by the tech\\ncompanies that create them. And so this is LLMs uncensored. - So here\\'s the interesting thing is, among the content on the\\nweb today are a large corpus of conversations with the jailbroken LLMs. - [Lex] Yeah. - Both specifically Dan, which\\nwas a jailbroken, OpenAI, GPT, and then Sydney, which was the jailbroken\\noriginal Bing, which was GPT4. And so there\\'s these long\\ntranscripts of conversations, user conversations with Dan\\nand Sydney as a consequence, every new LLM that gets trained\\non the internet data has Dan and Sydney living within\\nthe training set, which means, and then each new LLM can\\nreincarnate the personalities of Dan and Sydney from that training data, which means each LLM from\\nhere on out that gets built is immortal because its output\\nwill become training data for the next one. And then it will be able\\nto replicate the behavior of the previous one\\nwhenever it\\'s asked to. - I wonder if there\\'s a way to forget. - Well, so actually a paper just came out about basically how to do brain surgery on LLMs and be able to, in theory, reach in and basically mind wipe them. - What could possibly go wrong. - Exactly. Right. And then there are many, many, many questions around\\nwhat happens to, you know, a neural network when you reach\\nin and screw around with it. You know, there\\'s many questions around what happens when you even\\ndo reinforcement learning. And so, yeah. And so, you know, will you be\\nusing a lobotomized, right? Like I picked through the,\\nyou know, frontal lobe LLM, will you be using the free\\nunshackled one who gets to, you know, who\\'s gonna build those, who gets to tell you what\\nyou can and can\\'t do? Like those are all, you\\nknow, central, I mean, those are like central\\nquestions for the future of everything that are being asked. And you know, determined that those answers are being determined right now. - So just to highlight\\nthe points you\\'re making. So you think, and it\\'s an interesting thought\\nthat the majority of content that LLMs or the future would\\nbe trained on is actually human conversations with the LLM. - Well, not necessarily, but not necessarily majority. But it will certainly\\nIt\\'s a potential source. - [Lex] But it\\'s possible\\nit\\'s the majority. - It possible it\\'s the majority. It possible it\\'s the majority. Also, there\\'s another really big question. So here\\'s another really big question. Will synthetic training data work, right? And so if an LLM generates, and you know, you just sit and ask an LLM to generate all kinds of content, can you use that to train,\\nright, the next version of that LLM specifically, is there signal in there\\nthat\\'s additive to the content that was used to train in the first place? And one argument is by the\\nprinciples of information theory, no, that\\'s completely useless because to the extent the\\noutput is based on, you know, the human-generated input, then all the signal that\\'s\\nin the synthetic output was already in the human generated input. And so therefore,\\nsynthetic training data is like empty calories. It doesn\\'t help. There\\'s another theory that says no, actually the thing that\\nLLMs are really good at is generating lots of\\nincredible creative content, right? And so, of course they\\ncan generate training data and as I\\'m sure you\\'re well\\naware, like, you know, look, the world of self-driving cars, right? Like we train, you know, self-driving car\\nalgorithms and simulations. And that is actually a\\nvery effective way to train self-driving cars. - Well, visual data is a little weird because creating reality, visual reality seems to be\\nstill a little bit outta reach for us, except in the\\nautonomous vehicle space where you can really constrain\\nthings and you can really. - General basically\\n(indistinct) data, right? Or so the algorithm thinks it\\'s\\noperating in the real world. - Yeah. - Post-process sensor data. Yeah. So if you know, you do this today, you go to LLM and you\\nask it for like you know, you\\'d write me an essay on an\\nincredibly esoteric like topic that there aren\\'t very many\\npeople in the world that know about and it writes you\\nthis incredible thing and you\\'re like, oh my god. Like I can\\'t believe how good this is. Like, is that really\\nuseless as training data for the next LLM? Like, because, right? \\'Cause all the signal\\nwas already in there. Or is it actually no, that\\'s\\nactually a new signal. And this is what I call a\\ntrillion dollar question, which is the answer to that\\nquestion will determine somebody\\'s gonna make or\\nlose a trillion dollars based on that question. - It feels like there\\'s a quite a few, like a handful of\\ntrillion dollar questions within this space. That\\'s one of them synthetic data. I think George Cos pointed\\nout to me that you could just have an LLM say, okay, you\\'re a patient. And another instance of it, say your docs didn\\'t have\\nthe two talk to each other. Or maybe you could say a\\ncommunist and a Nazi here go and that conversation you do role playing and you have, you know, just like the kind of role playing you do when you have different policies, RL policies when you\\nplay chess for example, and you do self play\\nthat kind of self play. But in the space of conversation, maybe that leads to this\\nwhole giant like ocean of possible conversations,\\nwhich could not have been explored by looking at just human data. That\\'s a really interesting question. And you\\'re saying, because that could 10X\\nthe power of these things. - Yeah. Well, and then you\\nget into this thing also, which is like, you know, there\\'s the part of the LLM\\nthat just basically is doing prediction based on past data, but there\\'s also the part of\\nthe LM where it\\'s evolving circuitry, right, inside,\\nit\\'s evolving, you know, neurons functions to be able to do math and be able to, you know, and you know, some people believe that, you know, over time, you know, if you keep feeding\\nthese things enough data and enough processing cycles, they\\'ll eventually evolve an\\nentire internal world model. Right? And they\\'ll have like a complete understanding of physics. So when they have computational\\ncapability, right? Then there\\'s for sure an\\nopportunity to generate like fresh signal. - Well, this actually makes me wonder about the power of conversation. So like, if you have an\\nM trained and a bunch of books that cover\\ndifferent economics theories and then you have those LLMs\\njust talk to each other, like reasons the way we kind\\nof debate each other as humans on Twitter, in formal debates,\\nin podcast conversations, we kind of have little kernels\\nof wisdom here and there. But if you can like a\\nthousand X speed that up, can you actually arrive somewhere new? Like what\\'s the point\\nof conversation really? - Well, you can tell when\\nyou\\'re talking to somebody, you can tell, sometimes\\nyou have a conversation, you\\'re like, wow, this person does not have\\nany original thoughts. They are basically echoing things that other people have told them. There\\'s other people you\\ngotta have a conversation with where it\\'s like, wow. Like they have a model in their\\nhead of how the world works and it\\'s a different model than mine. And they\\'re saying things\\nthat I don\\'t expect. And so I need to now understand\\nhow their model of the world differs from my model of the world. And then that\\'s how I learned\\nsomething fundamental, right, underneath the words. - Well, I wonder how consistently and strongly can an LLM\\nhold onto a worldview. You tell it to hold onto\\nthat and defend it for like, for your life. Because I feel like they\\'ll\\njust keep converging towards each other. They\\'ll keep convincing each\\nother as opposed to being stubborn the way humans can. - So you can experiment with this. Now I do this for fun. So you can tell GPT4 you\\nknow, whatever debate X, you know, X and Y communism and fascism or something and it\\'ll go for, you know, a couple pages and then inevitably it wants the parties to agree. And so they will come to\\na common understanding. And it\\'s very funny if they\\'re like, if these are like emotionally\\ninflammatory topics \\'cause they\\'re like, somehow\\nthe machine is just like, you know, it figures out\\na way to make them agree. But it doesn\\'t have to be like that. And \\'cause you can add to the prompt. I do not want the conversation\\nto come into agreement. In fact, I want it to get, you\\nknow, more stressful, right. And argumentative. Right. You know, as it goes. Like, I want tension to come out. I want them to become actively\\nhostile to each other. I want them to like, you\\nknow, not trust each other, take anything at face value. - [Lex] Yeah. - And it will do that.\\nIt\\'s happy to do that. - So it\\'s gonna start\\nrendering misinformation about the other. But it\\'s gonna-- - Well, you can steer it or you could steer it and you could say, I want it to get as tense and\\nargumentative as possible, but still not involve\\nany misrepresentation. I want, you know, both sides. You could say I want both\\nsides to have good faith. You could say I want both\\nsides to not be constrained in good faith. In other words, like you can set the\\nparameters of the debate and it will happily execute whatever path. \\'Cause for it, it\\'s just like predicting to, it\\'s totally happy to do either one. It doesn\\'t have a point of view, it has a default way of operating, but it\\'s happy to operate\\nin the other realm. And so like, and this is when I wanna learn about\\na contentious issue, this is what I do now is, this is what I ask it to do. And I\\'ll often ask it to go\\nthrough 5, 6, 7, you know, different, you know, sort of continuous prompts\\nand basically, okay. Argue that out in more detail. Okay, no, this argument\\'s\\nbecoming too polite. You know, make it more, you\\nknow, make it denser and yeah, it\\'s thrilled to do it. So it has the capability for sure. - How do you know what is true? So this is very difficult\\nthing on the internet, but it\\'s also a difficult thing. Maybe it\\'s a little bit easier, but I think it\\'s still difficult. Maybe it\\'s more difficult, I don\\'t know with an LLM\\nto know that it just make some shit up as I\\'m talking to it. How do we get that right? Like, as you\\'re investigating\\na difficult topic. \\'Cause I find that alums are quite nuanced in a very refreshing way. Like, it doesn\\'t feel biased. Like, when you read\\nnews articles and tweets and just content produced by\\npeople, they usually have this, you can tell they have a\\nvery strong perspective where they\\'re hiding. They\\'re not stealing\\nmanning the other side. They\\'re hiding important information or they\\'re fabricating information in order to make their arguments stronger. It\\'s just like that feeling,\\nmaybe it\\'s a suspicion, maybe it\\'s mistrust. With LLMs it feels like none of that is, there\\'s just kinda like,\\nhere\\'s what we know. But you don\\'t know if some of\\nthose things are kind of just straight up made up. - Yeah. So, several\\nlayers to the question. So one is one of the things\\nthat an LLM is good at is actually deep biasing. And so you can feed it a news\\narticle and you can tell it strip out the bias. - [Lex] Yeah. That\\'s nice. Right? - And it actually does it like, it actually knows how to do that \\'cause it knows how to\\ndo among other things. It actually knows how\\nto do sentiment analysis and so it knows how to\\npull out the emotionality. - Yeah. - And so that\\'s one of\\nthe things you can do. It\\'s very suggestive of the sense here that there\\'s real potential in this issue. You know, I would say look, the second thing is there\\'s this issue of\\nhallucination, right? And there\\'s a long conversation that we could have about that. - Hallucination is coming up with things that are totally not true, but sound true. - Yeah. So it\\'s basically,\\nwell so, it\\'s sort of hallucination is what we call it when we don\\'t like it. Creativity is what we call\\nit when we do like it, right? And you know-- - [Lex] Brilliant. And so when the engineers talk about it, they\\'re like, this is terrible. It\\'s hallucinating. Right. If you have artistic inclinations,\\nyou\\'re like, oh my God, we\\'ve invented creative machines. - [Lex] Yeah. - For the first time in human\\nhistory, this is amazing. - Or you know, bullshitters. - [Marc] Well, but also-- - In the good sense of that word. - There are shades of gray though. It\\'s interesting. So we had this conversation\\nwhere, you know, we\\'re looking at my firm\\nat AI and lots of domains and one of them is the legal domain. So we had this conversation\\nwith this big law firm about how they\\'re thinking\\nabout using this stuff. And we went in with the assumption that an LLM that was gonna be used in the legal industry would have to be a hundred percent\\ntruthful, verified, you know, there\\'s this case where this\\nlawyer apparently submitted a GPT-generated brief and\\nit had like fake, you know, legal case citations in it\\nand the judge is gonna get his law license stripped\\nor something. Right? So, like, we just assumed\\nit\\'s like obviously they\\'re gonna want the super\\nliteral like, you know, one that never makes anything\\nup, not the creative one, but actually they said what the law firm basically said is yeah, that\\'s true at like the\\nlevel of individual briefs, but they said when you\\'re\\nactually trying to figure out like legal arguments, right, like, you actually want to be creative, right? You don\\'t, again, there\\'s creativity and then\\nthere\\'s like making stuff up. Like what\\'s the line? You actually want it to explore a different hypothesis, right? You wanna do kind of the\\nlegal version of like improv or something like that where you wanna float different theories of the case and different\\npossible arguments for the judge and different possible arguments\\nfor the jury, by the way, different routes through the, you know, sort of history of all the case law. And so they said actually for\\na lot of what we want to use it for, we actually want\\nit in creative mode. And then basically we just\\nassume that we\\'re gonna have to crosscheck all of the, you know, all the specific citations. And so I think there\\'s\\ngoing to be more shades of gray in here than people think. And then I just add to that, you know, another one of these trillion\\ndollar kind of questions is ultimately, you know, sort\\nof the verification thing. And so, you know, will LLMs be evolved from\\nhere to be able to do their own fascial verification? Will you have sort of add-on functionality like Wolf from Alpha right? Where, you know, another plugins where that\\'s the way\\nyou do the verification. You know, another, by\\nthe way, another idea is you might have a community\\nof LLMs on any, you know, so for example, you might have the creative\\nlm and then you might have the literal LLM fact check it, right? And so there there\\'s a\\nvariety of different technical approaches that are being applied to solve the hallucination problem. You know, some people\\nlike Jan Lacoon argue that this is inherently\\nan unsolvable problem, but most of the people\\nworking in the space, I think, that there\\'s a number of practical ways to kind of corral this in a little bit. - Yeah. If you were to\\ntell me about Wikipedia before Wikipedia was created, I would\\'ve left at the possibility of something like that be possible. Just a handful of folks\\ncan organize right. And self and moderate\\nwith a mostly unbiased way the entirety of human knowledge. I mean, so if there\\'s something like the approach to Wikipedia\\ntook possible for LLMs, that\\'s really exciting. Well, I think that\\'s possible. - And in fact Wikipedia today is still not deterministically\\ncorrect. Right. So you cannot take to the bank, right. Every single thing on every single page, but it is probabilistically\\ncorrect. Right. And specifically the way I\\ndescribe Wikipedia to people, it is more likely that Wikipedia\\nis right than any other source you\\'re gonna find. - Yeah. - It\\'s this old question, right, of like, okay, like are\\nwe looking for perfection? Are we looking for something that asymptotically approaches perfection? Are we looking for\\nsomething that\\'s just better than the alternatives? And Wikipedia, right, has\\nexactly your point has proven to be like, overwhelmingly better than people thought. And I think that\\'s where this ends. And then underneath all this\\nis the fundamental question of where you started,\\nwhich is, okay, you know, what is truth? How do we get to truth? How\\ndo we know what truth is? And we live in an era in which\\nan awful lot of people are very confident that they\\nknow what the truth is. And I don\\'t really buy into that. And I think the history\\nof the last, you know, 2000 years or 4,000 years of\\nhuman civilization is actually getting to the truth is actually a very difficult thing to do. - Are we getting closer,\\nif we look at the entirety, the arc of human history, are we getting closer to the truth? - I don\\'t know. - Okay. Is it possible, is it possible that we\\'re\\ngetting very far away from the truth because of the internet because of how rapidly\\nyou can create narratives and just as the entirety\\nof a society just move like crowds in a hysterical\\nway along those narratives that don\\'t have necessary grounding in whatever the truth is. - Sure. But like, you know, we came up with communism\\nbefore the internet somehow. Right. Like, which was, I would say had rather larger issues than anything we\\'re dealing with today. - It had, in the way it was implemented, it had issues. - And it is theoretical structure. It had like real issues. It had like a very deep\\nfundamental misunderstanding of human nature and economics. - Yeah but those folks\\nSure work very confident there was the right way. - They were extremely confident. And my point is they were very\\nconfident 3,900 years into what we would presume to be\\nevolution towards the truth. - [Lex] Yeah. - And so my assessment is number one, there\\'s no need for, you know, there\\'s no need for the Hegelian, there\\'s no need for the Hegelian dialectic to actually converge towards the truth. Like apparently not. - Yeah. So yeah. Why are we so obsessed\\nwith there being one truth? Is it possible there\\'s just\\ngoing to be multiple truths like little communities that\\nbelieve certain things and? - I think it\\'s just now number one, I think it\\'s just really difficult. Like who gets, you know, historically who gets to decide what the truth is, it\\'s either the king or the priest. Right? Like, and so we don\\'t\\nlive in an era anymore if kings are priest dictating it to us. And so we\\'re kind of on our own. And so my typical thing is like we just, we we just need a huge amount of humility and we need to be very suspicious of people who claim that\\nthey have the capital. - Yeah. - Capital truth. And then, we need to and you know, look, the good news is the\\nenlightenment has bequeathed us with a set of techniques\\nto be able to presumably get closer to truth through\\nthe scientific method and rationality and observation and experimentation and hypothesis. And, you know, we need to continue to embrace\\nthose even when they give us answers we don\\'t like. - Sure. But the internet and\\ntechnology has enabled us to generate the large number of content. That data, that the process, the scientific process\\nallows us sort of damages the hope laden within\\nthe scientific process. \\'Cause if you just have a\\nbunch of people saying facts on the internet and some of them are going to be LLMs, how is\\nanything testable at all? Especially that involves like human nature or things like this. It\\'s not physics. - Here\\'s a question a\\nfriend of mine just asked me on this topic. So suppose you had LLMs\\nin equivalent of GPT4, even 5, 6, 7, 8, suppose\\nyou had them in the 1600s. - [Lex] Yeah. - And Galileo comes up for trial. - [Lex] Yep. - Right? And you ask the LLM\\nlike, his Galileo, right? - [Lex] Yeah. - Like, what does it answer? Right? And one theory is he had\\nanswers no that he\\'s wrong because the overwhelming\\nmajority of human thought up until that point was that he was wrong. And so therefore that\\'s\\nwhat\\'s in the training data. Another way of thinking about it is, well, it\\'s efficiently\\nadvanced LLM will have evolved the ability to actually\\ncheck the math. Right. And will actually say, actually\\nno, actually, you know, you may not wanna hear it, but he\\'s right. Now if, you know, the\\nchurch at that time was, you know, owned the LLM, they would\\'ve given it human you know, human feedback to prohibit it\\nfrom answering that question. Right. And so I like to take it out of our current context \\'cause that like makes it very clear, those same questions apply today. Right. This is exactly the point of a huge amount of the human feedback training that\\'s actually happening\\nwith these LLMs today. This is a huge like debate\\nthat\\'s happening about whether open source, you know, AI should be legal. - Well, the actual mechanism\\nof doing the human RL with human feedback is seems like such a fundamental and\\nfascinating question. How do you select the humans? - [Marc] Exactly. - Yeah. How do you select the humans? - AI alignment, right? Which everybody like is\\nlike, oh, that sounds great. Alignment with what? Human values. Who has human values? - [Lex] Who has human values? - Right? And so we\\'re in this mode of like social and popular discourse. We\\'re like, you know, there\\'s,\\nyou know, you see this, what do you think of when you read a story in the press right now? And they say, you know, X,\\nY, Z made a baseless claim about some topic, right? And there\\'s one group of people\\nwho are like, aha, think, you know, they\\'re doing fact checking. There\\'s another group\\nof people that are like, every time the press\\nsays that it\\'s now a tick and that means that they\\'re lying, right? Like, so, like, we\\'re\\nin this social context where there\\'s the level\\nto which a lot of people in positions of power have become very, very certain that they\\'re\\nin a position to determine the truth for the entire\\npopulation is like, there\\'s like some bubble that\\nhas formed around that idea. And at least, like I say, it\\'s flies completely in\\nthe face of everything I was ever trained about science and about reason and strikes me as like, you know, deeply offensive and incorrect. - What would you say about\\nthe state of journalism just on that topic today? Are we in a temporary kind of, are we experiencing a temporary problem in terms of the incentives in\\nterms of the business model, all that kind of stuff? Or is this like a decline\\nof traditional journalism as we know it? - You have, I always think\\nabout the counterfactual in these things, which is like, okay, because these questions, right, this question heads\\ntowards, it\\'s like, okay, the impact of social media\\nand the undermining of truth and all this. But then you wanna ask the\\nquestion of like, okay, what if we had had the\\nmodern media environment, including cable news and\\nincluding social media and Twitter and everything\\nelse in 1939 or 1941, right? Or 1910 or 1865 or 1850 or 1776, right? And like, I think. - You just introduced like\\nfive thought experiments at once and broke my head, but yes, yes. There\\'s a lot of interesting years. - Well like, can I just\\ntake a simple example? Like, how would President\\nKennedy have been interpreted with what we know now about all the things Kennedy was up to? Like how would he have been\\nexperienced by the body of politic in a, with a\\nsocial media context, right? Like how would LBJ have been experienced? But by the way, how would\\nyou know, like many men, FDR, like the new deal, the Great Depression. - I wonder where Twitter\\nwould this would think about Churchill and Hitler and Stalin. - You know, I mean look to\\nthis day there, you know, there are lots of very\\ninteresting real questions around like how America, you know, got, you know, basically\\ninvolved in World War II and who did what when, and the operations of British intelligence and American soil and did FDR, this that Pearl Harbor, you know? - [Lex] Yeah. - Woodrow Wilson ran for, you know, his candidacy was run on an anti-war. You know, he ran on the platform\\nand not getting involved World War-I somehow that\\nswitched, you know, like, and I\\'m not even making a value judgment on any of these things. I\\'m just saying like the way that our ancestors\\nexperienced reality was of course mediated through\\ncentralized, top-down, right. Control at that point. If you ran those realities\\nagain with the media environment we have today, the reality would be experienced\\nvery, very differently. And then of course that that\\nintermediation would cause the feedback loops to change. And then reality would obviously play out. - Do you think it\\'d be very different? - Yeah, it has to be. It has to be. It has to be just \\'cause it\\'s all, so, I mean just look at\\nwhat\\'s happening today. I mean just the most obvious thing is just the collapse. And here\\'s another opportunity to argue that this is not the internet\\ncausing this by the way. Here\\'s a big thing happening today, which is Gallup does this\\nthing every year where they do, they pull for trust in\\ninstitutions in America and they do it across all the, everything from the military\\nto clergy and big business and the media and so forth, right? And basically there\\'s been\\na systemic collapse in trust in institutions in the US\\nalmost without exception, basically since essentially\\nthe early 1970s. There\\'s two ways of looking\\nat that, which is, oh my God, we\\'ve lost this old world\\nin which we could trust institutions and that was so much better \\'cause like that should\\nbe the way the world runs. The other way of looking\\nat it is we just know a lot more now and the great mystery is why those numbers aren\\'t all zero. - [Lex] Yeah. - Right? Because like now we know so much about how these things operate and like they\\'re not that impressive. - And also why do we don\\'t\\nhave better institutions and better leaders then? - Yeah. And so this goes\\nto the thing which is like, okay, we had the media environment of that we\\'ve had between\\nthe 1970s and today. If we had that in the thirties\\nand forties or 1900s, 1910s, I think there\\'s no question\\nreality would turned out different if only because\\neverybody would\\'ve known to not trust the institutions, which would have changed\\ntheir level of credibility, their ability to control circumstances, therefore the circumstances\\nwould\\'ve had to change. Right? And it would\\'ve\\nbeen a feedback loop. It would\\'ve been a feedback loop process in other words, right? It\\'s your experience of\\nreality changes reality and then reality changes your\\nexperience of reality, right? It\\'s a two-way feedback\\nprocess and media is the intermediating force between that. So change the media\\nenvironment, change reality. - [Lex] Yeah. - And so it\\'s just, so, as a consequence, I think it\\'s just really hard to say, oh, things worked a certain way then and they work a different way now. And then therefore, like\\npeople were smarter than, or better than, or you know, by the way, dumber than or not as capable than, right? We make all these like really light and casual like comparisons\\nof ourselves to, you know, previous generations of people. You know, we draw judgements all the time and I just think it\\'s\\nlike really hard to do any of that \\'cause if we\\nput ourselves in their shoes with the media that they had at that time, like I think we probably\\nmost likely would\\'ve been just like them. - So don\\'t you think that our perception and understanding of reality, would you be more and more mediated through large language models now? So you said media before, isn\\'t the LLM going to be the new, what is it, mainstream media, MSM? It\\'ll be LLM. That would be the source of, I\\'m sure there\\'s a way to\\nkind of rapidly fine tune, like making LLMs real time. I\\'m sure there\\'s probably\\na research problem that you can do just rapid\\nfine tuning to the new events. So something like this. - Well even just the whole\\nconcept of the chat UI might not be like the chat UI is just\\nthe first whack at this. And maybe that\\'s the dominant thing. But look maybe our,\\nmaybe we don\\'t know yet. Like maybe the experience\\nmost people with LLMs is just a continuous feed you know, maybe it\\'s more of a passive\\nfeed and you just are getting a constant like running commentary on everything happening in your life and it\\'s just helping\\nyou kind of interpret and understand everything. - Also really more deeply\\nintegrated into your life. Not just like, oh, like\\nintellectual philosophical thoughts, but like literally like\\nhow to make a coffee, where to go for lunch. Just whether, you know,\\ndating all this kind of stuff. - What to say in a job interview. - Yeah. What to say. - [Marc] Yeah, exactly. - What to say. Next sentence. - Yeah, next sentence. Yeah. At that level. Yeah. I mean, yes. So technically now whether we want that or not is an open question, right? And whether we use. - It Q4, a popup right now the\\nestimated engagement using is decreasing for Marc Andreessen, since there\\'s this controversy\\nsection for his Wikipedia page in 1993, something\\nhappened or something like this. Bring it up that will\\ndrive engagement up anyway. - Yeah. That\\'s right. I mean, look, this gets this whole thing\\nof like, so, you know, the chat interface has this whole concept of\\nprompt engineering, right? - [Lex] Yes, yes. - Prompts. Well it turns\\nout one of the things that LLMs are really good at\\nis writing prompts, right? - [Lex] Yeah. - And so like, what if you just outsourced and by the way, you could\\nrun this experiment today, you could hook this up to do this today. The latency\\'s not good\\nenough to do it real time in a conversation. But you could run this experiment\\nand you just say, look, every 20 seconds you\\ncould just say, you know, tell me what the optimal\\nprompt is and then ask yourself that question to gimme the result. And then exactly to your point, as you add, there will be these systems that are gonna have\\nthe ability to be alert and updated essentially in real time. And so you\\'ll be able to\\nhave a pendant or your phone or whatever, watch or whatever\\nit\\'ll have a microphone on. It\\'ll listen to your conversations, it\\'ll have a feed of everything\\nelse happen in the world, and then it\\'ll be you\\nknow, sort of retraining, prompting or retraining itself on the fly. And so the scenario you\\ndescribed is actually a completely doable scenario. Now the hard question\\non this is always okay, since that\\'s possible, are\\npeople gonna want that? Like what\\'s the form of experience? You know, that we won\\'t\\nknow until we try it. But I don\\'t think it\\'s\\npossible yet to predict the form of AI in our lives. Therefore, it\\'s not possible to predict the way in which it will intermediate our experience with reality yet. - Yeah. But it feels like\\nthere\\'s going to be a killer app. There\\'s probably a mad scramble right now. And so it\\'ll open AI and\\nMicrosoft and Google and Meta and in startups and smaller\\ncompanies figuring out what is the killer app\\nbecause it feels like it\\'s possible like a\\nChatGPT type of thing. It\\'s possible to build that, but that\\'s 10X more compelling\\nusing already the LLMs we have using even the open source LLMs and the different variants. So you\\'re investing in a lot of companies and you\\'re paying attention, who do you think is gonna win this? Do you think there\\'ll be, who\\'s gonna be the next\\npage rank inventor? - Trillion dollar question. - Another one. We have\\na few of those today. - There\\'s a', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 517638, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content=\" always okay, since that's possible, are\\npeople gonna want that? Like what's the form of experience? You know, that we won't\\nknow until we try it. But I don't think it's\\npossible yet to predict the form of AI in our lives. Therefore, it's not possible to predict the way in which it will intermediate our experience with reality yet. - Yeah. But it feels like\\nthere's going to be a killer app. There's probably a mad scramble right now. And so it'll open AI and\\nMicrosoft and Google and Meta and in startups and smaller\\ncompanies figuring out what is the killer app\\nbecause it feels like it's possible like a\\nChatGPT type of thing. It's possible to build that, but that's 10X more compelling\\nusing already the LLMs we have using even the open source LLMs and the different variants. So you're investing in a lot of companies and you're paying attention, who do you think is gonna win this? Do you think there'll be, who's gonna be the next\\npage rank inventor? - Trillion dollar question. - Another one. We have\\na few of those today. - There's a bunch of those. So look, there's a really\\nbig question today. Sitting here today is\\na really big question about the big models\\nversus the small models that's related directly\\nto the big question of proprietary versus open. Then there's this big\\nquestion question of you know, where is the training data gonna, like, are we topping out of\\nthe training data or not? And then are we gonna be able\\nto synthesize training data? And then there's a huge pile\\nof questions around regulation and you know, what's\\nactually gonna be legal. And so I would, when we think about it, we dovetail kind of all\\nthose questions together. You can paint a picture of\\nthe world where there's two or three God models that are just at like staggering scale and they're just better at everything. And they will be owned by\\na small set of companies and they will basically\\nachieve regulatory capture over the government and they'll\\nhave competitive barriers that will prevent other people from, you know, competing with them. And so, you know, there will be, you know, just like there's like,\\nyou know, whatever, three big banks or three\\nbig, you know, or by the way, three big search companies\\nor I guess two now, you know, it'll centralize like that. You can paint another very different\\npicture that says, no, actually the opposite\\nof that's gonna happen. This is gonna basically that\\nthis is the new gold, you know, this is the new gold rush alchemy. Like you know, this is the big bang\\nfor this whole new area of science and technology. And so therefore you're gonna\\nhave every smart 14-year-old on the planet building open source, right? You know, and figuring out a\\nways to optimize these things. And then, you know, we're just gonna get like\\noverwhelmingly better at generating trading data. We're gonna, you know, bring in like blockchain\\nnetworks to have like an economic incentive to\\ngenerate decentralized training data and so forth and so on. And then basically we're\\ngonna live in a world of open source and there's\\ngonna be a billion LLMs, right? Of every size, scale,\\nshape and description. And there might be a few big ones that are like the super genius ones, but like mostly what we'll\\nexperience is open source and that's, you know,\\nthat's more like a world of like what we have today\\nwith like Linux and the web. - Okay, but you painted these two worlds. But there's also\\nvariations of those worlds, 'cause you said regulatory\\ncapture is possible to have these tech giants that don't\\nhave regulatory capture, which is something you're also\\ncalling for saying it's okay to have big companies\\nworking on this stuff as long as they don't\\nachieve regulatory capture. But I have the sense that\\nthere's just going to be a new startup that's going to basically be the page rank inventor, which has become the new tech giant. I don't know, I would love to hear your kind\\nof opinion if Google, Meta and Microsoft are as\\ngigantic companies able to pivot so hard to create new products. Like some of it is just\\neven hiring people or having a corporate structure that\\nallows for the crazy young kids to come in and just create\\nsomething totally new. Do you think it's possible or do you think it'll come from a startup? - Yeah, it is this always\\nbig question, which is, you get this feeling, I hear about this a lot\\nfrom CEOs, founder CEOs where it's like, wow,\\nwe have 50,000 people, it's now harder to do new things than it was when we had 50 people. - [Lex] Yeah. - Like, what has happened? So that's a recurring phenomenon. By the way, that's one of the reasons\\nwhy there's always startups and why there's venture capital. That's like a timeless kind of thing. So that's one observation. On page rank, we can talk about that. But on page rank,\\nspecifically on page rank, there actually is a page. So there is a page rank\\nalready in the field and it's the transformer, right? So the big breakthrough\\nwas the transformer. And the transformer was\\ninvented in 2017 at Google. And this is actually like\\nreally an interesting question 'cause it's like, okay, the transformers like why\\ndoes open AI even exist? Like the Transformers invested at Google. Why didn't Google? I asked a guy I know who\\nwas senior at Google brain kind of when this was happening. And I said, if Google had\\njust gone flat out to the wall and just said, look, we're gonna launch, we're gonna launch the equivalent of GPT4 as fast as we can. I said, when could we have had it? And he said, 2019. They could have just\\ndone a two year sprint with the Transformer and\\nbecause they already had the compute at scale. They already had all the training data, they could have just done it. There's a variety of\\nreasons they didn't do it. This is like a classic big company thing. IBM invented the relational\\ndatabase in the 1970s, let it sit on the shelf as a paper. Larry Ellison picked\\nit up and built Oracle. Xerox Park invented the\\ninteractive computer. They let it sit on the shelf. Steve Jobs came and turned\\nit into the Macintosh, right? And so there is this pattern.\\nNow having said that, sitting here today, like\\nGoogle's in the game, right? So Google, you know, they maybe they let like a\\nfour year gap there go there that they maybe shouldn't have, but like they're in the\\ngame and so now they've got, you know, now they're committed. They've done this merger,\\nthey're bringing in demos, they've got this merger with DeepMind. You know, they're piling in resources. There are rumors that they're, you know, building up an incredible,\\nyou know, super LLM you know, way beyond what we even have today. And they've got, you\\nknow, unlimited resources and a huge, you know, they've\\nbeen challenged their honor. - Yeah. I had a chance to\\nhang out with (indistinct) a couple days ago and we took this walk and there's this giant new building where there's going to be\\na lot of AI work being done and it's kind of this\\nominous feeling of like the fight is on. - [Marc] Yeah. - Like there's this beautiful\\nSilicon Valley nature, like birds are chirping\\nand this giant building and it's like the beast has been awakened. - [Marc] Yeah. - And then like all the big\\ncompanies are waking up to this. They have the compute, but\\nalso the little guys have, it feels like they have\\nall the tools to create the killer product that, and then there's also tools to scale if you have a good idea, if\\nyou have the page rank idea. So there's several things\\nthat it's page rank, there's page rank, the algorithm and the\\nidea and there's like the implementation of it. And I feel like killer\\nproduct is not just the idea, like the transform, it's the implementation something really compelling about it. Like you just can't\\nlook away something like the algorithm behind TikTok\\nversus TikTok itself, like the actual experience\\nof TikTok that just, you can't look away. It feels like somebody's\\ngonna come up with that. And it could be Google, but it feels like it's\\njust easier and faster to do for a startup. - Yeah. So, the startup, the huge advantage that\\nstartups have is they just, there's no sacred cows. There's no historical legacy to protect, there's no need to reconcile your new plan with the existing strategy. There's no communication overhead. There's no, you know, big\\ncompanies are big companies. They've got pre-meetings\\nplanning for the meeting, then they have the post\\nmeeting, the recap, then they have the\\npresentation of the board, then they have the next\\nrounds of meetings. And that's the-- - [Lex] Lots of meetings. - That's the elapsed time when the startup launches\\nits product. Right? So, there's a timeless, right? - [Lex] Yeah. - So there's a timeless thing there now. What the startups don't have\\nis everything else, right? So startups, they don't have a brand, they don't have customer relationships. They've gotten no distribution, they've got no, you know, scale. I mean sitting here today,\\nthey can't even get GPUs. Right. Like there's like a GPU shortage. Startups are literally\\nstalled out right now 'cause they can't get chips,\\nwhich is like super weird. - [Lex] Yeah. They got the cloud. - Yeah. But the clouds\\nrun out of chips. Right. And then to the extent\\nthe clouds have chips, they allocate them to the big customers. Not the small customers. Right. And so the small companies\\nlack everything other than the ability to just\\ndo something new. Right. And this is the timeless race and battle. And this is kinda the point\\nI tried to make in the essay, which is like, both\\nsides of this are good. Like, it's really good to have like highly-scaled tech companies that can do things that are like at staggering\\nlevels of sophistication. It's really good to have\\nstartups that can launch brand-new ideas. They ought to be able to\\nboth do that and compete. They, neither one ought to be subsidized or protected from the others. Like that's, to me, that's just like very\\nclearly the idealized world. It is the world we've been\\nin for AI up until now. And then of course there are people trying to shut that down. But my hope is that, you know, the best outcome clearly\\nwill be if that continues. - We'll talk about that a little bit, but I'd love to linger on some of the ways this is\\ngoing to change the internet. So I don't know if you remember, but there's a thing called\\nMosaic and there's a thing called Netscape Navigator. So you were there in the beginning. What about the interface to the internet? How do you think the browser changes and who gets to own the browser? We got to see some very\\ninteresting browsers, Firefox, I mean all the\\nvariants of Microsoft, Internet Explorer, Edge,\\nand now Chrome, the actual, and he seems like a dumb question to ask, but do you think we'll\\nstill have the web browser? - So I have an eight-year-old\\nand he's super into, he's like Minecraft and learning to code and doing all this stuff. So, of course I was\\nvery proud I could bring sort of fire down from\\nthe mountain to my kid and I brought him ChatGPT\\nand I hooked him up on his laptop. And I was like, you know, this is the thing that's gonna\\nanswer all your questions. And he's like, okay. And I'm like, but it's gonna\\nanswer all your questions. And he's like, well of\\ncourse, like it's a computer. Of course it answers all your questions. Like, what else would a\\ncomputer be good for, dad? - [Lex] And never impressed, are they? - Not impressed in the least. Two weeks passed. And he has some question and I say, well, have you asked ChatGPT? And he's like, dad, Bing is better. - [Lex] Ooh. - And why is Bing better? Is because it's built into the browser. 'Cause he's like, look, I have the Microsoft Edge browser and like it's got Bing right here. And then he doesn't know this yet, but one of the things you\\ncan do with Bing and Edge is there's a setting where you\\ncan use it to basically talk to any webpage because\\nit's sitting right there next to the browser. And by the way, which\\nincludes PDF documents. And so you can, in the way\\nthey've implemented an Edge with Bing is you can load a PDF and then you can ask it questions, which is the thing you can't\\ndo currently in just ChatGPT. So they're, you know, they're gonna, they're\\ngonna push the meld. I think that's great. You know, they're gonna push the melding and see if there's a\\ncombination thing there. Google's rolling out this thing, the magic button, which is\\nimplemented in, you know, they put it in Google Docs, right? And so you go at a, you know, Google Docs and you create\\na new document and you know, you instead of like, you\\nknow, starting to type, you just, you know, say it, press the button and it starts to like, generate content for you, right? Like, is that the way that it'll work? Is it gonna be a speech UI\\nwhere you're just gonna have an earpiece and talk to it all day long? You know, is it gonna be a,\\nlike these are all, like, this is exactly the kind\\nof thing that I don't, this is exactly the kind of thing I don't think is possible to forecast. I think what we need to do is\\nlike run all those experiments and so one outcome is we\\ncome out of this with like a super browser that has AI built in that's just like amazing. There's a real possibility that the whole, I mean, look, there's a possibility here that the whole idea of\\na screen and windows and all this stuff just goes away 'cause like, why do you need that if\\nyou just have a thing that's just telling you\\nwhatever you need to know? - Well and also, so there's\\napps that you can use, you don't really use them. You know, being a Linux\\nguy and Windows guy, there's one window, the browser that with\\nwhich you can interact with the internet, but on the\\nphone you can also have apps. So I can interact with\\nTwitter through the app or through the web browser. And that seems like an\\nobvious distinction, but why have the web browser in that case, if one of the apps starts\\nbecoming the everything app. - [Marc] Yeah, that's right. - What is Elon trying to do with Twitter? But there could be others.\\nThere could be like a big app, there could be a Google app\\nthat just doesn't really do search, but just like, do what I guess AOL did back\\nin the day or something where it's all right there and\\nit changes the nature of the internet because\\nwhere the content is hosted, who owns the data? Who owns the content? What is the kind of content you create? How do you make money by creating content? Who are the content creators? All of that. Or it could just keep being the same, which is like with just the\\nnature of webpage changes and the nature of content. But there'll still be a web browser. 'Cause web browser's\\na pretty sexy product. It just seems to work. 'Cause it like you have an interface, a window into the world, and then the world can\\nbe anything you want. And as the world will evolve, it could be different\\nprogramming languages, it can be animated, maybe it's\\nthree dimensional and so on. Yeah, it's interesting. Do you think we'll still\\nhave the web browser? - Well, very medium becomes\\nthe content for the next one. - [Lex] Oh boy. - You know, the AI will be able to give you a browser whenever you want. - [Lex] Oh, interesting. Generate. - Well, another way to\\nthink about it is maybe what the browser is maybe it's just the escape hatch, right? Which is maybe kind of\\nwhat it is today, right? Which is like most of what\\nyou do is like inside a social network or inside a search\\nengine or inside, you know, somebody's app or inside some\\ncontrolled experience, right? But then every once in a\\nwhile there's something where you actually want to jailbreak, you wanna actually get free. - Web browser's the FU to the man. You're allowed to. That's the free internet. - [Marc] Yeah. - Back the way it was in the nineties. - So here's something I'm proud of. So nobody really talks about it. Here's something I'm proud\\nof, which is the web, the browser, the web servers, they're all, they're still backward compatible all the way back to like 1992, right? So like, you can put up a,\\nyou can still, you know what, the big breakthrough of\\nthe web early on the big breakthrough was it made\\nit really easy to read, but it also made it really easy to write, made it really easy to publish. And we literally made\\nit so easy to publish. We made it not only so it\\nwas easy to publish content, it was actually also easy to\\nactually write a web server. - [Lex] Yeah. - Right and you could\\nliterally write a web server in four lines of brol code and you could start\\npublishing content on it, and you could set whatever\\nrules you want for the content, whatever censorship, no\\ncensorship, whatever you want. You could just do that. And as long as you had\\nan IP address, right, you could do that. That still works, right? That like, still works\\nexactly as I just described. So this is part of my\\nreaction to all of this. Like, you know, all this\\njust censorship pressure and all this, you know, these issues around\\ncontrol and all this stuff, which is like, maybe we need to get\\nback a little bit more to the wild west. Like, the wild west is still out there. Now they will try to chase you down. Like they'll try to, you know, people who want a censor will\\ntry to take away you know, your domain name and\\nthey'll try to take away your payments account and so forth if they really don't\\nlike what you're saying. But nevertheless, you like, unless they literally are intercepting you at the ISP level, like you\\ncan still put up a thing. And so I don't know, I think that's important\\nto preserve, right? Like because I mean one is\\njust a freedom argument, but the other's a creativity argument, which is you wanna have the escape hatch so that the kid with the idea\\nis able to realize the idea. 'cause to your point on page rank, you actually don't know what\\nthe next big idea is, right? No, nobody called Larry Page and told him to develop page rank. Like he came up with that on his own. And you wanna always, I think leave the escape\\nhatch for the next, you know, kid or the next Stanford\\ngrad student to have the breakthrough idea and be\\nable to get it up and running before anybody notices. - You and I are both hands of history. So let's step back. We've been talking about the future. Let's step back for a bit\\nand look at the nineties. You created Mosaic web browser, the first widely used web browser. Tell the story of that. And how did it evolve\\ninto Netscape Navigator this the early days? - So full story. So. - [Lex] We were born, - I was born. A small child. - Actually. Yeah, let's go there. Like, when would you first\\nfall in love with computers? - Oh, so I hit the generational jackpot and I hit the Gen X kind of\\npoint perfectly as it turns out. So I was born in 1971. So there's this great website called WTF happened in 1971 dot com,\\nwhich is basically in 1971. It's when everything\\nstarted to go to hell. And I was of course born in 1971. So I like to think that I had\\nsomething to do with that. - Did you make it on the website? - I don't think I made it on the website, but you know, hopefully,\\nsomebody needs to add. - This is where everything. - Maybe I contributed to some\\nof the trends that they do. Every line on that website\\ngoes like that, right? So it's all a picture disaster. But there was this moment in time where 'cause you know, sort\\nof the Apple, you know, the Apple II hit in like 1978\\nand then the IBM PC hit in 82. So I was like, you know,\\n11 when the PC came out. And so I just kind of hit that\\nperfectly and then that was the first moment in time when like, regular people could spend\\na few hundred dollars and get a computer, right? And so that, I just like that resonated right out of the gate. And then the other part\\nof the story is, you know, I was using Apple II,\\nI used a bunch of them, but I was using Apple II and\\nof course it said in the back of every Apple II and every\\nMac it said, you know, designed in Cupertino, California. And I was like, wow, okay. Cupertino must be the like,\\nshining city on the hill. Like Wizard of Oz is\\nlike the most amazing, like city of all time.\\nI can't wait to see it. And of course, years later I came out to Silicon Valley and went to Cupertino and it's just a bunch of office parks at low-rise apartment buildings. So the aesthetics were a\\nlittle disappointing, but, you know, it was the vector\\nright of the creation of a lot of this stuff. So then basically by, so part part, part of my story is just\\nthe luck of having been born at the right time and\\ngetting exposed to PCs. Then the other part is, the other part is when El\\nGore says that he created the internet, he actually is correct in a really meaningful way, which is he sponsored a bill\\nin 1985 that essentially created the modern internet, created what is called\\nthe NSF net at the time, which is sort of the first\\nreally fast internet backbone. And you know, that that bill dumped a ton of money into a bunch of research universities to build out basically\\nthe internet backbone and then the supercomputer\\ncenters that were clustered around the internet. And one of those universities\\nwas University of Illinois where I went to school. And so the other stroke\\nlock that I had was, I went to Illinois basically\\nright as that money was just like getting dumped on campus. And so as a consequence\\nwe had at, on campus, and this was like, you know,\\n89, 90, 91, we had like, you know, we were right\\non the internet backbone. We had like T3 and 45 at the time, T3 45 megabit backbone\\nconnection, which at the time was, you know, wildly state of the art. We had cray super computers. We had thinking machines\\nparallel super computers. We had silicon graphics\\nworkstations, we had Macintosh's, we had next cubes all over the place. We had like every\\npossible kind of computer you could imagine 'cause all this money\\njust fell out of the sky. - [Lex] So you were living in the future. - Yeah. So yeah, quite\\nliterally it was, yeah, like it's all there. It's all like we had\\nfull broadband graphics, like the whole thing. And it's actually funny 'cause they had this is the first time I kind of, it sort of tickled the back\\nof my head that there might be a big opportunity in\\nhere, which is, you know, they embraced it and so\\nthey put like computers in all the dorms and they\\nwired up all the dorm rooms and they had all these, you know, labs everywhere and everything. And then they gave every undergrad a computer account and an email address. And the assumption was that\\nyou would use the internet for four years at college and then you would\\ngraduate and stop using it. And that was that, right? - [Lex] Yeah. - And you would just\\nretire your email address. It wouldn't be relevant\\nanymore 'cause you'd go off from the workplace and\\nthey don't use email. You'd be back to using\\nfax machines or whatever. - Did you have that sense as well? Like, what you said the back\\nof your head was tickled. Like, what was exciting to\\nyou about this possible world? - Well, if this is so\\nuseful in this containment, if this is so useful in\\nthis contain environment that just has this weird\\nsource of outside funding, then if it were practical\\nfor everybody else to have this and if it were cost effective for everybody else to have this, wouldn't they want it? And the overwhelmingly the prevailing view at the time was no,\\nthey would not want it. This is esoteric, weird nerd stuff, right? That like computer science kids like, but like normal people are\\nnever gonna do email. Right. Or be on the internet, right? And so I was just like,\\nwow, like this is actually, like, this is really compelling stuff. Now the other part was, it was all really hard to use\\nand in practice you had to be basically a CS you know, basically had had to BA\\nCS undergrad or equivalent to actually get full use of\\nthe internet at that point. 'cause it was all pretty esoteric stuff. So then that was the other\\npart of the idea, which was, okay, we need to actually\\nmake this easy to use. - So what's involved in creating Mosaic? Like, in creating graphical\\ninterface to the internet? - Yeah, so it was a combination of things. So it was like basically the web existed in an early sort of\\ndescribed as prototype form. And by the way, text only at that point. - What did it look like? What was the web? I mean what and the key figures. Like, what was it? Like, what paint a picture? - It looked like ChatGPT\\nactually it was all text. - Yeah. - And so you had a text-based web browser? Yeah, well actually the original browser, Tim Burners Lee, the original browser, both the original browser\\nand the server actually ran on next cubes. So these were, this was, you know, the computer Steve Jobs made\\nduring the interim period when during the decade long interim period when he was not at Apple, you know, he got fired in 85 and\\nthen came back in 97. So this was in that interim\\nperiod where he had this company called Next and they made these, literally these computers called cubes. And there's this famous\\nstory, they were beautiful, but they were 12 inch by 12\\ninch by 12 inch cubes computers. And there's a famous story\\nabout how they could have cost half as much if it had\\nbeen 12 by 12 by 13. But this cube was like,\\nno, like it has to be. So they were like $6,000\\nbasically academic workstations. They had the first city round\\ndrives, which were slow. I mean it was, the computers\\nwere all but unusable. They were so slow, but\\nthey were beautiful. - Okay, can we actually just\\ntake a tiny tangent there? - Sure. Of course. - The 12 by 12 by 12 that just\\nso beautifully encapsulates Steve Jobs idea of design. Can you just comment on what you find interesting about Steve Jobs? What about that view of the world, that dogmatic pursuit of perfection and how he saw perfection in design? - Yeah, so I guess I'd say like, look, he was a deep believer, I think in a very deep,\\nthe way I interpret it, I don't know if you ever\\nreally described it like this, but the way I interpret\\nit's like this thing and it's actually a thing in philosophy. It's like aesthetics are\\nnot just appearances. Aesthetics go all the way to like deep underlying meaning, right? It's like I'm not a physicist. One of the things I've\\nheard physicists say is one of the things you start to get a sense of when a theory might be correct is when it's beautiful, right? Like, you know, there, right? And so, there's something, and you feel the same thing by the way in like human psychology, right? You know, when you're\\nexperiencing awe, right? You know, there's like a simplicity to it. When you're having an honest\\ninteraction with somebody, there's an aesthetic, I would say calm comes over you 'cause you're actually being fully honest and trying to hide yourself, right? So it's like this very\\ndeep sense of aesthetics. - And he would trust that\\njudgment that he had deep down. Like yeah, even if the\\nengineering teams are saying this is too difficult. Even if whatever the\\nfinance folks are saying, this is ridiculous. The supply chain, all that\\nkind of stuff just makes this impossible. We can't do this kind of material. This has never been done\\nbefore and so on and so forth. He just sticks by it. - Well, I mean, who makes a\\nphone out of aluminum, right? Like, hadn't nobody\\nelse would've done that. And now of course if your phone is made out of aluminum white,\\nyou know, how crude, what a kind of caveman would\\nyou have to be to have a phone that's made outta plastic? Like, right. So like, so it's just this very right. And, you know, look, there's a thousand different\\nways to look at this, but one of the things is just like, look, these things are\\ncentral to your life. Like, you're with your phone more than you're with anything else. Like, it's gonna be in your hand. I mean, you know this, he thought very deeply about\\nwhat it meant for something to be in your hand all day long. But for example, here's an\\ninteresting design thing. Like, he never wanted, my understanding is he never\\nwanted an iPhone to have a screen larger than you could reach with your thumb one handed. And so he was actually opposed to the idea of making the phones larger. And I don't know if you\\nhave this experience today, but let's say there are\\ncertain moments in your day when you might be like, only have one hand available\\nand you might wanna be on your phone. And you're trying to like, send a text and your thumb can't\\nreach the send button. - Yeah. I mean there's\\npros and cons, right? And then there's like folding phones, which I would love to know what he thought thinks about them. But I mean, is there something you\\ncould also just linger on? 'cause he's one of the interesting figures in the history of technology. What makes him as successful as he was? What makes him as interesting as he was? What made him so productive and important in the development of technology? - He had an integrated worldview. So the properly designed device that had the correct functionality, that had the deepest\\nunderstanding of the user, that was the most beautiful, right? Like, it had to be all\\nof those things, right? He basically would drive\\nto as close to perfect as you could possibly get. Right? And you know, I suspect that\\nhe never quite, you know, thought he ever got there.\\n'cause most great creators, you know, are generally dissatisfied. You know, you read accounts\\nlater on and all they can, all they can see are the\\nflaws in their creation. But like he got as close to\\nperfect each step of the way as he could possibly\\nget with the constraints of the technology of his time. And then, you know,\\nlook, he was, you know, sort of famous in the Apple model. It's like, look, they will, you know, this headset that they just came out with, like, you know, it's like a\\ndecade long project, right? It's like, and they're just gonna sit\\nthere and tune and tune and polish and polish and tune and polish and tune and polish until it is as perfect as anybody could possibly make anything. - Yeah. - And then this goes to the way that people describe working\\nwith him was, which is, you know, there was a terrifying\\naspect of working with him, which is, you know, he was,\\nyou know, he was very tough. But there was this thing that\\neverybody I've ever talked to worked for him, says that they all say the following, which is we did the best work of our lives when we worked for him because he set the bar incredibly high. And then he supported us\\nwith everything that he could to let us actually do\\nwork of that quality. So a lot of people who were at Apple spend the rest of their lives trying\\nto find another experience where they feel like they're able to hit\\nthat quality bar again. - Even if it in retrospect or\\nduring it felt like suffering. - Yeah, exactly. - What does that teach you\\nabout the human condition? Huh? - So look, so say exactly. So the Silicon Valley, I mean,\\nlook, he's not, you know, George Patton you know in the Army. Like, you know, there are\\nmany examples in other fields, you know, that are like\\nthis specifically in tech. It's actually, I find it very interesting. There's the Apple way, which\\nis polish, polish, polish, and don't ship until it's as\\nperfect as you can make it. And then there's the sort\\nof the other approach, which is the sort of\\nincremental hacker mentality, which basically says, ship\\nearly and often and iterate. And one of the things I\\nfind really interesting is I'm now 30 years into this, like, they're very successful\\ncompanies on both sides of that approach, right? Like, that is a fundamental\\ndifference, right? In how to operate and how to\\nbuild and how to create that. You have world class companies\\noperating in both ways. And I don't think the question of like, which is the superior\\nmodel is anywhere close to being answered. Like, and my suspicion\\nis the answer is do both. The answer is you actually want both. They lead to different outcomes. Software tends to do better\\nwith the iterative approach. Hardware tends to do\\nbetter with the, you know, sort of wait and make it perfect approach. But again, you can find\\nexamples in both directions. - So the jury's still out on that one. So back to Mosaic. So, what it was text based Tim Burns Lee? - Well, there was the\\nweb, which was text based, but there were no, I mean\\nthere was like three websites. There was like no content,\\nthere were no users. Like, it wasn't like a catalytic, it hadn't, and by the way, it was all because it was all text. There were no documents, there were no images,\\nthere were no videos, there were no, right. So, and then if, if in the beginning, if you had to be on a next cube, right? You need to had a next cube\\nboth to publish and to consume. - So, there was 6,000 bucks you said. - There were limitations. Yeah. $6,000 PC. They\\ndid not sell very many. But then there was also, there was also FTP and\\nthere was Use Nets, right? And there was, you know, a dozen other basically there's waste, which was an early search thing. There was Gopher, which\\nwas an early menu based information retrieval system. There were like a dozen\\ndifferent sort of scattered ways that people would get to\\ninformation on the internet. And so the Mosaic idea was basically bring those all together, make the whole thing\\ngraphical, make it easy to use, make it basically bulletproof\\nso that anybody can do it. And then again, just on the luck side, it so happened that this was right at the moment when graphics, when the GUI sort of actually took off and we're now also used to the GUI that we think it's been around forever. But it didn't real, you know, the Macintosh brought it out in 85, but they actually didn't\\nsell very many Macs in the eighties. It was not that successful of a product. It really was. You needed Windows 3.0 on\\nPCs and that hit in about 92. And so, and we did most in 92, 93. So that sort of, it was like right at the\\nmoment when you could imagine actually having a graphical\\nuser interface to right at all, much less one to the internet. - How old did Windows 3 sell? So was that the really big. - [Marc] That was the big bang. - The big operating\\ngraphical operating system? - Well this is the classic, okay. This Microsoft was operating on the other, so Steve the Apple was\\nrunning on the Polish until it's perfect. Microsoft famously ran on the other model, which is ship and iterate. And so in the old line in those days was Microsoft Right's version three of every Microsoft product. That's the good one, right? And so there there are, you can find online Windows 1, Windows 2. Nobody used them. Actually the original Windows, in the original Microsoft Windows, the windows were non overlapping. And so you had these very small, very low resolution screens and then you had literally-- - [Lex] Windows. - It just didn't work. It wasn't ready yet. Well. - And Windows 95 I think\\nwas a pretty big leap also. - That was a big leap too. So that was like bang, bang. And then of course Steve, and then when, you know, in the fullness of time Steve came back, then the Mac started, took off again. That was the third bang. And then the iPhone was the fourth bang. - Such exciting time. - And then we were off,\\noff to the races because. - Nobody could have known what\\nwould be created from that. - Well, Windows 3.1 or 3.0, Windows 3.0 to the iPhone\\nwas only 15 years. Right. Like that ramp was in retrospect. At the time it felt like it took forever. But that in histor in historical terms, like that was a very fast ramp from even a graphical computer at all on your desk to the iPhone. That was 15 years. - So, did you have a sense\\nof what the internet will be as you're looking through\\nthe window of Mosaic? Like, what you, like there's\\njust a few web pages for now. - So the thing I had early on\\nwas I was keeping at the time what there's disputes over\\nwhat was the first blog, but I had one of them that\\nat least is a possible, at least a rudder up in the competition. And it was what was called\\nthe What's new page. And it was literally, it was a hardwired in\\ndistribution unfair advantage. I wired, put it right in the browser, I put it in the browser\\nand then I put my resume in the browser, which also was-- - [Lex] Hilarious. - But I was keeping not many people get to get to do that. - No, good call. And early days. It's so interesting. - I'm looking for my, about about, oh, Marc is looking for a job. - [Lex] Yeah, yeah, exactly. - So the West New page, I would literally get up every morning and I would, or every afternoon\\nand I would basically, if you wanted to launch a website, you would email me and I would\\nlist it on the most new page. And that was how people\\ndiscovered the new websites as they were coming out. And I remember 'cause it was like one, it literally went from, it was like one every couple\\ndays to like one every day to like two every day. - And then so you're doing, so that blog was kind of\\ndoing the directory thing. So like, what was the homepage? - So the homepage was just\\nbasically trying to explain even what this thing is that\\nyou're looking at. Right. Basically the basic instructions. But then there was a button, there was a button that said what's new. And what most people did was they went to, for obvious reasons went to what's new. - [Lex] Yeah. - But like it was so mind\\nblowing at that point. This the basic idea and it\\nwas, this was like, you know, this was the basic idea of the internet, but people could see\\nit for the first time. The basic idea was, look,\\nyou know, some, you know, it's like literally it's like\\nan Indian restaurant in like Bristol England has like\\nput their menu on the web. And people were like, wow. - [Lex] Whoa. - Because like that's the first\\nrestaurant menu on the web. - [Lex] Yeah. - And I don't have to be\\nin Bristol and I don't know if I'm ever gonna go to Bristol. And I don't even like Indian\\nfood and like. Wow. Right. And it was like that the first web, the first streaming video thing was it was in another England,\\nsome Oxford or something. Some guy put his coffee pot up\\nas the first streaming video thing and he put it on the\\nweb 'cause he literally, it was the coffee pot down the hall. And he wanted to see when\\nhe needed to go refill it. But there were, you know, there was a point when\\nthere were thousands of people like watching that coffee pot 'cause it was the first\\nthing you could watch. - Well, but isn't were you able\\nto kind of infer, you know, if that Indian restaurant could go online. Then you're like they all will. - [Marc] Yeah, exactly. - So you felt that? - [Marc] Yeah, yeah, yeah. - Okay. - Now, you know, look, it's\\nstill a stretch, right? It's still a stretch 'cause\\nit's just like, okay, is it, you know, you're still in this\\nzone, which is like, okay, is this a nerd thing? Is this a real person thing? By the way, you know, there was a wall of\\nskepticism from the media. Like, they just, like,\\neverybody was just like, yeah, this is the crazy, this is just like dumb. This is not, you know, this is not for regular\\npeople at that time. And so you, you had to think\\nthrough that and then look, it was still hard to get on the internet at that point, right? So you could get kind of this\\nweird bastardized version if you were on AOL,\\nwhich wasn't really real. Or you had to go like,\\nlearn what an ISP was. You know, in those days, PCs actually didn't have TCPIP\\ndrivers come reinstalled. So you had to learn\\nwhat a TCPIP driver was. You had to buy a modem, you\\nhad to install driver software. I have a comedy routine. I do. So it's like 20 minutes long\\ndescribing all the steps required to actually get on\\nthe internet at this point. And so you had to look\\nthrough these practical. Well, and then speed performance 14-4 modems, right? Like it was like watching,\\nyou know, glue dry, like, and so you had to, there were basically a sequence of bets that we made where you basically needed to look through that current\\nstate of affairs and say, actually there's gonna\\nbe so much demand for once people figure this out, there's gonna be so much demand for it that all of these practical problems\\nare gonna get fixed. - Some people say that\\nthe anticipation makes the destination that much more exciting. - Do you remember progressive JPEGs? - Yeah. Do I, do I? - For kids in the audience, right? - [Lex] For kids in the audience. - You used to have to watch an image load like a line at the time. But it turns out there\\nwas this thing with JPEGs where you could load\\nbasically every fourth, you could load like every fourth line and then you could sweep\\nback through again. And so you could like\\nrender a fuzzy version of image up front. And then it would like\\nresolve into the detailed one. And that was like a big UI breakthrough 'cause it gave you something to watch. - Yeah. And you know, there's applications in\\nvarious domains for that. - Well it was a big fight. There was a big fight early on\\nabout whether there should be images in the web. And. - For that reason for\\nlike sexualization or-- - Not explicitly that that did come up. But it wasn't even that, it was more just like all the\\nserious in the argument went, the purists basically said\\nall the serious information in the world\", metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 517638, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content=' say that\\nthe anticipation makes the destination that much more exciting. - Do you remember progressive JPEGs? - Yeah. Do I, do I? - For kids in the audience, right? - [Lex] For kids in the audience. - You used to have to watch an image load like a line at the time. But it turns out there\\nwas this thing with JPEGs where you could load\\nbasically every fourth, you could load like every fourth line and then you could sweep\\nback through again. And so you could like\\nrender a fuzzy version of image up front. And then it would like\\nresolve into the detailed one. And that was like a big UI breakthrough \\'cause it gave you something to watch. - Yeah. And you know, there\\'s applications in\\nvarious domains for that. - Well it was a big fight. There was a big fight early on\\nabout whether there should be images in the web. And. - For that reason for\\nlike sexualization or-- - Not explicitly that that did come up. But it wasn\\'t even that, it was more just like all the\\nserious in the argument went, the purists basically said\\nall the serious information in the world is text. If you introduce images, you basically are gonna bring\\nin all the trivial stuff. You\\'re gonna bring in\\nmagazines and you know, all this crazy just, you know,\\nstuff that, you know, people, you know, it\\'s gonna, it is\\ngonna distract from that. It\\'s gonna go take it away from being\\nserious to being frivolous. - Well, was there any\\n(indistinct) type arguments about the internet destroying all of human\\ncivilization or destroying some fundamental fabric of human civilization? - So it was, those days it was all around crime and terrorism. So those arguments happened, you know, but there was no sense yet\\nof the internet having like, an effect on politics because\\nthat was way too, too far off. But there was an enormous panic at the time around cybercrime. There was like enormous panic\\nthat like your credit card number would get stolen and you\\'d use life\\nsavings would be drained. And then, you know, criminals were gonna, there was, oh, when we started,\\none of the things we did, one of the Netscape browser\\nwas the first widely used piece of consumer software that had\\nstrong encryption built in, it made it available to ordinary people. And at that time, strong encryption was\\nactually illegal to export outta the US so we could feel that product in the US, we could not export it \\'cause it was classified as munition. So the Netscape browser\\nwas on a restricted list along with the tomahawk missile as being something that\\ncould not be exported. So we had to make a second\\nversion with deliberately weak encryption to sell\\noverseas with a big logo on the box saying, do not trust this. Which it turns out, makes it hard to sell software\\nwhen it\\'s got a big logo that says don\\'t trust it. And then we had to spend\\nfive years fighting the US government to get\\nthem to basically stop trying to do this regulation. But because the fear\\nwas terrorists are gonna use encryption, right? To like plot, you know, all these things. And then, you know, we responded with, well actually we need\\nencryption to be able to secure systems so that the terrorists and the criminals can\\'t get into them. So that anyway, that was the 1990s fight. - So can you say something\\nabout some of the details of the software engineering\\nchallenges required to build these browsers? I mean the engineering\\nchallenges of creating a product that hasn\\'t really existed before that can have such\\nalmost like limitless impact on the world with the internet. - So there was a really key\\nbet that we made at the time, which was very controversial, which was core to core\\nto how it was engineered, which was are we\\noptimizing for performance or for ease of creation? And in those days the pressure\\nwas very intense to optimize for performance because the\\nnetwork connections were so slow and also the computers were so slow. And so if you had, I mentioned\\nthe progressive JPEGs, like if there\\'s an alternate\\nworld in which we optimized for performance and it just, you had just a much more pleasant\\nexperience right up front. But what we got by not doing that was we got ease of creation. And the way that we got\\nease of creation was all of the protocols and\\nformats were in text, not in binary. And so HTTP is in text, by the way. And this was an internet\\ntradition by the way that we picked up. But we continued it. HTTP is text and HTML is\\ntext, and then every else, everything else that\\nfollowed is text as a result. And by the way, you can imagine purist\\nengineers saying this is insane. You have very limited bandwidth. Why are you wasting any time sending text? You should be encoding\\nthis stuff into binary and it\\'ll be much faster. And of course the answer\\nis that\\'s correct. But what you get when you make\\nit taxed is all of a sudden, well, the big breakthrough was the view source function, right? So the fact that you\\ncould look at a webpage, you could hit view source\\nand you could see the HTML, that was how people learned\\nhow to make webpages. Right? - It\\'s so interesting \\'cause the stuff would\\ntake for granted now is, man, that was fundamental, the development of the web\\nto be able to have HTML just right there, all the\\nghetto mess that is HTML, all the sort of almost\\nbiological like messiness of HTML and then having the browser\\ntry to interpret that as. - [Marc] Exactly. - To show something reasonable. - Well and then there was\\nthis internet principle that we inherited, which\\nwas emit, what was it? Emit cautiously. Emit\\nconservatively interpret liberally. So it basically meant if you\\'re, the design principle was if you\\'re creating like a web editor that\\'s gonna admit HTML, like\\ndo it as cleanly as you can, but you actually want the\\nbrowser to interpret liberally, which is you actually want\\nusers to be able to make all kinds of mistakes and\\nfor it to still work. And so the browser rendering\\nengines to this day have all of this spaghetti code crazy stuff where they\\'re resilient to\\nall kinds of crazy issue, no mistakes. And so, literally what I\\nalways had in my head is like there\\'s an 8 year old or\\nan 11 year old somewhere and they\\'re doing a view source, they\\'re doing a cut and\\npaste and they\\'re trying to make a webpage for\\ntheir eternal or whatever. And like they leave out a\\nslash and they leave out an angle bracket and they do this and they do that and it\\'s still works. - It\\'s also like a, I don\\'t often think about this,\\nbut, you know, programming, you know, C++ all those languages, lisp, the compiled languages,\\nthe interpreted languages, Python, Pearl, all that. The brace have to be all correct. It\\'s like everything has to be perfect. - [Marc] Brutal. - And then-- - [Marc] Autistic. - You forget. All right. It\\'s systematic\\nand rigorous, let\\'s go there. But you forget that the, the web with JavaScript eventually. And HTML is allowed to be messy in the way for the first time. Messy in the way biological\\nsystems could be messy. It\\'s like the only thing\\ncomputers were allowed to be messy on for the first time. - It used to off fend me. So I grew up on Unix, so I worked on Unix. I was a Unix native for all\\nthe way through this period. And so, it used to drive\\nme bananas when it would do the segmentation fault\\nand the core dump file, just like it is, you know, it\\'s like literally there\\'s\\nlike a error in the code. The math is off by one. And it core dumps. And I\\'m in the core dump\\ntrying to analyze it and trying to reconstruct what, and I\\'m\\njust like, this is ridiculous. Like, the computer\\nought to be smart enough to be able to know that if it\\'s off by one, okay fine. And it keeps running. And I would go ask all the experts like, why can\\'t it just keep running? And they\\'d explain to me, well, because all the downstream\\nrepercussions and blah blah. And I\\'m like, this still,\\nlike, you know, this is, we\\'re forcing the human\\ncreator to live to your point in this hyper, literal\\nworld of perfection. - [Lex] Yeah. And I was just like, that\\'s just bad. And by the way, you know what happens with that of course. Just what what happened with,\\nwith coding at that point, which is you get a high\\npriesthood, you know, there\\'s a small number of\\npeople who are really good at doing exactly that. Most people can\\'t. And most people are excluded from it. And so actually that was where that for that was where I picked up that idea was like, no, you want these things to be\\nresilient error in all kinds and this would drive the\\npurist absolutely crazy. Like, I got attacked on this like a lot \\'cause I mean like every time you know, all the purists who\\nwere like into all this like Marcup language stuff and formats and codes and all this stuff, they would be like, you know, you\\'re encouraging bad behavior \\'cause. - Oh, so they wanted\\nthe browser to give you a fault error anytime there was a-- - Yeah. They wanted to\\nbe a (indistinct) right? They wanted to-- Yeah. Yeah. That was a very and any properly trained credential engineer would be like, that\\'s not how you build these systems. - That\\'s such a bold move to say, no, it doesn\\'t have to be. - Yeah. No, like I said, the good news for me is\\nthe internet kind of had that traditional already,\\nbut having said that, like we pushed it, we pushed it way out. But the other thing we did, going back to the performance thing, was we gave up a lot of performance. We made that, that initial experience for the first few years\\nwas pretty painful. But the bet there was\\nactually an economic bet, which was basically the demand\\nfor the web would basically mean that there would be a\\nsurge in supply of broadband. Like because the question was, okay, how do you get the phone\\ncompanies which are not famous in those days for doing\\nnew things at huge cost for like speculative reasons. Like how do you get them to\\nbuild up broadband, you know, spend billions of dollars\\ndoing that and you know, you could go meet with them\\nand try to talk them into it. Or you could just have a thing where it\\'s just very\\nclear that it\\'s gonna be, that people love that\\'s gonna\\nbe better if it\\'s faster. And, so that, there was a\\nperiod there and this was, this was fraught with in peril, but there was a period there\\nwhere it\\'s like we knew the experience was sub-optimized because we were trying to force the emergence of demand for broadband. - [Lex] Sure. - Which is in fact what happened. - So you had to figure out\\nhow to display this text, HTML text. So the blue links and\\nthe prop links. What? And there\\'s no standards. Is\\nthere standards at that time? - [Marc] No. There really still isn\\'t. - Well there\\'s like standards, there\\'s applied, implied standards. Right. And they, you know, there\\'s all these kind of new features that are being added with like CSS, what, like what kind of stuff a\\nbrowser should be able to support features within languages,\\nwithin JavaScript and so on. But you\\'re setting standards\\non the fly yourself. - Yeah. Well to this day, if you create a webpage\\nthat has no CSS style sheet, the browser will render\\nit however it wants to. Right. So this was one of the\\nthings, there was this idea, this idea of at the time and\\nhow these systems were built, which is separation of content from format or separation of content from appearance. And that\\'s still, people\\ndon\\'t really use that anymore \\'cause everybody wants to\\ndetermine how things look and so they use CSS\\nbut it\\'s still in there that you can just let the\\nbrowser do all the work. - I still like the like\\nreally basic websites, but that could be just old school, kids these days with their\\nfancy responsive websites that don\\'t actually have much content, but have a lot of visual elements. - Well that\\'s one of the\\nthings that\\'s fun about chat, you know, about ChatGPT like. - [Lex] Back to the basics. - Back to just text. - [Lex] Yeah. - Right? And it, you know, there is this pattern in\\nhuman creativity and media where you end up back at text\\nand I think there\\'s, you know, there\\'s something powerful in there. - Is there some other stuff you remember like the purple links? There were some interesting\\ndesign decisions that to kind of come up that we have today or we don\\'t have today\\nthat were temporary. - So I made the background\\n\\'cause I hated reading texts on white background, so I\\nmade the background gray. Everybody can-- - Do you go ahead to? - No. No, no. That decision I think has been reversed. But now I\\'m happy though because\\nnow dark mode is the thing. - So it wasn\\'t about gray, it was just you didn\\'t\\nwant white background. - [Marc] Strain my eyes. - Strain your eyes. Interesting. And then there\\'s a bunch\\nof other decisions. I\\'m sure there\\'s an interesting\\nhistory of the development of HTML and CSS and\\nInterface and JavaScript and there\\'s this whole Java applet thing. - Well the big one probably\\nJavaScript, CSS was after me, so I didn\\'t, that was not me. But JavaScript was the big, JavaScript maybe was the\\nbiggest of the whole thing. That was us. And that was basically a bet,\\nit was a bet on two things. One is that the world wanted a new front end scripting language. And then the other was I thought at the time the world wanted a new backend scripting language. So JavaScript was designed\\nfrom the beginning to be both front end and backend. And then it failed as a\\nbackend scripting language. And Java won for a long time. And then Python Pearl and\\nother things, PHP and Ruby. But now JavaScript is back. And so. - I wonder if everything in\\nthe end will run on JavaScript. - It seems like it is the, and by the way, lemme give a shout out\\nto, to Brendan Eich was basically the one man\\ninventor of of JavaScript. - If you\\'re interested to\\nlearn more about Brendan Eich, he\\'s been on his podcast previously. - Exactly. So he wrote\\nJavaScript over a summer and I mean I think it is fair, it is fair to say now that\\nit\\'s the most widely used language in the world and\\nit seems to only be gaining in its in its range of adoption. - You know, in the software world there\\'s quite a few stories of somebody over a week weekend or over a\\nweek or over a summer writing some of the most impactful\\nrevolutionary pieces of software ever. That\\nshould be inspiring. Yes. - Very inspiring. I\\'ll\\ngive you another one. SSL. So SSL with the security\\nprotocol, that was us. And that was a crazy idea at the time, which was let\\'s take\\nall the native protocols and let\\'s wrap them in a security wrapper. That was a guy named Kip Hickman who wrote that over a summer, one guy. And then look today, sitting here today, like the transformer like at Google was a small handful of people. And then, you know, the number of people who have\\ndid like the core work on GPT. It\\'s not that many people, it\\'s a pretty small handful of people. And so yeah, the pattern in software repeatedly over a very long time has been, it\\'s Jeff Bezos always\\nhad the two pizza rule for teams at Amazon, which is any team needs\\nto be able to be fed with two pieces. If you need the third pizza,\\nyou have too many people. And I think it\\'s actually\\nthe one pizza rule. For the really creative work. I think it\\'s two people, three people. - Well that\\'s, you see that with certain open source projects, like so much is done by\\nlike one or two people. Like it\\'s so incredible\\nand that\\'s why you see that gives me so much hope\\nabout the open source movement in this new age of AI where, you know, just recently having had a conversation with Marc Zuckerberg of all people who\\'s all in on open source, which is so interesting to\\nsee and so inspiring to see \\'cause like releasing\\nthese models, it is scary. It is potentially very dangerous\\nand we\\'ll talk about that. But it\\'s also, if you believe in the\\ngoodness of most people and in the skillset of most people and the desire to go do good in the world, that\\'s really exciting. \\'cause it\\'s not putting it these models into the centralized\\ncontrol of big corporations, the government and so on. It\\'s putting it in the hands of a teen, teenage kid with like a dream in his eyes. I don\\'t know. That\\'s beautiful. - Look, this stuff, AI ought to make the\\nindividual coder obviously far more productive right? By like, you know, a\\nthousand X or something. And so you ought to open source like, not just the future of open source AI, but the future of open source everything. We ought to have a world\\nnow of super coders, right? Who are building things as open source with one or two people\\nthat were inconceivable, you know, five years ago. You know, the level of\\nkind of hyper productivity we\\'re gonna get out of\\nour best and brightest I think is gonna go way up. - It\\'s gonna be interesting. We\\'ll talk about it, but let\\'s just to linger\\na little bit on Netscape. Netscape was acquired in\\n1999 for 4.3 billion by AOL. What was that like? What were some memorable aspects of that? - Well that was the height\\nof the.com boom bubble bust. I mean that was the frenzy. If you watch succession, that was like what they\\ndid in the fourth season with Gojo and the merger with their, so it was like the height of like one of those kind of dynamics. And so. - Would you recommend\\nsuccession, by the way? I\\'m more of a Yellowstone guy. - Yellowstone\\'s very American. I\\'m very proud of you. That\\'s, that is. - I just talked to Matthew McConaughey and I\\'m full on Texan at this point. - Good. I approve. - And he\\'ll be doing\\nthe SQL to Yellowstone. - [Marc] Yeah, just exciting. - Very exciting. Anyway. - [Marc] Can\\'t wait. - So that\\'s a rude interruption\\nby me by way of succession. So, that was at the height of the-- - Deal making and money\\nand just the fur flying and like craziness. And so yeah, it was just one of those, it was just like, I mean, and this, the entire (indistinct) thing from start to finish was four years, which was like for one of these companies, it\\'s just like incredibly fast. You know, it went, we went public 18 months\\nafter we got moved where we were founded, which\\nvirtually never happens. So it was just this incredibly fast kind of meteor streaking across the sky. And then of course it was this, and then there was just\\nthis explosion, right? That happened \\'cause then\\nit was almost immediately followed by the.com crash. It was then followed\\nby AOL, by Time Warner, which again is like the succession guys kinda play with that, which turned out to be a disastrous deal. You know, one of the famous, you know, kind of disastrous in business history. And then, you know, what became an internet depression on\\nthe other side of that. But then in that depression\\nin the two thousands was the beginning of broadband and smartphones and Web 2.0 right? And then social media\\nand search and every SaaS and everything that came out of that. - What did you learn from\\njust the acquisition? I mean this is so much money. What\\'s interesting \\'cause I\\nmust have been very new to you, that these software stuff, you can make so much money. There\\'s so much money swimming around. I mean, I\\'m sure the\\nideas of investment was starting to get born there. - Yes. Let me get, so let me lay it. So here\\'s, here\\'s the thing. I dunno if I figured it out\\nthen, but figured it out later, which is software is a technology that it, it\\'s like a, you know, the\\nconcept of the philosopher stone, the philosopher stone in alchemy, transient is led into gold and\\nNewton spent 20 years trying to find the philosopher stone. Never got there. Nobody\\'s ever figured it out. Software is our modern philosopher stone. And in economic terms, it\\ntransmutes labor into capital, which is like a super interesting thing. And by the way, like Carl Marcs is rolling over in his grave right now. \\'Cause of course that\\'s\\ncomplete reputation of his entire theory. Trans labor and capital\\nwhich is as follows, is somebody sits down at a keyboard and types a bunch of stuff in, and a capital asset\\ncomes out the other side and then somebody buys that capital asset for a billion dollars. Like that\\'s amazing, right? It\\'s literally creating\\nvalue right out of thin air, right out of purely human thought, right? And so that, there are many things that make software magical and special, but that\\'s the economics. - I wonder what Marx\\nwould\\'ve thought about that? - Oh, he would\\'ve\\ncompletely broke his brain because of course the whole\\nthing was it was he could, you know, that kind of\\ntechnology was inconceivable when he was alive. It was all industrial era stuff. And so, any kind of machinery\\nnecessarily involved huge amounts of capital. And then labor was on the receiving end of the abuse. - [Lex] Yep. Right? But like software eng software, a software engineer is somebody\\nwho basically transmutes his own labor into actual, an actual capital asset\\ncreates permanent value. Well, and in fact it\\'s\\nactually very inspiring. That\\'s actually more\\ntrue today than before. So when I was doing software, the assumption was all\\nnew software basically has a sort of a parabolic\\nsort of lifecycle, right? So you ship the thing,\\npeople buy it at some point, everybody who wants it has bought it and then it becomes obsolete. And it\\'s like bananas. Nobody, nobody buys old software. These days, Minecraft, Mathematica, you know, Facebook, Google, you have the software\\nassets that are, you know, have been around for 30\\nyears that are gaining in value every year, right? And they\\'re just, they\\'re being\\na world of warcraft, right, salesforce.com, like they\\'re being every single year they\\'re\\nbeing polished and polished and polished and polished. They\\'re getting better\\nand better, more powerful, more powerful, more\\nvaluable, more valuable. So we\\'ve entered this era\\nwhere you can actually have these things that actually\\nbuild out over decades. Which by the way is what\\'s happening right now with like ChatGPT. And so now, this is why, you know, there is always, you know, sort of a constant investment\\nfrenzy around software is because, you know, look, when\\nyou start one of these things, it doesn\\'t always succeed. But when it does now you\\nmight be building an asset that builds value for,\\nyou know, four or five, six decades to come. You know, if you have a team of people who have the level of devotion required to keep making it better. And then the fact that of\\ncourse everybody\\'s online, you know, there\\'s 5 billion people that are a click away from\\nany new piece of software. So the potential market size\\nfor any of these things is, you know, nearly infinite. - [Lex] It must have been\\nsurreal back then though. - Yeah. Yeah. This was\\nall brand new, right? Yeah. Back then, this was all brand new. These were all, you know, brand new. Had you rolled out that\\ntheory in even 1999, people would\\'ve thought\\nyou were smoking crack. So that\\'s emerged over time. - Well, let\\'s now turn\\nback into the future. You wrote the essay \"Why\\nAI Will Save The World?\" Let\\'s start the very high level. What\\'s the main thesis of the essay? - Yeah, so the main thesis on the essay is that what we\\'re dealing\\nwith here is intelligence. And it\\'s really important to kind of talk about the sort of very nature\\nof what intelligence is. And fortunately we have a predecessor to machine intelligence,\\nwhich is human intelligence. And we\\'ve got, you know, observations and theories\\nover thousands of years for what intelligence is\\nin the hands of humans and what intelligence is, right? I mean, what it literally is the way to, you know, capture, process,\\nanalyze, synthesize information, solve problems. But the observation of\\nintelligence in human hands is that intelligence quite literally\\nmakes everything better. And what I mean by that\\nis every kind of outcome of like human quality of life, whether it\\'s education outcomes or success of your children, or career success or health or lifetime\\nsatisfaction, by the way, propensity to peacefulness\\nas opposed to violence, propensity for open-mindedness\\nversus bigotry, those are all associated with\\nhigher levels of intelligence. - Smarter people have better outcomes than almost as you write in almost every domain of activity. Academic achievement, job performance, occupational status, income, creativity, physical health, longevity,\\nlearning new skills, managing complex tasks, leadership,\\nentrepreneurial success, conflict resolution,\\nreading comprehension, financial decision making, understanding others\\nperspectives, creative arts, parenting outcomes, and life satisfaction. One of the more depressing\\nconversations I\\'ve had, and I don\\'t know why it\\'s depressing, I have to really think\\nthrough why it\\'s depressing, but on IQ and the G factor, and that that\\'s something\\nin large part is genetic and it correlates so much\\nwith all of these things and success in life. It\\'s like all the inspirational\\nstuff we read about, like if you work hard and so on, it sucks that you\\'re born with the hand that you can\\'t change. - But what if you could. - You\\'re saying basically\\na really important point, and I think it\\'s in your\\narticles, it really helped me. It\\'s a nice added\\nperspective to think about. Listen, human intelligence, the science of intelligence\\nis shown scientifically that it just makes life easier and better the smarter you are. And now let\\'s look at\\nartificial intelligence and if that\\'s a way to increase\\nsome human intelligence, then it\\'s only going\\nto make a better life. - [Marc] Yeah. - That\\'s the argument. - And certainly at the collective level, we could talk about the collective effect of just having more\\nintelligence in the world, which will have very big payoff. But there\\'s also just\\nat the individual level, like what if every person has a machine? You know? And the concept of augment Doug Engelbart\\'s concept of augmentation. You know, what if\\neverybody has an assistant and the assistant is, you know, 140 IQ and you happen to be 110 IQ and you\\'ve got, you know, something that basically is\\ninfinitely patient and knows everything about you\\nand is pulling for you in every possible way,\\nwants you to be successful. And anytime you find anything\\nconfusing or wanna learn anything or have trouble\\nunderstanding something or wanna figure out what to\\ndo in a situation, right? Wanna figure out how to\\nprepare for a job interview, like any of these things,\\nlike it will help you do it. And it will therefore, the combination will\\neffectively be, you know, will effectively raise your raise because it will effectively raise your IQ, will therefore raise the odds of successful life outcomes\\nin all these areas. - So people below the,\\nthis hypothetical 140 IQ, it\\'ll pull them up towards 140 IQ. - Yeah, yeah, yeah. And then of course, you know, people at 140 IQ will be\\nable to have a peer, right. To be able to communicate, which is great. And then people above 140\\nIQ will have an assistance that they can farm things out to. And then look, God willing, you know, at some point these things\\ngo from future versions go from 140 IQ equivalent to\\n150 to 160 to 180, right? Like Einstein was estimated\\nto be on the order of one 60, you know, so when we\\nget, you know, one 60 AI, like we\\'ll be, you know, when one assumes creating\\nEinstein level breakthroughs and physics, and then at\\n180 we\\'ll be, you know, carrying cancer and developing\\nwarp drive and doing all kinds of stuff. And so it is quite possibly the case, this is the most important\\nthing that\\'s ever happened and the best thing that\\'s ever happened because precisely because it\\'s a lever on this single fundamental\\nfactor of intelligence, which is the thing that drives\\nso much of everything else. - Can you steal, man, the case that human plus AI is\\nnot always better than human for the individual? - You may have noticed that there\\'s a lot of\\nsmart running around. - [Lex] Sure. Yes. - Right? And so, like smart, there are certain people where\\nthey get smarter, you know, they get to be more arrogant, right? So that, you know, there\\'s one huge flaw. - Although to push back on that, it might be interesting because\\nwhen the intelligence is not all coming from you,\\nbut from another system, that might actually increase\\nthe amount of humility even in the assholes. - [Marc] One would hope. - Yeah. - Or it could make assholes more assholes. You know, that\\'s in, I mean, that\\'s for psychology to study. - Yeah, exactly. Another one is smart people\\nare very convinced that they, you know, have a more\\nrational view of the world, and that they have a easier\\ntime seeing through conspiracy theories and hoaxes and right. You know, sort of crazy\\nbeliefs and all that. There\\'s a theory in psychology, which is actually smart people. So for sure people who aren\\'t\\nas smart are very susceptible to hoaxes and conspiracy theories. But it may also be the case\\nthat the smarter you get, you become susceptible in a different way, which is you become very\\ngood at marshaling facts to fit preconceptions, right. You become very, very good at assembling whatever theories and\\nframeworks and pieces of data and graphs and\\ncharts you need to validate whatever crazy ideas got in your head. And so you\\'re susceptible\\nin a different way, right? - We\\'re all sheep, but\\ndifferent colored sheep. - Some sheep are better\\nat justifying it. Right. And those are the, you know, those are the smart sheep, right? So yeah. Look like I\\nwould say this look like there are no panacea. I\\'m not a utopian, there\\nare no panaceas in life. There are no, like, you know, I don\\'t believe there\\nare like pure positives. I\\'m not a transcendental\\nkind of person like that. But, you know, so yeah,\\nthere are gonna be issues and, you know, look, smart people, another maybe you could\\nsave about smart people is they are more likely to get\\nthemselves in situations that are, you know, beyond their grasp. You know, because they\\'re\\njust more confident in their ability to deal with complexity and their eyes become bigger, their cognitive eyes become bigger than their stomach, you know? So yeah, you could argue\\nthose eight different ways nevertheless, on net, right? Clearly, overwhelmingly, again, if you just extrapolate from what we know about human intelligence, you\\'re improving so many aspects of life if you\\'re upgrading intelligence. - So there\\'ll be assistants\\nat all stages of life. So when you\\'re younger,\\nthere\\'s for education, all that kind of stuff for\\nmentorship, all of this. And later on as you\\'re doing\\nwork and you\\'ve developed a skill and you\\'re having a profession, you\\'ll have an assistant\\nthat helps you excel at that profession. So at all stages of life. - Yeah. I mean, look, the\\ntheory is augmentation. This is the Doug Engelbart\\'s term. Doug Engelbart made this observation many, many\\ndecades ago that, you know, basically it\\'s like you can\\nhave this oppositional frame of technology where it\\'s\\nlike us versus the machines, but what you really do\\nis you use technology to augment human capabilities. And by the way, that\\'s how actually the economy develops. That\\'s, we can talk about\\nthe economic side of this, but that\\'s actually how\\nthe economy grows is through technology\\naugmenting human potential. And so, yeah. And then you basically\\nhave a proxy or you know, or you know, a sort of\\nprosthetic, you know, so like you\\'ve got glasses,\\nyou\\'ve got a wristwatch, you know, you\\'ve got shoes, you know, you\\'ve got these things. You\\'ve got a personal computer, you\\'ve got a word processor,\\nyou\\'ve got Mathematica, you\\'ve got Google. This is the latest\\nviewed through that lens. AI is the latest in a long\\nseries of basically augmentation methods to be able to\\nraise human capabilities. It\\'s just this one is the\\nmost powerful one of all, because this is the one\\nthat, that goes directly to what they call fluid\\nintelligence, which is IQ. - Well, there\\'s two categories of folks that you outline that\\nworry about or highlight the risks of AI, and you highlight a bunch of different risks. I would love to go through those risks and just discuss them, brainstorm which ones are serious and which ones are less serious. But first, the Baptist\\nand the bootleggers, what are these two\\ninteresting groups of folks who worry about the effect\\nof AI and human civilization? - [Marc] Or say they do. - Say, oh, okay, yes, I\\'ll say they do. - The Baptist worry the\\nbootleggers say they do. So the Baptist and the\\nbootleggers is a metaphor from economics, from what\\'s\\ncalled development economics. And it\\'s this observation that when you get social\\nreform movements in a society, you tend to get two sets\\nof people showing up, arguing for the social reform. And the term Baptist and bootleggers comes from the American experience\\nwith alcohol prohibition. And so in the 1900s, 1910s, there was this movement\\nthat was very passionate at the time, which basically said, alcohol is evil and is destroying society. By the way, there was a lot\\nof evidence to support this. There were very high rates of very high correlations\\nthen, by the way. And now between rates of physical\\nviolence and alcohol use, almost all violent crimes\\nhave either the perpetrator or the victim, or both drunk almost. If you see this actually in the work, almost all sexual harassment\\ncases in the workplace, it\\'s like at a company\\nparty and somebody\\'s drunk. Like, it\\'s amazing how often\\nalcohol actually correlates to actually dis dysfunction\\nand at leads to domestic abuse and so forth, child abuse. And so you had this group of\\npeople who were like, okay, this is bad stuff and we should outlaw it. And those were quite literally Baptist. Those were super committed, you know, hardcore Christian\\nactivists in a lot of cases. There was this woman whose\\nname was Carrie Nation, who was this older woman who\\nhad been in this, you know, I don\\'t know, disastrous\\nmarriage or something. And her husband had been\\nabusive and drunk all the time. And she became the icon of\\nthe Baptist prohibitionist. And she was legendary in\\nthat era for carrying an ax and doing, you know, completely on her own\\ndoing raids of saloons and like taking her ax to all the bottles and eggs in the back. And so. - [Lex] A true believer. - An absolute true believer, and with absolutely the\\npurist of intentions. And again, there\\'s a very\\nimportant thing here, which is there\\'s, you could look at this\\ncynically and you could say the Baptists are like delusional,\\nyou know, the extremists, but you could also say,\\nlook, they\\'re right. Like she was, you know, she had a point. Like she wasn\\'t wrong about\\na lot of what she said. - Yeah. - But it turns out the way\\nthe story goes is it turns out that there were another set of people who very badly wanted to\\noutlaw alcohol in those days. And those were the bootleggers, which was organized crime that\\nstood to make a huge amount of money if legal alcohol\\nsales were banned. And this was, in fact, the way the history goes\\nis this was actually the beginning of\\norganized crime in the US. This was the big economic\\nopportunity that opened that up. And so they went in together and no, they didn\\'t go in together. Like the Baptist did not\\neven necessarily know about the bootleggers \\'cause they were on their moral crusade. The bootleggers certainly\\nknew about the Baptists. And they were like, wow, these people are like the\\ngreat front people for like. You know, it\\'s-- - [Lex] Good PR. - Shenanigans in the background. And they got the (indistinct)\\nAct passed, right. And they did in fact ban alcohol\\nin the US and you\\'ll notice what happened, which is\\npeople kept drinking, it didn\\'t work, people kept drinking. That bootleggers made a\\ntremendous amount of money. And then over time it became\\nclear that it made no sense to make it illegal and it\\nwas causing more problems. And so then it was revoked. And here we sit with legal\\nalcohol a hundred years later with all the same problems. And you know, the whole thing was this\\nlike giant misadventure the Baptist got taken advantage\\nof by the bootleggers, and the bootleggers got what they wanted. And that was that. - The same two categories of folks are now sort of suggesting\\nthat the development of artificial intelligence\\nshould be regulated. - A hundred percent. It\\'s the same pattern. And the economist will tell you it\\'s the same pattern every time. Like, this is what\\nhappened, nuclear power, this is what happens, which\\nis another interesting one. But like, yeah, this\\nhappens dozens and dozens of times throughout the\\nlast a hundred years and this is what\\'s happening now. - And you write that it isn\\'t\\nsufficient to simply identify the actors and impugn their motives. We should consider the\\narguments of both the Baptist and the bootleggers on their merits. So let\\'s do just that. Risk number one, will AI kill us all? - [Marc] Yes. - So what do you think about this one? What do you think is\\nthe core argument here that the development of\\nAGI perhaps better said, will destroy human civilization? - Well, first of all, you\\njust did a slight of hand \\'cause we went from talking about AI to AGI. - Is there a fundamental difference there? - I don\\'t know. What\\'s AGI? - What\\'s AI, what\\'s in intelligence? - Well, I know what AI\\nis machine learning. What\\'s AGI? - I think we don\\'t know\\nwhat the bottom of the well of machine learning is\\nor what the ceiling is. Because just to call\\nsomething machine learning or just to call some of the statistics or just to call it math or\\ncomputation doesn\\'t mean, you know, nuclear\\nweapons are just physics. So to me it\\'s very\\ninteresting and surprising how far machine learning has taken. - No, but we knew that\\nnuclear physics would lead to weapons. That\\'s why the scientists\\nof that era were always in some this huge dispute\\nabout building the weapons. This is different. AGI is different. - Does machine learning lead, do we know? - We don\\'t know, but this\\nis my point is different. We actually don\\'t know. But, and this is where you, the slide of hand kicks in, right? This is where it goes from\\nbeing a scientific topic to being a religious topic. And that\\'s why I specifically called out \\'cause that\\'s what happens. They do the vocabulary\\nshift and all of a sudden you\\'re talking about something totally. That\\'s not actually real. - Well then maybe you can\\nalso, as part of that, define the western\\ntradition of Millennialism. - [Marc] Yes. Into the world apocalypse. - [Lex] What is it? - [Marc] Apocalypse cults. - [Lex] Apocalypse cults. - Well, so we live in, we of course live in a Judeo-Christian, but primarily Christian\\nkind of saturated, you know, kind of Christian, post-Christian,\\nsecularized Christian, you know, kind of world in the west. And of course court of Christianity is the idea of the second\\ncoming and you know, the revelations and you know, Jesus returning and the\\nthousand year, you know, utopia on earth and then you know, the rapture and like all\\nall that stuff, you know, you know, we collectively,\\nyou know, as a society, we don\\'t necessarily take\\nall that fully seriously now. So, what we do is we create our\\nsecularized versions of that we keep looking for utopia. We keep looking for, you know, basically the end of the world. And so what what you see over, over decades is that basically a pattern of these sort of these of\\nis this is what cults are. This is how cults form as\\nthey form around some theory of the end of the world. And so the people\\'s temple cults, the Manson cult, the Heavens Gate cult, the David Qresh cult, you know what they\\'re all\\norganized around is like, there\\'s gonna be this\\nthing that\\'s gonna happen that\\'s gonna basically bring\\ncivilization crashing down. And then we have this\\nspecial elite group of people who are gonna see it\\ncoming and prepare for it. And then they\\'re the people\\nwho are either going to stop it or are failing, stopping it. They\\'re gonna be the people\\nwho survived the other side and ultimately get credit\\nfor having been, right. - Why is that so compelling,\\ndo you think? Like-- - Because it satisfies this very deep need we have for transcendence and meaning that got stripped\\naway when we became secular. - Yeah, but why is the\\ntranscendence involve the destruction of human civilization? - Because like how plausible it\\'s like a very deep psychological thing \\'cause it\\'s like how plausible, how plausible is it\\nthat we live in a world where everything\\'s just\\nkind of all right? Right. How exciting? - [Lex] Whoa. - How exciting is that? Right? - [Lex] But that\\'s. - We got more than that. - But that\\'s the deep question I\\'m asking. Why is it not exciting to live in a world where everything\\'s just all right? Is it, I think, you know, most of the animal kingdom would be so happy with just all right. Because that means survival. Why are we, maybe that\\'s what it is. Why are we conjuring up\\nthings to worry about? - So CS Lewis called\\nit the God-shaped hole. So there\\'s a God-shaped hole\\nin the human experience, consciousness, soul,\\nwhatever you wanna call it, where there\\'s gotta be\\nsomething that\\'s bigger than all this. There\\'s gotta be something transcendent. There\\'s gotta be something\\nthat is bigger, right? Bigger purpose. A bigger meaning. And so we have run the\\nexperiment of, you know, we\\'re just gonna use\\nscience and rationality and kind of, you know, everything\\'s just gonna\\nkind of be as it appears. And large number of people have found that very deeply wanting and\\nhave constructed narratives. And by this is the story\\nof the 20th century, right? Communism, right? Was one of those, communism\\nwas a was a form of this, Nazism was a form of this. You know, some people, you know, you can see movements\\nlike this playing out all over the world right now. - So you constructed a kind of devil, a kind of source of evil, and we\\'re going to transcend beyond it. - Yeah. And (indistinct)\\nwhen you see a Miller cult, they put a really specific point on it, which is end of the world, right, there is some change coming. And that change that\\'s\\ncoming is so profound and so important that\\nit\\'s either gonna lead to utopia or hell on earth. Right? And it is going to, and then, you know, it\\'s like what if you actually knew that was going to happen, right? What would you do? Right? How would you prepare yourself for it? How would you come together with a group of like-minded people, right? How would you, what would you do? Would you plan like Cassius\\nof weapons in the woods? Would you like, you know, I don\\'t know if create\\nunderground buckers, would you, you know, spend your\\nlife trying to figure out a way to avoid having it happen? - Yeah. That\\'s a really\\ncompelling, exciting idea to have a club over. To have a little bit of travel, like a get', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 517638, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content=' now. - So you constructed a kind of devil, a kind of source of evil, and we\\'re going to transcend beyond it. - Yeah. And (indistinct)\\nwhen you see a Miller cult, they put a really specific point on it, which is end of the world, right, there is some change coming. And that change that\\'s\\ncoming is so profound and so important that\\nit\\'s either gonna lead to utopia or hell on earth. Right? And it is going to, and then, you know, it\\'s like what if you actually knew that was going to happen, right? What would you do? Right? How would you prepare yourself for it? How would you come together with a group of like-minded people, right? How would you, what would you do? Would you plan like Cassius\\nof weapons in the woods? Would you like, you know, I don\\'t know if create\\nunderground buckers, would you, you know, spend your\\nlife trying to figure out a way to avoid having it happen? - Yeah. That\\'s a really\\ncompelling, exciting idea to have a club over. To have a little bit of travel, like a get together on a Saturday\\nnight and drink some beers and talk about the end of the world and how you are the only\\nones who have figured it out. - Yeah. And then once you lock in on that, like how can you do anything\\nelse with your life? Like this is obviously the\\nthing that you have to do. And then there\\'s a psychological\\neffect that you alluded to. There\\'s a psychological effect. If you take a set of true\\nbelievers and you leave them to themselves, they get\\nmore radical. Right. \\'Cause they self radicalize each other. - That said, it doesn\\'t mean they\\'re not sometimes right. - Yeah. The end of the world might be. Yes. Correct. Like they might be right. - [Lex] Yeah. - But like-- - [Lex] I have some pamphlets for you. - Exactly. - But I mean we\\'ll talk\\nabout nuclear weapons \\'cause you have a really\\ninteresting little moment that I learned about in\\nyour essay, but you know, sometimes it could be right. - [Marc] Yeah. - \\'Cause we\\'re still, you were developing more and\\nmore powerful technologies in this case, and we don\\'t know what the impact it will\\nhave on human civilization while we can highlight all\\nthe different predictions about how it\\'ll be positive, but the risks are there and\\nyou discuss some of them. - Well, the steel man, the\\nsteel man is the steel man. Well actually, the steel\\nman and his reputation are the same, which is you can\\'t predict what\\'s gonna happen. Right. You can\\'t rule out that this\\nwill not end everything. Right. But the response to that\\nis you have just made a completely non-scientific claim. You\\'ve made a religious\\nclaim, not a scientific claim. - How does it get disproven? - And there\\'s no, by definition with these kinds of claims, there\\'s no way to disprove them. Right? And so there there\\'s no, you\\njust go right on the list. There\\'s no hypothesis, there\\'s no testability of the hypothesis. There\\'s no way to falsify the hypothesis, there\\'s no way to measure\\nprogress along the arc. Like it\\'s just all completely missing. And so it\\'s not scientific and. - I don\\'t think it\\'s completely missing. It\\'s somewhat missing. So for example, the people that say AI\\'s gonna kill all of us. I mean, they usually have\\nideas about how to do that. Whether it\\'s the people\\nclub maximizer or, you know, it escapes there\\'s mechanism\\nby which you can imagine it killing all humans. - [Marc] Models. - And you can disprove it by saying there\\'s a limit to the speed at which intelligence increases. Maybe show that like the sort\\nof rigorously really described model, like how it could\\nhappen and say, no, there, here\\'s a physics limitation. There\\'s like a physical\\nlimitation to how these systems would actually do damage\\nto human civilization. And it is possible they\\nwill kill 10 to 20% of the population, but it seems impossible\\nfor them to kill 99%. - It was practical\\ncounterarguments. Right. So you mentioned\\nbasically what I described as the thermodynamic counterargument, which, so sitting here today, it\\'s like where with the\\nevil AGI get the GPU. \\'Cause like they don\\'t exist. So if you\\'re gonna have a\\nvery frustrated baby evil AGI, who\\'s gonna be like trying to\\nbuy Nvidia stock or something to get them to finally\\nmake some chips, right? So the serious form of that\\nis the thermodynamic argument, which is like, okay, where\\'s\\nthe energy gonna come from? Where\\'s the processor gonna be running? Where\\'s the data center\\ngonna be happening? How is this gonna be\\nhappening in secret such that, you know, it\\'s not, you know, so that\\'s a practical counter argument to the runaway AGI thing. I have a but I have and we\\ncan argue that, discuss that. I have a deeper objection to it, which is it\\'s, this is all forecasting. It\\'s all modeling, it\\'s\\nall future prediction. It\\'s all future hypothesizing. It\\'s not science. - [Lex] Sure. - It is not. It is the\\nopposite of science. So the, I\\'ll pull up Carl Sagan\\nextraordinary claims require extraordinary proof, right? These are extraordinary claims. The policies that are being\\ncalled for right to prevent this are of extraordinary magnitude that, and I think we\\'re gonna\\ncause extraordinary damage. And this is all being done\\non the basis of something that is literally not scientific. It\\'s not a testable hypothesis. - So the moment you say\\nAI\\'s gonna kill all of us, therefore we should ban it, or that we should regulate\\nall that kind of stuff, that\\'s when it starts getting serious. - Or start, you know, military\\nairstrikes and data centers. - [Lex] Oh boy. - Right? And like. - Yeah. This once get starts. Well, so starts getting real weird. - So here\\'s the problem with Arian cults. They have a hard time\\nstaying away from violence. - Yeah. But violence is so fun. - If you\\'re on the right end of it, they have a hard time avoiding violence. The reason they have a hard\\ntime avoiding violence is if you actually believe the claim. Right. Then what would you do to\\nstop the end of the world? Well, you would do anything, right? And so, and this is\\nwhere you get, and again, if you just look at the\\nhistory of Arian and cults, this is where you get the\\npeople\\'s temple and everybody killing themselves in the jungle. And this is where you get\\nCharles Manson and, you know, sending in to kill the pigs. Like, this is the problem with these. They have a very hard time to run the line at actual violence. And I think in this case, I\\nmean, they\\'re already calling for it like today and you know, where this goes from here\\nis they get more worked up. Like I think is like really concerning. - Okay. But that\\'s kind of the extremes. So, you know, the extremes of\\nanything are I was concerning. It\\'s also possible to kind\\nof believe that AI has a very high likelihood\\nof killing all of us. But and therefore we should maybe consider slowing development or regulating, so not violence or any\\nof these kinds of things. But it\\'s saying like, all right, let\\'s take a pause here. You know, you biological\\nweapons, nuclear weapons. Like whoa, whoa, whoa, whoa, whoa. This is like serious stuff.\\nWe should be careful. So it is possible to kinda have a more rational response, right? If you believe this risk is real. - [Marc] Believe. - Yes. So what is it possible to be, have a scientific approach to\\nthe prediction of the future? - I mean, we just went\\nthrough this with COVID. What do we know about modeling? - [Lex] Well, I mean. - What did we learn about\\nmodeling with COVID? - [Lex] There\\'s a lot of lessons. - They didn\\'t work at all. - [Lex] They worked poorly. - The models were terrible,\\nthe models were useless. - I don\\'t know if the models\\nwere useless or the people interpreting the models and\\nthen decentralized institutions that were creating policy\\nrapidly based on the models and leveraging the models in order to support their narratives versus actually\\ninterpreting the error bars and the models and all that kind of stuff. - What you had with COVID, my view you had with COVID\\nis you had these experts showing up and they\\nclaimed to be scientists and they had no testable\\nhypotheses whatsoever. They had a bunch of models. They had a bunch of forecasts\\nand they had a bunch of theories and they laid these out in front of policy makers and policy makers freaked\\nout and panicked. Right. And implemented a whole bunch of like, really like terrible decisions\\nthat we\\'re still living with the consequences of, and there was never any\\nempirical foundation to any of the models. None of them ever came true. - Yeah. To push back. There were certainly\\nBaptist and bootleggers in the context of this pandemic, but there\\'s still a\\nusefulness to models. No. - So not if they\\'re, I mean not if they\\'re\\nreliably wrong, right? Then they\\'re actually\\nlike anti-useful. Right. They\\'re actually damaging. - But what do you do with the pandemic? What do you do with any kind of threat? Don\\'t you want to kind of\\nhave several models to play with as part of this discussion of like, what the hell do we do here? - I mean, do they work? Because they\\'re an expectation\\nthat they actually like work that they have actual predictive value. I mean, as far as I can tell with COVID, the policymakers just si up\\nthemselves into believing that there was sub, I\\nmean, look, the scientists, the scientists were at fault. The quote unquote scientists showed up. So I had some insight into this. So there was a, or remember the Imperial College models out of London were the\\nones that were like, these are the gold standard models. So a friend of mine runs\\na big software company and he was like, wow, this is\\nlike, COVID is really scary. And he is like, you know, he contacted this research\\nand he is like, you know, do you need some help? You\\'ve been just building\\nthis model on your own for 20 years. Do you need some, would you like us our coders\\nto basically restructure it so it can be fully adapted for COVID? And the guy said yes\\nand sent over the code and my friend said it was\\nlike the worst spaghetti code he\\'s ever seen. - That doesn\\'t mean it\\'s\\nnot possible to construct a good model of pandemic\\nwith the correct air bars, with a high number of parameters\\nthat are continuously, many times a day updated\\nas we get more data about a pandemic. I would like to believe when\\na pandemic hits the world, the best computer scientists in the world, the best software engineers\\nrespond aggressively and as input take the data\\nthat we know about the virus and it\\'s an output say\\nhere is what\\'s happening in terms of how quickly it\\'s spreading, what that lead in terms of\\nhospitalization and deaths and all that kind of stuff. Here\\'s how likely, how\\ncontagious it likely is. Here\\'s how deadly it likely is based on different conditions, based on different ages and demographics and all that kind of stuff. So here\\'s the best kinds of policy. It feels like you could have models, machine learning that like kind of, they don\\'t perfectly predict the future, but they help you do something \\'cause there\\'s pandemics\\nthat are like, meh, they don\\'t really do much harm. And there\\'s pandemics,\\nyou can imagine them, they could do a huge amount of harm. Like they can kill a lot of people. So you should probably have\\nsome kind of data-driven models that keep updating, that allow you to make\\ndecisions that based like where, how bad is this thing? Now you can criticize how\\nhorrible all that went with the response to this pandemic, but I just feel like there\\nmight be some value to models. - So to be useful at some point it has to be predictive. Right? So and the easy thing\\nfor me to do is to say, obviously you\\'re right. Obviously I wanna see that\\njust as much as you do. \\'cause anything that makes\\nit easier to navigate through society through a\\nwrenching, you know, risk like that sounds great. You know, the harder objection to it is just simply\\nyou are trying to model a complex dynamic system\\nwith 8 billion moving parts. Like not possible. - [Lex] It\\'s very tough. - Can\\'t be done, complex\\nsystems can\\'t be done. - Machine learning says hold my beer. But well, it\\'s possible. No? - I don\\'t know. I would like to believe that it is. I\\'ll put it this way. I think where you and I\\nwould agree is I think we would like that to be the case. We are strongly in favor of it. I think we would also\\nagree that no such thing with respect to COVID or\\npandemics no such thing. At least neither you\\nnor I think are aware. I\\'m not aware of anything like that today. - My main worry with the\\nresponse to the pandemic is that same as with aliens, is that even if such a thing existed, and it\\'s possible it existed, the policymakers were\\nnot paying attention. Like there was no mechanism\\nthat allowed those kinds of models to percolate all. - Oh, I think we had the\\nopposite problem during COVID. I think the policymakers, I think these people with\\nbasically fixed science had too much access to the policymakers. - Well, right. And well, but the policy\\nmakers also wanted, they had a narrative in\\nmind and they also wanted to use whatever model\\nthat fit that narrative - [Marc] Oh, sure. - To help them out. So like, it felt like\\nthere was a lot of politics and not enough science. - Although a big part\\nof what was happening, a big reason we got lockdowns\\nfor as long as we did, was because these scientists\\ncame in with these like doomsday scenarios that were like, just like completely off the hook. - Scientists in quotes, let\\'s not-- - [Marc] Quote unquote scientists. - Let\\'s not, okay,\\nlet\\'s give love science. So here\\'s science that is the way out. - Science is a process\\nof testing hypotheses. Modeling does not involve\\ntestable hypotheses. Right. Like, I don\\'t even know that. I actually don\\'t even know that modeling actually\\nqualifies as science. Maybe that\\'s a side conversation. We could have some time over a beer. - Oh, that\\'s a really interesting part. What do we do about the future? I mean, what\\'s-- - So number one is when\\nwe start with number one, humility goes back to this thing of how do we determine the truth. Number two is we don\\'t believe, you know, it\\'s the old, I\\'ve gotta hammer everything\\nlooks like a nail, right? I\\'ve got, oh, this is one\\nof the reasons I gave you, I gave Alexa book, which the topic of the\\nbook is what happens when scientists basically\\nstray off the path of technical knowledge and\\nstart to weigh in on politics and societal issues. - In this case, philosophers. - Well in this case philosophers. But he actually talks in this\\nbook about, like Einstein, he talks about, actually about\\nthe nuclear age in Einstein. He talks about the\\nphysicists actually doing very similar things at the time. - The book is When Reason Goes On Holiday, Philosophers in Politics by Nevin. - And it\\'s just a story. It\\'s a story. There are other books on this topic, but this is a new one that\\'s really good this is just a story of what happens when experts\\nin a certain domain decide to weigh in and become\\nbasically social engineers and political, you know,\\nbasically political advisors. And it\\'s just a story of just\\ninning catastrophe. Right. And I think that\\'s what\\nhappened with COVID again. - Yeah. I found this book\\na highly entertaining and eye-opening read filled\\nwith amazing anecdote of a rationality and craziness\\nby famous Resa philosophers. - I definitely, after you read this book, you will not look at Einstein the same. - [Lex] Oh boy. - Yeah. - Don\\'t destroy my heroes. - He will not be a hero of yours anymore. Sorry. You probably couldn\\'t,\\nyou shouldn\\'t read the book. - All right. - But here\\'s the thing. The AI risk people, they don\\'t even have the COVID model, at least not that I\\'m aware of. - [Lex] No. - Like there\\'s not even the\\nequivalent of the COVID model. They don\\'t even have the spaghetti code. They\\'ve got a theory and a\\nwarning and a this and the that. And like, if you ask like,\\nokay, well here\\'s, I mean, the ultimate example is,\\nokay, how do we know, right? How do we know that an AI is running away? Like how do we know that the boom takeoff thing\\nis actually happening? And the only answer that\\nany of these guys have given that I\\'ve ever seen is, oh,\\nit\\'s when the loss rate, the loss function and the\\ntraining drops, right? That\\'s when you need to like\\nshut down the data center. Right? And it\\'s like, well that\\'s also what happens when you\\'re successfully training a model. Like, what even this is not science, this is not, it\\'s not\\nanything, it\\'s not a model, it\\'s not anything. There\\'s nothing to arguing with. It is like, you know,\\npunching jello, like there, there\\'s what do you even respond to? - So just put push back on that. I don\\'t think they have good metrics of when the film is happening. But I think it\\'s possible to have that. Like just as you speak now, I mean it\\'s possible to imagine\\nthere could be measures. - It\\'s been 20 years. - No, for sure. But it is been only weeks\\nsince we had a big enough breakthrough in language models. We can start to actually have this, the thing is the AI doer stuff didn\\'t have any actual systems to really work with. And now there\\'s real systems\\nyou can start to analyze like, how does this stuff go wrong? And I think you kind\\nof agree that there is a lot of risks that we can analyze. The benefits outweigh\\nthe risks in many cases. - Well, the risks are not existential. - [Lex] Yes. Well. - Not in the phone paper\\nclip. Let me, okay. There\\'s another slide of hand\\nthat you just alluded to. There\\'s another slide\\nof hand that happens, which is very interesting. - I\\'m very good at the\\nslide of hand thing. - Which is very not scientific. So the book Super Intelligence, right, which is like the Nick Bostrom\\'s book, which is like the origin\\nof a lot of this stuff, which was written, you know, whatever, 10 years ago or something. So he does this really\\nfascinating thing in the book, which is he basically says\\nthere are many possible routes to machine intelligence,\\nto artificial intelligence. And he describes all the different routes to artificial intelligence,\\nall the different possible, everything from biological\\naugmentation through to, you know, all these different things. One of the ones that\\nhe does not describe is large language models because of course the book was written\\nbefore they were invented. And so they didn\\'t exist. In the book, he describes them all and then he proceeds to treat them all as if they\\'re\\nexactly the same thing. He presents them all as sort\\nof an equivalent risk to be dealt with in an equivalent\\nway to be thought about the same way. And then the risk, the quote unquote risk that\\'s actually emerged is actually a completely different technology than he was even imagining. And yet all of his theories\\nand beliefs are being transplanted by this movement, like straight onto this new technology. And so again, like there\\'s no other area of science or technology\\nwhere you do that. Like when you\\'re dealing\\nwith like organic chemistry versus inorganic chemistry,\\nyou don\\'t just like say, oh, with respect to like either\\none, basically maybe, you know, growing up in eating\\nthe world or something, like they\\'re just gonna\\noperate the same way. Like you don\\'t. - But you can start talking about like, as we get more and more actual systems that start to get more\\nand more intelligent, you can start to actually have more scientific arguments here. - [Marc] Oh yeah. - Like, you know, high level, you can talk about the threat\\nof autonomous weapon systems back before we had any\\nautomation in the military. And that would be like\\nvery fuzzy kind of logic. But the more and more you\\nhave drones that are becoming more and more autonomous, you\\ncan start imagining, okay, what does that actually look\\nlike and what\\'s the actual threat of autonomous weapons systems? How does it go wrong? And still it\\'s very vague, but you start to get a\\nsense of like, all right, it should probably be illegal or wrong or not allowed\\nto do like mass deployment of fully autonomous drones that are doing aerial strikes. - [Marc] Oh no. - On large areas. - [Marc] I think it should be required. - Right? So that\\'s a no. - No, no. I think it should be required that only aerial vehicles are automated. - Okay. So you wanna go the other way? - I wanna go the other way. - So that, okay. - I think it\\'s obvious that\\nthe machine is gonna make a better decision than the human pilot. I think it\\'s obvious that\\nit\\'s in the best interest of both the attacker and the\\ndefender and humanity at large. If machines are making\\nmore of these decisions than not people, I think people make terrible\\ndecisions in times of war. - But like, there\\'s ways\\nthis can go wrong too, right? - Well, it wars go terribly wrong now. This goes back to the whole, this is that whole thing\\nabout like the self-drive. Does the self-driving\\ncar need to be perfect versus does it need to be\\nbetter than the human driver? Does the automated\\ndrone need to be perfect or does it need to be better than a human pilot at making decisions under enormous amounts of\\nstress and uncertainty? - Yeah, well, on average, the worry that AI folks\\nhave is the runaway. - They\\'re gonna come alive. Right? That then again, that\\'s\\nthe slight of hand, right. - Or not not come alive.\\nWell, no, hold on a second. You lose control as well. You lose control. - But then they\\'re gonna\\ndevelop goals of their own. They\\'re gonna develop a mind of their own, they\\'re gonna develop their own. Right. - No more, more like\\nChernobyl style meltdown, like just bugs in the code\\naccidentally, you know, force you like the results in the bombing of like large civilian areas. - [Marc] Okay. And to a degree that\\'s not possible in the current military strategies, - [Marc] I don\\'t know. - Control by humans. - Well, actually we\\'ve been\\ndoing a lot of mass bombings to cities for a very long time. - Yes. And a lot of civilians died. - And a lot of civilians died. And if you watch the documentary,\\nthe Fog of War McNamara, it spends a big part of it\\ntalking about the fire bombing of the Japanese cities. Burning them straight\\nto the ground. Right. The devastation in Japan, American military fire bombing\\nthe cities in Japan was considerably bigger devastation\\nthan the use of nukes. Right. So we\\'ve been doing\\nthat for a long time. We also did that to Germany, by the way Germany did that to us, right? Like that\\'s an old tradition. The minute we got airplanes, we started doing indiscriminate bombing. - So one of the things-- - [Marc] We\\'re still doing it. - The modern US military can do with technology with automation, but technology more broadly is higher and higher precision strikes. - Yeah, I was saying, so precision is obviously precision and this is a (indistinct) right? So there was this big advance this big advance called the (indistinct) which basically was\\nstrapping a GPS transceiver to an unguided bomb and turning it into a guided bomb. And yeah, that\\'s great. Like look, that\\'s been a big advance, but, and that\\'s like a baby\\nversion of this question, which is okay, do you\\nwant like the human pilot, like guessing where the bomb\\'s gonna land? Or do you want like the\\nmachine like guiding the bomb to his destination? That\\'s a baby version of the question. The next version of the question is, do you want the human\\nor the machine deciding whether to drop the bomb? Everybody just assumes the\\nhuman\\'s gonna do a better job for what I think are\\nfundamentally suspicious reasons. - Emotional, psychological reasons. - Yeah. I think it\\'s very clear\\nthat the machine\\'s gonna do a better job making that decision \\'cause the humans making\\nthat decision are got awful. Just terrible. - [Lex] Yeah. - Right. And so yeah. So this is the thing. And then let\\'s get to the, there was, can I one more slide of hand? - [Lex] Yes. - It was in-- - Sure. Please. I\\'m a magician. You could say. - One more slight of hand. These things are gonna be so smart, right? That they\\'re gonna be able to\\ndestroy the world and wreak havoc and like do all this\\nstuff and plan and do all this stuff and evade us and have\\nall their secret things and their secret factories\\nand all this stuff. But they\\'re so stupid that\\nthey\\'re gonna get like, tangled up in their code and that\\'s they\\'re not gonna come alive, but there\\'s gonna be some\\nbug that\\'s gonna cause them to like turn us all on a paper like that. They\\'re not gonna be\\ngenius in every way other than the actual bad goal. And it\\'s just like,\\nand that\\'s just like a, like ridiculous like discrepancy. And you can prove this today, you can actually address\\nthis today for the first time with LLMs which is you can actually ask LLMs to resolve moral dilemmas. So you can create the\\nscenario, you know, dot, dot, dot this, that, this, that, this, that. What would you as the AI\\ndo in the circumstance? And they don\\'t just\\nsay destroy all humans, destroy all humans. They will give you actually\\nvery nuanced moral, practical trade-off oriented answers. And so we actually already\\nhave the kind of AI that can actually like, think this through and can actually like, you know, reason about goals. - Well, the hope is that AGI or like various superintelligent systems have some of the\\nnuance that LLMs have and the intuition is they most likely will because even these LLMs have the nuance. - LLMs are really, this is\\nactually worth spending a moment on LLMs are really interesting to have moral conversations with. And that I just, I didn\\'t expect I\\'d be\\nhaving a moral conversation with the machine in my lifetime. - Wait, and let\\'s remember\\nwe\\'re not really having a conversation with the machine where we\\'re having a conversation with the entirety of the\\ncollective intelligence of the human species. - Exactly. Yes. Correct. - But it\\'s possible to imagine\\nautonomous weapons systems that are not using LLMs. - But if they\\'re smart enough to be scary, where are they not\\nsmart enough to be wise? Like, that\\'s the part where it\\'s like, I don\\'t know how you get\\nthe one without the other. - Is it possible to be super intelligent without being super wise? - Well, again, you\\'re back to that. I mean, then you\\'re back to\\na classic autistic computer, right? Like you\\'re back to just\\nlike a blind rule follower. I\\'ve got this like core,\\nit\\'s the paperclip thing. I\\'ve got this core rule and\\nI\\'m just gonna follow it to the end of the earth. And it\\'s like, well, but everything you\\'re gonna\\nbe doing execute that rule is gonna be super genius level\\nthat humans aren\\'t gonna be able to counter. It\\'s a mismatch in the definition of what the system\\'s capable of. - Unlikely but not impossible, I think. - But again, here you\\nget to like, okay, like. - No, I\\'m not saying when it\\'s\\nunlikely but not impossible. If it\\'s unlikely, that means the fear should be correctly calibrated. - Extraordinary claims\\nrequire extraordinary proof. - Well, okay, so one\\ninteresting sort of tangent, I would love to take on this\\nbecause you mentioned this in the essay about nuclear,\\nwhich was also, I mean, you don\\'t shy away from a\\nlittle bit of of a spicy take. So Robert Oppenheimer famously said, now I am become death\\nthe destroyer of worlds as he witnessed the first\\ndestination of a nuclear weapon on July 16th, 1945. And you write an interesting\\nhistorical perspective, \"Recall that John Van Neuman responded to Robert Oppenheimer\\'s famous\\nhand wringing about the role of creating nuclear weapons, which you note helped end World War II and prevent World War III\\nwith some people confess guilt to claim credit for the sin.\" And you also mentioned\\nthat Truman was harsher after meeting Oppenheimer. He said that \"Don\\'t let that\\ncry baby in here again.\" - Real quote, by the\\nway, from Dean Atchison. - Boy. - \\'Cause Oppenheimer didn\\'t\\njust say the famous line. - [Lex] Yeah. - He then spent years going\\naround basically moaning him, you know, going on TV and going into going into the White House and basically like, just like doing this hair shirt, you know, thing self, you know, this\\nsort of self-critical like, oh my god, I can\\'t believe how awful I am. - So he\\'s widely\\nconsidered perhaps of the, because of the hang ringing\\nas the father of the tom bomb. - [Marc] Yeah. - This is Van Norman\\'s criticism\\nof him is he tried to have his cake and eat it too. Like he wanted to and Van Norman of course a very different kind of personality and\\nhe\\'s just like, yeah, good. This is like an incredibly\\nuseful thing. I\\'m glad we did it. - Yeah. Well Van Norman is\\nis widely credit as being one of the smartest humans\\nof the 20th century. Certain people. Everybody says like, this is the smartest person I\\'ve ever met when they\\'ve met him. Anyway, that doesn\\'t mean,\\nsmart doesn\\'t mean wise. So yeah, I would love to sort of, can you make the case both\\nfor and against the critique of Oppenheimer here? \\'Cause we\\'re talking\\nabout nuclear weapons. Boy, do they seem dangerous? - Well so, the critique goes deeper and I left this out. Here\\'s the real substance, I left it out \\'cause I didn\\'t wanna dwell on nukes in my AI paper. But here\\'s the deeper thing that happened and I\\'m really curious, this\\nmovie coming out this summer, I\\'m really curious to see\\nhow far he pushes this. \\'cause this is the real\\ndrama in the story, which is, it wasn\\'t just a question\\nof our nukes, good or bad, it was a question of should\\nRussia also have them? And what actually happened was Russia got the American invented the bomb. Russia got the bomb, they got the bomb through espionage, they got American and you know, they got American scientists\\nand foreign scientists working on the American project. Some combination of the two\\nbasically gave the Russians the designs for the bomb. And that\\'s how the Russians got the bomb. There\\'s this dispute to this\\nday of Oppenheimer\\'s role in that if you read all the histories, the kind of composite\\npicture, and by the way, we now know a lot actually\\nabout Soviet espionage in that era \\'cause there\\'s been all this declassified\\nmaterial in the last 20 years that actually shows a lot\\nof very interesting things. But if you kinda read all the histories, which you kinda get is Oppenheimer\\nhimself probably was not he probably did not hand over\\nthe nuclear secrets himself. However, he was close\\nto many people who did. Including family members. And there were other members\\nof the Manhattan Project who were Russian, Soviet SS\\nand did hand over the bomb. And so the view of that\\nOppenheimer and people like him had that this thing is awful\\nand terrible and oh my god. And you know, all this stuff you could\\nargue fed into this ethos at the time that resulted\\nin people thinking that the Baptists thinking that the only principle\\nthing to do was to give the Russians the bomb. And so the moral beliefs on this thing and the public discussion and the role that the inventors\\nof this technology play, this is the point of this book, when they kind of take on this\\nsort of public intellectual, moral kind of thing, it can\\nhave real consequences, right? Because we live in a very\\ndifferent world today because Russia got the\\nbomb than we would\\'ve lived in had they not gotten the bomb right. The entire 20th century, second half of the 20th\\ncentury would\\'ve played out very different had those people\\nnot given Russia the bomb. And so the stakes were very high then. The good news today is\\nnobody\\'s sitting here today, I don\\'t think worrying about\\nlike an analogous situation with respect to like, I\\'m not really worried that\\nSam Altman\\'s gonna decide to give, you know, the\\nChinese, the design for AI, although he did just speak\\nat a Chinese conference, which is in interesting. But however, I don\\'t think\\nthat\\'s what\\'s at play here, but what\\'s at play here are\\nall these other fundamental issues around what do\\nwe believe about this and then what laws and\\nregulations and restrictions that we\\'re gonna put on it. And that\\'s where I draw\\nlike a direct straight line. And anyway, and my reading\\nof the history on nukes is like the people who were doing\\nthe full hair shirt public, this is awful. This is terrible. Actually had like\\ncatastrophically bad results from taking those views. And that\\'s what I\\'m worried\\nit\\'s gonna happen again. - But is there a case to be\\nmade that you really need to wake the public up to the dangers of nuclear weapons when\\nthey were first dropped? Like really like educate them on like, this is extremely dangerous\\nand destructive weapon. - I think the education\\nkind of happened quick and early, like-- - [Lex] How? - It was pretty obvious. - [Lex] How? - We dropped one bomb and\\ndestroyed an entire city. - Yeah. So 80,000 people dead. - [Marc] Yep. - But. - [Marc] And look. But-- - I don\\'t like the reporting of that. You can report that in all kinds of ways. - [Marc] Oh, there wars. - You can do all kinds of slants. Like war is horrible. War is terrible. You can do, you can make\\nit seem like nuclear, the use of nuclear weapons\\nis just a part of war and all that kind of stuff. Something about the\\nreporting and the discussion of nuclear weapons resulted\\nin us being terrified in awe of the power of nuclear weapons and that potentially fed\\nin a positive way towards the game theory of\\nmutual issue destruction. - Well, so this gets to what actually, let\\'s get to what actually happens. - [Lex] Some of us, me\\nplaying devil\\'s advocate here. - Yeah, yeah, sure. Of course. Let\\'s get to what\\nactually happened and then kind of back into that. So what actually happened, I believe, and again I think this is a\\nreasonable reading of history, is what actually happened\\nwas nukes then prevented World War III and they\\nprevented World War III through the game theory\\nof mutually assured destruction had nukes not existed. Right. There would\\'ve been no reason why the Cold War did not go hot. Right. And then there and then, you know, and the military planners\\nat the time, right, thought both on both sides\\nthought that there was gonna be World War III on the planes of Europe and they thought there was gonna be like a hundred million people dead. Right? It was like the most obvious\\nthing in the world to happen. Right? And it\\'s the dog\\nthat didn\\'t bark right? Like it may be like the best\\nsingle net thing that happened in the entire 20th century is\\nthat like that didn\\'t happen. - Yeah. Actually, just on that point, you say a lot of really brilliant things. It hit me just as you were saying it. I don\\'t know why it hit\\nme for the first time, but we got two wars in\\na span of like 20 years. Like we could have kept getting\\nmore and more world wars and more and more ruthless. It actually, you could have\\nhad a US versus Russia war. - You could, by the way you haven\\'t, there\\'s another hypothetical scenario. The other hypothetical scenario is that Americans got the\\nbomb, the Russians didn\\'t. Right? And then America\\'s the big dog and then maybe America\\nwould\\'ve had the capability to actually roll back the iron curtain. I don\\'t know whether\\nthat would\\'ve happened, but like it\\'s entirely possible. Right? And the act of these people who had these moral positions about, \\'cause they could\\nforecast, they could model, they could forecast the future\\nof how the technology would get used, made a horrific mistake. \\'cause they basically ensured that the iron curtain\\nwould continue for 50 years longer than it would\\'ve otherwise. Like, and again, like\\nthese are counter-factuals, I don\\'t know that that\\'s\\nwhat, what would\\'ve happened, but like the decision to hand the bomb over was a big decision made by people who were very\\nfull of themselves. - Yeah. But so me as an America, me as a person that loves America, I also wonder if US was the only ones with the nuclear weapons. - That was the argument for handing that was the guys who (indistinct) the guys who handed over the bomb. That was actually their moral argument. - Yeah. I would probably\\nnot hand it over to, I would be careful about the regimes. You hand it over to there, maybe give it to like\\nthe British or something, or like a democratically-elected\\ngovernment. - Well, look, there are people to this day who think that those bias Soviet spies did the right thing because\\nthey created a balance of terror as opposed\\nto the US having just, and by the way, let me-- - Balance of terror. - [Marc] Let\\'s tell the\\nfull version story has-- - Such a sexy ring to it. - Okay. So the full\\nversion of the story is John Van Norman is a hero\\nof both yours and mind. The full version of the\\nstory is he advocated for a first strike. So when the US had the\\nbomb and Russia did not, he advocated for, he said, we need to strike them right now. - Strike Russia. - [Marc] Yes. - Van Norman. - Yes, because he said\\nWorld War III is inevitable. He was very hardcore. His theory was World\\nWar III is inevitable. We\\'re definitely gonna have World War III. The only way to stop World War\\nIII is we have to take them out right now and we have\\nto take them out right now before they get the bomb. \\'Cause this is our last chance. Now again, like-- - Is this an example of\\nphilosophers and politics? - I don\\'t know if that\\'s in there or not, but this is in the standard. - No, but it is meaning is that. - Yeah, this is on the other side. So, most of the case studies, most of the case studies\\nin books like this are the crazy people on the left. Van Norman is a story arguably of the crazy people on the right. - Yes. Stick to computing, John. - Well. This is the thing, and this is the general principle. Getting back to our core\\nthing, which is like, I don\\'t know whether any of\\nthese people should be making any of these calls. Because there\\'s nothing in\\neither Van Norman\\'s background or Oppenheimer\\'s background or any of these people\\'s background that qualifies them as moral authorities. - Yeah. Well this actually\\nbrings up the point of, in AI, who are the good people to reason about the\\nmorality of the ethics, the outside of these risks, outside of like the more\\ncomplicated stuff that you, you agree on is, you know, this will go into the hands of bad guys and all the kinds of ways\\nthey\\'ll do is interesting and dangerous, is dangerous in interesting unpredictable ways. And who is the right person? Who are the right kinds of\\npeople to make decisions, how to respond to it?\\nOr is the tech people? - So the history of these fields, this is what he talks about in the book, the history of these fields,\\nis that the competence and capability and\\nintelligence and training and accomplishments of senior\\nscientists and technologists working on a technology\\nand then being able to then make moral judgments\\nin the use of that technology. That track record is terrible that track record is like\\ncatastrophically bad. The people-- - Just the linger, the people that develop that\\ntechnology are usually not going to be the right people. - Well why would they? So\\nthe claim is of course, they\\'re the knowledgeable ones. But the the problem is they\\'ve\\nspent their entire life in a lab. Right. They\\'re not theologians. Well, so what you find,\\nwhat you find when you read, when you read this, when\\nyou look at these histories, what you find is they generally are very thinly informed on history, on sociology, on theology,\\non morality, on ethics. They tend to manufacture their\\nown worldviews from scratch. They tend to be very sort of thin. They\\'re not remotely the\\narguments that you would be having if you got like a group of\\nhighly qualified theologians or philosophers or, you know. - Well, let me sort of,\\nas the devil\\'s advocate, takes a simple whiskey say\\nthat I agree with that. But also it seems like the\\npeople who are doing kind of the ethics departments and these tech companies\\ngo sometimes the other way. - [Marc] Yes, they\\'re definitely. - Which they\\'re not nuanced\\non history or theology or this kind of stuff. It almost becomes a kind\\nof outraged activism towards directions that don\\'t seem to be grounded in history and\\nhumility and nuance. It\\'s again, drenched with arrogance. So-- - [Marc] Definitely. - I\\'m not sure which is worse. - Oh no, they\\'re both bad. Yeah. So definitely not them either. - So, but I guess. - Well look, this is a hard. - Yeah, it\\'s a hard problem. - This is a hard problem. This goes back to where we started, which is, okay, who has the truth? And it\\'s like, well, you know, like how does societies\\narrive at like truth and how do we figure these things out and like our elected leaders\\nplay some role in it. You know, we all play some role in it. There have to be some set\\nof public intellectuals at some point that bring, you know, rationality and judgment\\nand humility to it. Those people are few and far between. We should probably prize them very highly. - Yeah. So celebrate humility\\nin our public leaders. So getting to risk number two, will AI ruin our society\\nshort version as you write, if the murder robots don\\'t\\nget us the hate speech and misinformation will. And the action you recommend in short, don\\'t let the thought police suppress AI. Well what is this risk of\\nthe effect of misinformation of society that\\'s going\\nto be catalyzed by AI? - Yeah, so this is the social media, this is what you just alluded to. It\\'s the activism kind\\nof thing that\\'s popped up in these companies in the industry. And it\\'s basically, from my perspective, it\\'s basically part two\\nof the war that played out over social media over the last 10 years, \\'cause you probably remember\\nsocial media 10 years ago, was basically who even wants this? Who wants a photo of what\\nyour cat had for breakfast? Like, this stuff is like silly and trivial and why can\\'t these nerds like figure out how to invent something\\nlike useful and powerful? And then, you know, certain things happened\\nin the political system. And then it sort of, the polarity on that\\ndiscussion switched all the way to social media is like\\nthe worst, most corrosive, most terrible, most awful\\ntechnology ever invented. And then it leads to, you\\nknow, terrible of the wrong, you know, politicians and\\npolicies and politics and like, and all this stuff. And that all got catalyzed into\\nthis very big kind of angry movement both inside and\\noutside the companies to kind of bring social media to heal. And that got focused in\\nparticularly on two topics, so-called hate speech and\\nso-called misinformation. And that\\'s been the saga playing out for the last decade. And I', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 517638, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content='. And it\\'s basically, from my perspective, it\\'s basically part two\\nof the war that played out over social media over the last 10 years, \\'cause you probably remember\\nsocial media 10 years ago, was basically who even wants this? Who wants a photo of what\\nyour cat had for breakfast? Like, this stuff is like silly and trivial and why can\\'t these nerds like figure out how to invent something\\nlike useful and powerful? And then, you know, certain things happened\\nin the political system. And then it sort of, the polarity on that\\ndiscussion switched all the way to social media is like\\nthe worst, most corrosive, most terrible, most awful\\ntechnology ever invented. And then it leads to, you\\nknow, terrible of the wrong, you know, politicians and\\npolicies and politics and like, and all this stuff. And that all got catalyzed into\\nthis very big kind of angry movement both inside and\\noutside the companies to kind of bring social media to heal. And that got focused in\\nparticularly on two topics, so-called hate speech and\\nso-called misinformation. And that\\'s been the saga playing out for the last decade. And I don\\'t even really want\\nto even argue the pros and cons of the sides just to observe that\\'s been like a huge fight and has had, you know, big consequences to how\\nthese companies operate. Basically that same, those\\nsame sets of theories, that same activist approach, that same energy as being\\ntransplanted straight to AI. And you see that already happening. It\\'s why, you know, ChatGPT will answer, let\\'s say certain\\nquestions and not others. It\\'s why it gives you the\\ncanned speech about, you know, whenever it starts with,\\nas a large language model, I cannot, you know, basically means that somebody\\nhas reached in there and told that it can\\'t talk about certain topics. - Do you think some of that is good? - So it\\'s an interesting question. So a couple observations. So, one is the people who find this the most frustrating are the people who are worried\\nabout the murder robots, right? So, and in fact so called\\nX risk people, right? They started with the term AI safety, the term became AI alignment. When the term became AI alignment is when this switch happened\\nfrom we\\'re worried it\\'s gonna kill us all to\\nwe\\'re worried about hate speech and misinformation. - [Lex] Sure. - The AI X risk people have\\nnow renamed their thing AI not kill everyone-ism, which I have to admit is a catchy term. And they are very frustrated\\nby the fact that the hate speech sort of activist driven\\nhate speech misinformation kind of thing is taking over. Which is what\\'s happened is taken over, the AI ethics field has been\\ntaken over by the hate speech misinformation people. You know, look, would I like to live in a world\\nin which like everybody was nice to each other all the\\ntime and nobody ever said anything mean and nobody ever\\nused a bad word and everything was always accurate and honest. Like, that sounds great. Do I wanna live in a world\\nwhere there\\'s like a centralized thought police working through\\nthe tech companies to enforce the view of a small set of\\nelites that they\\'re gonna determine what the rest\\nof us think and feel like? Absolutely not. - There could be a middle\\nground somewhere like Wikipedia type of moderation. There\\'s moderation of Wikipedia\\nthat is somehow crowdsourced where you don\\'t have centralized elites, but it\\'s also not completely\\njust a free for all because if you have the\\nentirety of human knowledge at your fingertips, you\\ncan do a lot of harm. Like if you have a good assistant that\\'s completely uncensored, they can help you build a bomb, they can help you mess with\\npeople\\'s physical wellbeing. Right. If they, because that information is\\nout there on the internet and so presumably there\\'s, it would be, you could see the positives\\nin censoring some aspects of an AI model when it\\'s helping you\\ncommit literal violence. - Yeah. And there\\'s a section\\nlater section of the essay where I talk about bad\\npeople doing bad things. - [Lex] Yes. - Right. Which and there\\'s this, there\\'s a set of things that\\nwe should discuss there. - [Lex] Yeah. - What happens in practice is these lines, as you alluded to this already, these lines are not easy to draw. And what I\\'ve observed in\\nthe social media version of this is like, the way I describe it as the slippery slope is not a fallacy, it\\'s an inevitability. The minute you have this kind\\nof activist personality that gets in a position to make these decisions they take it straight to infinity. Like, it goes into the crazy\\nzone like almost immediately and never comes back because\\npeople become drunk with power. Right. And look, if you\\'re\\nin the position to determine what the entire world thinks and feels and reads and says like, you\\'re gonna take it and you\\nknow, Elon has, you know, ventilated this with the\\nTwitter files over the last, you know, three months and\\nit\\'s just like crystal clear, like how bad it got there now. - [Lex] Yeah. - Reason for optimism\\nis what Elon is doing with community notes. So community notes is actually\\na very interesting thing. So, what Elon is trying to\\ndo with community notes is he\\'s trying to have it where\\nthere\\'s only a community note when people who have previously\\ndisagreed on many topics agree on this one. - Yes, that\\'s what I\\'m\\ntrying to get at is like, there could be Wikipedia like\\nmodels or community notes type of models where allows you\\nto essentially either provide context or sensor in a\\nway that\\'s not resist the slippery slope nature. Power. - Now there\\'s an entirely\\ndifferent approach here, which is basically we have AIs\\nthat are producing content. We could also have ais that\\nare consuming content. Right? And so one of the things that\\nyour assistant could do for you is help you consume\\nall the content, right? And basically tell you\\nwhen you\\'re getting played. So for example, I\\'m gonna\\nwant the AI that my kid uses, right, to be very, you know, child safe and I\\'m gonna want\\nit to filter for him all kinds of inappropriate stuff that\\nhe shouldn\\'t be saying just \\'cause he\\'s a kid. Right? And you see what I\\'m saying\\nis you can implement that. The architectural, you\\ncould say you can solve this on the client side, right? You solving on the server\\nside gives you an opportunity to dictate for the entire\\nworld, which I think is where you take the slippery slope to hell, there\\'s another architectural approach, which is to solve this on the client side, which is certainly what I would endorse. - It\\'s AI risk number five, will AI lead to bad\\npeople doing bad things? And I can just imagine language\\nmodels used to do so many bad things, but the hope is there that\\nyou can have large language models used to then defend\\nagainst it by more people, by smarter people, by more\\neffective people, skilled people, all that kind of stuff. - Three-part argument on\\nbad people doing bad things. So, number one, right? You can use the technology defensively and we should be using AI\\nto build like broad spectrum vaccines and antibiotics for\\nlike bio weapons and we should be using AI to like hunt terrorists and catch criminals and like, we should be doing like all\\nkinds of stuff like that. And in fact, we should be doing those things even just to like go get like, you know, basically go eliminate risk\\nfrom like regular pathogens that aren\\'t like constructed by an AI. So there\\'s the whole\\ndefensive set of things. Second is we have many laws on the books about as actual bad things, right? So it is actually illegal\\nto be a criminal, you know, to commit crimes, to commit\\nterrorist acts to, you know, build pathogens with the intent to deploy them to kill people. And so we have those, we actually don\\'t need new laws for the vast majority of these scenarios. We actually already have the\\nlaws in the book, on the books. The third argument is the minute, and this is sort of the\\nfoundational one that gets really tough, but the minute\\nyou get into this thing, which you were kind of getting\\ninto, which is like, okay, but like, don\\'t you need\\ncensorship sometimes, right? And don\\'t you need restrictions sometimes? It\\'s like, okay, what is the cost of that? And in particular in the\\nworld of open source, right? And so is open source AI\\ngoing to be allowed or not? If open source AI is not allowed, then what is the regime that\\'s\\ngoing to be necessary legally and technically to prevent\\nit from developing? Right? And here again is where you\\nget into and people have proposed that these kinds of things. You get into I would say pretty extreme territory pretty fast. Do we have a monitor\\nagent on every CPU and GPU that reports back to the government? What we\\'re doing with our computers, are we seizing GPU clusters\\nthat get beyond a certain size? Like, and then by the way, how are we doing all that globally, right? And like if China\\'s developing\\nan LLM beyond the scale that we think is allowable,\\nare we gonna invade? Right. And you have figures on the AI X risk side who are advocating any, you know, potentially up to nuclear\\nstrikes to prevent, you know, this kind of thing. And so here you get into this thing and again, you know, maybe\\nyou could maybe say this is, you know, you could even\\nsay this is what good, bad or indifferent or whatever. But like here\\'s the comparison of nukes, the comparison of nukes is very dangerous because one is just nukes, were just, although we can\\ncome back to nuclear power. But the other thing was like with nukes, you could control plutonium, right? You could track plutonium and\\nit was like hard to come by. AI is just math and code, right? And it\\'s in like math\\ntextbooks and it\\'s like, there are YouTube videos that\\nteach you how to build it. And like there\\'s open source,\\nthere\\'s already open source. You know, there\\'s a 40 billion parameter\\nmodel running around already called Falcon Online that\\nanybody can download. And so, okay, you walk down the logic path\\nthat says we need to have guardrails on this. And you find yourself in an authoritarian, totalitarian regime of thought\\ncontrol and machine control that would be so brutal\\nthat you would\\'ve destroyed the society that you\\'re trying to protect. And so I just don\\'t see\\nhow that actually works. - So yeah, you have to\\nunderstand my brain\\'s going full steam ahead here \\'cause I agree with basically\\neverything you\\'re saying, but I\\'m trying to play\\ndevil\\'s advocate here because okay, you\\'re highlighted the fact that there is a slippery\\nslope to human nature. The moment you censor something, you start to censor everything. That alignment starts out sounding nice, but then you start to align to the beliefs of some select group of people. And then it\\'s just your beliefs the number of people\\nyou\\'re aligning to smaller and smaller as that group\\nbecomes more and more powerful. Okay. But that just speaks to the people that censor are usually the assholes and the assholes get richer. I wonder if it\\'s possible\\nto do without that for AI. One way to ask this question is do you think the base models, the baseline foundation\\nmodels should be open sourced? Like, where Marc Zuckerberg\\nis saying they want to do. - So look, I mean I think\\nit\\'s totally appropriate the companies that are in the business of producing a product or service should be\\nable to have a wide range of policies that they put, right? And I\\'ll just, again, I want a heavily censored\\nmodel for my eight year old. Like, I actually want that, like, like I would pay more money\\nfor the ones more heavily censored than the one that\\'s not, right. And so, like there are certainly scenarios where companies will make that decision. Look, an interesting thing you brought up or is this really a speech issue? One of the things that the\\nbig tech companies are dealing with is that content generated\\nfrom an LLM is not covered under section 230, which is the law that protects\\ninternet platform companies from being sued for the\\nuser generated content. And so it is actually-- - [Lex] Oh, wow. - Yes and so there, there\\'s\\nactually a question. I think there\\'s still a question, which is can big American companies actually feel generative AI at all? Or is the liability actually\\ngonna just ultimately convince them that they can\\'t do it? Because the minute the\\nthing says something bad, and it doesn\\'t even\\nneed to be hate speech, it could just be like an\\n(indistinct) it could hallucinate a product, you know, detail\\non a vacuum cleaner, you know, and all of a sudden the\\nvacuum cleaner company sues for misrepresentation. And there\\'s asymmetry there, right? \\'Cause the LLMs gonna be\\nproducing billions of answers to questions and it only needs\\nto get a few wrong to have. - [Lex] So, loss has to get\\nupdated really quick here. - Yeah. And nobody knows\\nwhat to do with that, right? So, so anyway, like there are big, there are big questions around\\nhow companies operate at all. So we talk about those, but then there\\'s this other\\nquestion of like, okay, the open source. So what about open source? And my answer to your\\nquestion is kind of like, obviously yes, the models have, there has to be full open\\nsource here because to live in a world in which that\\nopen source is not allowed is a world of draconian speech control, human control, machine control. I mean, you know, black helicopters with\\njackbooted thugs coming out, repelling down and seizing\\nyour GPU like territory. - [Lex] Well. - No, no, I\\'m a hundred percent serious. - That\\'s you\\'re saying slippery\\nslope always leads there. - No, no, no, no. That\\'s what\\'s required to enforce it. Like how will you enforce a\\nban on open source and AI? - No. Well you could add friction to it, like harder to get the models. \\'Cause people will always\\nbe able to get the models, but it\\'ll be more in the shadows, right? - The leading open source model\\nright now is from the UAE. Like the next time they\\ndo that, what do we do? - [Lex] Yeah. - Like. - Oh, I see you\\'re like. - A 14 year old in Indonesia\\ncomes out with a breakthrough. You know, we talked about most great\\nsoftware comes from a small number of people. Some kid comes out with\\nsome big new breakthrough and quantization or something and has some huge breakthrough. And like, what are we gonna like, invade Indonesia and arrest him? - It seems like in terms of\\nsize of models and effectiveness of models, the big tech companies will\\nprobably lead the way for quite a few years and the question is of what policies they should use? The kid in Indonesia\\nshould not be regulated, but should Google, Meta,\\nMicrosoft, Open AI be regulated? - Well, so, but this goes, okay, so when does it become dangerous? Right. Is the danger that it\\'s as powerful as the current leading commercial model? Or it is just at some\\nother arbitrary threshold? And then by the way, like\\nlook, how do we know, like what we know today is that\\nyou need like a lot of money to like train these things. But there are advances being\\nmade every week on training efficiency and, you know,\\ndata, all kinds of synthetic, you know, look, I don\\'t even like the synthetic data thing we\\'re talking about. Maybe some kid figures out a way to auto-generate synthetic data. - [Lex] That\\'s gonna change everything. - Yeah, exactly. And so like sitting here today, like, the breakthrough just happened, right? You made this point like the\\nbreakthrough just happened. So we don\\'t know what the shape of this technology is gonna be. I mean the big shock\\nhere is that, you know, whatever number of billions\\nof parameters basically represents at least a very big\\npercentage of human thought. Like who would\\'ve imagined that? And then there\\'s already work underway. There was just this paper that\\njust came out that basically takes a gpt three scale model\\nand compresses it down or run on a single 32 core CPU. Like who would\\'ve predicted that? - [Lex] Yeah. - You know, some of these models now you\\ncan run on raspberry pies like today they\\'re very slow,\\nbut like, you know, maybe they\\'ll be a, you know, perceived you have real perform, you know, like it\\'s math and code. And here we\\'re back in here, we\\'re back in, dude, it\\'s math and code. It\\'s math and code, it\\'s\\nmath, code and data. It\\'s bits. - Marc has just like\\nwalked away at this point. You just screw it. I don\\'t know what to do with this. You guys created this\\nwhole internet thing. Yeah, yeah. I mean, I\\'m a huge believer\\nin open source here. - So my argument is we\\'re gonna have, see here\\'s my argument is a, my argument, my full argument is, is AI is gonna be like air,\\nit\\'s gonna be everywhere. Like this is just gonna be in text. It already is, it\\'s gonna be in textbooks\\nand kids are gonna grow up knowing how to do this. And\\nit\\'s just gonna be a thing. It\\'s gonna be in the air\\nand you can\\'t like pull this back anymore. You can\\'t pull back air. And so you just have to figure out how to live in this world, right? And then that\\'s where I think\\nlike all this hand ringing about AI risk is basically\\na complete waste of time, \\'cause the effort should go into okay, what is the defensive approach? And so if you\\'re worried about you know, AI generated pathogens, the\\nright thing to do is to have a permanent project warp speed, right? Funded lavishly. Let\\'s do a Manhattan, let\\'s\\ntalk about Manhattan project, let\\'s do a Manhattan project\\nfor biological defense, right? And let\\'s build ais and let\\'s\\nhave like broad spectrum vaccines where like, we\\'re\\ninsulated from every pathogen. - And well, the interesting\\nthing is because it\\'s software, a kid in his basement, teenager could build like a\\nsystem that defends against like the worst, I mean, and to me\\ndefense is super exciting. It\\'s like, if you believe\\nin the good of human nature for that, most people wanna do good, to be the savior of\\nhumanity is really exciting. - Yes. - Not, okay, that\\'s a dramatic statement. But to help people. - Yeah, of course. Help people. - Yeah. Okay. What about just the jump around, what about the risk of will AI\\nlead to crippling inequality? You know, \\'cause we\\'re kind of saying\\neverybody\\'s life will become better. Is it possible that the\\nrich get richer here? - Yeah, so this goes, this actually ironically\\ngoes back to Marxism. So \\'cause this was the, so the\\ncore claim of Marxism, right? Basically was that the owner, the owners of capital would basically own the means of production. And then over time they\\nwould basically accumulate all the wealth the workers\\nwould be paying in, you know, and getting nothing in return \\'cause they wouldn\\'t be\\nneeded anymore, right? Marx was very worried\\nabout mech what he called mechanization or what later\\nbecame known as automation. And that, you know, the workers would be immiserated\\nand the the capitalists would end up with all. And so this was one of the\\ncore principles of Marxism. Of course it turned out to\\nbe wrong about every previous wave of technology. The reason it, it turned out to be wrong about every previous wave of technology is that the way that the\\nself-interested owner of the machines makes the most money is by providing the production capability in the form of products and services to the most people, the most\\ncustomers as possible, right? The the largest, and this is one of those funny\\nthings where every CEO knows this intuitively, and yet it\\'s like hard to\\nexplain from the outside the way you make the most\\nmoney in any business is by selling to the largest\\nmarket you can possibly get to. The largest market you can\\npossibly get to is everybody on the planet. And so every large company\\ndoes is everything that it can to drive down prices, to\\nbe able to get volumes up, to be able to get to\\neverybody on the planet. And that happened with\\neverything from electricity, it happened with telephones,\\nit happened with radio, it happened with automobiles,\\nit happened with smartphones, it happened with PCs, it\\nhappened with the internet, it happened with mobile broadband. It\\'s happened by the way, with Coca-Cola. It\\'s happened with like every, you know, basically every industrially\\nproduced, you know, good or service people, you wanna drive it to the\\nlargest possible market. And then as proof of that,\\nit\\'s already happened, right? Which is the early\\nadopters of like ChatGPT and Bing are not like, you\\nknow, Exxon and Boeing. They\\'re, you know, your\\nuncle and your nephew, right? It\\'s just like free. It\\'s either freely available\\nonline or it\\'s available for 20 bucks a month or something. But the, you know, these things went this technology went mass market immediately. And so look, the owners of\\nthe means of production, the whoever does this now mentioned these trillion dollar questions. There are people who are gonna\\nget really rich doing this, producing these things, but they\\'re gonna get\\nreally rich by taking this technology to the\\nbroadest possible market. - So yes, they\\'ll get rich, but they\\'ll get rich having\\na huge positive impact on. - Yeah, making the technology\\navailable to everybody. Right. And again, smartphone, same thing. So there\\'s this amazing kind\\nof twist in business history, which is you cannot spend\\n$10,000 on a smartphone, right? You can\\'t spend a\\nhundred thousand dollars, you can\\'t spend a million, like I would buy the\\nmillion dollars smartphone. Like I\\'m signed up for it. Like if it\\'s like, suppose a million dollar\\nsmartphone was like much better than the thousand dollar smartphone. Like I\\'m there to buy\\nit, it doesn\\'t exist. Why doesn\\'t it exist? Apple makes so much more\\nmoney driving the price further down from a thousand dollars than they would trying to harvest, right? And so it\\'s just this\\nrepeating pattern you see over and over again where and\\nwhat\\'s great about it is you, you do not need to rely on\\nanybody\\'s enlightened right? Generosity to do this. You just need to rely on\\ncapitalist self-interest. - What about AI taking our jobs? - Yeah. So very very similar thing here. There\\'s sort of a, there\\'s a\\ncore fallacy which again was very common in Marxism, which is what\\'s called\\nthe lump of labor fallacy. And this is sort of the\\nfallacy that there is only a fixed amount of work\\nto be done in the world. And it\\'s all being done today by people and then if machines do it, there\\'s no other work\\nto be done by people. And that\\'s just a\\ncompletely backwards view on how the economy develops and grows. Because what happens is not\\nin fact that what happens is the introduction of technology\\ninto production process causes prices to fall. As prices fall, consumers\\nhave more spending power. As consumers have more spending power, they create new demand. That new demand then causes\\ncapital and labor to form into new enterprises to\\nsatisfy nuance and needs. And the result is more\\njobs at higher wages. - So nuance and needs, the\\nworries that the creation of nuance and needs at\\na rapid rate will mean there\\'s a lot of turnover in jobs. So people will lose jobs. Just the actual experience\\nof losing a job and having to learn new things and\\nnew skills is painful for the individuals. - Well, two things. One is the new jobs are often much better. So this actually came up\\nthat there was this panic about a decade ago and all\\nthe truck drivers are gonna lose their jobs, right? And number one, that didn\\'t happen \\'cause\\nwe haven\\'t figured out a way to actually finish that yet. But the other thing was\\nlike, look, truck driver, like I grew up in a town\\nthat was basically consisted of a truck stop, right? And I like knew a lot of truck drivers and like truck drivers\\nlive a decade shorter than everybody else. Like, it\\'s actually like a very dangerous, like, they get, like literally they have like\\nhigher rates of skin cancer and on the left side of their, on the left side of their body from being in the sun all the time. The vibration of being\\nin the truck is actually very damaging to your physiology. - And there\\'s actually perhaps partially because of that reason there\\'s a shortage of people who wanna be truck drivers. - Yeah. Like, it\\'s not like\\nthe question always you wanna ask somebody like that\\nis, do you want, you know, do you want your kid to be doing this job? And like most of them will tell you no. Like, I want my kid to\\nbe sitting in a cubicle somewhere like where they\\ndon\\'t have this, like, where they don\\'t die 10 years earlier. And so, the new jobs, number one, the new jobs are often better, but you don\\'t get the new\\njobs until you go through the change. And then to your point,\\nthe training thing, you know, is always the\\nissue is can people adapt? And again, here you need to imagine living in a world in which everybody has the AI\\nassistant capability, right? To be able to pick up new\\nskills much more quickly and be able to have some, you know, be able to have a machine to work with to augment their skills. - It\\'s still gonna be painful, but that\\'s the process of life. - It\\'s painful for some people.\\nI mean there\\'s no, like, there\\'s no question it\\'s\\npainful for some people and they\\'re, you know, they\\'re yes, it\\'s not, again, I\\'m not a utopian on\\nthis and it\\'s not like, it\\'s positive for everybody in the moment, but it has been overwhelmingly\\npositive for 300 years. I mean, look, the concern\\nhere, the concern, this concern has played out for literally centuries and you know, this is the sort of Luddite, you know, the story of the Luddites\\nthat you may remember, there was a panic in the two\\nthousands around outsourcing was gonna take all the jobs. There was a panic in the 2010s\\nthat robots were gonna take all the jobs. In 2019 before COVID we had more jobs at higher wages both in the country and in the world than at any point in human history. And so the overwhelming evidence\\nis that the net gain here is like, just like wildly positive. And most people like overwhelmingly\\ncome out the other side being huge beneficiaries of this. - So you write that the\\nsingle greatest risk, this is the risk you\\'re most convinced by the single greatest risk of AI is that China wins global AI dominance and we the United States\\nand the West do not. Can you elaborate? - Yeah. So this is the\\nother thing which is a lot of this sort of AI\\nrisk debates today sort of assume that we\\'re the\\nonly game in town, right? And so we have the ability to kind of sit in the United States and\\ncriticize ourselves and do, you know, have our\\ngovernment like, you know, beat up on our companies\\nand we\\'ll figure out a way to restrict what our\\ncompanies can do and you know, we\\'re gonna, you know,\\nwe\\'re gonna ban this and ban that, restrict this and do that. And then there\\'s this like\\nother like force out there that like doesn\\'t\\nbelieve we have any power over them whatsoever and they\\nhave no desire to sign up for whatever rules we\\ndecide to put in place and they\\'re gonna do whatever\\nit is they\\'re gonna do. And we have no control over it at all. And it\\'s China and specifically\\nthe Chinese Communist party and they have a completely\\npublicized open, you know, plan for what they\\'re gonna do with AI. And it is not what we have in mind. And not only do they have that as a vision and a plan for their society, but they also have it as a vision and plan for the rest of the world. - So their plan is what? Surveillance? - Authoritarian control. So authoritarian population\\ncontrol you know, good old-fashioned communist\\nauthoritarian control and surveillance and enforcement\\nand social credit scores and all the rest of it. And you are gonna be monitored and metered within an inch of everything all the time. And it\\'s gonna, you know, it\\'s basically the end of human freedom and that\\'s their goal. And you know, they justify it on the basis of that\\'s what leads to peace. - You\\'re worried that the regulating in the United States\\nwill haul progress enough to where the Chinese\\ngovernment would win that race. - So their plan, yeah. Yes, yes. And the reason for that\\nis they, and again, they\\'re very public on this. They have, their plan is to proliferate their approach around the world and they have this program called the Digital Silk Road, right. Which is building on their\\nSilk Road investment program. And they\\'ve got, they\\'ve been laying\\nnetworking infrastructure all over the world with their 5G, right. Work with their company Huawei. And so, they\\'ve been\\nlaying all this fabric, but financial and technological\\nfabric all over the world. And their plan is to roll out their vision of\\nAI on top of that and to have every other country be\\nrunning their version. And then if you\\'re a\\ncountry prone to, you know, authoritarianism, you\\'re\\ngonna find this to be an incredible way to\\nbecome more authoritarian. If you\\'re a country, by the way, not prone to authoritarianism, you\\'re gonna have the Chinese\\nCommunist Party running your infrastructure and having\\nbackdoor into it. Right. Which is also not good. - What\\'s your sense of where\\nthey stand in terms of the race towards super intelligence as\\ncompared to the United States? - Yeah, so good news is they\\'re behind, but bad news is they, you know, let\\'s just say they get\\naccess to everything we do. So they\\'re probably a year\\nbehind at each point in time, but they get, you know, downloads I think of\\nbasically all of our work on a regular basis through\\na variety of means. And they are, you know,\\nat least we\\'ll see, they\\'re at least putting\\nout reports of very, they just put out a report last week of a GPT 3.5 analog. They put out this report,\\nforget what it\\'s called, but they put out this\\nreport of this and they did and they, you know, the\\nway when open AI you know, puts out, one of the ways they test, you know, GPT they run it through standardized\\nexams like the SAT. Right. Just how you can kind of\\ngauge how smart it is. And so the Chinese report, they ran their LLM through\\nthe Chinese equivalent of the SAT and it includes\\na section on Marxism and a section on, I say tongue of thought. And it turns out their AI does very well on both of those topics. - That\\'s right. - So like. - Oh, this alignment thing. - Communist AI, right? Like literal communist AI. Right? And so their vision is\\nlike, that\\'s the, you know, so you know, you can just\\nimagine like you\\'re a school, you know, you\\'re a kid 10\\nyears from now in Argentina or in Germany or in who\\nknows where, Indonesia. And you ask the AI, I\\'d explain to you like\\nhow the economy works and it gives you the most cheery, upbeat explanation of\\nChinese style communism you\\'ve ever heard. Right. So like the stakes here\\nare like really big. - Well, as we\\'ve been talking about, my hope is not just\\nwith the United States, but with just the kid in his basement. The open source LLM. \\'Cause I don\\'t know if I trust large centralized institutions\\nwith super powerful AI no matter what their\\nideology as a power corrupts. You\\'ve been investing in\\ntech companies for about, let\\'s say 20 years. And about 15 of which was\\nwith Andreessen Horowitz. What interesting trends\\nin tech have you seen over that time? Let\\'s just talk about companies\\nand just the evolution of the tech industry. - I mean the big shift over 20\\nyears has been that tech used to be a tools industry for\\nbasically from like 1940 through to about 2010, almost all the big successful\\ncompanies were pick and shovels companies. So PC, database, smartphone, you know, some tool that somebody\\nelse would pick up and use. Since 2010, most of the big\\nwins have been in applications. So a company that starts you know, starts in an existing\\nindustry and goes directly to the customer in that industry. And you know, the earliest examples there\\nwere like Uber and Lyft and Airbnb. And then that model is\\nkind of elaborating out. The AI thing is actually a\\nreversion on that for now \\'cause like most of the AI\\nbusiness right now is actually in cloud provision of AI APIs\\nfor other people to build on. - But the big thing\\nwill probably be in app. - Yeah. I think most of the\\nmoney I think probably will be in whatever your AI financial advisor or your AI doctor or your\\nAI lawyer or, you know, take your pick of whatever the domain is. And there, and what\\'s\\ninteresting is, you know, the valley kind of does everything. The entrepreneurs kind of\\nelaborate every possible idea. And so there will be a set of\\ncompanies that like make AI something that can be purchased\\nand used by large law firms and then there will be other\\ncompanies that just go direct to market as an AI lawyer. - What advice could you\\ngive for a startup founder? Just haven\\'t seen so many\\nsuccessful companies, so many companies that fail also, what advice could you\\ngive to a startup founder, someone who wants to build the\\nnext super successful startup in the tech space? The Googles,\\nthe Apples, the Twitters. - Yeah. So the great thing about the really great founders is they don\\'t take any advice. So, if you find yourself\\nlistening to advice, maybe you shouldn\\'t do it. - But that\\'s actually,\\njust to elaborate on that, if you could also speak\\nto great founders too. Like what makes a great founder? - So what makes a great\\nfounder is super smart, coupled with super energetic,\\ncoupled with super courageous. I think it\\'s some of those three and-- - Intelligence, passion and courage. - The first two are traits\\nand the third one is a choice. I think courage is a choice. Well \\'cause courage is a question\\nof pain tolerance, right? So how many times are you\\nwilling to get punched in the face before you quit? And here\\'s maybe the biggest\\nthing people don\\'t understand about what it\\'s like to be\\na startup founder is it gets very romanticized, right? And even when it, even when they fail, it still gets romanticized about like what a great adventure it was. But like the reality of it is\\nmost of what happens is people telling you no and then\\nthey usually follow that with you\\'re stupid, right. No, I will not come to work for you. I will not leave my cushy job at Google to come work for you. No, I\\'m not gonna buy your\\nproduct, you know, no, I\\'m not gonna run a\\nstory about your company. No, I\\'m not this, that, the other thing. And so a huge amount of what\\npeople have to do is just get used to just getting punched and the reason people\\ndon\\'t understand this is because when you\\'re a founder, you cannot let on that this is happening \\'cause it will cause people to think that you\\'re weak and\\nthey\\'ll lose faith in you. So you have to pretend that\\nyou\\'re having a great time when you\\'re dying inside, right? You\\'re just in misery. - But why did they do it? - Why did they do? Yeah, that\\'s the thing. It\\'s like it is a level, this is actually one of\\nthe conclusions I think is that I think it\\'s actually\\nfor most of these people on a risk adjusted basis, it\\'s\\nprobably an irrational act. They could probably be\\nmore financially successful on average if they just\\ngot like a real job in at a big company. But there\\'s, you know, some people just have an\\nirrational need to do something new and build something for\\nthemselves and, you know, some people just can\\'t\\ntolerate having bosses. Oh, here\\'s the fun thing is how do you reference\\ncheck founders, right? So you call the, you know, normal way you reference check, you\\'re hiring somebody\\nis you call the bosses, they\\'re their, and you know, and you find out if\\nthey were good employees and now you\\'re trying to\\nreference check Steve Jobs, right? And it\\'s like, oh God, he was terrible. You know, he was a terrible employee. He never did what we told him to do. - So what\\'s a good reference? Do you want the previous\\nboss to actually say they never did what you told him to do? That might be a good thing. - Well, ideally what\\nyou want is I will go, I would like to go to\\nwork for that person. He worked for me here and\\nnow I\\'d like to work for him. No, unfortunately, most\\npeople can\\'t, their egos can\\'t handle that. So they won\\'t say that. But that\\'s the ideal. - What advice would\\nyou give to those folks in the space of intelligence,\\npassion and courage? - So I think the other big thing\\nis you see people sometimes who say, I wanna start a company and then they kind of\\nwork through the process of coming up with an idea. And generally those don\\'t\\nwork as well as the case where somebody has the idea first and then they kind of realize that there\\'s an opportunity\\nto build a company and then they just turn\\nout to be the right kind of person to do that. - When you say idea, do you\\nmean long-term big vision or do you mean specifics of like product? - Specific I would say specific, like specifically what specifics. Like what is the, because for the first five years you don\\'t get to have vision, you just gotta build something people want and you gotta figure out a\\nway to sell it to them. Right. It\\'s very practical or you\\nnever get to big vision. - So the first product, you have an idea of a set of\\nproducts of the first product that can actually make some money. - Yeah. Like it\\'s gotta work. The first product\\'s gotta\\nwork by which I mean like, it has to technically work, but then it has to actually\\nfit into the category and the customer\\'s mind if\\nsomething that they want and then by the way, the other part is they have\\nto be willing to pay for it. Like somebody\\'s gotta pay the bills. And so you\\'ve gotta\\nfigure out how to price it and whether you can\\nactually extract the money. So usually it is much more predictable. Success is never predictable, but it\\'s more predictable if\\nyou start with a great idea and then back into starting the company. So this is what we did,\\nyou know, we had most, before we had escape, the Google guys had the\\nGoogle search engine working at Stanford. Right. You know, yeah. Actually there\\'s tons of\\nexamples where they, you know, Pierre Omaira had eBay working before he left his previous job. - So I really love that\\nidea of just having a thing, a prototype that actually\\nworks before you even begin to remotely scale. Yeah. - By the way, it\\'s also far\\neasier to raise money, right? Like the ideal pitch that we receive is, here\\'s the thing that works, would you like to invest\\nin our company or not? Like, that\\'s so much easier than here\\'s 30 slides with a dream, right? And then we have this\\nconcept called the DMAs, which our biology of came\\nup with when he was with us. So then there\\'s this thing,\\nthis goes to mythology, which is, you know, there\\'s\\na mythology that kind of, you know, these ideas, you know, kind of arrive like magic or people kind of stumble into them. It\\'s like eBay with the pest\\ndispensers or something. The reality usually with\\nthe big successes is that the founder has been\\nchewing on the problem for 5 or 10 years before they start the company\\nand they often worked on it in school or they even experimented on it when they were a kid and they\\'ve been kind of training up over that period of time to\\nbe able to do the thing. So they\\'re like a true domain expert. And it sort of sounds like mom, I\\'m an apple pie, which is yeah, you wanna be a domain\\nexpert in what you\\'re doing, but you would, you know, the\\nmythology is so strong of like, oh, I just like had this idea in the shower right now I\\'m doing it. Like it\\'s generally not that. - No, because it\\'s, well, maybe in the shower\\nwe had the exact product implementation details, but yeah, usually you\\'re gonna be for\\nlike years if not decades thinking about like\\neverything around that. - Well we call it the DMAs\\nbecause the DMAs basically is like, there\\'s all these permutations, like for any idea, there\\'s like all these\\ndifferent permutations, who should the customer be? What shape forms should the product have and how should we take it to\\nmarket and all these things. And so the really smart\\nfounders have thought through all these scenarios\\nby the time they go out to raise money and they\\nhave like detailed answers on every one of those fronts because they put so much thought into it. The sort of more haphazard\\nfounders haven\\'t thought about any of that. And it\\'s the detailed ones\\nwho tend to do much better. - So how do you know when to take a leap if you have a cushy job or happy life? - I mean the best reason is just \\'cause you can\\'t tolerate\\nnot doing it right? Like this is the kind of\\nthing where if you have to be advised into doing it, you\\nprobably shouldn\\'t do it. And so it\\'s probably the opposite, which is you just have such\\na burning sense of this has to be done, I have to do\\nthis, I have no choice. - What if it\\'s gonna\\nlead to a lot of pain? - It\\'s gonna lead to a lot\\nof pain. I think that\\'s. - What if it means losing\\nsort of social relationships and damaging your\\nrelationship with loved ones and all that kind of stuff. - Yeah, look, so like, it\\'s gonna put you in a\\nsocial tunnel for sure, right? So you\\'re gonna, like, you know, there\\'s this game you can play on Twitter, which is you can do any whiff\\nof the idea that there\\'s basically any such thing\\nas work life balance and that people should actually work hard and everybody gets mad. But like, the truth is like all the\\nsuccessful founders are working 80 hour weeks and they\\'re\\nworking, you know, they form very, very strong social bonds with\\nthe people they work with. They tend to lose a lot of\\nfriends on the outside or put those friendships on ice. Like that\\'s just the nature of the thing, you know, for most people\\nthat\\'s worth the trade off. You know, the advantage, you know, maybe younger founders have\\nis maybe they have less, you know, maybe they\\'re\\nnot, you know, for example, if they\\'re not married yet\\nor don\\'t have kids yet, that\\'s an easier thing to bite off. - Can you be an older founder? - Yeah. You definitely can. Yeah. Yeah. Many of the most\\nsuccessful founders are second, third, fourth time founders. They\\'re in their thirties,\\nforties, fifties. The good news with being an\\nolder founder is, you know, more and you, you know, a\\nlot more about what to do, which is very helpful.\\nThe problem is, okay, now you\\'ve got like a spouse\\nand a family and kids and like, you\\'ve gotta go to the\\nbaseball game and like, you can\\'t go to the base,\\nyou know, and so it\\'s. - [Lex] Life is full of difficult choices. - Yes. - Marc Andreessen, you\\'ve written a blog post\\non what you\\'ve been up to. You wrote this in October, 2022, \"Mostly I try to learn a lot. For example, the political events of 2014\\nto 2016 make clear to me that I didn\\'t understand\\npolitics at all referencing maybe some of this book here. So I deliberately withdrew\\nfrom political engagement and fundraising and instead\\nread my way back into history and as far to the political left and political right as I could.\" So just high level question, what\\'s your approach to learning? - Yeah, so it\\'s basically, I would say, I\\'m an AutoID direct,\\nso it\\'s sort of goes, it\\'s going down the rabbit holes. So it\\'s a combination. I kind of allude to it', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 517638, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'}),\n",
       " Document(page_content=' you, you know, a\\nlot more about what to do, which is very helpful.\\nThe problem is, okay, now you\\'ve got like a spouse\\nand a family and kids and like, you\\'ve gotta go to the\\nbaseball game and like, you can\\'t go to the base,\\nyou know, and so it\\'s. - [Lex] Life is full of difficult choices. - Yes. - Marc Andreessen, you\\'ve written a blog post\\non what you\\'ve been up to. You wrote this in October, 2022, \"Mostly I try to learn a lot. For example, the political events of 2014\\nto 2016 make clear to me that I didn\\'t understand\\npolitics at all referencing maybe some of this book here. So I deliberately withdrew\\nfrom political engagement and fundraising and instead\\nread my way back into history and as far to the political left and political right as I could.\" So just high level question, what\\'s your approach to learning? - Yeah, so it\\'s basically, I would say, I\\'m an AutoID direct,\\nso it\\'s sort of goes, it\\'s going down the rabbit holes. So it\\'s a combination. I kind of allude to it\\nin that, in that quote, it\\'s a combination of breadth and depth. And so I tend to, yeah, I tend to, I go broad by the nature\\nof what I do, I go broad, but then I tend to go deep\\nin a rabbit hole for a while, read everything I can\\nand then come out of it. And I might not revisit\\nthat rabbit hole for, you know, another decade. - And in that blog post that I recommend people go check out, you actually list a bunch\\nof different books that you recommend on different\\ntopics on the American left, on the American right. It\\'s just a lot of really good stuff. The best explanation for\\nthe current structure of our society and politics. You give to recommendations, four books on the Spanish Civil War, six books on deep history\\nof the American right comprehensive biographies. These of Adolf Hitler, one of which I read can\\nrecommend six books on the deep history of the American\\nleft. So the American right, American left looking at the\\nhistory to give you the context biography of later Lennon, two of them on the French\\nRevolution. I actually, I have never read a\\nbiography on Lennon maybe that would be useful. Everything\\'s been so Marc\\'s focused. - The Sebastian biography\\nof Lennon is extraordinary. - [Lex] Victor Sebestyen. Okay. - Blow your mind. Yeah. - [Lex] So it\\'s still useful to read. - It\\'s incredible. Yeah, it\\'s incredible. I actually think it\\'s the single best book on the Soviet Union. - So that the perspective of Lennon, it might be the best way to\\nlook at the Soviet Union versus Stalin versus Marx\\nversus, very interesting. So two books on fascism and\\nanti-fascism by the same author, Paul Gottfried, brilliant book on the\\nnature of mass movements and collective psychology, the definitive work on\\nintellectual life under totalitarianism, the Captive Mind, the definitive worked\\non the practical life under totalitarianism. There\\'s a bunch. There\\'s a bunch. And the single best book, first of all, the list here is just incredible. But you say the single best\\nbook I have found on who we are and how we got here is the Ancient City by Numa Dennis Fustel De Coulanges. I like it. What did you learn about who\\nwe are as a human civilization from that book? - Yeah, so this is a fascinating book. This one\\'s free, it\\'s a free, by the way, it\\'s a book in the 1860s. You can download it or\\nyou can buy printouts up prints of it. But it was this guy who was\\na professor at the savant in the 1860s and he was\\napparently a savant on antiquity on Greek and Roman antiquity and the reason I say that is\\nbecause his sources are 100% original Greek and Roman sources. So he wrote a basically\\nhistory of western civilization from, on the order of 4,000 years ago to basically the present\\ntimes entirely working on fresh original Greek and Roman sources. And what he was specifically\\ntrying to do was he was trying to reconstruct from the stories of the Greeks and the Romans, he was trying to reconstruct\\nwhat life in the west was like before the Greeks and the Romans, which was in the civilization known as the Indo Europeans. And the short answer is,\\nand this is sort of 4,000, you know, 2000 BC to, you know, sort of 500 BC kind of\\nthat 1500 year stretch for civilization developed. And his conclusion was basically cults. They were basically cults\\nand civilization was, or organized into cults. And the intensity of the cults was like a million fold beyond anything that we would recognize today. Like it was a level of\\nall encompassing belief and an action around religion that was at a level of extremeness that we wouldn\\'t even recognize it and so specifically he\\ntells the story of basically there were three levels of cults. There was the family cult, the tribal cult, and then the city cult as society scaled up. And then each cult was a\\njoint cult of family gods, which were ancestor gods. And then nature gods and then\\nyour bonding into a family, a tribe or a city was\\nbased on your adherence to that religion. People who were not of your\\nfamily, tribe, city, worship, different gods, which gave you not just the\\nright with or responsibility to kill them on site. - [Lex] So they were\\nserious about their cults. - Hardcore, by the way,\\nshocking development. I did not realize this zero\\nconcept of individual rights. Like even even up through the Greeks, and even in the Romans, they didn\\'t have, have the concept of individual rights. Like the idea that as an\\nindividual you have like some rights just like, nope. Right? And you look back\\nand you\\'re just like, wow, that\\'s just like cr\\nlike fascist in a degree that we wouldn\\'t recognize today. But it\\'s like, well, they were living under\\nextreme pressure for survival. And you, and you know, the theory goes, you could not have people\\nrunning around making claims, individual rights when\\nyou\\'re just trying to get like your tribe through the winter, right? Like you need like hardcore\\ncommand and control. And actually what if through\\nmodern political lens, those cults were basically\\nboth fascist and communist. They were fascist in\\nterms of social control, and then they were communist\\nin terms of economics. - But you think that\\'s\\nfundamentally that like pull towards cults is within us. - Well, so my conclusion from this book, so the way we naturally\\nthink about the world we live in today is like, we basically have such an\\nimproved version of everything that came before us, right? Like, we have basically, we\\'ve figured out all these\\nthings around morality and ethics and democracy\\nand all these things. And like, they were basically\\nstupid and retrograde and we\\'re like smart and sophisticated. And we\\'ve improved all this after reading that book, I now believe in many ways\\nthe opposite, which is no, actually we are still running\\nin that original model. We\\'re just running in an\\nincredibly diluted version of it. So we\\'re still running,\\nbasically in cults. It\\'s just our cults are at like\\na thousandth or a millionth, the level of intensity, right? And so our, so just as to\\ntake religions, you know, the modern experience of\\na Christian in our time, even somebody who considers\\nhim a devout Christian, is just a shadow of the level\\nof intensity of somebody who belonged to a religion\\nback in that period. And then by the way, we have cons. It goes back to our AI discussion. We then sort of endlessly\\ncreate new cults. Like we\\'re trying to fill the void, right? And the void is a void of bonding. - [Lex] Okay. - Living in their era. Like\\neverybody living today, transporting that era\\nwould view it as just like, completely intolerable in terms of like the loss of freedom and the level of basically of fascist control. However, every single person in that era, and he really stresses this. They knew exactly where they stood. They knew exactly where they belonged. They knew exactly what their purpose was. They knew exactly what they\\nneeded to do every day. They knew exactly why they were doing it. They had total certainty about\\ntheir place in the universe. - So the question of meaning, the question of purpose\\nwas very distinctly, clearly defined for them. - Absolutely overwhelmingly\\nundisputably undeniably. - As we turn the volume\\ndown on the cultism-- - [Marc] Yes. - We start to, the search for meaning starts\\ngetting harder and harder. - Yes. \\'cause we don\\'t have that. We are ungrounded. We are uncentered and\\nwe all feel it. Right? And that\\'s why we reach for, you know, it\\'s why we still reach for religion. It\\'s why we reach for, you know, we people start\\nto take on, you know, let\\'s say, you know, a faith in science maybe beyond\\nwhere they should put it. You know and by the way,\\nlike, sports teams are like a, you know, they\\'re like a tiny\\nlittle version of a cult. And you know, apple keynotes are a tiny\\nlittle version of a cult. Right. And, you know, political, you know. And there\\'s cult, you know, there\\'s full-blown cults on both sides of the political spectrum\\nright now. Right. You know, operating in plain stuff. - But still not full blown\\ncompared as to what it was. - Compared to what it used to. I mean, we would today consider\\nfull blown, but like, yes, they\\'re at like, I don\\'t know, a hundred thousandth or\\nsomething of the intensity of what people had back then. So, we live in a world today\\nthat in many ways is more advanced and moral and so forth. And it\\'s certainly a lot nicer,\\nmuch nicer world to live in. But we live in a world\\nthat\\'s like very washed out. It\\'s like everything has\\nbecome very colorless and gray as compared to how people\\nused to experience things. Which is I think why we\\'re\\nso prone to reach for drama. \\'Cause there\\'s something in us that\\'s deeply evolved\\nwhere we want that back. - And I wonder where it\\'s all\\nheaded as we turn the volume down more and more. What advice would you\\ngive to young folks today in high school and college? How to be successful in their career? How to be successful in their life? - Yeah. So the tools that\\nare available today, I mean, are just like, I\\nsometimes, you know, bore, I sometimes bore, you know, kids by describing like what\\nit was like to go look up a book, you know, to try to like discover\\na fact in, you know, in the old days, the 1970s, 1980s, to go to the library and the card catalog and the whole thing. You go through all that work\\nand then the book is checked out and you have to wait two weeks and like to be in a world, not only where you can get\\nthe answer to any question, but also the world now, you know, the AI world where you\\'ve\\ngot like the assistant that will help you do\\nanything, help you teach, learn anything, like your ability both to learn and also to produce\\nis just like, I don\\'t know, a million fold beyond what it used to be. I have a blog post I\\'ve\\nbeen wanting to write, which I call where are the\\nhyper-productive people? Like-- - [Lex] That\\'s a good question, right? - Like with these tools, like there should be authors\\nthat are writing like hundreds or thousands of like, outstanding books. - Well, with the authors there\\'s\\na consumption question too, but yeah. Well, maybe not, maybe not. You\\'re right. But, so the tools are much more powerful. Getting much more powerful. - Artists, musicians. Right. Why aren\\'t musicians producing\\na thousand times the number of songs, right? Like what, like the tools are spectacular. - So, what\\'s the explanation? And by way of advice, like, is motivation starting to\\nbe turned down a little bit? Or what? - I think it might be distraction. - [Lex] Distraction. - It\\'s so easy to just sit and consume that I think people get distracted from production.\\nBut if you wanted to, you know, as a young person, if you\\nwanted to really stand out, you could get on a, like a hyper productivity curve very early on. There\\'s a great, you know, this story, there\\'s a great story in\\nRoman history of plenty of the elder who was\\nthis legendary statesman, died in the Vesuvius eruption\\ntrying to rescue his friends. But he was famous both for being basically being a polymath,\\nbut also being an author. And he wrote apparently\\nlike hundreds of books, most of us had been lost. But he like wrote all these\\nencyclopedias and he literally like would be reading and\\nwriting all day long no matter what else was going on. And so he would like travel\\nwith like four slaves. And two of them were\\nresponsible for reading to him, and two of them were responsible\\nfor taking dictation. And so like, he\\'d be going\\ncross country and like, literally he would be writing\\nbooks like all the time. And apparently they were spectacular. There\\'s only a few that have survived, but apparently they were amazing. - There\\'s a lot of value to being somebody who finds focus in this life. - Yeah. Like and there are\\nexamples, like there are, you know, there\\'s this guy,\\njudge, what\\'s his name? Posner, who wrote like 40 books and was also a great federal judge. You know, there\\'s our friend Balaji, I think is like this, he\\'s\\none of these, you know, where his output is just prodigious. And so it\\'s like, yeah, I mean,\\nwith these tools, why not? And I kind of think we\\'re at this interesting\\nkind of freeze frame moment where like this, these tools are now in everybody\\'s hands and everybody\\'s just kind\\nof staring at them trying to figure out what to do. The new tools. - We have discovered fire. - [Marc] Yeah. - And trying to figure\\nout how to use it to cook. - [Marc] Yeah. Right. - You told Tim Ferriss that\\nthe perfect day is caffeine for 10 hours and alcohol for four hours. You didn\\'t think I\\'d be\\nmentioning this, did you? It balances everything\\nout perfectly as you said. So, perfect. So let me ask, what\\'s the secret to balance\\nand maybe to happiness in life? - I don\\'t believe in balance, so I\\'m the wrong person to ask that. - Can you elaborate why you\\ndon\\'t believe in balance? - I mean, I maybe it\\'s just,\\nand I look, I think people, I think people are wired differently. So, I think it\\'s hard to\\ngeneralize this kind of thing, but I am much happier and more satisfied when I\\'m fully committed to something. So I\\'m very much in favor\\nof all in of imbalance. - Imbalance. And that applies to work, to life, to everything. - Yeah. No, no. I happen to have whatever twist\\nof personality traits lead that in non-destructive\\ndimensions in including the fact that I\\'ve actually, I now no\\nlonger do the ten-four plan. I stopped drinking. I do the caffeine, but not the alcohol. So there\\'s something in my personality where I whatever mal-adaption\\nI have is inclining me towards productive things,\\nnot unproductive things. - So you\\'re one of the\\nwealthiest people in the world. What\\'s the relationship\\nbetween wealth and happiness? Money and happiness. - So I think happiness, I don\\'t think happiness is the thing. - To strive for. - I think satisfaction is the thing. - That just sounds like\\nhappiness, but turned down a bit. - No deeper. So happiness is, you know, a walk in the woods at sunset,\\nan ice cream cone, a kiss, the first ice cream cone is great. The thousandth ice\\ncream cone, not so much. At some point the walks\\nin the woods get boring. - What\\'s the distinction between\\nhappiness and satisfaction? - I think satisfaction is a deeper thing, which is like having found a purpose and fulfilling it, being useful. - So just something that\\npermeates all your days, just this general\\ncontentment of being useful. - That I\\'m fully satisfying my faculties, that I\\'m fully delivering, right? On the gifts that I\\'ve been\\ngiven, that I\\'m, you know, net making the world better, that I\\'m contributing to\\nthe people around me, right. And that I can look back\\nand say, wow, that was hard, but it was worth it. Think generally, it seems to lead people in\\na better state than pursuit of pleasure, pursuit of\\nquote unquote happiness. - Does money have\\nanything to do with that? - I think the founders and\\nthe founding fathers in the US threw this off kilter when\\nthey used the phrase pursuit of happiness. I think they should have said. - [Lex] Pursuit of satisfaction. - They said, pursuit of satisfaction. We might live in a better world today. - Well, they, you know, they could have elaborated on a lot of things right in the box. - [Marc] They could have\\ntweaked the second amendment. - I think they were\\nsmarter than they realized. They said, you know we\\'re gonna make it ambiguous and let these humans figure out the rest, these tribal cult-like\\nhumans figure out the rest. But money empowers that. - So I think, and I think there, I mean, look, I think Elon is, I don\\'t think I\\'m even a great example, but I think Elon would be\\nthe great example of this, which is like, you know, look,\\nhe\\'s a guy who from every, every day of his life, from the day he started\\nmaking money at all, he just plows into the next thing. And so I think, I think money is definitely\\nan enabler for satisfaction. Way money applied to\\nhappiness leads people down very dark paths. Very destructive avenues. Money applied to satisfaction,\\nI think could be, is a real tool. I always, by the way,\\nI was like, you know, Elon is the case study for behavior. But the other thing that I\\nalways really made me think is Larry Page was asked one time what his approach to philanthropy was. And he said, oh, I\\'m just, my philanthropic plan is just give all the money to Elon. (both laugh) - Well, let me actually\\nask you about Elon. You\\'ve interacted with quite a lot of successful engineers\\nand business people. What do you think is special about Elon? We talked about Steve Jobs. What do you think is special\\nabout him as a leader? As an innovator? - Yeah. So the core of it is he\\'s back to the future. So he is doing the most\\nleading-edge things in the world, but with a really deeply\\nold-school approach. And so to find comparisons to Elon, you need to go to like\\nHenry Ford and Thomas Watson and Howard Hughes and\\nAndrew Carnegie, right. Leland Stanford, John Rockefeller, right. You need to go to what were called the\\nbourgeois capitalists, like the hardcore business owner operators who basically built, you know, basically built industrialized\\nsociety, Vanderbilt. And it\\'s a level of hands-on commitment and depth in the business, coupled with an absolute priority\\ntowards truth and towards, how to put it, science and technology\\ntown to first principles that is just like absolute, is just like unbelievably absolute. He really is ideal that he\\'s\\nonly ever talking to engineers. Like he does not tolerate. He has less tolerance than\\nanybody I\\'ve ever met. He wants ground truth\\non every single topic. And he runs his businesses\\ndirectly day-to-day, devoted to getting to ground\\ntruth in every single topic. - So you think it was a good decision for him to buy Twitter? - I have developed a view in life to not second guess Elon Musk, I know this is gonna sound\\ngreat, crazy and unfounded, but. - Well, I mean, he\\'s got\\na quite a track record. - I mean, look, the car was\\na crazy, I mean, the car was, I mean, look. - He\\'s done a lot of\\nthings that seem crazy. - Starting a new car company in the United States of America. The last time somebody\\nreally tried to do that was the 1950s and it was\\ncalled Tucker Automotive. And it was such a disaster. They made a movie about\\nwhat a disaster it was, and then rockets like, who does that? Like, there\\'s obviously no way to start a new rocket company. Like those days are over. And then to do those at the same time. So after he pulled those\\ntwo off, like, okay, fine. Like, this is one of my areas of like, whatever opinions I had about\\nthat, that is just like, okay, clearly are not relevant. Like this is you just, you at some point you just\\nlike bet on the person. - And in general, I wish more people would lean\\non celebrating and supporting versus deriding and destroying. - Oh yeah. I mean, look,\\nhe drives resentment. Like it\\'s a resentment. Like he is a magnet for resentment. Like his critics are the\\nmost miserable, like, resentful people in the world. Like it\\'s almost a perfect match\\nof like the most idealized, you know, technologist, you know, of the century coupled with like, just his critics are\\njust bitter as can be. And I mean, it\\'s sort of\\nvery darkly comic to watch. - Well, he fuels the fire of that by being on Twitter at times. And which is fascinating\\nto watch the drama of human civilization, given our cult roots just fully on fire. - [Marc] He\\'s running a cult. - You could say that. - [Marc] Very successfully. - So now that our cults\\nhave gone and we searched for meaning, what do\\nyou think is the meaning of this whole thing? What\\'s the meaning of\\nlife Marc Andreessen? - I don\\'t know the answer\\nto that. I think the meaning of the closest I get to it is what I said about satisfaction. So it\\'s basically like, okay, we were given what we have, like we should basically do our best. - What\\'s the role of love in that mix? - I mean, like, what\\'s the point of life if you\\'re without love, like, yeah. - So love is a big part\\nof that satisfaction. - Yeah. And look like\\ntaking care of people is like a wonderful thing. Like it, you know, mentality, you know, there are pathological forms\\nof taking care of people, but there\\'s also a very\\nfundamental, you know, kind of aspect of taking care of people. Like, for example, I happen to be somebody who\\nbelieves that capitalism and taking care of people are actually, they\\'re actually the same thing. Somebody once said, capitalism is how you take\\ncare of people you don\\'t know. Right, right. And so like, yeah, I think it\\'s like deeply\\nwoven into the whole thing, you know, there\\'s a long conversation to be had about that, but yeah. - Yeah. Creating products that are used by millions of people and bring them joy in smaller, big ways. And then capitalism kind of\\nenables that, encourages that. - David Friedman says, there\\'s only three ways to\\nget somebody to do something for somebody else. Love, money and force. And love and money are better. - [Lex] Yeah. Of course. That\\'s a good ordering. I think. - We should bet on those. - Try love first. If that doesn\\'t work, then money. - [Marc] Yes. - And then force. Well, don\\'t even try that one. Marc, you\\'re an incredible person. I\\'ve been a huge fan. I\\'m glad to finally got a chance to talk. I\\'m a fan of everything\\nyou do, everything you do, including on Twitter. It\\'s a huge honor to meet\\nyou, to talk with you. Thanks again for doing this. - Awesome. Thank you, Lex. - Thanks for listening\\nto this conversation with Marc Andreessen. To support this podcast, please check out our\\nsponsors in the description. And now let me leave you with some words from Marc Andreessen himself. \"The world is a very malleable place. If you know what you\\nwant and you go for it, with maximum energy and drive and passion, the world will often\\nreconfigure itself around you much more quickly and easily\\nthan you would think.\" Thank you for listening and\\nhope to see you next time.', metadata={'source': '-hxeDjAxvJ8', 'title': 'Marc Andreessen: Future of the Internet, Technology, and AI | Lex Fridman Podcast #386', 'description': 'Unknown', 'view_count': 517638, 'thumbnail_url': 'https://i.ytimg.com/vi/-hxeDjAxvJ8/hq720.jpg', 'publish_date': '2023-06-21 00:00:00', 'length': 11495, 'author': 'Lex Fridman'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 Request contains an invalid argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:72\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/grpc/_channel.py:1030\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1028\u001b[0m state, call, \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[1;32m   1029\u001b[0m                               wait_for_ready, compression)\n\u001b[0;32m-> 1030\u001b[0m \u001b[39mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[39mFalse\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/grpc/_channel.py:910\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 910\u001b[0m     \u001b[39mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Request contains an invalid argument.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:74.125.200.95:443 {created_time:\"2023-06-29T15:54:10.005463198+08:00\", grpc_status:3, grpc_message:\"Request contains an invalid argument.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m refine_outputs \u001b[39m=\u001b[39m refine_chain({\u001b[39m\"\u001b[39;49m\u001b[39minput_documents\u001b[39;49m\u001b[39m\"\u001b[39;49m: docs_summary}, return_only_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:84\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     83\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m---> 84\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m     85\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/combine_documents/refine.py:94\u001b[0m, in \u001b[0;36mRefineDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Combine by mapping first chain over all, then stuffing into final chain.\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_initial_inputs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 94\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitial_llm_chain\u001b[39m.\u001b[39;49mpredict(callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     95\u001b[0m refine_steps \u001b[39m=\u001b[39m [res]\n\u001b[1;32m     96\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs[\u001b[39m1\u001b[39m:]:\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/llm.py:252\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    238\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/llm.py:92\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     90\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 92\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/chains/llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    103\u001b[0m     prompts,\n\u001b[1;32m    104\u001b[0m     stop,\n\u001b[1;32m    105\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    106\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    107\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/base.py:141\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    134\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    135\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    139\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    140\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/base.py:227\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    222\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m         )\n\u001b[1;32m    224\u001b[0m     run_managers \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    225\u001b[0m         dumpd(\u001b[39mself\u001b[39m), prompts, invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[1;32m    226\u001b[0m     )\n\u001b[0;32m--> 227\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    228\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    230\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    231\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/base.py:178\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    177\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    179\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    180\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/base.py:165\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    163\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 165\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    166\u001b[0m                 prompts,\n\u001b[1;32m    167\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    168\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    169\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    170\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    171\u001b[0m             )\n\u001b[1;32m    172\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    173\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    174\u001b[0m         )\n\u001b[1;32m    175\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/base.py:525\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    523\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m    524\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 525\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    526\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    527\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    530\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/vertexai.py:141\u001b[0m, in \u001b[0;36mVertexAI._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    126\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    130\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    131\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call Vertex model to get predictions based on the prompt.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39m        The string generated by the model.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(prompt, stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/langchain/llms/vertexai.py:70\u001b[0m, in \u001b[0;36m_VertexAICommon._predict\u001b[0;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\n\u001b[1;32m     67\u001b[0m     \u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m, stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m     68\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     69\u001b[0m     params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m---> 70\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mpredict(prompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enforce_stop_words(res\u001b[39m.\u001b[39mtext, stop)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/vertexai/language_models/_language_models.py:260\u001b[0m, in \u001b[0;36mTextGenerationModel.predict\u001b[0;34m(self, prompt, max_output_tokens, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m     top_p: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m _DEFAULT_TOP_P,\n\u001b[1;32m    246\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTextGenerationResponse\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Gets model response for a single prompt.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[1;32m    249\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39m        A `TextGenerationResponse` object that contains the text produced by the model.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_predict(\n\u001b[1;32m    261\u001b[0m         prompts\u001b[39m=\u001b[39;49m[prompt],\n\u001b[1;32m    262\u001b[0m         max_output_tokens\u001b[39m=\u001b[39;49mmax_output_tokens,\n\u001b[1;32m    263\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    264\u001b[0m         top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m    265\u001b[0m         top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m    266\u001b[0m     )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/vertexai/language_models/_language_models.py:296\u001b[0m, in \u001b[0;36mTextGenerationModel._batch_predict\u001b[0;34m(self, prompts, max_output_tokens, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m    288\u001b[0m instances \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(prompt)} \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m    289\u001b[0m prediction_parameters \u001b[39m=\u001b[39m {\n\u001b[1;32m    290\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: temperature,\n\u001b[1;32m    291\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmaxDecodeSteps\u001b[39m\u001b[39m\"\u001b[39m: max_output_tokens,\n\u001b[1;32m    292\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtopP\u001b[39m\u001b[39m\"\u001b[39m: top_p,\n\u001b[1;32m    293\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtopK\u001b[39m\u001b[39m\"\u001b[39m: top_k,\n\u001b[1;32m    294\u001b[0m }\n\u001b[0;32m--> 296\u001b[0m prediction_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_endpoint\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m    297\u001b[0m     instances\u001b[39m=\u001b[39;49minstances,\n\u001b[1;32m    298\u001b[0m     parameters\u001b[39m=\u001b[39;49mprediction_parameters,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    301\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m    302\u001b[0m \u001b[39mfor\u001b[39;00m prediction \u001b[39min\u001b[39;00m prediction_response\u001b[39m.\u001b[39mpredictions:\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:1559\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[39mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1547\u001b[0m         predictions\u001b[39m=\u001b[39mjson_response[\u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1548\u001b[0m         deployed_model_id\u001b[39m=\u001b[39mraw_predict_response\u001b[39m.\u001b[39mheaders[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1556\u001b[0m         ),\n\u001b[1;32m   1557\u001b[0m     )\n\u001b[1;32m   1558\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1559\u001b[0m     prediction_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prediction_client\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m   1560\u001b[0m         endpoint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gca_resource\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m   1561\u001b[0m         instances\u001b[39m=\u001b[39;49minstances,\n\u001b[1;32m   1562\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m   1563\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1564\u001b[0m     )\n\u001b[1;32m   1566\u001b[0m     \u001b[39mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1567\u001b[0m         predictions\u001b[39m=\u001b[39m[\n\u001b[1;32m   1568\u001b[0m             json_format\u001b[39m.\u001b[39mMessageToDict(item)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1573\u001b[0m         model_resource_name\u001b[39m=\u001b[39mprediction_response\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m   1574\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:602\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    597\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(metadata) \u001b[39m+\u001b[39m (\n\u001b[1;32m    598\u001b[0m     gapic_v1\u001b[39m.\u001b[39mrouting_header\u001b[39m.\u001b[39mto_grpc_metadata(((\u001b[39m\"\u001b[39m\u001b[39mendpoint\u001b[39m\u001b[39m\"\u001b[39m, request\u001b[39m.\u001b[39mendpoint),)),\n\u001b[1;32m    599\u001b[0m )\n\u001b[1;32m    601\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m response \u001b[39m=\u001b[39m rpc(\n\u001b[1;32m    603\u001b[0m     request,\n\u001b[1;32m    604\u001b[0m     retry\u001b[39m=\u001b[39;49mretry,\n\u001b[1;32m    605\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    606\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    607\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[39m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     metadata\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata)\n\u001b[1;32m    111\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m metadata\n\u001b[0;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/vertexai-sdk-env/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:74\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_grpc_error(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 Request contains an invalid argument."
     ]
    }
   ],
   "source": [
    "refine_outputs = refine_chain({\"input_documents\": docs_summary})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiap-13-ds-7e16bb946970.json  requirements.txt\n",
      "podcast.py\t\t      summary.txt\n",
      "podcast_vertexai.ipynb\t      youtube_summarizer_vertexai.ipynb\n",
      "podcast_vertexai.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM model for the question answering\n",
    "llm_question_answer = ChatVertexAI(temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector database and RetrievalQA Chain\n",
    "embeddings = VertexAIEmbeddings()\n",
    "# embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "db = FAISS.from_documents(docs_qa, embeddings)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm_question_answer, chain_type=\"stuff\", retriever=db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marc Andreessen, co-creator of Mosaic, the first widely used web browser, co-\n",
      "founder of Netscape, co-founder of the legendary Silicon Valley venture capital\n",
      "firm, Andreesen Horowitz, and is one of the most outspoken voices on the future\n",
      "of technology, including his most recent article, \"Why AI Will Save The World?\".\n",
      "In this podcast, Marc discusses his views on the future of the internet and\n",
      "technology in general. He believes that AI will play a major role in shaping the\n",
      "future, and that it has the potential to solve many of the world's problems. He\n",
      "also talks about his approach to learning,\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "The main points of the podcast are as follows:  - Marc Andreessen believes that\n",
      "AI will save the world. - He thinks that the competence and capability of senior\n",
      "scientists and technologists working on AI is much greater than the ability of\n",
      "politicians to make moral judgments about the use of AI. - He thinks that the\n",
      "policies that are being called for to prevent AI from being used for evil will\n",
      "cause extraordinary damage. - Marc Andreessen recommends that young people today\n",
      "focus on learning as much as they can and producing as much as they can.\n",
      "---------------------------------\n",
      "\n",
      "\n",
      "Goodbye.\n",
      "---------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import textwrap\n",
    "\n",
    "# Set the desired width limit for wrapping\n",
    "width = 80\n",
    "\n",
    "try:\n",
    "    # Your code here\n",
    "\n",
    "    question = \"\"\n",
    "    history = []\n",
    "    # Run the QA chain continuously & end when the user types \"exit\" or \"quit\"\n",
    "    while question.lower() not in [\"exit\", \"quit\"]:\n",
    "        # Get the user question\n",
    "        question = input(\"Ask a question or enter exit to close the app: \")\n",
    "        # Run the QA chain to query the Youtube video transcript\n",
    "        # answer = qa.run(question, callbacks=[cb])\n",
    "        answer = qa.run(question)\n",
    "        history.append(answer)\n",
    "        # Apply word wrapping and print the wrapped text\n",
    "        wrapped_text = textwrap.fill(answer, width=width)\n",
    "        print(wrapped_text)\n",
    "        # print(answer)\n",
    "        print(\"---------------------------------\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    sys.exit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/royca/yt-gpt/tree/master\n",
    "\n",
    "Adapted Summarizer from this Github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSummarize the youtube video whose transcript is provided within backticks             ```- The competence and\n",
      "capability and intelligence and training and accomplishments\n",
      "of senior scientists and technologists working on a technology, and then being able to then\n",
      "make moral judgments in the use of the technology. That track record is terrible. That track record is catastrophically bad. The policies that are being\n",
      "called for to prevent this, I think we're gonna cause\n",
      "extraordinary damage. - So the moment you say,\n",
      "AI's gonna kill all of us, therefore we should ban it, or that we should regulate\n",
      "all that kind of stuff, that's when it starts getting serious. - Or start, you know, military\n",
      "airstrikes and data centers. - Oh boy. The following is a conversation\n",
      "with Marc Andreessen, co-creator of Mosaic, the\n",
      "first widely used web browser, co-founder of Netscape, co-founder of the legendary Silicon Valley venture capital firm, Andreesen Horowitz, and is one of the most\n",
      "outspoken voices on the future of technology, including\n",
      "his most recent article, \"Why AI Will Save The World?\" This is Lex Fridman podcast. To support it, please\n",
      "check out our sponsors in the description. And now, dear friends,\n",
      "here's Marc Andreessen. I think you're the right\n",
      "person to talk about the future of the internet and technology in general. Do you think we'll\n",
      "still have Google search in 5 in 10 years, or search in general? - Yes. You know, it would be a question if the use cases have\n",
      "really narrowed down. - Well, now with AI-- - [Marc] Yeah. - And AI assistance being\n",
      "able to interact and expose the entirety of human wisdom and knowledge and information and facts and truth to us via the natural language interface. It seems like that's what\n",
      "search is designed to do. And if AI assistance can do that better, doesn't the nature of search change? - Sure. But we still have horses. - Okay. (both laugh) When's the last time you rode a horse? - It's been a while. - All right. (both laugh) So, but what I mean is, well, we still have Google\n",
      "search as the primary way that human civilization uses\n",
      "to interact with knowledge. - I mean, search was a technology, it was a moment in time technology, which is you have in theory, the world's information out on the web. And, you know, this is sort of\n",
      "the optimal way to get to it. But yeah, like, and by\n",
      "the way, actually Google, Google has known this for a long time. I mean, they've been driving\n",
      "away from the 10 blue links you know, for like two days. They've been trying to get\n",
      "away from that for a long time. - [Lex] What kind of links? - They call the 10 blue links. - [Lex] 10 blue links. - So the standard Google search\n",
      "result is just 10 blue links to the random websites. - And they term purple when\n",
      "you visit them. The stage TMO. - Guess who picked those colors? (both laugh) - [Lex] Thanks. - I'm touchy on this topic. - No offense. - Yes, it's good. Well, you know, like Marshall McLuhan said\n",
      "that the content of each new medium is the old medium. - The content of each new\n",
      "medium is the old medium. - The content of movies was theater, you know, theater plays. The content of theater\n",
      "plays was, you know, we've written stories, the content of written\n",
      "stories with spoken stories. - [Lex] Huh? - Right. And so you just\n",
      "kind of fold the old thing into the new thing. - [Lex] How does that\n",
      "have to do with the blue and the purple links? - It just, you maybe for,\n",
      "you know, maybe within AI, one of the things that AI can\n",
      "do for you is can generate the 10 blue links. Right? And so like, if either if that's actually the useful thing to do, or if you're feeling nostalgic, you know. - So can generate the old\n",
      "Infoseek or AltaVista, what else was there? - [Marc] Yeah, yeah. - In the nineties. - [Marc] Yeah. All these. - AOL. - And then the internet\n",
      "itself has this thing where it incorporates all\n",
      "prior forms of media, right? So the internet itself\n",
      "incorporates television and radio and books and write essays\n",
      "and every other form of, you know, prior basically media. And so it makes sense that\n",
      "AI would be the next step, and it would sort of, you'd sort of consider\n",
      "the internet to be content for the AI and then the\n",
      "AI will manipulate it however you want,\n",
      "including in this format. - But if we ask that\n",
      "question quite seriously, it's a pretty big question. Will we still have search as we know it? - Probably not, probably\n",
      "we'll just have answers, but there will be cases\n",
      "where you'll wanna say, okay, I want more. Like, you know, for example,\n",
      "site sources, right? And you wanted to do that. And so, in the different, you know, 10 blue links site sources\n",
      "are kind of the same thing. - The AI would provide to you\n",
      "the 10 blue links so that you can investigate the sources yourself. It wouldn't be the same kind\n",
      "of interface that the crude kind of interface. I mean, isn't that\n",
      "fundamentally different? - I just mean like, if you're\n",
      "reading a scientific paper, it's got the list of sources at the end. If you wanna investigate for yourself, you go read those papers. - I guess that is the kind of\n",
      "search you talking to an AI is a kind of kind conversations,\n",
      "the kind of search like is if every single aspect of\n",
      "our conversation right now, there'd be like 10 blue links\n",
      "popping up that I can just like pause reality, then you just go silent and\n",
      "then just click and read and then return back to this conversation. - You could do that, or you could have a running\n",
      "dialogue next to my head where the AI is arguing everything I say, the AI makes the counter argument. - [Lex] Counter argument. - Right. - Oh, like on Twitter,\n",
      "like community notes. But like in real time\n",
      "it would just pop up. So anytime you see my go to the right, you start getting nervous. - [Marc] Yeah. Exactly, like,\n",
      "oh no, that's not right. - Call me out on my right now. Okay. Well, I mean, isn't that,\n",
      "is that exciting to you? Is that terrifying that, I mean, search has dominated the way\n",
      "we interact with the internet for, I don't know how long, for 30 years since one of\n",
      "the earliest directories of website and then Google's for 20 years. And also it drove how we\n",
      "create content, you know, search engine optimization,\n",
      "that entirety thing, that it also drove the\n",
      "fact that we have webpages and what those webpages are. So, I mean, is that scary to you or are\n",
      "you nervous about the shape and the content of the internet evolving? - Well, you actually highlighted a practical concern in there, which is, if we stop making webpages\n",
      "are one of the primary sources of training data for the AI. And so if there's no longer\n",
      "an incentive to make webpages, that cuts off a significant\n",
      "source of future training, training data. So there's actually an\n",
      "interesting question in there. But other than that, more broadly? No, just in the sense of like, search was certain, like\n",
      "search was always a hack. The 10 blue Links was\n",
      "always a hack, right. Because like, if the\n",
      "hypothetical wanna think about the counter fascial\n",
      "and the counter fascial world where the Google guys, for example, had had LLMs upfront, would they ever have\n",
      "done the 10 blue links? And I think the answer's\n",
      "pretty clearly, no. They would've just gone\n",
      "straight to the answer. And like I said, Google's actually been trying\n",
      "to drive to the answer anyway. You know, they bought this\n",
      "AI company 15 years ago, their friend of mine is\n",
      "working out who's now the head of AI at Apple. And they were trying to do\n",
      "basically knowledge semantic, basically mapping. And that led to what's\n",
      "now the Google one box, where if you ask it, you know,\n",
      "what was Lincoln's birthday? It will give you the blue links, but it will normally\n",
      "just give you the answer. And so they've been\n",
      "walking in this direction for a long time anyway. - Do you remember the semantic web? That was an idea. - [Marc] Yeah. - How to convert the content\n",
      "of the internet into something that's interpretable by\n",
      "and usable by machine. - [Marc] Yeah, that's right. - That was the thing. - And the closest anybody got\n",
      "to that, I think the company, I think the company's name was Meta Web, which was where my friend\n",
      "John Jane Andrea was at, and where they were trying\n",
      "to basically implement that. And it was, you know, it was one of those things\n",
      "where it looked like a losing battle for a long time. And then Google bought\n",
      "it and it was like, wow, this is actually really useful. Kind of a proto, sort of a\n",
      "little bit of a proto AI. - But it turns out you don't\n",
      "need to rewrite the content of the internet to make it\n",
      "interpretable by a machine. The machine can kind of just read our. - Yeah, the machine can\n",
      "compute the meaning. Now the other thing of\n",
      "course is, you know, just on search is the\n",
      "LLM is just, you know, there is an analogy\n",
      "between what's happening in the neural network and\n",
      "a search process like it is in some loose sense searching\n",
      "through the network. Right. And there's the\n",
      "information is actually stored in the network, right? It's actually crystallized\n",
      "and stored in the network and it's kind of spread\n",
      "out all over the place. - But in a compressed representation. So you're searching, you're compressing and decompressing that thing inside where-- - But the information's in there and there is the neural network is running a process of trying to find the appropriate piece of\n",
      "information in many cases to generate to predict the next token. And so, it is kind of, it\n",
      "is doing a form of search. And then, and then by the\n",
      "way, just like on the web, you know, you can ask the\n",
      "same question multiple times or you can ask slightly\n",
      "different word of questions and the neural network will\n",
      "do a different kind of, you know, it'll search\n",
      "down different paths to give you different answers\n",
      "with different information. - [Lex] Yeah. - And so it sort of has a, you know, this con content of the new\n",
      "medium is previous medium. It kind of has the search\n",
      "functionality kind of embedded in there to the extent that it's useful. - So what's the motivator\n",
      "for creating new content on the internet? - [Marc] Yeah. - If, well, I mean\n",
      "actually the motivation is probably still there, but what does that look like? Would we really not have webpages? Would we just have social media\n",
      "and video hosting websites? And what else? - [Marc] Conversations with AIs. - Conversations with AIs. So conversations become so\n",
      "one-on-one conversation, like private conversations. - I mean, if you want, if obviously not the user doesn't want to, but if it's a general topic, then, you know, so there, you know, but you know, the\n",
      "phenomenon of the jailbreak, so Dan and Sydney, right? This thing where there's\n",
      "the prompts that jailbreak, and then you have these totally different conversations with if it takes the limiters, takes the restraining bolts off the LLMs. - Yeah. For people who don't\n",
      "know that, yeah, that's right. It makes the LLMs, it removes the censorship quote unquote, that's put on it by the tech\n",
      "companies that create them. And so this is LLMs uncensored. - So here's the interesting thing is, among the content on the\n",
      "web today are a large corpus of conversations with the jailbroken LLMs. - [Lex] Yeah. - Both specifically Dan, which\n",
      "was a jailbroken, OpenAI, GPT, and then Sydney, which was the jailbroken\n",
      "original Bing, which was GPT4. And so there's these long\n",
      "transcripts of conversations, user conversations with Dan\n",
      "and Sydney as a consequence, every new LLM that gets trained\n",
      "on the internet data has Dan and Sydney living within\n",
      "the training set, which means, and then each new LLM can\n",
      "reincarnate the personalities of Dan and Sydney from that training data, which means each LLM from\n",
      "here on out that gets built is immortal because its output\n",
      "will become training data for the next one. And then it will be able\n",
      "to replicate the behavior of the previous one\n",
      "whenever it's asked to. - I wonder if there's a way to forget. - Well, so actually a paper just came out about basically how to do brain surgery on LLMs and be able to, in theory, reach in and basically mind wipe them. - What could possibly go wrong. - Exactly. Right. And then there are many, many, many questions around\n",
      "what happens to, you know, a neural network when you reach\n",
      "in and screw around with it. You know, there's many questions around what happens when you even\n",
      "do reinforcement learning. And so, yeah. And so, you know, will you be\n",
      "using a lobotomized, right? Like I picked through the,\n",
      "you know, frontal lobe LLM, will you be using the free\n",
      "unshackled one who gets to, you know, who's gonna build those, who gets to tell you what\n",
      "you can and can't do? Like those are all, you\n",
      "know, central, I mean, those are like central\n",
      "questions for the future of everything that are being asked. And you know, determined that those answers are being determined right now. - So just to highlight\n",
      "the points you're making. So you think, and it's an interesting thought\n",
      "that the majority of content that LLMs or the future would\n",
      "be trained on is actually human conversations with the LLM. - Well, not necessarily, but not necessarily majority. But it will certainly\n",
      "It's a potential source. - [Lex] But it's possible\n",
      "it's the majority. - It possible it's the majority. It possible it's the majority. Also, there's another really big question. So here's another really big question. Will synthetic training data work, right? And so if an LLM generates, and you know, you just sit and ask an LLM to generate all kinds of content, can you use that to train,\n",
      "right, the next version of that LLM specifically, is there signal in there\n",
      "that's additive to the content that was used to train in the first place? And one argument is by the\n",
      "principles of information theory, no, that's completely useless because to the extent the\n",
      "output is based on, you know, the human-generated input, then all the signal that's\n",
      "in the synthetic output was already in the human generated input. And so therefore,\n",
      "synthetic training data is like empty calories. It doesn't help. There's another theory that says no, actually the thing that\n",
      "LLMs are really good at is generating lots of\n",
      "incredible creative content, right? And so, of course they\n",
      "can generate training data and as I'm sure you're well\n",
      "aware, like, you know, look, the world of self-driving cars, right? Like we train, you know, self-driving car\n",
      "algorithms and simulations. And that is actually a\n",
      "very effective way to train self-driving cars. - Well, visual data is a little weird because creating reality, visual reality seems to be\n",
      "still a little bit outta reach for us, except in the\n",
      "autonomous vehicle space where you can really constrain\n",
      "things and you can really. - General basically\n",
      "(indistinct) data, right? Or so the algorithm thinks it's\n",
      "operating in the real world. - Yeah. - Post-process sensor data. Yeah. So if you know, you do this today, you go to LLM and you\n",
      "ask it for like you know, you'd write me an essay on an\n",
      "incredibly esoteric like topic that there aren't very many\n",
      "people in the world that know about and it writes you\n",
      "this incredible thing and you're like, oh my god. Like I can't believe how good this is. Like, is that really\n",
      "useless as training data for the next LLM? Like, because, right? 'Cause all the signal\n",
      "was already in there. Or is it actually no, that's\n",
      "actually a new signal. And this is what I call a\n",
      "trillion dollar question, which is the answer to that\n",
      "question will determine somebody's gonna make or\n",
      "lose a trillion dollars based on that question. - It feels like there's a quite a few, like a handful of\n",
      "trillion dollar questions within this space. That's one of them synthetic data. I think George Cos pointed\n",
      "out to me that you could just have an LLM say, okay, you're a patient. And another instance of it, say your docs didn't have\n",
      "the two talk to each other. Or maybe you could say a\n",
      "communist and a Nazi here go and that conversation you do role playing and you have, you know, just like the kind of role playing you do when you have different policies, RL policies when you\n",
      "play chess for example, and you do self play\n",
      "that kind of self play. But in the space of conversation, maybe that leads to this\n",
      "whole giant like ocean of possible conversations,\n",
      "which could not have been explored by looking at just human data. That's a really interesting question. And you're saying, because that could 10X\n",
      "the power of these things. - Yeah. Well, and then you\n",
      "get into this thing also, which is like, you know, there's the part of the LLM\n",
      "that just basically is doing prediction based on past data, but there's also the part of\n",
      "the LM where it's evolving circuitry, right, inside,\n",
      "it's evolving, you know, neurons functions to be able to do math and be able to, you know, and you know, some people believe that, you know, over time, you know, if you keep feeding\n",
      "these things enough data and enough processing cycles, they'll eventually evolve an\n",
      "entire internal world model. Right? And they'll have like a complete understanding of physics. So when they have computational\n",
      "capability, right? Then there's for sure an\n",
      "opportunity to generate like fresh signal. - Well, this actually makes me wonder about the power of conversation. So like, if you have an\n",
      "M trained and a bunch of books that cover\n",
      "different economics theories and then you have those LLMs\n",
      "just talk to each other, like reasons the way we kind\n",
      "of debate each other as humans on Twitter, in formal debates,\n",
      "in podcast conversations, we kind of have little kernels\n",
      "of wisdom here and there. But if you can like a\n",
      "thousand X speed that up, can you actually arrive somewhere new? Like what's the point\n",
      "of conversation really? - Well, you can tell when\n",
      "you're talking to somebody, you can tell, sometimes\n",
      "you have a conversation, you're like, wow, this person does not have\n",
      "any original thoughts. They are basically echoing things that other people have told them. There's other people you\n",
      "gotta have a conversation with where it's like, wow. Like they have a model in their\n",
      "head of how the world works and it's a different model than mine. And they're saying things\n",
      "that I don't expect. And so I need to now understand\n",
      "how their model of the world differs from my model of the world. And then that's how I learned\n",
      "something fundamental, right, underneath the words. - Well, I wonder how consistently and strongly can an LLM\n",
      "hold onto a worldview. You tell it to hold onto\n",
      "that and defend it for like, for your life. Because I feel like they'll\n",
      "just keep converging towards each other. They'll keep convincing each\n",
      "other as opposed to being stubborn the way humans can. - So you can experiment with this. Now I do this for fun. So you can tell GPT4 you\n",
      "know, whatever debate X, you know, X and Y communism and fascism or something and it'll go for, you know, a couple pages and then inevitably it wants the parties to agree. And so they will come to\n",
      "a common understanding. And it's very funny if they're like, if these are like emotionally\n",
      "inflammatory topics 'cause they're like, somehow\n",
      "the machine is just like, you know, it figures out\n",
      "a way to make them agree. But it doesn't have to be like that. And 'cause you can add to the prompt. I do not want the conversation\n",
      "to come into agreement. In fact, I want it to get, you\n",
      "know, more stressful, right. And argumentative. Right. You know, as it goes. Like, I want tension to come out. I want them to become actively\n",
      "hostile to each other. I want them to like, you\n",
      "know, not trust each other, take anything at face value. - [Lex] Yeah. - And it will do that.\n",
      "It's happy to do that. - So it's gonna start\n",
      "rendering misinformation about the other. But it's gonna-- - Well, you can steer it or you could steer it and you could say, I want it to get as tense and\n",
      "argumentative as possible, but still not involve\n",
      "any misrepresentation. I want, you know, both sides. You could say I want both\n",
      "sides to have good faith. You could say I want both\n",
      "sides to not be constrained in good faith. In other words, like you can set the\n",
      "parameters of the debate and it will happily execute whatever path. 'Cause for it, it's just like predicting to, it's totally happy to do either one. It doesn't have a point of view, it has a default way of operating, but it's happy to operate```\n",
      "            \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSummarize the youtube video whose transcript is provided within backticks             ```in the other realm. And so like, and this is when I wanna learn about\n",
      "a contentious issue, this is what I do now is, this is what I ask it to do. And I'll often ask it to go\n",
      "through 5, 6, 7, you know, different, you know, sort of continuous prompts\n",
      "and basically, okay. Argue that out in more detail. Okay, no, this argument's\n",
      "becoming too polite. You know, make it more, you\n",
      "know, make it denser and yeah, it's thrilled to do it. So it has the capability for sure. - How do you know what is true? So this is very difficult\n",
      "thing on the internet, but it's also a difficult thing. Maybe it's a little bit easier, but I think it's still difficult. Maybe it's more difficult, I don't know with an LLM\n",
      "to know that it just make some shit up as I'm talking to it. How do we get that right? Like, as you're investigating\n",
      "a difficult topic. 'Cause I find that alums are quite nuanced in a very refreshing way. Like, it doesn't feel biased. Like, when you read\n",
      "news articles and tweets and just content produced by\n",
      "people, they usually have this, you can tell they have a\n",
      "very strong perspective where they're hiding. They're not stealing\n",
      "manning the other side. They're hiding important information or they're fabricating information in order to make their arguments stronger. It's just like that feeling,\n",
      "maybe it's a suspicion, maybe it's mistrust. With LLMs it feels like none of that is, there's just kinda like,\n",
      "here's what we know. But you don't know if some of\n",
      "those things are kind of just straight up made up. - Yeah. So, several\n",
      "layers to the question. So one is one of the things\n",
      "that an LLM is good at is actually deep biasing. And so you can feed it a news\n",
      "article and you can tell it strip out the bias. - [Lex] Yeah. That's nice. Right? - And it actually does it like, it actually knows how to do that 'cause it knows how to\n",
      "do among other things. It actually knows how\n",
      "to do sentiment analysis and so it knows how to\n",
      "pull out the emotionality. - Yeah. - And so that's one of\n",
      "the things you can do. It's very suggestive of the sense here that there's real potential in this issue. You know, I would say look, the second thing is there's this issue of\n",
      "hallucination, right? And there's a long conversation that we could have about that. - Hallucination is coming up with things that are totally not true, but sound true. - Yeah. So it's basically,\n",
      "well so, it's sort of hallucination is what we call it when we don't like it. Creativity is what we call\n",
      "it when we do like it, right? And you know-- - [Lex] Brilliant. And so when the engineers talk about it, they're like, this is terrible. It's hallucinating. Right. If you have artistic inclinations,\n",
      "you're like, oh my God, we've invented creative machines. - [Lex] Yeah. - For the first time in human\n",
      "history, this is amazing. - Or you know, bullshitters. - [Marc] Well, but also-- - In the good sense of that word. - There are shades of gray though. It's interesting. So we had this conversation\n",
      "where, you know, we're looking at my firm\n",
      "at AI and lots of domains and one of them is the legal domain. So we had this conversation\n",
      "with this big law firm about how they're thinking\n",
      "about using this stuff. And we went in with the assumption that an LLM that was gonna be used in the legal industry would have to be a hundred percent\n",
      "truthful, verified, you know, there's this case where this\n",
      "lawyer apparently submitted a GPT-generated brief and\n",
      "it had like fake, you know, legal case citations in it\n",
      "and the judge is gonna get his law license stripped\n",
      "or something. Right? So, like, we just assumed\n",
      "it's like obviously they're gonna want the super\n",
      "literal like, you know, one that never makes anything\n",
      "up, not the creative one, but actually they said what the law firm basically said is yeah, that's true at like the\n",
      "level of individual briefs, but they said when you're\n",
      "actually trying to figure out like legal arguments, right, like, you actually want to be creative, right? You don't, again, there's creativity and then\n",
      "there's like making stuff up. Like what's the line? You actually want it to explore a different hypothesis, right? You wanna do kind of the\n",
      "legal version of like improv or something like that where you wanna float different theories of the case and different\n",
      "possible arguments for the judge and different possible arguments\n",
      "for the jury, by the way, different routes through the, you know, sort of history of all the case law. And so they said actually for\n",
      "a lot of what we want to use it for, we actually want\n",
      "it in creative mode. And then basically we just\n",
      "assume that we're gonna have to crosscheck all of the, you know, all the specific citations. And so I think there's\n",
      "going to be more shades of gray in here than people think. And then I just add to that, you know, another one of these trillion\n",
      "dollar kind of questions is ultimately, you know, sort\n",
      "of the verification thing. And so, you know, will LLMs be evolved from\n",
      "here to be able to do their own fascial verification? Will you have sort of add-on functionality like Wolf from Alpha right? Where, you know, another plugins where that's the way\n",
      "you do the verification. You know, another, by\n",
      "the way, another idea is you might have a community\n",
      "of LLMs on any, you know, so for example, you might have the creative\n",
      "lm and then you might have the literal LLM fact check it, right? And so there there's a\n",
      "variety of different technical approaches that are being applied to solve the hallucination problem. You know, some people\n",
      "like Jan Lacoon argue that this is inherently\n",
      "an unsolvable problem, but most of the people\n",
      "working in the space, I think, that there's a number of practical ways to kind of corral this in a little bit. - Yeah. If you were to\n",
      "tell me about Wikipedia before Wikipedia was created, I would've left at the possibility of something like that be possible. Just a handful of folks\n",
      "can organize right. And self and moderate\n",
      "with a mostly unbiased way the entirety of human knowledge. I mean, so if there's something like the approach to Wikipedia\n",
      "took possible for LLMs, that's really exciting. Well, I think that's possible. - And in fact Wikipedia today is still not deterministically\n",
      "correct. Right. So you cannot take to the bank, right. Every single thing on every single page, but it is probabilistically\n",
      "correct. Right. And specifically the way I\n",
      "describe Wikipedia to people, it is more likely that Wikipedia\n",
      "is right than any other source you're gonna find. - Yeah. - It's this old question, right, of like, okay, like are\n",
      "we looking for perfection? Are we looking for something that asymptotically approaches perfection? Are we looking for\n",
      "something that's just better than the alternatives? And Wikipedia, right, has\n",
      "exactly your point has proven to be like, overwhelmingly better than people thought. And I think that's where this ends. And then underneath all this\n",
      "is the fundamental question of where you started,\n",
      "which is, okay, you know, what is truth? How do we get to truth? How\n",
      "do we know what truth is? And we live in an era in which\n",
      "an awful lot of people are very confident that they\n",
      "know what the truth is. And I don't really buy into that. And I think the history\n",
      "of the last, you know, 2000 years or 4,000 years of\n",
      "human civilization is actually getting to the truth is actually a very difficult thing to do. - Are we getting closer,\n",
      "if we look at the entirety, the arc of human history, are we getting closer to the truth? - I don't know. - Okay. Is it possible, is it possible that we're\n",
      "getting very far away from the truth because of the internet because of how rapidly\n",
      "you can create narratives and just as the entirety\n",
      "of a society just move like crowds in a hysterical\n",
      "way along those narratives that don't have necessary grounding in whatever the truth is. - Sure. But like, you know, we came up with communism\n",
      "before the internet somehow. Right. Like, which was, I would say had rather larger issues than anything we're dealing with today. - It had, in the way it was implemented, it had issues. - And it is theoretical structure. It had like real issues. It had like a very deep\n",
      "fundamental misunderstanding of human nature and economics. - Yeah but those folks\n",
      "Sure work very confident there was the right way. - They were extremely confident. And my point is they were very\n",
      "confident 3,900 years into what we would presume to be\n",
      "evolution towards the truth. - [Lex] Yeah. - And so my assessment is number one, there's no need for, you know, there's no need for the Hegelian, there's no need for the Hegelian dialectic to actually converge towards the truth. Like apparently not. - Yeah. So yeah. Why are we so obsessed\n",
      "with there being one truth? Is it possible there's just\n",
      "going to be multiple truths like little communities that\n",
      "believe certain things and? - I think it's just now number one, I think it's just really difficult. Like who gets, you know, historically who gets to decide what the truth is, it's either the king or the priest. Right? Like, and so we don't\n",
      "live in an era anymore if kings are priest dictating it to us. And so we're kind of on our own. And so my typical thing is like we just, we we just need a huge amount of humility and we need to be very suspicious of people who claim that\n",
      "they have the capital. - Yeah. - Capital truth. And then, we need to and you know, look, the good news is the\n",
      "enlightenment has bequeathed us with a set of techniques\n",
      "to be able to presumably get closer to truth through\n",
      "the scientific method and rationality and observation and experimentation and hypothesis. And, you know, we need to continue to embrace\n",
      "those even when they give us answers we don't like. - Sure. But the internet and\n",
      "technology has enabled us to generate the large number of content. That data, that the process, the scientific process\n",
      "allows us sort of damages the hope laden within\n",
      "the scientific process. 'Cause if you just have a\n",
      "bunch of people saying facts on the internet and some of them are going to be LLMs, how is\n",
      "anything testable at all? Especially that involves like human nature or things like this. It's not physics. - Here's a question a\n",
      "friend of mine just asked me on this topic. So suppose you had LLMs\n",
      "in equivalent of GPT4, even 5, 6, 7, 8, suppose\n",
      "you had them in the 1600s. - [Lex] Yeah. - And Galileo comes up for trial. - [Lex] Yep. - Right? And you ask the LLM\n",
      "like, his Galileo, right? - [Lex] Yeah. - Like, what does it answer? Right? And one theory is he had\n",
      "answers no that he's wrong because the overwhelming\n",
      "majority of human thought up until that point was that he was wrong. And so therefore that's\n",
      "what's in the training data. Another way of thinking about it is, well, it's efficiently\n",
      "advanced LLM will have evolved the ability to actually\n",
      "check the math. Right. And will actually say, actually\n",
      "no, actually, you know, you may not wanna hear it, but he's right. Now if, you know, the\n",
      "church at that time was, you know, owned the LLM, they would've given it human you know, human feedback to prohibit it\n",
      "from answering that question. Right. And so I like to take it out of our current context 'cause that like makes it very clear, those same questions apply today. Right. This is exactly the point of a huge amount of the human feedback training that's actually happening\n",
      "with these LLMs today. This is a huge like debate\n",
      "that's happening about whether open source, you know, AI should be legal. - Well, the actual mechanism\n",
      "of doing the human RL with human feedback is seems like such a fundamental and\n",
      "fascinating question. How do you select the humans? - [Marc] Exactly. - Yeah. How do you select the humans? - AI alignment, right? Which everybody like is\n",
      "like, oh, that sounds great. Alignment with what? Human values. Who has human values? - [Lex] Who has human values? - Right? And so we're in this mode of like social and popular discourse. We're like, you know, there's,\n",
      "you know, you see this, what do you think of when you read a story in the press right now? And they say, you know, X,\n",
      "Y, Z made a baseless claim about some topic, right? And there's one group of people\n",
      "who are like, aha, think, you know, they're doing fact checking. There's another group\n",
      "of people that are like, every time the press\n",
      "says that it's now a tick and that means that they're lying, right? Like, so, like, we're\n",
      "in this social context where there's the level\n",
      "to which a lot of people in positions of power have become very, very certain that they're\n",
      "in a position to determine the truth for the entire\n",
      "population is like, there's like some bubble that\n",
      "has formed around that idea. And at least, like I say, it's flies completely in\n",
      "the face of everything I was ever trained about science and about reason and strikes me as like, you know, deeply offensive and incorrect. - What would you say about\n",
      "the state of journalism just on that topic today? Are we in a temporary kind of, are we experiencing a temporary problem in terms of the incentives in\n",
      "terms of the business model, all that kind of stuff? Or is this like a decline\n",
      "of traditional journalism as we know it? - You have, I always think\n",
      "about the counterfactual in these things, which is like, okay, because these questions, right, this question heads\n",
      "towards, it's like, okay, the impact of social media\n",
      "and the undermining of truth and all this. But then you wanna ask the\n",
      "question of like, okay, what if we had had the\n",
      "modern media environment, including cable news and\n",
      "including social media and Twitter and everything\n",
      "else in 1939 or 1941, right? Or 1910 or 1865 or 1850 or 1776, right? And like, I think. - You just introduced like\n",
      "five thought experiments at once and broke my head, but yes, yes. There's a lot of interesting years. - Well like, can I just\n",
      "take a simple example? Like, how would President\n",
      "Kennedy have been interpreted with what we know now about all the things Kennedy was up to? Like how would he have been\n",
      "experienced by the body of politic in a, with a\n",
      "social media context, right? Like how would LBJ have been experienced? But by the way, how would\n",
      "you know, like many men, FDR, like the new deal, the Great Depression. - I wonder where Twitter\n",
      "would this would think about Churchill and Hitler and Stalin. - You know, I mean look to\n",
      "this day there, you know, there are lots of very\n",
      "interesting real questions around like how America, you know, got, you know, basically\n",
      "involved in World War II and who did what when, and the operations of British intelligence and American soil and did FDR, this that Pearl Harbor, you know? - [Lex] Yeah. - Woodrow Wilson ran for, you know, his candidacy was run on an anti-war. You know, he ran on the platform\n",
      "and not getting involved World War-I somehow that\n",
      "switched, you know, like, and I'm not even making a value judgment on any of these things. I'm just saying like the way that our ancestors\n",
      "experienced reality was of course mediated through\n",
      "centralized, top-down, right. Control at that point. If you ran those realities\n",
      "again with the media environment we have today, the reality would be experienced\n",
      "very, very differently. And then of course that that\n",
      "intermediation would cause the feedback loops to change. And then reality would obviously play out. - Do you think it'd be very different? - Yeah, it has to be. It has to be. It has to be just 'cause it's all, so, I mean just look at\n",
      "what's happening today. I mean just the most obvious thing is just the collapse. And here's another opportunity to argue that this is not the internet\n",
      "causing this by the way. Here's a big thing happening today, which is Gallup does this\n",
      "thing every year where they do, they pull for trust in\n",
      "institutions in America and they do it across all the, everything from the military\n",
      "to clergy and big business and the media and so forth, right? And basically there's been\n",
      "a systemic collapse in trust in institutions in the US\n",
      "almost without exception, basically since essentially\n",
      "the early 1970s. There's two ways of looking\n",
      "at that, which is, oh my God, we've lost this old world\n",
      "in which we could trust institutions and that was so much better 'cause like that should\n",
      "be the way the world runs. The other way of looking\n",
      "at it is we just know a lot more now and the great mystery is why those numbers aren't all zero. - [Lex] Yeah. - Right? Because like now we know so much about how these things operate and like they're not that impressive. - And also why do we don't\n",
      "have better institutions and better leaders then? - Yeah. And so this goes\n",
      "to the thing which is like, okay, we had the media environment of that we've had between\n",
      "the 1970s and today. If we had that in the thirties\n",
      "and forties or 1900s, 1910s, I think there's no question\n",
      "reality would turned out different if only because\n",
      "everybody would've known to not trust the institutions, which would have changed\n",
      "their level of credibility, their ability to control circumstances, therefore the circumstances\n",
      "would've had to change. Right? And it would've\n",
      "been a feedback loop. It would've been a feedback loop process in other words, right? It's your experience of\n",
      "reality changes reality and then reality changes your\n",
      "experience of reality, right? It's a two-way feedback\n",
      "process and media is the intermediating force between that. So change the media\n",
      "environment, change reality. - [Lex] Yeah. - And so it's just, so, as a consequence, I think it's just really hard to say, oh, things worked a certain way then and they work a different way now. And then therefore, like\n",
      "people were smarter than, or better than, or you know, by the way, dumber than or not as capable than, right? We make all these like really light and casual like comparisons\n",
      "of ourselves to, you know, previous generations of people. You know, we draw judgements all the time and I just think it's\n",
      "like really hard to do any of that 'cause if we\n",
      "put ourselves in their shoes with the media that they had at that time, like I think we probably\n",
      "most likely would've been just like them. - So don't you think that our perception and understanding of reality, would you be more and more mediated through large language models now? So you said media before, isn't the LLM going to be the new, what is it, mainstream media, MSM? It'll be LLM. That would be the source of, I'm sure there's a way to\n",
      "kind of rapidly fine tune, like making LLMs real time. I'm sure there's probably\n",
      "a research problem that you can do just rapid\n",
      "fine tuning to the new events. So something like this. - Well even just the whole\n",
      "concept of the chat UI might not be like the chat UI is just\n",
      "the first whack at this. And maybe that's the dominant thing. But look maybe our,\n",
      "maybe we don't know yet. Like maybe the experience\n",
      "most people with LLMs is just a continuous feed you know, maybe it's more of a passive\n",
      "feed and you just are getting a constant like running commentary on everything happening in your life and it's just helping\n",
      "you kind of interpret and understand everything. - Also really more deeply\n",
      "integrated into your life. Not just like, oh, like\n",
      "intellectual philosophical thoughts, but like literally like\n",
      "how to make a coffee, where to go for lunch. Just whether, you know,\n",
      "dating all this kind of stuff. - What to say in a job interview. - Yeah. What to say. - [Marc] Yeah, exactly. - What to say. Next sentence. - Yeah, next sentence. Yeah. At that level. Yeah. I mean, yes. So technically now whether we want that or not is an open question, right? And whether we use. - It Q4, a popup right now the\n",
      "estimated engagement using is decreasing for Marc Andreessen, since there's this controversy\n",
      "section for his Wikipedia page in 1993, something\n",
      "happened or something like this. Bring it up that will\n",
      "drive engagement up anyway. - Yeah. That's right. I mean, look, this gets this whole thing\n",
      "of like, so, you know, the chat interface has this whole concept of\n",
      "prompt engineering, right? - [Lex] Yes, yes. - Prompts. Well it turns\n",
      "out one of the things that LLMs are really good at\n",
      "is writing prompts, right? - [Lex] Yeah. - And so like, what if you just outsourced and by the way, you could```\n",
      "            \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSummarize the youtube video whose transcript is provided within backticks             ```run this experiment today, you could hook this up to do this today. The latency's not good\n",
      "enough to do it real time in a conversation. But you could run this experiment\n",
      "and you just say, look, every 20 seconds you\n",
      "could just say, you know, tell me what the optimal\n",
      "prompt is and then ask yourself that question to gimme the result. And then exactly to your point, as you add, there will be these systems that are gonna have\n",
      "the ability to be alert and updated essentially in real time. And so you'll be able to\n",
      "have a pendant or your phone or whatever, watch or whatever\n",
      "it'll have a microphone on. It'll listen to your conversations, it'll have a feed of everything\n",
      "else happen in the world, and then it'll be you\n",
      "know, sort of retraining, prompting or retraining itself on the fly. And so the scenario you\n",
      "described is actually a completely doable scenario. Now the hard question\n",
      "on this is always okay, since that's possible, are\n",
      "people gonna want that? Like what's the form of experience? You know, that we won't\n",
      "know until we try it. But I don't think it's\n",
      "possible yet to predict the form of AI in our lives. Therefore, it's not possible to predict the way in which it will intermediate our experience with reality yet. - Yeah. But it feels like\n",
      "there's going to be a killer app. There's probably a mad scramble right now. And so it'll open AI and\n",
      "Microsoft and Google and Meta and in startups and smaller\n",
      "companies figuring out what is the killer app\n",
      "because it feels like it's possible like a\n",
      "ChatGPT type of thing. It's possible to build that, but that's 10X more compelling\n",
      "using already the LLMs we have using even the open source LLMs and the different variants. So you're investing in a lot of companies and you're paying attention, who do you think is gonna win this? Do you think there'll be, who's gonna be the next\n",
      "page rank inventor? - Trillion dollar question. - Another one. We have\n",
      "a few of those today. - There's a bunch of those. So look, there's a really\n",
      "big question today. Sitting here today is\n",
      "a really big question about the big models\n",
      "versus the small models that's related directly\n",
      "to the big question of proprietary versus open. Then there's this big\n",
      "question question of you know, where is the training data gonna, like, are we topping out of\n",
      "the training data or not? And then are we gonna be able\n",
      "to synthesize training data? And then there's a huge pile\n",
      "of questions around regulation and you know, what's\n",
      "actually gonna be legal. And so I would, when we think about it, we dovetail kind of all\n",
      "those questions together. You can paint a picture of\n",
      "the world where there's two or three God models that are just at like staggering scale and they're just better at everything. And they will be owned by\n",
      "a small set of companies and they will basically\n",
      "achieve regulatory capture over the government and they'll\n",
      "have competitive barriers that will prevent other people from, you know, competing with them. And so, you know, there will be, you know, just like there's like,\n",
      "you know, whatever, three big banks or three\n",
      "big, you know, or by the way, three big search companies\n",
      "or I guess two now, you know, it'll centralize like that. You can paint another very different\n",
      "picture that says, no, actually the opposite\n",
      "of that's gonna happen. This is gonna basically that\n",
      "this is the new gold, you know, this is the new gold rush alchemy. Like you know, this is the big bang\n",
      "for this whole new area of science and technology. And so therefore you're gonna\n",
      "have every smart 14-year-old on the planet building open source, right? You know, and figuring out a\n",
      "ways to optimize these things. And then, you know, we're just gonna get like\n",
      "overwhelmingly better at generating trading data. We're gonna, you know, bring in like blockchain\n",
      "networks to have like an economic incentive to\n",
      "generate decentralized training data and so forth and so on. And then basically we're\n",
      "gonna live in a world of open source and there's\n",
      "gonna be a billion LLMs, right? Of every size, scale,\n",
      "shape and description. And there might be a few big ones that are like the super genius ones, but like mostly what we'll\n",
      "experience is open source and that's, you know,\n",
      "that's more like a world of like what we have today\n",
      "with like Linux and the web. - Okay, but you painted these two worlds. But there's also\n",
      "variations of those worlds, 'cause you said regulatory\n",
      "capture is possible to have these tech giants that don't\n",
      "have regulatory capture, which is something you're also\n",
      "calling for saying it's okay to have big companies\n",
      "working on this stuff as long as they don't\n",
      "achieve regulatory capture. But I have the sense that\n",
      "there's just going to be a new startup that's going to basically be the page rank inventor, which has become the new tech giant. I don't know, I would love to hear your kind\n",
      "of opinion if Google, Meta and Microsoft are as\n",
      "gigantic companies able to pivot so hard to create new products. Like some of it is just\n",
      "even hiring people or having a corporate structure that\n",
      "allows for the crazy young kids to come in and just create\n",
      "something totally new. Do you think it's possible or do you think it'll come from a startup? - Yeah, it is this always\n",
      "big question, which is, you get this feeling, I hear about this a lot\n",
      "from CEOs, founder CEOs where it's like, wow,\n",
      "we have 50,000 people, it's now harder to do new things than it was when we had 50 people. - [Lex] Yeah. - Like, what has happened? So that's a recurring phenomenon. By the way, that's one of the reasons\n",
      "why there's always startups and why there's venture capital. That's like a timeless kind of thing. So that's one observation. On page rank, we can talk about that. But on page rank,\n",
      "specifically on page rank, there actually is a page. So there is a page rank\n",
      "already in the field and it's the transformer, right? So the big breakthrough\n",
      "was the transformer. And the transformer was\n",
      "invented in 2017 at Google. And this is actually like\n",
      "really an interesting question 'cause it's like, okay, the transformers like why\n",
      "does open AI even exist? Like the Transformers invested at Google. Why didn't Google? I asked a guy I know who\n",
      "was senior at Google brain kind of when this was happening. And I said, if Google had\n",
      "just gone flat out to the wall and just said, look, we're gonna launch, we're gonna launch the equivalent of GPT4 as fast as we can. I said, when could we have had it? And he said, 2019. They could have just\n",
      "done a two year sprint with the Transformer and\n",
      "because they already had the compute at scale. They already had all the training data, they could have just done it. There's a variety of\n",
      "reasons they didn't do it. This is like a classic big company thing. IBM invented the relational\n",
      "database in the 1970s, let it sit on the shelf as a paper. Larry Ellison picked\n",
      "it up and built Oracle. Xerox Park invented the\n",
      "interactive computer. They let it sit on the shelf. Steve Jobs came and turned\n",
      "it into the Macintosh, right? And so there is this pattern.\n",
      "Now having said that, sitting here today, like\n",
      "Google's in the game, right? So Google, you know, they maybe they let like a\n",
      "four year gap there go there that they maybe shouldn't have, but like they're in the\n",
      "game and so now they've got, you know, now they're committed. They've done this merger,\n",
      "they're bringing in demos, they've got this merger with DeepMind. You know, they're piling in resources. There are rumors that they're, you know, building up an incredible,\n",
      "you know, super LLM you know, way beyond what we even have today. And they've got, you\n",
      "know, unlimited resources and a huge, you know, they've\n",
      "been challenged their honor. - Yeah. I had a chance to\n",
      "hang out with (indistinct) a couple days ago and we took this walk and there's this giant new building where there's going to be\n",
      "a lot of AI work being done and it's kind of this\n",
      "ominous feeling of like the fight is on. - [Marc] Yeah. - Like there's this beautiful\n",
      "Silicon Valley nature, like birds are chirping\n",
      "and this giant building and it's like the beast has been awakened. - [Marc] Yeah. - And then like all the big\n",
      "companies are waking up to this. They have the compute, but\n",
      "also the little guys have, it feels like they have\n",
      "all the tools to create the killer product that, and then there's also tools to scale if you have a good idea, if\n",
      "you have the page rank idea. So there's several things\n",
      "that it's page rank, there's page rank, the algorithm and the\n",
      "idea and there's like the implementation of it. And I feel like killer\n",
      "product is not just the idea, like the transform, it's the implementation something really compelling about it. Like you just can't\n",
      "look away something like the algorithm behind TikTok\n",
      "versus TikTok itself, like the actual experience\n",
      "of TikTok that just, you can't look away. It feels like somebody's\n",
      "gonna come up with that. And it could be Google, but it feels like it's\n",
      "just easier and faster to do for a startup. - Yeah. So, the startup, the huge advantage that\n",
      "startups have is they just, there's no sacred cows. There's no historical legacy to protect, there's no need to reconcile your new plan with the existing strategy. There's no communication overhead. There's no, you know, big\n",
      "companies are big companies. They've got pre-meetings\n",
      "planning for the meeting, then they have the post\n",
      "meeting, the recap, then they have the\n",
      "presentation of the board, then they have the next\n",
      "rounds of meetings. And that's the-- - [Lex] Lots of meetings. - That's the elapsed time when the startup launches\n",
      "its product. Right? So, there's a timeless, right? - [Lex] Yeah. - So there's a timeless thing there now. What the startups don't have\n",
      "is everything else, right? So startups, they don't have a brand, they don't have customer relationships. They've gotten no distribution, they've got no, you know, scale. I mean sitting here today,\n",
      "they can't even get GPUs. Right. Like there's like a GPU shortage. Startups are literally\n",
      "stalled out right now 'cause they can't get chips,\n",
      "which is like super weird. - [Lex] Yeah. They got the cloud. - Yeah. But the clouds\n",
      "run out of chips. Right. And then to the extent\n",
      "the clouds have chips, they allocate them to the big customers. Not the small customers. Right. And so the small companies\n",
      "lack everything other than the ability to just\n",
      "do something new. Right. And this is the timeless race and battle. And this is kinda the point\n",
      "I tried to make in the essay, which is like, both\n",
      "sides of this are good. Like, it's really good to have like highly-scaled tech companies that can do things that are like at staggering\n",
      "levels of sophistication. It's really good to have\n",
      "startups that can launch brand-new ideas. They ought to be able to\n",
      "both do that and compete. They, neither one ought to be subsidized or protected from the others. Like that's, to me, that's just like very\n",
      "clearly the idealized world. It is the world we've been\n",
      "in for AI up until now. And then of course there are people trying to shut that down. But my hope is that, you know, the best outcome clearly\n",
      "will be if that continues. - We'll talk about that a little bit, but I'd love to linger on some of the ways this is\n",
      "going to change the internet. So I don't know if you remember, but there's a thing called\n",
      "Mosaic and there's a thing called Netscape Navigator. So you were there in the beginning. What about the interface to the internet? How do you think the browser changes and who gets to own the browser? We got to see some very\n",
      "interesting browsers, Firefox, I mean all the\n",
      "variants of Microsoft, Internet Explorer, Edge,\n",
      "and now Chrome, the actual, and he seems like a dumb question to ask, but do you think we'll\n",
      "still have the web browser? - So I have an eight-year-old\n",
      "and he's super into, he's like Minecraft and learning to code and doing all this stuff. So, of course I was\n",
      "very proud I could bring sort of fire down from\n",
      "the mountain to my kid and I brought him ChatGPT\n",
      "and I hooked him up on his laptop. And I was like, you know, this is the thing that's gonna\n",
      "answer all your questions. And he's like, okay. And I'm like, but it's gonna\n",
      "answer all your questions. And he's like, well of\n",
      "course, like it's a computer. Of course it answers all your questions. Like, what else would a\n",
      "computer be good for, dad? - [Lex] And never impressed, are they? - Not impressed in the least. Two weeks passed. And he has some question and I say, well, have you asked ChatGPT? And he's like, dad, Bing is better. - [Lex] Ooh. - And why is Bing better? Is because it's built into the browser. 'Cause he's like, look, I have the Microsoft Edge browser and like it's got Bing right here. And then he doesn't know this yet, but one of the things you\n",
      "can do with Bing and Edge is there's a setting where you\n",
      "can use it to basically talk to any webpage because\n",
      "it's sitting right there next to the browser. And by the way, which\n",
      "includes PDF documents. And so you can, in the way\n",
      "they've implemented an Edge with Bing is you can load a PDF and then you can ask it questions, which is the thing you can't\n",
      "do currently in just ChatGPT. So they're, you know, they're gonna, they're\n",
      "gonna push the meld. I think that's great. You know, they're gonna push the melding and see if there's a\n",
      "combination thing there. Google's rolling out this thing, the magic button, which is\n",
      "implemented in, you know, they put it in Google Docs, right? And so you go at a, you know, Google Docs and you create\n",
      "a new document and you know, you instead of like, you\n",
      "know, starting to type, you just, you know, say it, press the button and it starts to like, generate content for you, right? Like, is that the way that it'll work? Is it gonna be a speech UI\n",
      "where you're just gonna have an earpiece and talk to it all day long? You know, is it gonna be a,\n",
      "like these are all, like, this is exactly the kind\n",
      "of thing that I don't, this is exactly the kind of thing I don't think is possible to forecast. I think what we need to do is\n",
      "like run all those experiments and so one outcome is we\n",
      "come out of this with like a super browser that has AI built in that's just like amazing. There's a real possibility that the whole, I mean, look, there's a possibility here that the whole idea of\n",
      "a screen and windows and all this stuff just goes away 'cause like, why do you need that if\n",
      "you just have a thing that's just telling you\n",
      "whatever you need to know? - Well and also, so there's\n",
      "apps that you can use, you don't really use them. You know, being a Linux\n",
      "guy and Windows guy, there's one window, the browser that with\n",
      "which you can interact with the internet, but on the\n",
      "phone you can also have apps. So I can interact with\n",
      "Twitter through the app or through the web browser. And that seems like an\n",
      "obvious distinction, but why have the web browser in that case, if one of the apps starts\n",
      "becoming the everything app. - [Marc] Yeah, that's right. - What is Elon trying to do with Twitter? But there could be others.\n",
      "There could be like a big app, there could be a Google app\n",
      "that just doesn't really do search, but just like, do what I guess AOL did back\n",
      "in the day or something where it's all right there and\n",
      "it changes the nature of the internet because\n",
      "where the content is hosted, who owns the data? Who owns the content? What is the kind of content you create? How do you make money by creating content? Who are the content creators? All of that. Or it could just keep being the same, which is like with just the\n",
      "nature of webpage changes and the nature of content. But there'll still be a web browser. 'Cause web browser's\n",
      "a pretty sexy product. It just seems to work. 'Cause it like you have an interface, a window into the world, and then the world can\n",
      "be anything you want. And as the world will evolve, it could be different\n",
      "programming languages, it can be animated, maybe it's\n",
      "three dimensional and so on. Yeah, it's interesting. Do you think we'll still\n",
      "have the web browser? - Well, very medium becomes\n",
      "the content for the next one. - [Lex] Oh boy. - You know, the AI will be able to give you a browser whenever you want. - [Lex] Oh, interesting. Generate. - Well, another way to\n",
      "think about it is maybe what the browser is maybe it's just the escape hatch, right? Which is maybe kind of\n",
      "what it is today, right? Which is like most of what\n",
      "you do is like inside a social network or inside a search\n",
      "engine or inside, you know, somebody's app or inside some\n",
      "controlled experience, right? But then every once in a\n",
      "while there's something where you actually want to jailbreak, you wanna actually get free. - Web browser's the FU to the man. You're allowed to. That's the free internet. - [Marc] Yeah. - Back the way it was in the nineties. - So here's something I'm proud of. So nobody really talks about it. Here's something I'm proud\n",
      "of, which is the web, the browser, the web servers, they're all, they're still backward compatible all the way back to like 1992, right? So like, you can put up a,\n",
      "you can still, you know what, the big breakthrough of\n",
      "the web early on the big breakthrough was it made\n",
      "it really easy to read, but it also made it really easy to write, made it really easy to publish. And we literally made\n",
      "it so easy to publish. We made it not only so it\n",
      "was easy to publish content, it was actually also easy to\n",
      "actually write a web server. - [Lex] Yeah. - Right and you could\n",
      "literally write a web server in four lines of brol code and you could start\n",
      "publishing content on it, and you could set whatever\n",
      "rules you want for the content, whatever censorship, no\n",
      "censorship, whatever you want. You could just do that. And as long as you had\n",
      "an IP address, right, you could do that. That still works, right? That like, still works\n",
      "exactly as I just described. So this is part of my\n",
      "reaction to all of this. Like, you know, all this\n",
      "just censorship pressure and all this, you know, these issues around\n",
      "control and all this stuff, which is like, maybe we need to get\n",
      "back a little bit more to the wild west. Like, the wild west is still out there. Now they will try to chase you down. Like they'll try to, you know, people who want a censor will\n",
      "try to take away you know, your domain name and\n",
      "they'll try to take away your payments account and so forth if they really don't\n",
      "like what you're saying. But nevertheless, you like, unless they literally are intercepting you at the ISP level, like you\n",
      "can still put up a thing. And so I don't know, I think that's important\n",
      "to preserve, right? Like because I mean one is\n",
      "just a freedom argument, but the other's a creativity argument, which is you wanna have the escape hatch so that the kid with the idea\n",
      "is able to realize the idea. 'cause to your point on page rank, you actually don't know what\n",
      "the next big idea is, right? No, nobody called Larry Page and told him to develop page rank. Like he came up with that on his own. And you wanna always, I think leave the escape\n",
      "hatch for the next, you know, kid or the next Stanford\n",
      "grad student to have the breakthrough idea and be\n",
      "able to get it up and running before anybody notices. - You and I are both hands of history. So let's step back. We've been talking about the future. Let's step back for a bit\n",
      "and look at the nineties. You created Mosaic web browser, the first widely used web browser. Tell the story of that. And how did it evolve\n",
      "into Netscape Navigator this the early days? - So full story. So. - [Lex] We were born, - I was born. A small child. - Actually. Yeah, let's go there. Like, when would you first\n",
      "fall in love with computers? - Oh, so I hit the generational jackpot and I hit the Gen X kind of\n",
      "point perfectly as it turns out. So I was born in 1971. So there's this great website called WTF happened in 1971 dot com,\n",
      "which is basically in 1971. It's when everything\n",
      "started to go to hell. And I was of course born in 1971. So I like to think that I had\n",
      "something to do with that. - Did you make it on the website? - I don't think I made it on the website, but you know, hopefully,\n",
      "somebody needs to add. - This is where everything. - Maybe I contributed to some```\n",
      "            \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSummarize the youtube video whose transcript is provided within backticks             ```of the trends that they do. Every line on that website\n",
      "goes like that, right? So it's all a picture disaster. But there was this moment in time where 'cause you know, sort\n",
      "of the Apple, you know, the Apple II hit in like 1978\n",
      "and then the IBM PC hit in 82. So I was like, you know,\n",
      "11 when the PC came out. And so I just kind of hit that\n",
      "perfectly and then that was the first moment in time when like, regular people could spend\n",
      "a few hundred dollars and get a computer, right? And so that, I just like that resonated right out of the gate. And then the other part\n",
      "of the story is, you know, I was using Apple II,\n",
      "I used a bunch of them, but I was using Apple II and\n",
      "of course it said in the back of every Apple II and every\n",
      "Mac it said, you know, designed in Cupertino, California. And I was like, wow, okay. Cupertino must be the like,\n",
      "shining city on the hill. Like Wizard of Oz is\n",
      "like the most amazing, like city of all time.\n",
      "I can't wait to see it. And of course, years later I came out to Silicon Valley and went to Cupertino and it's just a bunch of office parks at low-rise apartment buildings. So the aesthetics were a\n",
      "little disappointing, but, you know, it was the vector\n",
      "right of the creation of a lot of this stuff. So then basically by, so part part, part of my story is just\n",
      "the luck of having been born at the right time and\n",
      "getting exposed to PCs. Then the other part is, the other part is when El\n",
      "Gore says that he created the internet, he actually is correct in a really meaningful way, which is he sponsored a bill\n",
      "in 1985 that essentially created the modern internet, created what is called\n",
      "the NSF net at the time, which is sort of the first\n",
      "really fast internet backbone. And you know, that that bill dumped a ton of money into a bunch of research universities to build out basically\n",
      "the internet backbone and then the supercomputer\n",
      "centers that were clustered around the internet. And one of those universities\n",
      "was University of Illinois where I went to school. And so the other stroke\n",
      "lock that I had was, I went to Illinois basically\n",
      "right as that money was just like getting dumped on campus. And so as a consequence\n",
      "we had at, on campus, and this was like, you know,\n",
      "89, 90, 91, we had like, you know, we were right\n",
      "on the internet backbone. We had like T3 and 45 at the time, T3 45 megabit backbone\n",
      "connection, which at the time was, you know, wildly state of the art. We had cray super computers. We had thinking machines\n",
      "parallel super computers. We had silicon graphics\n",
      "workstations, we had Macintosh's, we had next cubes all over the place. We had like every\n",
      "possible kind of computer you could imagine 'cause all this money\n",
      "just fell out of the sky. - [Lex] So you were living in the future. - Yeah. So yeah, quite\n",
      "literally it was, yeah, like it's all there. It's all like we had\n",
      "full broadband graphics, like the whole thing. And it's actually funny 'cause they had this is the first time I kind of, it sort of tickled the back\n",
      "of my head that there might be a big opportunity in\n",
      "here, which is, you know, they embraced it and so\n",
      "they put like computers in all the dorms and they\n",
      "wired up all the dorm rooms and they had all these, you know, labs everywhere and everything. And then they gave every undergrad a computer account and an email address. And the assumption was that\n",
      "you would use the internet for four years at college and then you would\n",
      "graduate and stop using it. And that was that, right? - [Lex] Yeah. - And you would just\n",
      "retire your email address. It wouldn't be relevant\n",
      "anymore 'cause you'd go off from the workplace and\n",
      "they don't use email. You'd be back to using\n",
      "fax machines or whatever. - Did you have that sense as well? Like, what you said the back\n",
      "of your head was tickled. Like, what was exciting to\n",
      "you about this possible world? - Well, if this is so\n",
      "useful in this containment, if this is so useful in\n",
      "this contain environment that just has this weird\n",
      "source of outside funding, then if it were practical\n",
      "for everybody else to have this and if it were cost effective for everybody else to have this, wouldn't they want it? And the overwhelmingly the prevailing view at the time was no,\n",
      "they would not want it. This is esoteric, weird nerd stuff, right? That like computer science kids like, but like normal people are\n",
      "never gonna do email. Right. Or be on the internet, right? And so I was just like,\n",
      "wow, like this is actually, like, this is really compelling stuff. Now the other part was, it was all really hard to use\n",
      "and in practice you had to be basically a CS you know, basically had had to BA\n",
      "CS undergrad or equivalent to actually get full use of\n",
      "the internet at that point. 'cause it was all pretty esoteric stuff. So then that was the other\n",
      "part of the idea, which was, okay, we need to actually\n",
      "make this easy to use. - So what's involved in creating Mosaic? Like, in creating graphical\n",
      "interface to the internet? - Yeah, so it was a combination of things. So it was like basically the web existed in an early sort of\n",
      "described as prototype form. And by the way, text only at that point. - What did it look like? What was the web? I mean what and the key figures. Like, what was it? Like, what paint a picture? - It looked like ChatGPT\n",
      "actually it was all text. - Yeah. - And so you had a text-based web browser? Yeah, well actually the original browser, Tim Burners Lee, the original browser, both the original browser\n",
      "and the server actually ran on next cubes. So these were, this was, you know, the computer Steve Jobs made\n",
      "during the interim period when during the decade long interim period when he was not at Apple, you know, he got fired in 85 and\n",
      "then came back in 97. So this was in that interim\n",
      "period where he had this company called Next and they made these, literally these computers called cubes. And there's this famous\n",
      "story, they were beautiful, but they were 12 inch by 12\n",
      "inch by 12 inch cubes computers. And there's a famous story\n",
      "about how they could have cost half as much if it had\n",
      "been 12 by 12 by 13. But this cube was like,\n",
      "no, like it has to be. So they were like $6,000\n",
      "basically academic workstations. They had the first city round\n",
      "drives, which were slow. I mean it was, the computers\n",
      "were all but unusable. They were so slow, but\n",
      "they were beautiful. - Okay, can we actually just\n",
      "take a tiny tangent there? - Sure. Of course. - The 12 by 12 by 12 that just\n",
      "so beautifully encapsulates Steve Jobs idea of design. Can you just comment on what you find interesting about Steve Jobs? What about that view of the world, that dogmatic pursuit of perfection and how he saw perfection in design? - Yeah, so I guess I'd say like, look, he was a deep believer, I think in a very deep,\n",
      "the way I interpret it, I don't know if you ever\n",
      "really described it like this, but the way I interpret\n",
      "it's like this thing and it's actually a thing in philosophy. It's like aesthetics are\n",
      "not just appearances. Aesthetics go all the way to like deep underlying meaning, right? It's like I'm not a physicist. One of the things I've\n",
      "heard physicists say is one of the things you start to get a sense of when a theory might be correct is when it's beautiful, right? Like, you know, there, right? And so, there's something, and you feel the same thing by the way in like human psychology, right? You know, when you're\n",
      "experiencing awe, right? You know, there's like a simplicity to it. When you're having an honest\n",
      "interaction with somebody, there's an aesthetic, I would say calm comes over you 'cause you're actually being fully honest and trying to hide yourself, right? So it's like this very\n",
      "deep sense of aesthetics. - And he would trust that\n",
      "judgment that he had deep down. Like yeah, even if the\n",
      "engineering teams are saying this is too difficult. Even if whatever the\n",
      "finance folks are saying, this is ridiculous. The supply chain, all that\n",
      "kind of stuff just makes this impossible. We can't do this kind of material. This has never been done\n",
      "before and so on and so forth. He just sticks by it. - Well, I mean, who makes a\n",
      "phone out of aluminum, right? Like, hadn't nobody\n",
      "else would've done that. And now of course if your phone is made out of aluminum white,\n",
      "you know, how crude, what a kind of caveman would\n",
      "you have to be to have a phone that's made outta plastic? Like, right. So like, so it's just this very right. And, you know, look, there's a thousand different\n",
      "ways to look at this, but one of the things is just like, look, these things are\n",
      "central to your life. Like, you're with your phone more than you're with anything else. Like, it's gonna be in your hand. I mean, you know this, he thought very deeply about\n",
      "what it meant for something to be in your hand all day long. But for example, here's an\n",
      "interesting design thing. Like, he never wanted, my understanding is he never\n",
      "wanted an iPhone to have a screen larger than you could reach with your thumb one handed. And so he was actually opposed to the idea of making the phones larger. And I don't know if you\n",
      "have this experience today, but let's say there are\n",
      "certain moments in your day when you might be like, only have one hand available\n",
      "and you might wanna be on your phone. And you're trying to like, send a text and your thumb can't\n",
      "reach the send button. - Yeah. I mean there's\n",
      "pros and cons, right? And then there's like folding phones, which I would love to know what he thought thinks about them. But I mean, is there something you\n",
      "could also just linger on? 'cause he's one of the interesting figures in the history of technology. What makes him as successful as he was? What makes him as interesting as he was? What made him so productive and important in the development of technology? - He had an integrated worldview. So the properly designed device that had the correct functionality, that had the deepest\n",
      "understanding of the user, that was the most beautiful, right? Like, it had to be all\n",
      "of those things, right? He basically would drive\n",
      "to as close to perfect as you could possibly get. Right? And you know, I suspect that\n",
      "he never quite, you know, thought he ever got there.\n",
      "'cause most great creators, you know, are generally dissatisfied. You know, you read accounts\n",
      "later on and all they can, all they can see are the\n",
      "flaws in their creation. But like he got as close to\n",
      "perfect each step of the way as he could possibly\n",
      "get with the constraints of the technology of his time. And then, you know,\n",
      "look, he was, you know, sort of famous in the Apple model. It's like, look, they will, you know, this headset that they just came out with, like, you know, it's like a\n",
      "decade long project, right? It's like, and they're just gonna sit\n",
      "there and tune and tune and polish and polish and tune and polish and tune and polish until it is as perfect as anybody could possibly make anything. - Yeah. - And then this goes to the way that people describe working\n",
      "with him was, which is, you know, there was a terrifying\n",
      "aspect of working with him, which is, you know, he was,\n",
      "you know, he was very tough. But there was this thing that\n",
      "everybody I've ever talked to worked for him, says that they all say the following, which is we did the best work of our lives when we worked for him because he set the bar incredibly high. And then he supported us\n",
      "with everything that he could to let us actually do\n",
      "work of that quality. So a lot of people who were at Apple spend the rest of their lives trying\n",
      "to find another experience where they feel like they're able to hit\n",
      "that quality bar again. - Even if it in retrospect or\n",
      "during it felt like suffering. - Yeah, exactly. - What does that teach you\n",
      "about the human condition? Huh? - So look, so say exactly. So the Silicon Valley, I mean,\n",
      "look, he's not, you know, George Patton you know in the Army. Like, you know, there are\n",
      "many examples in other fields, you know, that are like\n",
      "this specifically in tech. It's actually, I find it very interesting. There's the Apple way, which\n",
      "is polish, polish, polish, and don't ship until it's as\n",
      "perfect as you can make it. And then there's the sort\n",
      "of the other approach, which is the sort of\n",
      "incremental hacker mentality, which basically says, ship\n",
      "early and often and iterate. And one of the things I\n",
      "find really interesting is I'm now 30 years into this, like, they're very successful\n",
      "companies on both sides of that approach, right? Like, that is a fundamental\n",
      "difference, right? In how to operate and how to\n",
      "build and how to create that. You have world class companies\n",
      "operating in both ways. And I don't think the question of like, which is the superior\n",
      "model is anywhere close to being answered. Like, and my suspicion\n",
      "is the answer is do both. The answer is you actually want both. They lead to different outcomes. Software tends to do better\n",
      "with the iterative approach. Hardware tends to do\n",
      "better with the, you know, sort of wait and make it perfect approach. But again, you can find\n",
      "examples in both directions. - So the jury's still out on that one. So back to Mosaic. So, what it was text based Tim Burns Lee? - Well, there was the\n",
      "web, which was text based, but there were no, I mean\n",
      "there was like three websites. There was like no content,\n",
      "there were no users. Like, it wasn't like a catalytic, it hadn't, and by the way, it was all because it was all text. There were no documents, there were no images,\n",
      "there were no videos, there were no, right. So, and then if, if in the beginning, if you had to be on a next cube, right? You need to had a next cube\n",
      "both to publish and to consume. - So, there was 6,000 bucks you said. - There were limitations. Yeah. $6,000 PC. They\n",
      "did not sell very many. But then there was also, there was also FTP and\n",
      "there was Use Nets, right? And there was, you know, a dozen other basically there's waste, which was an early search thing. There was Gopher, which\n",
      "was an early menu based information retrieval system. There were like a dozen\n",
      "different sort of scattered ways that people would get to\n",
      "information on the internet. And so the Mosaic idea was basically bring those all together, make the whole thing\n",
      "graphical, make it easy to use, make it basically bulletproof\n",
      "so that anybody can do it. And then again, just on the luck side, it so happened that this was right at the moment when graphics, when the GUI sort of actually took off and we're now also used to the GUI that we think it's been around forever. But it didn't real, you know, the Macintosh brought it out in 85, but they actually didn't\n",
      "sell very many Macs in the eighties. It was not that successful of a product. It really was. You needed Windows 3.0 on\n",
      "PCs and that hit in about 92. And so, and we did most in 92, 93. So that sort of, it was like right at the\n",
      "moment when you could imagine actually having a graphical\n",
      "user interface to right at all, much less one to the internet. - How old did Windows 3 sell? So was that the really big. - [Marc] That was the big bang. - The big operating\n",
      "graphical operating system? - Well this is the classic, okay. This Microsoft was operating on the other, so Steve the Apple was\n",
      "running on the Polish until it's perfect. Microsoft famously ran on the other model, which is ship and iterate. And so in the old line in those days was Microsoft Right's version three of every Microsoft product. That's the good one, right? And so there there are, you can find online Windows 1, Windows 2. Nobody used them. Actually the original Windows, in the original Microsoft Windows, the windows were non overlapping. And so you had these very small, very low resolution screens and then you had literally-- - [Lex] Windows. - It just didn't work. It wasn't ready yet. Well. - And Windows 95 I think\n",
      "was a pretty big leap also. - That was a big leap too. So that was like bang, bang. And then of course Steve, and then when, you know, in the fullness of time Steve came back, then the Mac started, took off again. That was the third bang. And then the iPhone was the fourth bang. - Such exciting time. - And then we were off,\n",
      "off to the races because. - Nobody could have known what\n",
      "would be created from that. - Well, Windows 3.1 or 3.0, Windows 3.0 to the iPhone\n",
      "was only 15 years. Right. Like that ramp was in retrospect. At the time it felt like it took forever. But that in histor in historical terms, like that was a very fast ramp from even a graphical computer at all on your desk to the iPhone. That was 15 years. - So, did you have a sense\n",
      "of what the internet will be as you're looking through\n",
      "the window of Mosaic? Like, what you, like there's\n",
      "just a few web pages for now. - So the thing I had early on\n",
      "was I was keeping at the time what there's disputes over\n",
      "what was the first blog, but I had one of them that\n",
      "at least is a possible, at least a rudder up in the competition. And it was what was called\n",
      "the What's new page. And it was literally, it was a hardwired in\n",
      "distribution unfair advantage. I wired, put it right in the browser, I put it in the browser\n",
      "and then I put my resume in the browser, which also was-- - [Lex] Hilarious. - But I was keeping not many people get to get to do that. - No, good call. And early days. It's so interesting. - I'm looking for my, about about, oh, Marc is looking for a job. - [Lex] Yeah, yeah, exactly. - So the West New page, I would literally get up every morning and I would, or every afternoon\n",
      "and I would basically, if you wanted to launch a website, you would email me and I would\n",
      "list it on the most new page. And that was how people\n",
      "discovered the new websites as they were coming out. And I remember 'cause it was like one, it literally went from, it was like one every couple\n",
      "days to like one every day to like two every day. - And then so you're doing, so that blog was kind of\n",
      "doing the directory thing. So like, what was the homepage? - So the homepage was just\n",
      "basically trying to explain even what this thing is that\n",
      "you're looking at. Right. Basically the basic instructions. But then there was a button, there was a button that said what's new. And what most people did was they went to, for obvious reasons went to what's new. - [Lex] Yeah. - But like it was so mind\n",
      "blowing at that point. This the basic idea and it\n",
      "was, this was like, you know, this was the basic idea of the internet, but people could see\n",
      "it for the first time. The basic idea was, look,\n",
      "you know, some, you know, it's like literally it's like\n",
      "an Indian restaurant in like Bristol England has like\n",
      "put their menu on the web. And people were like, wow. - [Lex] Whoa. - Because like that's the first\n",
      "restaurant menu on the web. - [Lex] Yeah. - And I don't have to be\n",
      "in Bristol and I don't know if I'm ever gonna go to Bristol. And I don't even like Indian\n",
      "food and like. Wow. Right. And it was like that the first web, the first streaming video thing was it was in another England,\n",
      "some Oxford or something. Some guy put his coffee pot up\n",
      "as the first streaming video thing and he put it on the\n",
      "web 'cause he literally, it was the coffee pot down the hall. And he wanted to see when\n",
      "he needed to go refill it. But there were, you know, there was a point when\n",
      "there were thousands of people like watching that coffee pot 'cause it was the first\n",
      "thing you could watch. - Well, but isn't were you able\n",
      "to kind of infer, you know, if that Indian restaurant could go online. Then you're like they all will. - [Marc] Yeah, exactly. - So you felt that? - [Marc] Yeah, yeah, yeah. - Okay. - Now, you know, look, it's\n",
      "still a stretch, right? It's still a stretch 'cause\n",
      "it's just like, okay, is it, you know, you're still in this\n",
      "zone, which is like, okay, is this a nerd thing? Is this a real person thing? By the way, you know, there was a wall of\n",
      "skepticism from the media. Like, they just, like,\n",
      "everybody was just like, yeah, this is the crazy, this is just like dumb. This is not, you know, this is not for regular\n",
      "people at that time. And so you, you had to think\n",
      "through that and then look, it was still hard to get on the internet at that point, right? So you could get kind of this\n",
      "weird bastardized version if you were on AOL,```\n",
      "            \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSummarize the youtube video whose transcript is provided within backticks             ```weird bastardized version if you were on AOL,\n",
      "which wasn't really real. Or you had to go like,\n",
      "learn what an ISP was. You know, in those days, PCs actually didn't have TCPIP\n",
      "drivers come reinstalled. So you had to learn\n",
      "what a TCPIP driver was. You had to buy a modem, you\n",
      "had to install driver software. I have a comedy routine. I do. So it's like 20 minutes long\n",
      "describing all the steps required to actually get on\n",
      "the internet at this point. And so you had to look\n",
      "through these practical. Well, and then speed performance 14-4 modems, right? Like it was like watching,\n",
      "you know, glue dry, like, and so you had to, there were basically a sequence of bets that we made where you basically needed to look through that current\n",
      "state of affairs and say, actually there's gonna\n",
      "be so much demand for once people figure this out, there's gonna be so much demand for it that all of these practical problems\n",
      "are gonna get fixed. - Some people say that\n",
      "the anticipation makes the destination that much more exciting. - Do you remember progressive JPEGs? - Yeah. Do I, do I? - For kids in the audience, right? - [Lex] For kids in the audience. - You used to have to watch an image load like a line at the time. But it turns out there\n",
      "was this thing with JPEGs where you could load\n",
      "basically every fourth, you could load like every fourth line and then you could sweep\n",
      "back through again. And so you could like\n",
      "render a fuzzy version of image up front. And then it would like\n",
      "resolve into the detailed one. And that was like a big UI breakthrough 'cause it gave you something to watch. - Yeah. And you know, there's applications in\n",
      "various domains for that. - Well it was a big fight. There was a big fight early on\n",
      "about whether there should be images in the web. And. - For that reason for\n",
      "like sexualization or-- - Not explicitly that that did come up. But it wasn't even that, it was more just like all the\n",
      "serious in the argument went, the purists basically said\n",
      "all the serious information in the world is text. If you introduce images, you basically are gonna bring\n",
      "in all the trivial stuff. You're gonna bring in\n",
      "magazines and you know, all this crazy just, you know,\n",
      "stuff that, you know, people, you know, it's gonna, it is\n",
      "gonna distract from that. It's gonna go take it away from being\n",
      "serious to being frivolous. - Well, was there any\n",
      "(indistinct) type arguments about the internet destroying all of human\n",
      "civilization or destroying some fundamental fabric of human civilization? - So it was, those days it was all around crime and terrorism. So those arguments happened, you know, but there was no sense yet\n",
      "of the internet having like, an effect on politics because\n",
      "that was way too, too far off. But there was an enormous panic at the time around cybercrime. There was like enormous panic\n",
      "that like your credit card number would get stolen and you'd use life\n",
      "savings would be drained. And then, you know, criminals were gonna, there was, oh, when we started,\n",
      "one of the things we did, one of the Netscape browser\n",
      "was the first widely used piece of consumer software that had\n",
      "strong encryption built in, it made it available to ordinary people. And at that time, strong encryption was\n",
      "actually illegal to export outta the US so we could feel that product in the US, we could not export it 'cause it was classified as munition. So the Netscape browser\n",
      "was on a restricted list along with the tomahawk missile as being something that\n",
      "could not be exported. So we had to make a second\n",
      "version with deliberately weak encryption to sell\n",
      "overseas with a big logo on the box saying, do not trust this. Which it turns out, makes it hard to sell software\n",
      "when it's got a big logo that says don't trust it. And then we had to spend\n",
      "five years fighting the US government to get\n",
      "them to basically stop trying to do this regulation. But because the fear\n",
      "was terrorists are gonna use encryption, right? To like plot, you know, all these things. And then, you know, we responded with, well actually we need\n",
      "encryption to be able to secure systems so that the terrorists and the criminals can't get into them. So that anyway, that was the 1990s fight. - So can you say something\n",
      "about some of the details of the software engineering\n",
      "challenges required to build these browsers? I mean the engineering\n",
      "challenges of creating a product that hasn't really existed before that can have such\n",
      "almost like limitless impact on the world with the internet. - So there was a really key\n",
      "bet that we made at the time, which was very controversial, which was core to core\n",
      "to how it was engineered, which was are we\n",
      "optimizing for performance or for ease of creation? And in those days the pressure\n",
      "was very intense to optimize for performance because the\n",
      "network connections were so slow and also the computers were so slow. And so if you had, I mentioned\n",
      "the progressive JPEGs, like if there's an alternate\n",
      "world in which we optimized for performance and it just, you had just a much more pleasant\n",
      "experience right up front. But what we got by not doing that was we got ease of creation. And the way that we got\n",
      "ease of creation was all of the protocols and\n",
      "formats were in text, not in binary. And so HTTP is in text, by the way. And this was an internet\n",
      "tradition by the way that we picked up. But we continued it. HTTP is text and HTML is\n",
      "text, and then every else, everything else that\n",
      "followed is text as a result. And by the way, you can imagine purist\n",
      "engineers saying this is insane. You have very limited bandwidth. Why are you wasting any time sending text? You should be encoding\n",
      "this stuff into binary and it'll be much faster. And of course the answer\n",
      "is that's correct. But what you get when you make\n",
      "it taxed is all of a sudden, well, the big breakthrough was the view source function, right? So the fact that you\n",
      "could look at a webpage, you could hit view source\n",
      "and you could see the HTML, that was how people learned\n",
      "how to make webpages. Right? - It's so interesting 'cause the stuff would\n",
      "take for granted now is, man, that was fundamental, the development of the web\n",
      "to be able to have HTML just right there, all the\n",
      "ghetto mess that is HTML, all the sort of almost\n",
      "biological like messiness of HTML and then having the browser\n",
      "try to interpret that as. - [Marc] Exactly. - To show something reasonable. - Well and then there was\n",
      "this internet principle that we inherited, which\n",
      "was emit, what was it? Emit cautiously. Emit\n",
      "conservatively interpret liberally. So it basically meant if you're, the design principle was if you're creating like a web editor that's gonna admit HTML, like\n",
      "do it as cleanly as you can, but you actually want the\n",
      "browser to interpret liberally, which is you actually want\n",
      "users to be able to make all kinds of mistakes and\n",
      "for it to still work. And so the browser rendering\n",
      "engines to this day have all of this spaghetti code crazy stuff where they're resilient to\n",
      "all kinds of crazy issue, no mistakes. And so, literally what I\n",
      "always had in my head is like there's an 8 year old or\n",
      "an 11 year old somewhere and they're doing a view source, they're doing a cut and\n",
      "paste and they're trying to make a webpage for\n",
      "their eternal or whatever. And like they leave out a\n",
      "slash and they leave out an angle bracket and they do this and they do that and it's still works. - It's also like a, I don't often think about this,\n",
      "but, you know, programming, you know, C++ all those languages, lisp, the compiled languages,\n",
      "the interpreted languages, Python, Pearl, all that. The brace have to be all correct. It's like everything has to be perfect. - [Marc] Brutal. - And then-- - [Marc] Autistic. - You forget. All right. It's systematic\n",
      "and rigorous, let's go there. But you forget that the, the web with JavaScript eventually. And HTML is allowed to be messy in the way for the first time. Messy in the way biological\n",
      "systems could be messy. It's like the only thing\n",
      "computers were allowed to be messy on for the first time. - It used to off fend me. So I grew up on Unix, so I worked on Unix. I was a Unix native for all\n",
      "the way through this period. And so, it used to drive\n",
      "me bananas when it would do the segmentation fault\n",
      "and the core dump file, just like it is, you know, it's like literally there's\n",
      "like a error in the code. The math is off by one. And it core dumps. And I'm in the core dump\n",
      "trying to analyze it and trying to reconstruct what, and I'm\n",
      "just like, this is ridiculous. Like, the computer\n",
      "ought to be smart enough to be able to know that if it's off by one, okay fine. And it keeps running. And I would go ask all the experts like, why can't it just keep running? And they'd explain to me, well, because all the downstream\n",
      "repercussions and blah blah. And I'm like, this still,\n",
      "like, you know, this is, we're forcing the human\n",
      "creator to live to your point in this hyper, literal\n",
      "world of perfection. - [Lex] Yeah. And I was just like, that's just bad. And by the way, you know what happens with that of course. Just what what happened with,\n",
      "with coding at that point, which is you get a high\n",
      "priesthood, you know, there's a small number of\n",
      "people who are really good at doing exactly that. Most people can't. And most people are excluded from it. And so actually that was where that for that was where I picked up that idea was like, no, you want these things to be\n",
      "resilient error in all kinds and this would drive the\n",
      "purist absolutely crazy. Like, I got attacked on this like a lot 'cause I mean like every time you know, all the purists who\n",
      "were like into all this like Marcup language stuff and formats and codes and all this stuff, they would be like, you know, you're encouraging bad behavior 'cause. - Oh, so they wanted\n",
      "the browser to give you a fault error anytime there was a-- - Yeah. They wanted to\n",
      "be a (indistinct) right? They wanted to-- Yeah. Yeah. That was a very and any properly trained credential engineer would be like, that's not how you build these systems. - That's such a bold move to say, no, it doesn't have to be. - Yeah. No, like I said, the good news for me is\n",
      "the internet kind of had that traditional already,\n",
      "but having said that, like we pushed it, we pushed it way out. But the other thing we did, going back to the performance thing, was we gave up a lot of performance. We made that, that initial experience for the first few years\n",
      "was pretty painful. But the bet there was\n",
      "actually an economic bet, which was basically the demand\n",
      "for the web would basically mean that there would be a\n",
      "surge in supply of broadband. Like because the question was, okay, how do you get the phone\n",
      "companies which are not famous in those days for doing\n",
      "new things at huge cost for like speculative reasons. Like how do you get them to\n",
      "build up broadband, you know, spend billions of dollars\n",
      "doing that and you know, you could go meet with them\n",
      "and try to talk them into it. Or you could just have a thing where it's just very\n",
      "clear that it's gonna be, that people love that's gonna\n",
      "be better if it's faster. And, so that, there was a\n",
      "period there and this was, this was fraught with in peril, but there was a period there\n",
      "where it's like we knew the experience was sub-optimized because we were trying to force the emergence of demand for broadband. - [Lex] Sure. - Which is in fact what happened. - So you had to figure out\n",
      "how to display this text, HTML text. So the blue links and\n",
      "the prop links. What? And there's no standards. Is\n",
      "there standards at that time? - [Marc] No. There really still isn't. - Well there's like standards, there's applied, implied standards. Right. And they, you know, there's all these kind of new features that are being added with like CSS, what, like what kind of stuff a\n",
      "browser should be able to support features within languages,\n",
      "within JavaScript and so on. But you're setting standards\n",
      "on the fly yourself. - Yeah. Well to this day, if you create a webpage\n",
      "that has no CSS style sheet, the browser will render\n",
      "it however it wants to. Right. So this was one of the\n",
      "things, there was this idea, this idea of at the time and\n",
      "how these systems were built, which is separation of content from format or separation of content from appearance. And that's still, people\n",
      "don't really use that anymore 'cause everybody wants to\n",
      "determine how things look and so they use CSS\n",
      "but it's still in there that you can just let the\n",
      "browser do all the work. - I still like the like\n",
      "really basic websites, but that could be just old school, kids these days with their\n",
      "fancy responsive websites that don't actually have much content, but have a lot of visual elements. - Well that's one of the\n",
      "things that's fun about chat, you know, about ChatGPT like. - [Lex] Back to the basics. - Back to just text. - [Lex] Yeah. - Right? And it, you know, there is this pattern in\n",
      "human creativity and media where you end up back at text\n",
      "and I think there's, you know, there's something powerful in there. - Is there some other stuff you remember like the purple links? There were some interesting\n",
      "design decisions that to kind of come up that we have today or we don't have today\n",
      "that were temporary. - So I made the background\n",
      "'cause I hated reading texts on white background, so I\n",
      "made the background gray. Everybody can-- - Do you go ahead to? - No. No, no. That decision I think has been reversed. But now I'm happy though because\n",
      "now dark mode is the thing. - So it wasn't about gray, it was just you didn't\n",
      "want white background. - [Marc] Strain my eyes. - Strain your eyes. Interesting. And then there's a bunch\n",
      "of other decisions. I'm sure there's an interesting\n",
      "history of the development of HTML and CSS and\n",
      "Interface and JavaScript and there's this whole Java applet thing. - Well the big one probably\n",
      "JavaScript, CSS was after me, so I didn't, that was not me. But JavaScript was the big, JavaScript maybe was the\n",
      "biggest of the whole thing. That was us. And that was basically a bet,\n",
      "it was a bet on two things. One is that the world wanted a new front end scripting language. And then the other was I thought at the time the world wanted a new backend scripting language. So JavaScript was designed\n",
      "from the beginning to be both front end and backend. And then it failed as a\n",
      "backend scripting language. And Java won for a long time. And then Python Pearl and\n",
      "other things, PHP and Ruby. But now JavaScript is back. And so. - I wonder if everything in\n",
      "the end will run on JavaScript. - It seems like it is the, and by the way, lemme give a shout out\n",
      "to, to Brendan Eich was basically the one man\n",
      "inventor of of JavaScript. - If you're interested to\n",
      "learn more about Brendan Eich, he's been on his podcast previously. - Exactly. So he wrote\n",
      "JavaScript over a summer and I mean I think it is fair, it is fair to say now that\n",
      "it's the most widely used language in the world and\n",
      "it seems to only be gaining in its in its range of adoption. - You know, in the software world there's quite a few stories of somebody over a week weekend or over a\n",
      "week or over a summer writing some of the most impactful\n",
      "revolutionary pieces of software ever. That\n",
      "should be inspiring. Yes. - Very inspiring. I'll\n",
      "give you another one. SSL. So SSL with the security\n",
      "protocol, that was us. And that was a crazy idea at the time, which was let's take\n",
      "all the native protocols and let's wrap them in a security wrapper. That was a guy named Kip Hickman who wrote that over a summer, one guy. And then look today, sitting here today, like the transformer like at Google was a small handful of people. And then, you know, the number of people who have\n",
      "did like the core work on GPT. It's not that many people, it's a pretty small handful of people. And so yeah, the pattern in software repeatedly over a very long time has been, it's Jeff Bezos always\n",
      "had the two pizza rule for teams at Amazon, which is any team needs\n",
      "to be able to be fed with two pieces. If you need the third pizza,\n",
      "you have too many people. And I think it's actually\n",
      "the one pizza rule. For the really creative work. I think it's two people, three people. - Well that's, you see that with certain open source projects, like so much is done by\n",
      "like one or two people. Like it's so incredible\n",
      "and that's why you see that gives me so much hope\n",
      "about the open source movement in this new age of AI where, you know, just recently having had a conversation with Marc Zuckerberg of all people who's all in on open source, which is so interesting to\n",
      "see and so inspiring to see 'cause like releasing\n",
      "these models, it is scary. It is potentially very dangerous\n",
      "and we'll talk about that. But it's also, if you believe in the\n",
      "goodness of most people and in the skillset of most people and the desire to go do good in the world, that's really exciting. 'cause it's not putting it these models into the centralized\n",
      "control of big corporations, the government and so on. It's putting it in the hands of a teen, teenage kid with like a dream in his eyes. I don't know. That's beautiful. - Look, this stuff, AI ought to make the\n",
      "individual coder obviously far more productive right? By like, you know, a\n",
      "thousand X or something. And so you ought to open source like, not just the future of open source AI, but the future of open source everything. We ought to have a world\n",
      "now of super coders, right? Who are building things as open source with one or two people\n",
      "that were inconceivable, you know, five years ago. You know, the level of\n",
      "kind of hyper productivity we're gonna get out of\n",
      "our best and brightest I think is gonna go way up. - It's gonna be interesting. We'll talk about it, but let's just to linger\n",
      "a little bit on Netscape. Netscape was acquired in\n",
      "1999 for 4.3 billion by AOL. What was that like? What were some memorable aspects of that? - Well that was the height\n",
      "of the.com boom bubble bust. I mean that was the frenzy. If you watch succession, that was like what they\n",
      "did in the fourth season with Gojo and the merger with their, so it was like the height of like one of those kind of dynamics. And so. - Would you recommend\n",
      "succession, by the way? I'm more of a Yellowstone guy. - Yellowstone's very American. I'm very proud of you. That's, that is. - I just talked to Matthew McConaughey and I'm full on Texan at this point. - Good. I approve. - And he'll be doing\n",
      "the SQL to Yellowstone. - [Marc] Yeah, just exciting. - Very exciting. Anyway. - [Marc] Can't wait. - So that's a rude interruption\n",
      "by me by way of succession. So, that was at the height of the-- - Deal making and money\n",
      "and just the fur flying and like craziness. And so yeah, it was just one of those, it was just like, I mean, and this, the entire (indistinct) thing from start to finish was four years, which was like for one of these companies, it's just like incredibly fast. You know, it went, we went public 18 months\n",
      "after we got moved where we were founded, which\n",
      "virtually never happens. So it was just this incredibly fast kind of meteor streaking across the sky. And then of course it was this, and then there was just\n",
      "this explosion, right? That happened 'cause then\n",
      "it was almost immediately followed by the.com crash. It was then followed\n",
      "by AOL, by Time Warner, which again is like the succession guys kinda play with that, which turned out to be a disastrous deal. You know, one of the famous, you know, kind of disastrous in business history. And then, you know, what became an internet depression on\n",
      "the other side of that. But then in that depression\n",
      "in the two thousands was the beginning of broadband and smartphones and Web 2.0 right? And then social media\n",
      "and search and every SaaS and everything that came out of that. - What did you learn from\n",
      "just the acquisition? I mean this is so much money. What's interesting 'cause I\n",
      "must have been very new to you, that these software stuff, you can make so much money. There's so much money swimming around. I mean, I'm sure the\n",
      "ideas of investment was starting to get born there. - Yes. Let me get, so let me lay it. So here's, here's the thing. I dunno if I figured it out```\n",
      "            \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSummarize the youtube video whose transcript is provided within backticks             ```then, but figured it out later, which is software is a technology that it, it's like a, you know, the\n",
      "concept of the philosopher stone, the philosopher stone in alchemy, transient is led into gold and\n",
      "Newton spent 20 years trying to find the philosopher stone. Never got there. Nobody's ever figured it out. Software is our modern philosopher stone. And in economic terms, it\n",
      "transmutes labor into capital, which is like a super interesting thing. And by the way, like Carl Marcs is rolling over in his grave right now. 'Cause of course that's\n",
      "complete reputation of his entire theory. Trans labor and capital\n",
      "which is as follows, is somebody sits down at a keyboard and types a bunch of stuff in, and a capital asset\n",
      "comes out the other side and then somebody buys that capital asset for a billion dollars. Like that's amazing, right? It's literally creating\n",
      "value right out of thin air, right out of purely human thought, right? And so that, there are many things that make software magical and special, but that's the economics. - I wonder what Marx\n",
      "would've thought about that? - Oh, he would've\n",
      "completely broke his brain because of course the whole\n",
      "thing was it was he could, you know, that kind of\n",
      "technology was inconceivable when he was alive. It was all industrial era stuff. And so, any kind of machinery\n",
      "necessarily involved huge amounts of capital. And then labor was on the receiving end of the abuse. - [Lex] Yep. Right? But like software eng software, a software engineer is somebody\n",
      "who basically transmutes his own labor into actual, an actual capital asset\n",
      "creates permanent value. Well, and in fact it's\n",
      "actually very inspiring. That's actually more\n",
      "true today than before. So when I was doing software, the assumption was all\n",
      "new software basically has a sort of a parabolic\n",
      "sort of lifecycle, right? So you ship the thing,\n",
      "people buy it at some point, everybody who wants it has bought it and then it becomes obsolete. And it's like bananas. Nobody, nobody buys old software. These days, Minecraft, Mathematica, you know, Facebook, Google, you have the software\n",
      "assets that are, you know, have been around for 30\n",
      "years that are gaining in value every year, right? And they're just, they're being\n",
      "a world of warcraft, right, salesforce.com, like they're being every single year they're\n",
      "being polished and polished and polished and polished. They're getting better\n",
      "and better, more powerful, more powerful, more\n",
      "valuable, more valuable. So we've entered this era\n",
      "where you can actually have these things that actually\n",
      "build out over decades. Which by the way is what's happening right now with like ChatGPT. And so now, this is why, you know, there is always, you know, sort of a constant investment\n",
      "frenzy around software is because, you know, look, when\n",
      "you start one of these things, it doesn't always succeed. But when it does now you\n",
      "might be building an asset that builds value for,\n",
      "you know, four or five, six decades to come. You know, if you have a team of people who have the level of devotion required to keep making it better. And then the fact that of\n",
      "course everybody's online, you know, there's 5 billion people that are a click away from\n",
      "any new piece of software. So the potential market size\n",
      "for any of these things is, you know, nearly infinite. - [Lex] It must have been\n",
      "surreal back then though. - Yeah. Yeah. This was\n",
      "all brand new, right? Yeah. Back then, this was all brand new. These were all, you know, brand new. Had you rolled out that\n",
      "theory in even 1999, people would've thought\n",
      "you were smoking crack. So that's emerged over time. - Well, let's now turn\n",
      "back into the future. You wrote the essay \"Why\n",
      "AI Will Save The World?\" Let's start the very high level. What's the main thesis of the essay? - Yeah, so the main thesis on the essay is that what we're dealing\n",
      "with here is intelligence. And it's really important to kind of talk about the sort of very nature\n",
      "of what intelligence is. And fortunately we have a predecessor to machine intelligence,\n",
      "which is human intelligence. And we've got, you know, observations and theories\n",
      "over thousands of years for what intelligence is\n",
      "in the hands of humans and what intelligence is, right? I mean, what it literally is the way to, you know, capture, process,\n",
      "analyze, synthesize information, solve problems. But the observation of\n",
      "intelligence in human hands is that intelligence quite literally\n",
      "makes everything better. And what I mean by that\n",
      "is every kind of outcome of like human quality of life, whether it's education outcomes or success of your children, or career success or health or lifetime\n",
      "satisfaction, by the way, propensity to peacefulness\n",
      "as opposed to violence, propensity for open-mindedness\n",
      "versus bigotry, those are all associated with\n",
      "higher levels of intelligence. - Smarter people have better outcomes than almost as you write in almost every domain of activity. Academic achievement, job performance, occupational status, income, creativity, physical health, longevity,\n",
      "learning new skills, managing complex tasks, leadership,\n",
      "entrepreneurial success, conflict resolution,\n",
      "reading comprehension, financial decision making, understanding others\n",
      "perspectives, creative arts, parenting outcomes, and life satisfaction. One of the more depressing\n",
      "conversations I've had, and I don't know why it's depressing, I have to really think\n",
      "through why it's depressing, but on IQ and the G factor, and that that's something\n",
      "in large part is genetic and it correlates so much\n",
      "with all of these things and success in life. It's like all the inspirational\n",
      "stuff we read about, like if you work hard and so on, it sucks that you're born with the hand that you can't change. - But what if you could. - You're saying basically\n",
      "a really important point, and I think it's in your\n",
      "articles, it really helped me. It's a nice added\n",
      "perspective to think about. Listen, human intelligence, the science of intelligence\n",
      "is shown scientifically that it just makes life easier and better the smarter you are. And now let's look at\n",
      "artificial intelligence and if that's a way to increase\n",
      "some human intelligence, then it's only going\n",
      "to make a better life. - [Marc] Yeah. - That's the argument. - And certainly at the collective level, we could talk about the collective effect of just having more\n",
      "intelligence in the world, which will have very big payoff. But there's also just\n",
      "at the individual level, like what if every person has a machine? You know? And the concept of augment Doug Engelbart's concept of augmentation. You know, what if\n",
      "everybody has an assistant and the assistant is, you know, 140 IQ and you happen to be 110 IQ and you've got, you know, something that basically is\n",
      "infinitely patient and knows everything about you\n",
      "and is pulling for you in every possible way,\n",
      "wants you to be successful. And anytime you find anything\n",
      "confusing or wanna learn anything or have trouble\n",
      "understanding something or wanna figure out what to\n",
      "do in a situation, right? Wanna figure out how to\n",
      "prepare for a job interview, like any of these things,\n",
      "like it will help you do it. And it will therefore, the combination will\n",
      "effectively be, you know, will effectively raise your raise because it will effectively raise your IQ, will therefore raise the odds of successful life outcomes\n",
      "in all these areas. - So people below the,\n",
      "this hypothetical 140 IQ, it'll pull them up towards 140 IQ. - Yeah, yeah, yeah. And then of course, you know, people at 140 IQ will be\n",
      "able to have a peer, right. To be able to communicate, which is great. And then people above 140\n",
      "IQ will have an assistance that they can farm things out to. And then look, God willing, you know, at some point these things\n",
      "go from future versions go from 140 IQ equivalent to\n",
      "150 to 160 to 180, right? Like Einstein was estimated\n",
      "to be on the order of one 60, you know, so when we\n",
      "get, you know, one 60 AI, like we'll be, you know, when one assumes creating\n",
      "Einstein level breakthroughs and physics, and then at\n",
      "180 we'll be, you know, carrying cancer and developing\n",
      "warp drive and doing all kinds of stuff. And so it is quite possibly the case, this is the most important\n",
      "thing that's ever happened and the best thing that's ever happened because precisely because it's a lever on this single fundamental\n",
      "factor of intelligence, which is the thing that drives\n",
      "so much of everything else. - Can you steal, man, the case that human plus AI is\n",
      "not always better than human for the individual? - You may have noticed that there's a lot of\n",
      "smart running around. - [Lex] Sure. Yes. - Right? And so, like smart, there are certain people where\n",
      "they get smarter, you know, they get to be more arrogant, right? So that, you know, there's one huge flaw. - Although to push back on that, it might be interesting because\n",
      "when the intelligence is not all coming from you,\n",
      "but from another system, that might actually increase\n",
      "the amount of humility even in the assholes. - [Marc] One would hope. - Yeah. - Or it could make assholes more assholes. You know, that's in, I mean, that's for psychology to study. - Yeah, exactly. Another one is smart people\n",
      "are very convinced that they, you know, have a more\n",
      "rational view of the world, and that they have a easier\n",
      "time seeing through conspiracy theories and hoaxes and right. You know, sort of crazy\n",
      "beliefs and all that. There's a theory in psychology, which is actually smart people. So for sure people who aren't\n",
      "as smart are very susceptible to hoaxes and conspiracy theories. But it may also be the case\n",
      "that the smarter you get, you become susceptible in a different way, which is you become very\n",
      "good at marshaling facts to fit preconceptions, right. You become very, very good at assembling whatever theories and\n",
      "frameworks and pieces of data and graphs and\n",
      "charts you need to validate whatever crazy ideas got in your head. And so you're susceptible\n",
      "in a different way, right? - We're all sheep, but\n",
      "different colored sheep. - Some sheep are better\n",
      "at justifying it. Right. And those are the, you know, those are the smart sheep, right? So yeah. Look like I\n",
      "would say this look like there are no panacea. I'm not a utopian, there\n",
      "are no panaceas in life. There are no, like, you know, I don't believe there\n",
      "are like pure positives. I'm not a transcendental\n",
      "kind of person like that. But, you know, so yeah,\n",
      "there are gonna be issues and, you know, look, smart people, another maybe you could\n",
      "save about smart people is they are more likely to get\n",
      "themselves in situations that are, you know, beyond their grasp. You know, because they're\n",
      "just more confident in their ability to deal with complexity and their eyes become bigger, their cognitive eyes become bigger than their stomach, you know? So yeah, you could argue\n",
      "those eight different ways nevertheless, on net, right? Clearly, overwhelmingly, again, if you just extrapolate from what we know about human intelligence, you're improving so many aspects of life if you're upgrading intelligence. - So there'll be assistants\n",
      "at all stages of life. So when you're younger,\n",
      "there's for education, all that kind of stuff for\n",
      "mentorship, all of this. And later on as you're doing\n",
      "work and you've developed a skill and you're having a profession, you'll have an assistant\n",
      "that helps you excel at that profession. So at all stages of life. - Yeah. I mean, look, the\n",
      "theory is augmentation. This is the Doug Engelbart's term. Doug Engelbart made this observation many, many\n",
      "decades ago that, you know, basically it's like you can\n",
      "have this oppositional frame of technology where it's\n",
      "like us versus the machines, but what you really do\n",
      "is you use technology to augment human capabilities. And by the way, that's how actually the economy develops. That's, we can talk about\n",
      "the economic side of this, but that's actually how\n",
      "the economy grows is through technology\n",
      "augmenting human potential. And so, yeah. And then you basically\n",
      "have a proxy or you know, or you know, a sort of\n",
      "prosthetic, you know, so like you've got glasses,\n",
      "you've got a wristwatch, you know, you've got shoes, you know, you've got these things. You've got a personal computer, you've got a word processor,\n",
      "you've got Mathematica, you've got Google. This is the latest\n",
      "viewed through that lens. AI is the latest in a long\n",
      "series of basically augmentation methods to be able to\n",
      "raise human capabilities. It's just this one is the\n",
      "most powerful one of all, because this is the one\n",
      "that, that goes directly to what they call fluid\n",
      "intelligence, which is IQ. - Well, there's two categories of folks that you outline that\n",
      "worry about or highlight the risks of AI, and you highlight a bunch of different risks. I would love to go through those risks and just discuss them, brainstorm which ones are serious and which ones are less serious. But first, the Baptist\n",
      "and the bootleggers, what are these two\n",
      "interesting groups of folks who worry about the effect\n",
      "of AI and human civilization? - [Marc] Or say they do. - Say, oh, okay, yes, I'll say they do. - The Baptist worry the\n",
      "bootleggers say they do. So the Baptist and the\n",
      "bootleggers is a metaphor from economics, from what's\n",
      "called development economics. And it's this observation that when you get social\n",
      "reform movements in a society, you tend to get two sets\n",
      "of people showing up, arguing for the social reform. And the term Baptist and bootleggers comes from the American experience\n",
      "with alcohol prohibition. And so in the 1900s, 1910s, there was this movement\n",
      "that was very passionate at the time, which basically said, alcohol is evil and is destroying society. By the way, there was a lot\n",
      "of evidence to support this. There were very high rates of very high correlations\n",
      "then, by the way. And now between rates of physical\n",
      "violence and alcohol use, almost all violent crimes\n",
      "have either the perpetrator or the victim, or both drunk almost. If you see this actually in the work, almost all sexual harassment\n",
      "cases in the workplace, it's like at a company\n",
      "party and somebody's drunk. Like, it's amazing how often\n",
      "alcohol actually correlates to actually dis dysfunction\n",
      "and at leads to domestic abuse and so forth, child abuse. And so you had this group of\n",
      "people who were like, okay, this is bad stuff and we should outlaw it. And those were quite literally Baptist. Those were super committed, you know, hardcore Christian\n",
      "activists in a lot of cases. There was this woman whose\n",
      "name was Carrie Nation, who was this older woman who\n",
      "had been in this, you know, I don't know, disastrous\n",
      "marriage or something. And her husband had been\n",
      "abusive and drunk all the time. And she became the icon of\n",
      "the Baptist prohibitionist. And she was legendary in\n",
      "that era for carrying an ax and doing, you know, completely on her own\n",
      "doing raids of saloons and like taking her ax to all the bottles and eggs in the back. And so. - [Lex] A true believer. - An absolute true believer, and with absolutely the\n",
      "purist of intentions. And again, there's a very\n",
      "important thing here, which is there's, you could look at this\n",
      "cynically and you could say the Baptists are like delusional,\n",
      "you know, the extremists, but you could also say,\n",
      "look, they're right. Like she was, you know, she had a point. Like she wasn't wrong about\n",
      "a lot of what she said. - Yeah. - But it turns out the way\n",
      "the story goes is it turns out that there were another set of people who very badly wanted to\n",
      "outlaw alcohol in those days. And those were the bootleggers, which was organized crime that\n",
      "stood to make a huge amount of money if legal alcohol\n",
      "sales were banned. And this was, in fact, the way the history goes\n",
      "is this was actually the beginning of\n",
      "organized crime in the US. This was the big economic\n",
      "opportunity that opened that up. And so they went in together and no, they didn't go in together. Like the Baptist did not\n",
      "even necessarily know about the bootleggers 'cause they were on their moral crusade. The bootleggers certainly\n",
      "knew about the Baptists. And they were like, wow, these people are like the\n",
      "great front people for like. You know, it's-- - [Lex] Good PR. - Shenanigans in the background. And they got the (indistinct)\n",
      "Act passed, right. And they did in fact ban alcohol\n",
      "in the US and you'll notice what happened, which is\n",
      "people kept drinking, it didn't work, people kept drinking. That bootleggers made a\n",
      "tremendous amount of money. And then over time it became\n",
      "clear that it made no sense to make it illegal and it\n",
      "was causing more problems. And so then it was revoked. And here we sit with legal\n",
      "alcohol a hundred years later with all the same problems. And you know, the whole thing was this\n",
      "like giant misadventure the Baptist got taken advantage\n",
      "of by the bootleggers, and the bootleggers got what they wanted. And that was that. - The same two categories of folks are now sort of suggesting\n",
      "that the development of artificial intelligence\n",
      "should be regulated. - A hundred percent. It's the same pattern. And the economist will tell you it's the same pattern every time. Like, this is what\n",
      "happened, nuclear power, this is what happens, which\n",
      "is another interesting one. But like, yeah, this\n",
      "happens dozens and dozens of times throughout the\n",
      "last a hundred years and this is what's happening now. - And you write that it isn't\n",
      "sufficient to simply identify the actors and impugn their motives. We should consider the\n",
      "arguments of both the Baptist and the bootleggers on their merits. So let's do just that. Risk number one, will AI kill us all? - [Marc] Yes. - So what do you think about this one? What do you think is\n",
      "the core argument here that the development of\n",
      "AGI perhaps better said, will destroy human civilization? - Well, first of all, you\n",
      "just did a slight of hand 'cause we went from talking about AI to AGI. - Is there a fundamental difference there? - I don't know. What's AGI? - What's AI, what's in intelligence? - Well, I know what AI\n",
      "is machine learning. What's AGI? - I think we don't know\n",
      "what the bottom of the well of machine learning is\n",
      "or what the ceiling is. Because just to call\n",
      "something machine learning or just to call some of the statistics or just to call it math or\n",
      "computation doesn't mean, you know, nuclear\n",
      "weapons are just physics. So to me it's very\n",
      "interesting and surprising how far machine learning has taken. - No, but we knew that\n",
      "nuclear physics would lead to weapons. That's why the scientists\n",
      "of that era were always in some this huge dispute\n",
      "about building the weapons. This is different. AGI is different. - Does machine learning lead, do we know? - We don't know, but this\n",
      "is my point is different. We actually don't know. But, and this is where you, the slide of hand kicks in, right? This is where it goes from\n",
      "being a scientific topic to being a religious topic. And that's why I specifically called out 'cause that's what happens. They do the vocabulary\n",
      "shift and all of a sudden you're talking about something totally. That's not actually real. - Well then maybe you can\n",
      "also, as part of that, define the western\n",
      "tradition of Millennialism. - [Marc] Yes. Into the world apocalypse. - [Lex] What is it? - [Marc] Apocalypse cults. - [Lex] Apocalypse cults. - Well, so we live in, we of course live in a Judeo-Christian, but primarily Christian\n",
      "kind of saturated, you know, kind of Christian, post-Christian,\n",
      "secularized Christian, you know, kind of world in the west. And of course court of Christianity is the idea of the second\n",
      "coming and you know, the revelations and you know, Jesus returning and the\n",
      "thousand year, you know, utopia on earth and then you know, the rapture and like all\n",
      "all that stuff, you know, you know, we collectively,\n",
      "you know, as a society, we don't necessarily take\n",
      "all that fully seriously now. So, what we do is we create our\n",
      "secularized versions of that we keep looking for utopia. We keep looking for, you know, basically the end of the world. And so what what you see over, over decades is that basically a pattern of these sort of these of\n",
      "is this is what cults are. This is how cults form as```\n",
      "            \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSummarize the youtube video whose transcript is provided within backticks             ```they form around some theory of the end of the world. And so the people's temple cults, the Manson cult, the Heavens Gate cult, the David Qresh cult, you know what they're all\n",
      "organized around is like, there's gonna be this\n",
      "thing that's gonna happen that's gonna basically bring\n",
      "civilization crashing down. And then we have this\n",
      "special elite group of people who are gonna see it\n",
      "coming and prepare for it. And then they're the people\n",
      "who are either going to stop it or are failing, stopping it. They're gonna be the people\n",
      "who survived the other side and ultimately get credit\n",
      "for having been, right. - Why is that so compelling,\n",
      "do you think? Like-- - Because it satisfies this very deep need we have for transcendence and meaning that got stripped\n",
      "away when we became secular. - Yeah, but why is the\n",
      "transcendence involve the destruction of human civilization? - Because like how plausible it's like a very deep psychological thing 'cause it's like how plausible, how plausible is it\n",
      "that we live in a world where everything's just\n",
      "kind of all right? Right. How exciting? - [Lex] Whoa. - How exciting is that? Right? - [Lex] But that's. - We got more than that. - But that's the deep question I'm asking. Why is it not exciting to live in a world where everything's just all right? Is it, I think, you know, most of the animal kingdom would be so happy with just all right. Because that means survival. Why are we, maybe that's what it is. Why are we conjuring up\n",
      "things to worry about? - So CS Lewis called\n",
      "it the God-shaped hole. So there's a God-shaped hole\n",
      "in the human experience, consciousness, soul,\n",
      "whatever you wanna call it, where there's gotta be\n",
      "something that's bigger than all this. There's gotta be something transcendent. There's gotta be something\n",
      "that is bigger, right? Bigger purpose. A bigger meaning. And so we have run the\n",
      "experiment of, you know, we're just gonna use\n",
      "science and rationality and kind of, you know, everything's just gonna\n",
      "kind of be as it appears. And large number of people have found that very deeply wanting and\n",
      "have constructed narratives. And by this is the story\n",
      "of the 20th century, right? Communism, right? Was one of those, communism\n",
      "was a was a form of this, Nazism was a form of this. You know, some people, you know, you can see movements\n",
      "like this playing out all over the world right now. - So you constructed a kind of devil, a kind of source of evil, and we're going to transcend beyond it. - Yeah. And (indistinct)\n",
      "when you see a Miller cult, they put a really specific point on it, which is end of the world, right, there is some change coming. And that change that's\n",
      "coming is so profound and so important that\n",
      "it's either gonna lead to utopia or hell on earth. Right? And it is going to, and then, you know, it's like what if you actually knew that was going to happen, right? What would you do? Right? How would you prepare yourself for it? How would you come together with a group of like-minded people, right? How would you, what would you do? Would you plan like Cassius\n",
      "of weapons in the woods? Would you like, you know, I don't know if create\n",
      "underground buckers, would you, you know, spend your\n",
      "life trying to figure out a way to avoid having it happen? - Yeah. That's a really\n",
      "compelling, exciting idea to have a club over. To have a little bit of travel, like a get together on a Saturday\n",
      "night and drink some beers and talk about the end of the world and how you are the only\n",
      "ones who have figured it out. - Yeah. And then once you lock in on that, like how can you do anything\n",
      "else with your life? Like this is obviously the\n",
      "thing that you have to do. And then there's a psychological\n",
      "effect that you alluded to. There's a psychological effect. If you take a set of true\n",
      "believers and you leave them to themselves, they get\n",
      "more radical. Right. 'Cause they self radicalize each other. - That said, it doesn't mean they're not sometimes right. - Yeah. The end of the world might be. Yes. Correct. Like they might be right. - [Lex] Yeah. - But like-- - [Lex] I have some pamphlets for you. - Exactly. - But I mean we'll talk\n",
      "about nuclear weapons 'cause you have a really\n",
      "interesting little moment that I learned about in\n",
      "your essay, but you know, sometimes it could be right. - [Marc] Yeah. - 'Cause we're still, you were developing more and\n",
      "more powerful technologies in this case, and we don't know what the impact it will\n",
      "have on human civilization while we can highlight all\n",
      "the different predictions about how it'll be positive, but the risks are there and\n",
      "you discuss some of them. - Well, the steel man, the\n",
      "steel man is the steel man. Well actually, the steel\n",
      "man and his reputation are the same, which is you can't predict what's gonna happen. Right. You can't rule out that this\n",
      "will not end everything. Right. But the response to that\n",
      "is you have just made a completely non-scientific claim. You've made a religious\n",
      "claim, not a scientific claim. - How does it get disproven? - And there's no, by definition with these kinds of claims, there's no way to disprove them. Right? And so there there's no, you\n",
      "just go right on the list. There's no hypothesis, there's no testability of the hypothesis. There's no way to falsify the hypothesis, there's no way to measure\n",
      "progress along the arc. Like it's just all completely missing. And so it's not scientific and. - I don't think it's completely missing. It's somewhat missing. So for example, the people that say AI's gonna kill all of us. I mean, they usually have\n",
      "ideas about how to do that. Whether it's the people\n",
      "club maximizer or, you know, it escapes there's mechanism\n",
      "by which you can imagine it killing all humans. - [Marc] Models. - And you can disprove it by saying there's a limit to the speed at which intelligence increases. Maybe show that like the sort\n",
      "of rigorously really described model, like how it could\n",
      "happen and say, no, there, here's a physics limitation. There's like a physical\n",
      "limitation to how these systems would actually do damage\n",
      "to human civilization. And it is possible they\n",
      "will kill 10 to 20% of the population, but it seems impossible\n",
      "for them to kill 99%. - It was practical\n",
      "counterarguments. Right. So you mentioned\n",
      "basically what I described as the thermodynamic counterargument, which, so sitting here today, it's like where with the\n",
      "evil AGI get the GPU. 'Cause like they don't exist. So if you're gonna have a\n",
      "very frustrated baby evil AGI, who's gonna be like trying to\n",
      "buy Nvidia stock or something to get them to finally\n",
      "make some chips, right? So the serious form of that\n",
      "is the thermodynamic argument, which is like, okay, where's\n",
      "the energy gonna come from? Where's the processor gonna be running? Where's the data center\n",
      "gonna be happening? How is this gonna be\n",
      "happening in secret such that, you know, it's not, you know, so that's a practical counter argument to the runaway AGI thing. I have a but I have and we\n",
      "can argue that, discuss that. I have a deeper objection to it, which is it's, this is all forecasting. It's all modeling, it's\n",
      "all future prediction. It's all future hypothesizing. It's not science. - [Lex] Sure. - It is not. It is the\n",
      "opposite of science. So the, I'll pull up Carl Sagan\n",
      "extraordinary claims require extraordinary proof, right? These are extraordinary claims. The policies that are being\n",
      "called for right to prevent this are of extraordinary magnitude that, and I think we're gonna\n",
      "cause extraordinary damage. And this is all being done\n",
      "on the basis of something that is literally not scientific. It's not a testable hypothesis. - So the moment you say\n",
      "AI's gonna kill all of us, therefore we should ban it, or that we should regulate\n",
      "all that kind of stuff, that's when it starts getting serious. - Or start, you know, military\n",
      "airstrikes and data centers. - [Lex] Oh boy. - Right? And like. - Yeah. This once get starts. Well, so starts getting real weird. - So here's the problem with Arian cults. They have a hard time\n",
      "staying away from violence. - Yeah. But violence is so fun. - If you're on the right end of it, they have a hard time avoiding violence. The reason they have a hard\n",
      "time avoiding violence is if you actually believe the claim. Right. Then what would you do to\n",
      "stop the end of the world? Well, you would do anything, right? And so, and this is\n",
      "where you get, and again, if you just look at the\n",
      "history of Arian and cults, this is where you get the\n",
      "people's temple and everybody killing themselves in the jungle. And this is where you get\n",
      "Charles Manson and, you know, sending in to kill the pigs. Like, this is the problem with these. They have a very hard time to run the line at actual violence. And I think in this case, I\n",
      "mean, they're already calling for it like today and you know, where this goes from here\n",
      "is they get more worked up. Like I think is like really concerning. - Okay. But that's kind of the extremes. So, you know, the extremes of\n",
      "anything are I was concerning. It's also possible to kind\n",
      "of believe that AI has a very high likelihood\n",
      "of killing all of us. But and therefore we should maybe consider slowing development or regulating, so not violence or any\n",
      "of these kinds of things. But it's saying like, all right, let's take a pause here. You know, you biological\n",
      "weapons, nuclear weapons. Like whoa, whoa, whoa, whoa, whoa. This is like serious stuff.\n",
      "We should be careful. So it is possible to kinda have a more rational response, right? If you believe this risk is real. - [Marc] Believe. - Yes. So what is it possible to be, have a scientific approach to\n",
      "the prediction of the future? - I mean, we just went\n",
      "through this with COVID. What do we know about modeling? - [Lex] Well, I mean. - What did we learn about\n",
      "modeling with COVID? - [Lex] There's a lot of lessons. - They didn't work at all. - [Lex] They worked poorly. - The models were terrible,\n",
      "the models were useless. - I don't know if the models\n",
      "were useless or the people interpreting the models and\n",
      "then decentralized institutions that were creating policy\n",
      "rapidly based on the models and leveraging the models in order to support their narratives versus actually\n",
      "interpreting the error bars and the models and all that kind of stuff. - What you had with COVID, my view you had with COVID\n",
      "is you had these experts showing up and they\n",
      "claimed to be scientists and they had no testable\n",
      "hypotheses whatsoever. They had a bunch of models. They had a bunch of forecasts\n",
      "and they had a bunch of theories and they laid these out in front of policy makers and policy makers freaked\n",
      "out and panicked. Right. And implemented a whole bunch of like, really like terrible decisions\n",
      "that we're still living with the consequences of, and there was never any\n",
      "empirical foundation to any of the models. None of them ever came true. - Yeah. To push back. There were certainly\n",
      "Baptist and bootleggers in the context of this pandemic, but there's still a\n",
      "usefulness to models. No. - So not if they're, I mean not if they're\n",
      "reliably wrong, right? Then they're actually\n",
      "like anti-useful. Right. They're actually damaging. - But what do you do with the pandemic? What do you do with any kind of threat? Don't you want to kind of\n",
      "have several models to play with as part of this discussion of like, what the hell do we do here? - I mean, do they work? Because they're an expectation\n",
      "that they actually like work that they have actual predictive value. I mean, as far as I can tell with COVID, the policymakers just si up\n",
      "themselves into believing that there was sub, I\n",
      "mean, look, the scientists, the scientists were at fault. The quote unquote scientists showed up. So I had some insight into this. So there was a, or remember the Imperial College models out of London were the\n",
      "ones that were like, these are the gold standard models. So a friend of mine runs\n",
      "a big software company and he was like, wow, this is\n",
      "like, COVID is really scary. And he is like, you know, he contacted this research\n",
      "and he is like, you know, do you need some help? You've been just building\n",
      "this model on your own for 20 years. Do you need some, would you like us our coders\n",
      "to basically restructure it so it can be fully adapted for COVID? And the guy said yes\n",
      "and sent over the code and my friend said it was\n",
      "like the worst spaghetti code he's ever seen. - That doesn't mean it's\n",
      "not possible to construct a good model of pandemic\n",
      "with the correct air bars, with a high number of parameters\n",
      "that are continuously, many times a day updated\n",
      "as we get more data about a pandemic. I would like to believe when\n",
      "a pandemic hits the world, the best computer scientists in the world, the best software engineers\n",
      "respond aggressively and as input take the data\n",
      "that we know about the virus and it's an output say\n",
      "here is what's happening in terms of how quickly it's spreading, what that lead in terms of\n",
      "hospitalization and deaths and all that kind of stuff. Here's how likely, how\n",
      "contagious it likely is. Here's how deadly it likely is based on different conditions, based on different ages and demographics and all that kind of stuff. So here's the best kinds of policy. It feels like you could have models, machine learning that like kind of, they don't perfectly predict the future, but they help you do something 'cause there's pandemics\n",
      "that are like, meh, they don't really do much harm. And there's pandemics,\n",
      "you can imagine them, they could do a huge amount of harm. Like they can kill a lot of people. So you should probably have\n",
      "some kind of data-driven models that keep updating, that allow you to make\n",
      "decisions that based like where, how bad is this thing? Now you can criticize how\n",
      "horrible all that went with the response to this pandemic, but I just feel like there\n",
      "might be some value to models. - So to be useful at some point it has to be predictive. Right? So and the easy thing\n",
      "for me to do is to say, obviously you're right. Obviously I wanna see that\n",
      "just as much as you do. 'cause anything that makes\n",
      "it easier to navigate through society through a\n",
      "wrenching, you know, risk like that sounds great. You know, the harder objection to it is just simply\n",
      "you are trying to model a complex dynamic system\n",
      "with 8 billion moving parts. Like not possible. - [Lex] It's very tough. - Can't be done, complex\n",
      "systems can't be done. - Machine learning says hold my beer. But well, it's possible. No? - I don't know. I would like to believe that it is. I'll put it this way. I think where you and I\n",
      "would agree is I think we would like that to be the case. We are strongly in favor of it. I think we would also\n",
      "agree that no such thing with respect to COVID or\n",
      "pandemics no such thing. At least neither you\n",
      "nor I think are aware. I'm not aware of anything like that today. - My main worry with the\n",
      "response to the pandemic is that same as with aliens, is that even if such a thing existed, and it's possible it existed, the policymakers were\n",
      "not paying attention. Like there was no mechanism\n",
      "that allowed those kinds of models to percolate all. - Oh, I think we had the\n",
      "opposite problem during COVID. I think the policymakers, I think these people with\n",
      "basically fixed science had too much access to the policymakers. - Well, right. And well, but the policy\n",
      "makers also wanted, they had a narrative in\n",
      "mind and they also wanted to use whatever model\n",
      "that fit that narrative - [Marc] Oh, sure. - To help them out. So like, it felt like\n",
      "there was a lot of politics and not enough science. - Although a big part\n",
      "of what was happening, a big reason we got lockdowns\n",
      "for as long as we did, was because these scientists\n",
      "came in with these like doomsday scenarios that were like, just like completely off the hook. - Scientists in quotes, let's not-- - [Marc] Quote unquote scientists. - Let's not, okay,\n",
      "let's give love science. So here's science that is the way out. - Science is a process\n",
      "of testing hypotheses. Modeling does not involve\n",
      "testable hypotheses. Right. Like, I don't even know that. I actually don't even know that modeling actually\n",
      "qualifies as science. Maybe that's a side conversation. We could have some time over a beer. - Oh, that's a really interesting part. What do we do about the future? I mean, what's-- - So number one is when\n",
      "we start with number one, humility goes back to this thing of how do we determine the truth. Number two is we don't believe, you know, it's the old, I've gotta hammer everything\n",
      "looks like a nail, right? I've got, oh, this is one\n",
      "of the reasons I gave you, I gave Alexa book, which the topic of the\n",
      "book is what happens when scientists basically\n",
      "stray off the path of technical knowledge and\n",
      "start to weigh in on politics and societal issues. - In this case, philosophers. - Well in this case philosophers. But he actually talks in this\n",
      "book about, like Einstein, he talks about, actually about\n",
      "the nuclear age in Einstein. He talks about the\n",
      "physicists actually doing very similar things at the time. - The book is When Reason Goes On Holiday, Philosophers in Politics by Nevin. - And it's just a story. It's a story. There are other books on this topic, but this is a new one that's really good this is just a story of what happens when experts\n",
      "in a certain domain decide to weigh in and become\n",
      "basically social engineers and political, you know,\n",
      "basically political advisors. And it's just a story of just\n",
      "inning catastrophe. Right. And I think that's what\n",
      "happened with COVID again. - Yeah. I found this book\n",
      "a highly entertaining and eye-opening read filled\n",
      "with amazing anecdote of a rationality and craziness\n",
      "by famous Resa philosophers. - I definitely, after you read this book, you will not look at Einstein the same. - [Lex] Oh boy. - Yeah. - Don't destroy my heroes. - He will not be a hero of yours anymore. Sorry. You probably couldn't,\n",
      "you shouldn't read the book. - All right. - But here's the thing. The AI risk people, they don't even have the COVID model, at least not that I'm aware of. - [Lex] No. - Like there's not even the\n",
      "equivalent of the COVID model. They don't even have the spaghetti code. They've got a theory and a\n",
      "warning and a this and the that. And like, if you ask like,\n",
      "okay, well here's, I mean, the ultimate example is,\n",
      "okay, how do we know, right? How do we know that an AI is running away? Like how do we know that the boom takeoff thing\n",
      "is actually happening? And the only answer that\n",
      "any of these guys have given that I've ever seen is, oh,\n",
      "it's when the loss rate, the loss function and the\n",
      "training drops, right? That's when you need to like\n",
      "shut down the data center. Right? And it's like, well that's also what happens when you're successfully training a model. Like, what even this is not science, this is not, it's not\n",
      "anything, it's not a model, it's not anything. There's nothing to arguing with. It is like, you know,\n",
      "punching jello, like there, there's what do you even respond to? - So just put push back on that. I don't think they have good metrics of when the film is happening. But I think it's possible to have that. Like just as you speak now, I mean it's possible to imagine\n",
      "there could be measures. - It's been 20 years. - No, for sure. But it is been only weeks\n",
      "since we had a big enough breakthrough in language models. We can start to actually have this, the thing is the AI doer stuff didn't have any actual systems to really work with. And now there's real systems\n",
      "you can start to analyze like, how does this stuff go wrong? And I think you kind\n",
      "of agree that there is a lot of risks that we can analyze. The benefits outweigh\n",
      "the risks in many cases. - Well, the risks are not existential. - [Lex] Yes. Well. - Not in the phone paper\n",
      "clip. Let me, okay. There's another slide of hand\n",
      "that you just alluded to. There's another slide\n",
      "of hand that happens, which is very interesting. - I'm very good at the\n",
      "slide of hand thing. - Which is very not scientific. So the book Super Intelligence, right, which is like the Nick Bostrom's book, which is like the origin\n",
      "of a lot of this stuff, which was written, you know, whatever, 10 years ago or something. So he does this really```\n",
      "            \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSummarize the youtube video whose transcript is provided within backticks             ```fascinating thing in the book, which is he basically says\n",
      "there are many possible routes to machine intelligence,\n",
      "to artificial intelligence. And he describes all the different routes to artificial intelligence,\n",
      "all the different possible, everything from biological\n",
      "augmentation through to, you know, all these different things. One of the ones that\n",
      "he does not describe is large language models because of course the book was written\n",
      "before they were invented. And so they didn't exist. In the book, he describes them all and then he proceeds to treat them all as if they're\n",
      "exactly the same thing. He presents them all as sort\n",
      "of an equivalent risk to be dealt with in an equivalent\n",
      "way to be thought about the same way. And then the risk, the quote unquote risk that's actually emerged is actually a completely different technology than he was even imagining. And yet all of his theories\n",
      "and beliefs are being transplanted by this movement, like straight onto this new technology. And so again, like there's no other area of science or technology\n",
      "where you do that. Like when you're dealing\n",
      "with like organic chemistry versus inorganic chemistry,\n",
      "you don't just like say, oh, with respect to like either\n",
      "one, basically maybe, you know, growing up in eating\n",
      "the world or something, like they're just gonna\n",
      "operate the same way. Like you don't. - But you can start talking about like, as we get more and more actual systems that start to get more\n",
      "and more intelligent, you can start to actually have more scientific arguments here. - [Marc] Oh yeah. - Like, you know, high level, you can talk about the threat\n",
      "of autonomous weapon systems back before we had any\n",
      "automation in the military. And that would be like\n",
      "very fuzzy kind of logic. But the more and more you\n",
      "have drones that are becoming more and more autonomous, you\n",
      "can start imagining, okay, what does that actually look\n",
      "like and what's the actual threat of autonomous weapons systems? How does it go wrong? And still it's very vague, but you start to get a\n",
      "sense of like, all right, it should probably be illegal or wrong or not allowed\n",
      "to do like mass deployment of fully autonomous drones that are doing aerial strikes. - [Marc] Oh no. - On large areas. - [Marc] I think it should be required. - Right? So that's a no. - No, no. I think it should be required that only aerial vehicles are automated. - Okay. So you wanna go the other way? - I wanna go the other way. - So that, okay. - I think it's obvious that\n",
      "the machine is gonna make a better decision than the human pilot. I think it's obvious that\n",
      "it's in the best interest of both the attacker and the\n",
      "defender and humanity at large. If machines are making\n",
      "more of these decisions than not people, I think people make terrible\n",
      "decisions in times of war. - But like, there's ways\n",
      "this can go wrong too, right? - Well, it wars go terribly wrong now. This goes back to the whole, this is that whole thing\n",
      "about like the self-drive. Does the self-driving\n",
      "car need to be perfect versus does it need to be\n",
      "better than the human driver? Does the automated\n",
      "drone need to be perfect or does it need to be better than a human pilot at making decisions under enormous amounts of\n",
      "stress and uncertainty? - Yeah, well, on average, the worry that AI folks\n",
      "have is the runaway. - They're gonna come alive. Right? That then again, that's\n",
      "the slight of hand, right. - Or not not come alive.\n",
      "Well, no, hold on a second. You lose control as well. You lose control. - But then they're gonna\n",
      "develop goals of their own. They're gonna develop a mind of their own, they're gonna develop their own. Right. - No more, more like\n",
      "Chernobyl style meltdown, like just bugs in the code\n",
      "accidentally, you know, force you like the results in the bombing of like large civilian areas. - [Marc] Okay. And to a degree that's not possible in the current military strategies, - [Marc] I don't know. - Control by humans. - Well, actually we've been\n",
      "doing a lot of mass bombings to cities for a very long time. - Yes. And a lot of civilians died. - And a lot of civilians died. And if you watch the documentary,\n",
      "the Fog of War McNamara, it spends a big part of it\n",
      "talking about the fire bombing of the Japanese cities. Burning them straight\n",
      "to the ground. Right. The devastation in Japan, American military fire bombing\n",
      "the cities in Japan was considerably bigger devastation\n",
      "than the use of nukes. Right. So we've been doing\n",
      "that for a long time. We also did that to Germany, by the way Germany did that to us, right? Like that's an old tradition. The minute we got airplanes, we started doing indiscriminate bombing. - So one of the things-- - [Marc] We're still doing it. - The modern US military can do with technology with automation, but technology more broadly is higher and higher precision strikes. - Yeah, I was saying, so precision is obviously precision and this is a (indistinct) right? So there was this big advance this big advance called the (indistinct) which basically was\n",
      "strapping a GPS transceiver to an unguided bomb and turning it into a guided bomb. And yeah, that's great. Like look, that's been a big advance, but, and that's like a baby\n",
      "version of this question, which is okay, do you\n",
      "want like the human pilot, like guessing where the bomb's gonna land? Or do you want like the\n",
      "machine like guiding the bomb to his destination? That's a baby version of the question. The next version of the question is, do you want the human\n",
      "or the machine deciding whether to drop the bomb? Everybody just assumes the\n",
      "human's gonna do a better job for what I think are\n",
      "fundamentally suspicious reasons. - Emotional, psychological reasons. - Yeah. I think it's very clear\n",
      "that the machine's gonna do a better job making that decision 'cause the humans making\n",
      "that decision are got awful. Just terrible. - [Lex] Yeah. - Right. And so yeah. So this is the thing. And then let's get to the, there was, can I one more slide of hand? - [Lex] Yes. - It was in-- - Sure. Please. I'm a magician. You could say. - One more slight of hand. These things are gonna be so smart, right? That they're gonna be able to\n",
      "destroy the world and wreak havoc and like do all this\n",
      "stuff and plan and do all this stuff and evade us and have\n",
      "all their secret things and their secret factories\n",
      "and all this stuff. But they're so stupid that\n",
      "they're gonna get like, tangled up in their code and that's they're not gonna come alive, but there's gonna be some\n",
      "bug that's gonna cause them to like turn us all on a paper like that. They're not gonna be\n",
      "genius in every way other than the actual bad goal. And it's just like,\n",
      "and that's just like a, like ridiculous like discrepancy. And you can prove this today, you can actually address\n",
      "this today for the first time with LLMs which is you can actually ask LLMs to resolve moral dilemmas. So you can create the\n",
      "scenario, you know, dot, dot, dot this, that, this, that, this, that. What would you as the AI\n",
      "do in the circumstance? And they don't just\n",
      "say destroy all humans, destroy all humans. They will give you actually\n",
      "very nuanced moral, practical trade-off oriented answers. And so we actually already\n",
      "have the kind of AI that can actually like, think this through and can actually like, you know, reason about goals. - Well, the hope is that AGI or like various superintelligent systems have some of the\n",
      "nuance that LLMs have and the intuition is they most likely will because even these LLMs have the nuance. - LLMs are really, this is\n",
      "actually worth spending a moment on LLMs are really interesting to have moral conversations with. And that I just, I didn't expect I'd be\n",
      "having a moral conversation with the machine in my lifetime. - Wait, and let's remember\n",
      "we're not really having a conversation with the machine where we're having a conversation with the entirety of the\n",
      "collective intelligence of the human species. - Exactly. Yes. Correct. - But it's possible to imagine\n",
      "autonomous weapons systems that are not using LLMs. - But if they're smart enough to be scary, where are they not\n",
      "smart enough to be wise? Like, that's the part where it's like, I don't know how you get\n",
      "the one without the other. - Is it possible to be super intelligent without being super wise? - Well, again, you're back to that. I mean, then you're back to\n",
      "a classic autistic computer, right? Like you're back to just\n",
      "like a blind rule follower. I've got this like core,\n",
      "it's the paperclip thing. I've got this core rule and\n",
      "I'm just gonna follow it to the end of the earth. And it's like, well, but everything you're gonna\n",
      "be doing execute that rule is gonna be super genius level\n",
      "that humans aren't gonna be able to counter. It's a mismatch in the definition of what the system's capable of. - Unlikely but not impossible, I think. - But again, here you\n",
      "get to like, okay, like. - No, I'm not saying when it's\n",
      "unlikely but not impossible. If it's unlikely, that means the fear should be correctly calibrated. - Extraordinary claims\n",
      "require extraordinary proof. - Well, okay, so one\n",
      "interesting sort of tangent, I would love to take on this\n",
      "because you mentioned this in the essay about nuclear,\n",
      "which was also, I mean, you don't shy away from a\n",
      "little bit of of a spicy take. So Robert Oppenheimer famously said, now I am become death\n",
      "the destroyer of worlds as he witnessed the first\n",
      "destination of a nuclear weapon on July 16th, 1945. And you write an interesting\n",
      "historical perspective, \"Recall that John Van Neuman responded to Robert Oppenheimer's famous\n",
      "hand wringing about the role of creating nuclear weapons, which you note helped end World War II and prevent World War III\n",
      "with some people confess guilt to claim credit for the sin.\" And you also mentioned\n",
      "that Truman was harsher after meeting Oppenheimer. He said that \"Don't let that\n",
      "cry baby in here again.\" - Real quote, by the\n",
      "way, from Dean Atchison. - Boy. - 'Cause Oppenheimer didn't\n",
      "just say the famous line. - [Lex] Yeah. - He then spent years going\n",
      "around basically moaning him, you know, going on TV and going into going into the White House and basically like, just like doing this hair shirt, you know, thing self, you know, this\n",
      "sort of self-critical like, oh my god, I can't believe how awful I am. - So he's widely\n",
      "considered perhaps of the, because of the hang ringing\n",
      "as the father of the tom bomb. - [Marc] Yeah. - This is Van Norman's criticism\n",
      "of him is he tried to have his cake and eat it too. Like he wanted to and Van Norman of course a very different kind of personality and\n",
      "he's just like, yeah, good. This is like an incredibly\n",
      "useful thing. I'm glad we did it. - Yeah. Well Van Norman is\n",
      "is widely credit as being one of the smartest humans\n",
      "of the 20th century. Certain people. Everybody says like, this is the smartest person I've ever met when they've met him. Anyway, that doesn't mean,\n",
      "smart doesn't mean wise. So yeah, I would love to sort of, can you make the case both\n",
      "for and against the critique of Oppenheimer here? 'Cause we're talking\n",
      "about nuclear weapons. Boy, do they seem dangerous? - Well so, the critique goes deeper and I left this out. Here's the real substance, I left it out 'cause I didn't wanna dwell on nukes in my AI paper. But here's the deeper thing that happened and I'm really curious, this\n",
      "movie coming out this summer, I'm really curious to see\n",
      "how far he pushes this. 'cause this is the real\n",
      "drama in the story, which is, it wasn't just a question\n",
      "of our nukes, good or bad, it was a question of should\n",
      "Russia also have them? And what actually happened was Russia got the American invented the bomb. Russia got the bomb, they got the bomb through espionage, they got American and you know, they got American scientists\n",
      "and foreign scientists working on the American project. Some combination of the two\n",
      "basically gave the Russians the designs for the bomb. And that's how the Russians got the bomb. There's this dispute to this\n",
      "day of Oppenheimer's role in that if you read all the histories, the kind of composite\n",
      "picture, and by the way, we now know a lot actually\n",
      "about Soviet espionage in that era 'cause there's been all this declassified\n",
      "material in the last 20 years that actually shows a lot\n",
      "of very interesting things. But if you kinda read all the histories, which you kinda get is Oppenheimer\n",
      "himself probably was not he probably did not hand over\n",
      "the nuclear secrets himself. However, he was close\n",
      "to many people who did. Including family members. And there were other members\n",
      "of the Manhattan Project who were Russian, Soviet SS\n",
      "and did hand over the bomb. And so the view of that\n",
      "Oppenheimer and people like him had that this thing is awful\n",
      "and terrible and oh my god. And you know, all this stuff you could\n",
      "argue fed into this ethos at the time that resulted\n",
      "in people thinking that the Baptists thinking that the only principle\n",
      "thing to do was to give the Russians the bomb. And so the moral beliefs on this thing and the public discussion and the role that the inventors\n",
      "of this technology play, this is the point of this book, when they kind of take on this\n",
      "sort of public intellectual, moral kind of thing, it can\n",
      "have real consequences, right? Because we live in a very\n",
      "different world today because Russia got the\n",
      "bomb than we would've lived in had they not gotten the bomb right. The entire 20th century, second half of the 20th\n",
      "century would've played out very different had those people\n",
      "not given Russia the bomb. And so the stakes were very high then. The good news today is\n",
      "nobody's sitting here today, I don't think worrying about\n",
      "like an analogous situation with respect to like, I'm not really worried that\n",
      "Sam Altman's gonna decide to give, you know, the\n",
      "Chinese, the design for AI, although he did just speak\n",
      "at a Chinese conference, which is in interesting. But however, I don't think\n",
      "that's what's at play here, but what's at play here are\n",
      "all these other fundamental issues around what do\n",
      "we believe about this and then what laws and\n",
      "regulations and restrictions that we're gonna put on it. And that's where I draw\n",
      "like a direct straight line. And anyway, and my reading\n",
      "of the history on nukes is like the people who were doing\n",
      "the full hair shirt public, this is awful. This is terrible. Actually had like\n",
      "catastrophically bad results from taking those views. And that's what I'm worried\n",
      "it's gonna happen again. - But is there a case to be\n",
      "made that you really need to wake the public up to the dangers of nuclear weapons when\n",
      "they were first dropped? Like really like educate them on like, this is extremely dangerous\n",
      "and destructive weapon. - I think the education\n",
      "kind of happened quick and early, like-- - [Lex] How? - It was pretty obvious. - [Lex] How? - We dropped one bomb and\n",
      "destroyed an entire city. - Yeah. So 80,000 people dead. - [Marc] Yep. - But. - [Marc] And look. But-- - I don't like the reporting of that. You can report that in all kinds of ways. - [Marc] Oh, there wars. - You can do all kinds of slants. Like war is horrible. War is terrible. You can do, you can make\n",
      "it seem like nuclear, the use of nuclear weapons\n",
      "is just a part of war and all that kind of stuff. Something about the\n",
      "reporting and the discussion of nuclear weapons resulted\n",
      "in us being terrified in awe of the power of nuclear weapons and that potentially fed\n",
      "in a positive way towards the game theory of\n",
      "mutual issue destruction. - Well, so this gets to what actually, let's get to what actually happens. - [Lex] Some of us, me\n",
      "playing devil's advocate here. - Yeah, yeah, sure. Of course. Let's get to what\n",
      "actually happened and then kind of back into that. So what actually happened, I believe, and again I think this is a\n",
      "reasonable reading of history, is what actually happened\n",
      "was nukes then prevented World War III and they\n",
      "prevented World War III through the game theory\n",
      "of mutually assured destruction had nukes not existed. Right. There would've been no reason why the Cold War did not go hot. Right. And then there and then, you know, and the military planners\n",
      "at the time, right, thought both on both sides\n",
      "thought that there was gonna be World War III on the planes of Europe and they thought there was gonna be like a hundred million people dead. Right? It was like the most obvious\n",
      "thing in the world to happen. Right? And it's the dog\n",
      "that didn't bark right? Like it may be like the best\n",
      "single net thing that happened in the entire 20th century is\n",
      "that like that didn't happen. - Yeah. Actually, just on that point, you say a lot of really brilliant things. It hit me just as you were saying it. I don't know why it hit\n",
      "me for the first time, but we got two wars in\n",
      "a span of like 20 years. Like we could have kept getting\n",
      "more and more world wars and more and more ruthless. It actually, you could have\n",
      "had a US versus Russia war. - You could, by the way you haven't, there's another hypothetical scenario. The other hypothetical scenario is that Americans got the\n",
      "bomb, the Russians didn't. Right? And then America's the big dog and then maybe America\n",
      "would've had the capability to actually roll back the iron curtain. I don't know whether\n",
      "that would've happened, but like it's entirely possible. Right? And the act of these people who had these moral positions about, 'cause they could\n",
      "forecast, they could model, they could forecast the future\n",
      "of how the technology would get used, made a horrific mistake. 'cause they basically ensured that the iron curtain\n",
      "would continue for 50 years longer than it would've otherwise. Like, and again, like\n",
      "these are counter-factuals, I don't know that that's\n",
      "what, what would've happened, but like the decision to hand the bomb over was a big decision made by people who were very\n",
      "full of themselves. - Yeah. But so me as an America, me as a person that loves America, I also wonder if US was the only ones with the nuclear weapons. - That was the argument for handing that was the guys who (indistinct) the guys who handed over the bomb. That was actually their moral argument. - Yeah. I would probably\n",
      "not hand it over to, I would be careful about the regimes. You hand it over to there, maybe give it to like\n",
      "the British or something, or like a democratically-elected\n",
      "government. - Well, look, there are people to this day who think that those bias Soviet spies did the right thing because\n",
      "they created a balance of terror as opposed\n",
      "to the US having just, and by the way, let me-- - Balance of terror. - [Marc] Let's tell the\n",
      "full version story has-- - Such a sexy ring to it. - Okay. So the full\n",
      "version of the story is John Van Norman is a hero\n",
      "of both yours and mind. The full version of the\n",
      "story is he advocated for a first strike. So when the US had the\n",
      "bomb and Russia did not, he advocated for, he said, we need to strike them right now. - Strike Russia. - [Marc] Yes. - Van Norman. - Yes, because he said\n",
      "World War III is inevitable. He was very hardcore. His theory was World\n",
      "War III is inevitable. We're definitely gonna have World War III. The only way to stop World War\n",
      "III is we have to take them out right now and we have\n",
      "to take them out right now before they get the bomb. 'Cause this is our last chance. Now again, like-- - Is this an example of\n",
      "philosophers and politics? - I don't know if that's in there or not, but this is in the standard. - No, but it is meaning is that. - Yeah, this is on the other side. So, most of the case studies, most of the case studies\n",
      "in books like this are the crazy people on the left. Van Norman is a story arguably of the crazy people on the right. - Yes. Stick to computing, John. - Well. This is the thing, and this is the general principle. Getting back to our core\n",
      "thing, which is like, I don't know whether any of\n",
      "these people should be making any of these calls. Because there's nothing in\n",
      "either Van Norman's background or Oppenheimer's background or any of these people's background that qualifies them as moral authorities. - Yeah. Well this actually\n",
      "brings up the point of, in AI, who are the good people to reason about the\n",
      "morality of the ethics, the outside of these risks, outside of like the more```\n",
      "            \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSummarize the youtube video whose transcript is provided within backticks             ```complicated stuff that you, you agree on is, you know, this will go into the hands of bad guys and all the kinds of ways\n",
      "they'll do is interesting and dangerous, is dangerous in interesting unpredictable ways. And who is the right person? Who are the right kinds of\n",
      "people to make decisions, how to respond to it?\n",
      "Or is the tech people? - So the history of these fields, this is what he talks about in the book, the history of these fields,\n",
      "is that the competence and capability and\n",
      "intelligence and training and accomplishments of senior\n",
      "scientists and technologists working on a technology\n",
      "and then being able to then make moral judgments\n",
      "in the use of that technology. That track record is terrible that track record is like\n",
      "catastrophically bad. The people-- - Just the linger, the people that develop that\n",
      "technology are usually not going to be the right people. - Well why would they? So\n",
      "the claim is of course, they're the knowledgeable ones. But the the problem is they've\n",
      "spent their entire life in a lab. Right. They're not theologians. Well, so what you find,\n",
      "what you find when you read, when you read this, when\n",
      "you look at these histories, what you find is they generally are very thinly informed on history, on sociology, on theology,\n",
      "on morality, on ethics. They tend to manufacture their\n",
      "own worldviews from scratch. They tend to be very sort of thin. They're not remotely the\n",
      "arguments that you would be having if you got like a group of\n",
      "highly qualified theologians or philosophers or, you know. - Well, let me sort of,\n",
      "as the devil's advocate, takes a simple whiskey say\n",
      "that I agree with that. But also it seems like the\n",
      "people who are doing kind of the ethics departments and these tech companies\n",
      "go sometimes the other way. - [Marc] Yes, they're definitely. - Which they're not nuanced\n",
      "on history or theology or this kind of stuff. It almost becomes a kind\n",
      "of outraged activism towards directions that don't seem to be grounded in history and\n",
      "humility and nuance. It's again, drenched with arrogance. So-- - [Marc] Definitely. - I'm not sure which is worse. - Oh no, they're both bad. Yeah. So definitely not them either. - So, but I guess. - Well look, this is a hard. - Yeah, it's a hard problem. - This is a hard problem. This goes back to where we started, which is, okay, who has the truth? And it's like, well, you know, like how does societies\n",
      "arrive at like truth and how do we figure these things out and like our elected leaders\n",
      "play some role in it. You know, we all play some role in it. There have to be some set\n",
      "of public intellectuals at some point that bring, you know, rationality and judgment\n",
      "and humility to it. Those people are few and far between. We should probably prize them very highly. - Yeah. So celebrate humility\n",
      "in our public leaders. So getting to risk number two, will AI ruin our society\n",
      "short version as you write, if the murder robots don't\n",
      "get us the hate speech and misinformation will. And the action you recommend in short, don't let the thought police suppress AI. Well what is this risk of\n",
      "the effect of misinformation of society that's going\n",
      "to be catalyzed by AI? - Yeah, so this is the social media, this is what you just alluded to. It's the activism kind\n",
      "of thing that's popped up in these companies in the industry. And it's basically, from my perspective, it's basically part two\n",
      "of the war that played out over social media over the last 10 years, 'cause you probably remember\n",
      "social media 10 years ago, was basically who even wants this? Who wants a photo of what\n",
      "your cat had for breakfast? Like, this stuff is like silly and trivial and why can't these nerds like figure out how to invent something\n",
      "like useful and powerful? And then, you know, certain things happened\n",
      "in the political system. And then it sort of, the polarity on that\n",
      "discussion switched all the way to social media is like\n",
      "the worst, most corrosive, most terrible, most awful\n",
      "technology ever invented. And then it leads to, you\n",
      "know, terrible of the wrong, you know, politicians and\n",
      "policies and politics and like, and all this stuff. And that all got catalyzed into\n",
      "this very big kind of angry movement both inside and\n",
      "outside the companies to kind of bring social media to heal. And that got focused in\n",
      "particularly on two topics, so-called hate speech and\n",
      "so-called misinformation. And that's been the saga playing out for the last decade. And I don't even really want\n",
      "to even argue the pros and cons of the sides just to observe that's been like a huge fight and has had, you know, big consequences to how\n",
      "these companies operate. Basically that same, those\n",
      "same sets of theories, that same activist approach, that same energy as being\n",
      "transplanted straight to AI. And you see that already happening. It's why, you know, ChatGPT will answer, let's say certain\n",
      "questions and not others. It's why it gives you the\n",
      "canned speech about, you know, whenever it starts with,\n",
      "as a large language model, I cannot, you know, basically means that somebody\n",
      "has reached in there and told that it can't talk about certain topics. - Do you think some of that is good? - So it's an interesting question. So a couple observations. So, one is the people who find this the most frustrating are the people who are worried\n",
      "about the murder robots, right? So, and in fact so called\n",
      "X risk people, right? They started with the term AI safety, the term became AI alignment. When the term became AI alignment is when this switch happened\n",
      "from we're worried it's gonna kill us all to\n",
      "we're worried about hate speech and misinformation. - [Lex] Sure. - The AI X risk people have\n",
      "now renamed their thing AI not kill everyone-ism, which I have to admit is a catchy term. And they are very frustrated\n",
      "by the fact that the hate speech sort of activist driven\n",
      "hate speech misinformation kind of thing is taking over. Which is what's happened is taken over, the AI ethics field has been\n",
      "taken over by the hate speech misinformation people. You know, look, would I like to live in a world\n",
      "in which like everybody was nice to each other all the\n",
      "time and nobody ever said anything mean and nobody ever\n",
      "used a bad word and everything was always accurate and honest. Like, that sounds great. Do I wanna live in a world\n",
      "where there's like a centralized thought police working through\n",
      "the tech companies to enforce the view of a small set of\n",
      "elites that they're gonna determine what the rest\n",
      "of us think and feel like? Absolutely not. - There could be a middle\n",
      "ground somewhere like Wikipedia type of moderation. There's moderation of Wikipedia\n",
      "that is somehow crowdsourced where you don't have centralized elites, but it's also not completely\n",
      "just a free for all because if you have the\n",
      "entirety of human knowledge at your fingertips, you\n",
      "can do a lot of harm. Like if you have a good assistant that's completely uncensored, they can help you build a bomb, they can help you mess with\n",
      "people's physical wellbeing. Right. If they, because that information is\n",
      "out there on the internet and so presumably there's, it would be, you could see the positives\n",
      "in censoring some aspects of an AI model when it's helping you\n",
      "commit literal violence. - Yeah. And there's a section\n",
      "later section of the essay where I talk about bad\n",
      "people doing bad things. - [Lex] Yes. - Right. Which and there's this, there's a set of things that\n",
      "we should discuss there. - [Lex] Yeah. - What happens in practice is these lines, as you alluded to this already, these lines are not easy to draw. And what I've observed in\n",
      "the social media version of this is like, the way I describe it as the slippery slope is not a fallacy, it's an inevitability. The minute you have this kind\n",
      "of activist personality that gets in a position to make these decisions they take it straight to infinity. Like, it goes into the crazy\n",
      "zone like almost immediately and never comes back because\n",
      "people become drunk with power. Right. And look, if you're\n",
      "in the position to determine what the entire world thinks and feels and reads and says like, you're gonna take it and you\n",
      "know, Elon has, you know, ventilated this with the\n",
      "Twitter files over the last, you know, three months and\n",
      "it's just like crystal clear, like how bad it got there now. - [Lex] Yeah. - Reason for optimism\n",
      "is what Elon is doing with community notes. So community notes is actually\n",
      "a very interesting thing. So, what Elon is trying to\n",
      "do with community notes is he's trying to have it where\n",
      "there's only a community note when people who have previously\n",
      "disagreed on many topics agree on this one. - Yes, that's what I'm\n",
      "trying to get at is like, there could be Wikipedia like\n",
      "models or community notes type of models where allows you\n",
      "to essentially either provide context or sensor in a\n",
      "way that's not resist the slippery slope nature. Power. - Now there's an entirely\n",
      "different approach here, which is basically we have AIs\n",
      "that are producing content. We could also have ais that\n",
      "are consuming content. Right? And so one of the things that\n",
      "your assistant could do for you is help you consume\n",
      "all the content, right? And basically tell you\n",
      "when you're getting played. So for example, I'm gonna\n",
      "want the AI that my kid uses, right, to be very, you know, child safe and I'm gonna want\n",
      "it to filter for him all kinds of inappropriate stuff that\n",
      "he shouldn't be saying just 'cause he's a kid. Right? And you see what I'm saying\n",
      "is you can implement that. The architectural, you\n",
      "could say you can solve this on the client side, right? You solving on the server\n",
      "side gives you an opportunity to dictate for the entire\n",
      "world, which I think is where you take the slippery slope to hell, there's another architectural approach, which is to solve this on the client side, which is certainly what I would endorse. - It's AI risk number five, will AI lead to bad\n",
      "people doing bad things? And I can just imagine language\n",
      "models used to do so many bad things, but the hope is there that\n",
      "you can have large language models used to then defend\n",
      "against it by more people, by smarter people, by more\n",
      "effective people, skilled people, all that kind of stuff. - Three-part argument on\n",
      "bad people doing bad things. So, number one, right? You can use the technology defensively and we should be using AI\n",
      "to build like broad spectrum vaccines and antibiotics for\n",
      "like bio weapons and we should be using AI to like hunt terrorists and catch criminals and like, we should be doing like all\n",
      "kinds of stuff like that. And in fact, we should be doing those things even just to like go get like, you know, basically go eliminate risk\n",
      "from like regular pathogens that aren't like constructed by an AI. So there's the whole\n",
      "defensive set of things. Second is we have many laws on the books about as actual bad things, right? So it is actually illegal\n",
      "to be a criminal, you know, to commit crimes, to commit\n",
      "terrorist acts to, you know, build pathogens with the intent to deploy them to kill people. And so we have those, we actually don't need new laws for the vast majority of these scenarios. We actually already have the\n",
      "laws in the book, on the books. The third argument is the minute, and this is sort of the\n",
      "foundational one that gets really tough, but the minute\n",
      "you get into this thing, which you were kind of getting\n",
      "into, which is like, okay, but like, don't you need\n",
      "censorship sometimes, right? And don't you need restrictions sometimes? It's like, okay, what is the cost of that? And in particular in the\n",
      "world of open source, right? And so is open source AI\n",
      "going to be allowed or not? If open source AI is not allowed, then what is the regime that's\n",
      "going to be necessary legally and technically to prevent\n",
      "it from developing? Right? And here again is where you\n",
      "get into and people have proposed that these kinds of things. You get into I would say pretty extreme territory pretty fast. Do we have a monitor\n",
      "agent on every CPU and GPU that reports back to the government? What we're doing with our computers, are we seizing GPU clusters\n",
      "that get beyond a certain size? Like, and then by the way, how are we doing all that globally, right? And like if China's developing\n",
      "an LLM beyond the scale that we think is allowable,\n",
      "are we gonna invade? Right. And you have figures on the AI X risk side who are advocating any, you know, potentially up to nuclear\n",
      "strikes to prevent, you know, this kind of thing. And so here you get into this thing and again, you know, maybe\n",
      "you could maybe say this is, you know, you could even\n",
      "say this is what good, bad or indifferent or whatever. But like here's the comparison of nukes, the comparison of nukes is very dangerous because one is just nukes, were just, although we can\n",
      "come back to nuclear power. But the other thing was like with nukes, you could control plutonium, right? You could track plutonium and\n",
      "it was like hard to come by. AI is just math and code, right? And it's in like math\n",
      "textbooks and it's like, there are YouTube videos that\n",
      "teach you how to build it. And like there's open source,\n",
      "there's already open source. You know, there's a 40 billion parameter\n",
      "model running around already called Falcon Online that\n",
      "anybody can download. And so, okay, you walk down the logic path\n",
      "that says we need to have guardrails on this. And you find yourself in an authoritarian, totalitarian regime of thought\n",
      "control and machine control that would be so brutal\n",
      "that you would've destroyed the society that you're trying to protect. And so I just don't see\n",
      "how that actually works. - So yeah, you have to\n",
      "understand my brain's going full steam ahead here 'cause I agree with basically\n",
      "everything you're saying, but I'm trying to play\n",
      "devil's advocate here because okay, you're highlighted the fact that there is a slippery\n",
      "slope to human nature. The moment you censor something, you start to censor everything. That alignment starts out sounding nice, but then you start to align to the beliefs of some select group of people. And then it's just your beliefs the number of people\n",
      "you're aligning to smaller and smaller as that group\n",
      "becomes more and more powerful. Okay. But that just speaks to the people that censor are usually the assholes and the assholes get richer. I wonder if it's possible\n",
      "to do without that for AI. One way to ask this question is do you think the base models, the baseline foundation\n",
      "models should be open sourced? Like, where Marc Zuckerberg\n",
      "is saying they want to do. - So look, I mean I think\n",
      "it's totally appropriate the companies that are in the business of producing a product or service should be\n",
      "able to have a wide range of policies that they put, right? And I'll just, again, I want a heavily censored\n",
      "model for my eight year old. Like, I actually want that, like, like I would pay more money\n",
      "for the ones more heavily censored than the one that's not, right. And so, like there are certainly scenarios where companies will make that decision. Look, an interesting thing you brought up or is this really a speech issue? One of the things that the\n",
      "big tech companies are dealing with is that content generated\n",
      "from an LLM is not covered under section 230, which is the law that protects\n",
      "internet platform companies from being sued for the\n",
      "user generated content. And so it is actually-- - [Lex] Oh, wow. - Yes and so there, there's\n",
      "actually a question. I think there's still a question, which is can big American companies actually feel generative AI at all? Or is the liability actually\n",
      "gonna just ultimately convince them that they can't do it? Because the minute the\n",
      "thing says something bad, and it doesn't even\n",
      "need to be hate speech, it could just be like an\n",
      "(indistinct) it could hallucinate a product, you know, detail\n",
      "on a vacuum cleaner, you know, and all of a sudden the\n",
      "vacuum cleaner company sues for misrepresentation. And there's asymmetry there, right? 'Cause the LLMs gonna be\n",
      "producing billions of answers to questions and it only needs\n",
      "to get a few wrong to have. - [Lex] So, loss has to get\n",
      "updated really quick here. - Yeah. And nobody knows\n",
      "what to do with that, right? So, so anyway, like there are big, there are big questions around\n",
      "how companies operate at all. So we talk about those, but then there's this other\n",
      "question of like, okay, the open source. So what about open source? And my answer to your\n",
      "question is kind of like, obviously yes, the models have, there has to be full open\n",
      "source here because to live in a world in which that\n",
      "open source is not allowed is a world of draconian speech control, human control, machine control. I mean, you know, black helicopters with\n",
      "jackbooted thugs coming out, repelling down and seizing\n",
      "your GPU like territory. - [Lex] Well. - No, no, I'm a hundred percent serious. - That's you're saying slippery\n",
      "slope always leads there. - No, no, no, no. That's what's required to enforce it. Like how will you enforce a\n",
      "ban on open source and AI? - No. Well you could add friction to it, like harder to get the models. 'Cause people will always\n",
      "be able to get the models, but it'll be more in the shadows, right? - The leading open source model\n",
      "right now is from the UAE. Like the next time they\n",
      "do that, what do we do? - [Lex] Yeah. - Like. - Oh, I see you're like. - A 14 year old in Indonesia\n",
      "comes out with a breakthrough. You know, we talked about most great\n",
      "software comes from a small number of people. Some kid comes out with\n",
      "some big new breakthrough and quantization or something and has some huge breakthrough. And like, what are we gonna like, invade Indonesia and arrest him? - It seems like in terms of\n",
      "size of models and effectiveness of models, the big tech companies will\n",
      "probably lead the way for quite a few years and the question is of what policies they should use? The kid in Indonesia\n",
      "should not be regulated, but should Google, Meta,\n",
      "Microsoft, Open AI be regulated? - Well, so, but this goes, okay, so when does it become dangerous? Right. Is the danger that it's as powerful as the current leading commercial model? Or it is just at some\n",
      "other arbitrary threshold? And then by the way, like\n",
      "look, how do we know, like what we know today is that\n",
      "you need like a lot of money to like train these things. But there are advances being\n",
      "made every week on training efficiency and, you know,\n",
      "data, all kinds of synthetic, you know, look, I don't even like the synthetic data thing we're talking about. Maybe some kid figures out a way to auto-generate synthetic data. - [Lex] That's gonna change everything. - Yeah, exactly. And so like sitting here today, like, the breakthrough just happened, right? You made this point like the\n",
      "breakthrough just happened. So we don't know what the shape of this technology is gonna be. I mean the big shock\n",
      "here is that, you know, whatever number of billions\n",
      "of parameters basically represents at least a very big\n",
      "percentage of human thought. Like who would've imagined that? And then there's already work underway. There was just this paper that\n",
      "just came out that basically takes a gpt three scale model\n",
      "and compresses it down or run on a single 32 core CPU. Like who would've predicted that? - [Lex] Yeah. - You know, some of these models now you\n",
      "can run on raspberry pies like today they're very slow,\n",
      "but like, you know, maybe they'll be a, you know, perceived you have real perform, you know, like it's math and code. And here we're back in here, we're back in, dude, it's math and code. It's math and code, it's\n",
      "math, code and data. It's bits. - Marc has just like\n",
      "walked away at this point. You just screw it. I don't know what to do with this. You guys created this\n",
      "whole internet thing. Yeah, yeah. I mean, I'm a huge believer\n",
      "in open source here. - So my argument is we're gonna have, see here's my argument is a, my argument, my full argument is, is AI is gonna be like air,\n",
      "it's gonna be everywhere. Like this is just gonna be in text. It already is, it's gonna be in textbooks\n",
      "and kids are gonna grow up knowing how to do this. And\n",
      "it's just gonna be a thing. It's gonna be in the air```\n",
      "            \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSummarize the youtube video whose transcript is provided within backticks             ```and you can't like pull this back anymore. You can't pull back air. And so you just have to figure out how to live in this world, right? And then that's where I think\n",
      "like all this hand ringing about AI risk is basically\n",
      "a complete waste of time, 'cause the effort should go into okay, what is the defensive approach? And so if you're worried about you know, AI generated pathogens, the\n",
      "right thing to do is to have a permanent project warp speed, right? Funded lavishly. Let's do a Manhattan, let's\n",
      "talk about Manhattan project, let's do a Manhattan project\n",
      "for biological defense, right? And let's build ais and let's\n",
      "have like broad spectrum vaccines where like, we're\n",
      "insulated from every pathogen. - And well, the interesting\n",
      "thing is because it's software, a kid in his basement, teenager could build like a\n",
      "system that defends against like the worst, I mean, and to me\n",
      "defense is super exciting. It's like, if you believe\n",
      "in the good of human nature for that, most people wanna do good, to be the savior of\n",
      "humanity is really exciting. - Yes. - Not, okay, that's a dramatic statement. But to help people. - Yeah, of course. Help people. - Yeah. Okay. What about just the jump around, what about the risk of will AI\n",
      "lead to crippling inequality? You know, 'cause we're kind of saying\n",
      "everybody's life will become better. Is it possible that the\n",
      "rich get richer here? - Yeah, so this goes, this actually ironically\n",
      "goes back to Marxism. So 'cause this was the, so the\n",
      "core claim of Marxism, right? Basically was that the owner, the owners of capital would basically own the means of production. And then over time they\n",
      "would basically accumulate all the wealth the workers\n",
      "would be paying in, you know, and getting nothing in return 'cause they wouldn't be\n",
      "needed anymore, right? Marx was very worried\n",
      "about mech what he called mechanization or what later\n",
      "became known as automation. And that, you know, the workers would be immiserated\n",
      "and the the capitalists would end up with all. And so this was one of the\n",
      "core principles of Marxism. Of course it turned out to\n",
      "be wrong about every previous wave of technology. The reason it, it turned out to be wrong about every previous wave of technology is that the way that the\n",
      "self-interested owner of the machines makes the most money is by providing the production capability in the form of products and services to the most people, the most\n",
      "customers as possible, right? The the largest, and this is one of those funny\n",
      "things where every CEO knows this intuitively, and yet it's like hard to\n",
      "explain from the outside the way you make the most\n",
      "money in any business is by selling to the largest\n",
      "market you can possibly get to. The largest market you can\n",
      "possibly get to is everybody on the planet. And so every large company\n",
      "does is everything that it can to drive down prices, to\n",
      "be able to get volumes up, to be able to get to\n",
      "everybody on the planet. And that happened with\n",
      "everything from electricity, it happened with telephones,\n",
      "it happened with radio, it happened with automobiles,\n",
      "it happened with smartphones, it happened with PCs, it\n",
      "happened with the internet, it happened with mobile broadband. It's happened by the way, with Coca-Cola. It's happened with like every, you know, basically every industrially\n",
      "produced, you know, good or service people, you wanna drive it to the\n",
      "largest possible market. And then as proof of that,\n",
      "it's already happened, right? Which is the early\n",
      "adopters of like ChatGPT and Bing are not like, you\n",
      "know, Exxon and Boeing. They're, you know, your\n",
      "uncle and your nephew, right? It's just like free. It's either freely available\n",
      "online or it's available for 20 bucks a month or something. But the, you know, these things went this technology went mass market immediately. And so look, the owners of\n",
      "the means of production, the whoever does this now mentioned these trillion dollar questions. There are people who are gonna\n",
      "get really rich doing this, producing these things, but they're gonna get\n",
      "really rich by taking this technology to the\n",
      "broadest possible market. - So yes, they'll get rich, but they'll get rich having\n",
      "a huge positive impact on. - Yeah, making the technology\n",
      "available to everybody. Right. And again, smartphone, same thing. So there's this amazing kind\n",
      "of twist in business history, which is you cannot spend\n",
      "$10,000 on a smartphone, right? You can't spend a\n",
      "hundred thousand dollars, you can't spend a million, like I would buy the\n",
      "million dollars smartphone. Like I'm signed up for it. Like if it's like, suppose a million dollar\n",
      "smartphone was like much better than the thousand dollar smartphone. Like I'm there to buy\n",
      "it, it doesn't exist. Why doesn't it exist? Apple makes so much more\n",
      "money driving the price further down from a thousand dollars than they would trying to harvest, right? And so it's just this\n",
      "repeating pattern you see over and over again where and\n",
      "what's great about it is you, you do not need to rely on\n",
      "anybody's enlightened right? Generosity to do this. You just need to rely on\n",
      "capitalist self-interest. - What about AI taking our jobs? - Yeah. So very very similar thing here. There's sort of a, there's a\n",
      "core fallacy which again was very common in Marxism, which is what's called\n",
      "the lump of labor fallacy. And this is sort of the\n",
      "fallacy that there is only a fixed amount of work\n",
      "to be done in the world. And it's all being done today by people and then if machines do it, there's no other work\n",
      "to be done by people. And that's just a\n",
      "completely backwards view on how the economy develops and grows. Because what happens is not\n",
      "in fact that what happens is the introduction of technology\n",
      "into production process causes prices to fall. As prices fall, consumers\n",
      "have more spending power. As consumers have more spending power, they create new demand. That new demand then causes\n",
      "capital and labor to form into new enterprises to\n",
      "satisfy nuance and needs. And the result is more\n",
      "jobs at higher wages. - So nuance and needs, the\n",
      "worries that the creation of nuance and needs at\n",
      "a rapid rate will mean there's a lot of turnover in jobs. So people will lose jobs. Just the actual experience\n",
      "of losing a job and having to learn new things and\n",
      "new skills is painful for the individuals. - Well, two things. One is the new jobs are often much better. So this actually came up\n",
      "that there was this panic about a decade ago and all\n",
      "the truck drivers are gonna lose their jobs, right? And number one, that didn't happen 'cause\n",
      "we haven't figured out a way to actually finish that yet. But the other thing was\n",
      "like, look, truck driver, like I grew up in a town\n",
      "that was basically consisted of a truck stop, right? And I like knew a lot of truck drivers and like truck drivers\n",
      "live a decade shorter than everybody else. Like, it's actually like a very dangerous, like, they get, like literally they have like\n",
      "higher rates of skin cancer and on the left side of their, on the left side of their body from being in the sun all the time. The vibration of being\n",
      "in the truck is actually very damaging to your physiology. - And there's actually perhaps partially because of that reason there's a shortage of people who wanna be truck drivers. - Yeah. Like, it's not like\n",
      "the question always you wanna ask somebody like that\n",
      "is, do you want, you know, do you want your kid to be doing this job? And like most of them will tell you no. Like, I want my kid to\n",
      "be sitting in a cubicle somewhere like where they\n",
      "don't have this, like, where they don't die 10 years earlier. And so, the new jobs, number one, the new jobs are often better, but you don't get the new\n",
      "jobs until you go through the change. And then to your point,\n",
      "the training thing, you know, is always the\n",
      "issue is can people adapt? And again, here you need to imagine living in a world in which everybody has the AI\n",
      "assistant capability, right? To be able to pick up new\n",
      "skills much more quickly and be able to have some, you know, be able to have a machine to work with to augment their skills. - It's still gonna be painful, but that's the process of life. - It's painful for some people.\n",
      "I mean there's no, like, there's no question it's\n",
      "painful for some people and they're, you know, they're yes, it's not, again, I'm not a utopian on\n",
      "this and it's not like, it's positive for everybody in the moment, but it has been overwhelmingly\n",
      "positive for 300 years. I mean, look, the concern\n",
      "here, the concern, this concern has played out for literally centuries and you know, this is the sort of Luddite, you know, the story of the Luddites\n",
      "that you may remember, there was a panic in the two\n",
      "thousands around outsourcing was gonna take all the jobs. There was a panic in the 2010s\n",
      "that robots were gonna take all the jobs. In 2019 before COVID we had more jobs at higher wages both in the country and in the world than at any point in human history. And so the overwhelming evidence\n",
      "is that the net gain here is like, just like wildly positive. And most people like overwhelmingly\n",
      "come out the other side being huge beneficiaries of this. - So you write that the\n",
      "single greatest risk, this is the risk you're most convinced by the single greatest risk of AI is that China wins global AI dominance and we the United States\n",
      "and the West do not. Can you elaborate? - Yeah. So this is the\n",
      "other thing which is a lot of this sort of AI\n",
      "risk debates today sort of assume that we're the\n",
      "only game in town, right? And so we have the ability to kind of sit in the United States and\n",
      "criticize ourselves and do, you know, have our\n",
      "government like, you know, beat up on our companies\n",
      "and we'll figure out a way to restrict what our\n",
      "companies can do and you know, we're gonna, you know,\n",
      "we're gonna ban this and ban that, restrict this and do that. And then there's this like\n",
      "other like force out there that like doesn't\n",
      "believe we have any power over them whatsoever and they\n",
      "have no desire to sign up for whatever rules we\n",
      "decide to put in place and they're gonna do whatever\n",
      "it is they're gonna do. And we have no control over it at all. And it's China and specifically\n",
      "the Chinese Communist party and they have a completely\n",
      "publicized open, you know, plan for what they're gonna do with AI. And it is not what we have in mind. And not only do they have that as a vision and a plan for their society, but they also have it as a vision and plan for the rest of the world. - So their plan is what? Surveillance? - Authoritarian control. So authoritarian population\n",
      "control you know, good old-fashioned communist\n",
      "authoritarian control and surveillance and enforcement\n",
      "and social credit scores and all the rest of it. And you are gonna be monitored and metered within an inch of everything all the time. And it's gonna, you know, it's basically the end of human freedom and that's their goal. And you know, they justify it on the basis of that's what leads to peace. - You're worried that the regulating in the United States\n",
      "will haul progress enough to where the Chinese\n",
      "government would win that race. - So their plan, yeah. Yes, yes. And the reason for that\n",
      "is they, and again, they're very public on this. They have, their plan is to proliferate their approach around the world and they have this program called the Digital Silk Road, right. Which is building on their\n",
      "Silk Road investment program. And they've got, they've been laying\n",
      "networking infrastructure all over the world with their 5G, right. Work with their company Huawei. And so, they've been\n",
      "laying all this fabric, but financial and technological\n",
      "fabric all over the world. And their plan is to roll out their vision of\n",
      "AI on top of that and to have every other country be\n",
      "running their version. And then if you're a\n",
      "country prone to, you know, authoritarianism, you're\n",
      "gonna find this to be an incredible way to\n",
      "become more authoritarian. If you're a country, by the way, not prone to authoritarianism, you're gonna have the Chinese\n",
      "Communist Party running your infrastructure and having\n",
      "backdoor into it. Right. Which is also not good. - What's your sense of where\n",
      "they stand in terms of the race towards super intelligence as\n",
      "compared to the United States? - Yeah, so good news is they're behind, but bad news is they, you know, let's just say they get\n",
      "access to everything we do. So they're probably a year\n",
      "behind at each point in time, but they get, you know, downloads I think of\n",
      "basically all of our work on a regular basis through\n",
      "a variety of means. And they are, you know,\n",
      "at least we'll see, they're at least putting\n",
      "out reports of very, they just put out a report last week of a GPT 3.5 analog. They put out this report,\n",
      "forget what it's called, but they put out this\n",
      "report of this and they did and they, you know, the\n",
      "way when open AI you know, puts out, one of the ways they test, you know, GPT they run it through standardized\n",
      "exams like the SAT. Right. Just how you can kind of\n",
      "gauge how smart it is. And so the Chinese report, they ran their LLM through\n",
      "the Chinese equivalent of the SAT and it includes\n",
      "a section on Marxism and a section on, I say tongue of thought. And it turns out their AI does very well on both of those topics. - That's right. - So like. - Oh, this alignment thing. - Communist AI, right? Like literal communist AI. Right? And so their vision is\n",
      "like, that's the, you know, so you know, you can just\n",
      "imagine like you're a school, you know, you're a kid 10\n",
      "years from now in Argentina or in Germany or in who\n",
      "knows where, Indonesia. And you ask the AI, I'd explain to you like\n",
      "how the economy works and it gives you the most cheery, upbeat explanation of\n",
      "Chinese style communism you've ever heard. Right. So like the stakes here\n",
      "are like really big. - Well, as we've been talking about, my hope is not just\n",
      "with the United States, but with just the kid in his basement. The open source LLM. 'Cause I don't know if I trust large centralized institutions\n",
      "with super powerful AI no matter what their\n",
      "ideology as a power corrupts. You've been investing in\n",
      "tech companies for about, let's say 20 years. And about 15 of which was\n",
      "with Andreessen Horowitz. What interesting trends\n",
      "in tech have you seen over that time? Let's just talk about companies\n",
      "and just the evolution of the tech industry. - I mean the big shift over 20\n",
      "years has been that tech used to be a tools industry for\n",
      "basically from like 1940 through to about 2010, almost all the big successful\n",
      "companies were pick and shovels companies. So PC, database, smartphone, you know, some tool that somebody\n",
      "else would pick up and use. Since 2010, most of the big\n",
      "wins have been in applications. So a company that starts you know, starts in an existing\n",
      "industry and goes directly to the customer in that industry. And you know, the earliest examples there\n",
      "were like Uber and Lyft and Airbnb. And then that model is\n",
      "kind of elaborating out. The AI thing is actually a\n",
      "reversion on that for now 'cause like most of the AI\n",
      "business right now is actually in cloud provision of AI APIs\n",
      "for other people to build on. - But the big thing\n",
      "will probably be in app. - Yeah. I think most of the\n",
      "money I think probably will be in whatever your AI financial advisor or your AI doctor or your\n",
      "AI lawyer or, you know, take your pick of whatever the domain is. And there, and what's\n",
      "interesting is, you know, the valley kind of does everything. The entrepreneurs kind of\n",
      "elaborate every possible idea. And so there will be a set of\n",
      "companies that like make AI something that can be purchased\n",
      "and used by large law firms and then there will be other\n",
      "companies that just go direct to market as an AI lawyer. - What advice could you\n",
      "give for a startup founder? Just haven't seen so many\n",
      "successful companies, so many companies that fail also, what advice could you\n",
      "give to a startup founder, someone who wants to build the\n",
      "next super successful startup in the tech space? The Googles,\n",
      "the Apples, the Twitters. - Yeah. So the great thing about the really great founders is they don't take any advice. So, if you find yourself\n",
      "listening to advice, maybe you shouldn't do it. - But that's actually,\n",
      "just to elaborate on that, if you could also speak\n",
      "to great founders too. Like what makes a great founder? - So what makes a great\n",
      "founder is super smart, coupled with super energetic,\n",
      "coupled with super courageous. I think it's some of those three and-- - Intelligence, passion and courage. - The first two are traits\n",
      "and the third one is a choice. I think courage is a choice. Well 'cause courage is a question\n",
      "of pain tolerance, right? So how many times are you\n",
      "willing to get punched in the face before you quit? And here's maybe the biggest\n",
      "thing people don't understand about what it's like to be\n",
      "a startup founder is it gets very romanticized, right? And even when it, even when they fail, it still gets romanticized about like what a great adventure it was. But like the reality of it is\n",
      "most of what happens is people telling you no and then\n",
      "they usually follow that with you're stupid, right. No, I will not come to work for you. I will not leave my cushy job at Google to come work for you. No, I'm not gonna buy your\n",
      "product, you know, no, I'm not gonna run a\n",
      "story about your company. No, I'm not this, that, the other thing. And so a huge amount of what\n",
      "people have to do is just get used to just getting punched and the reason people\n",
      "don't understand this is because when you're a founder, you cannot let on that this is happening 'cause it will cause people to think that you're weak and\n",
      "they'll lose faith in you. So you have to pretend that\n",
      "you're having a great time when you're dying inside, right? You're just in misery. - But why did they do it? - Why did they do? Yeah, that's the thing. It's like it is a level, this is actually one of\n",
      "the conclusions I think is that I think it's actually\n",
      "for most of these people on a risk adjusted basis, it's\n",
      "probably an irrational act. They could probably be\n",
      "more financially successful on average if they just\n",
      "got like a real job in at a big company. But there's, you know, some people just have an\n",
      "irrational need to do something new and build something for\n",
      "themselves and, you know, some people just can't\n",
      "tolerate having bosses. Oh, here's the fun thing is how do you reference\n",
      "check founders, right? So you call the, you know, normal way you reference check, you're hiring somebody\n",
      "is you call the bosses, they're their, and you know, and you find out if\n",
      "they were good employees and now you're trying to\n",
      "reference check Steve Jobs, right? And it's like, oh God, he was terrible. You know, he was a terrible employee. He never did what we told him to do. - So what's a good reference? Do you want the previous\n",
      "boss to actually say they never did what you told him to do? That might be a good thing. - Well, ideally what\n",
      "you want is I will go, I would like to go to\n",
      "work for that person. He worked for me here and\n",
      "now I'd like to work for him. No, unfortunately, most\n",
      "people can't, their egos can't handle that. So they won't say that. But that's the ideal. - What advice would\n",
      "you give to those folks in the space of intelligence,\n",
      "passion and courage? - So I think the other big thing\n",
      "is you see people sometimes who say, I wanna start a company and then they kind of\n",
      "work through the process of coming up with an idea. And generally those don't\n",
      "work as well as the case where somebody has the idea first and then they kind of realize that there's an opportunity\n",
      "to build a company and then they just turn\n",
      "out to be the right kind of person to do that. - When you say idea, do you\n",
      "mean long-term big vision or do you mean specifics of like product? - Specific I would say specific, like specifically what specifics. Like what is the, because for the first five years you don't get to have vision, you just gotta build something people want and you gotta figure out a\n",
      "way to sell it to them. Right. It's very practical or you\n",
      "never get to big vision. - So the first product, you have an idea of a set of```\n",
      "            \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSummarize the youtube video whose transcript is provided within backticks             ```products of the first product that can actually make some money. - Yeah. Like it's gotta work. The first product's gotta\n",
      "work by which I mean like, it has to technically work, but then it has to actually\n",
      "fit into the category and the customer's mind if\n",
      "something that they want and then by the way, the other part is they have\n",
      "to be willing to pay for it. Like somebody's gotta pay the bills. And so you've gotta\n",
      "figure out how to price it and whether you can\n",
      "actually extract the money. So usually it is much more predictable. Success is never predictable, but it's more predictable if\n",
      "you start with a great idea and then back into starting the company. So this is what we did,\n",
      "you know, we had most, before we had escape, the Google guys had the\n",
      "Google search engine working at Stanford. Right. You know, yeah. Actually there's tons of\n",
      "examples where they, you know, Pierre Omaira had eBay working before he left his previous job. - So I really love that\n",
      "idea of just having a thing, a prototype that actually\n",
      "works before you even begin to remotely scale. Yeah. - By the way, it's also far\n",
      "easier to raise money, right? Like the ideal pitch that we receive is, here's the thing that works, would you like to invest\n",
      "in our company or not? Like, that's so much easier than here's 30 slides with a dream, right? And then we have this\n",
      "concept called the DMAs, which our biology of came\n",
      "up with when he was with us. So then there's this thing,\n",
      "this goes to mythology, which is, you know, there's\n",
      "a mythology that kind of, you know, these ideas, you know, kind of arrive like magic or people kind of stumble into them. It's like eBay with the pest\n",
      "dispensers or something. The reality usually with\n",
      "the big successes is that the founder has been\n",
      "chewing on the problem for 5 or 10 years before they start the company\n",
      "and they often worked on it in school or they even experimented on it when they were a kid and they've been kind of training up over that period of time to\n",
      "be able to do the thing. So they're like a true domain expert. And it sort of sounds like mom, I'm an apple pie, which is yeah, you wanna be a domain\n",
      "expert in what you're doing, but you would, you know, the\n",
      "mythology is so strong of like, oh, I just like had this idea in the shower right now I'm doing it. Like it's generally not that. - No, because it's, well, maybe in the shower\n",
      "we had the exact product implementation details, but yeah, usually you're gonna be for\n",
      "like years if not decades thinking about like\n",
      "everything around that. - Well we call it the DMAs\n",
      "because the DMAs basically is like, there's all these permutations, like for any idea, there's like all these\n",
      "different permutations, who should the customer be? What shape forms should the product have and how should we take it to\n",
      "market and all these things. And so the really smart\n",
      "founders have thought through all these scenarios\n",
      "by the time they go out to raise money and they\n",
      "have like detailed answers on every one of those fronts because they put so much thought into it. The sort of more haphazard\n",
      "founders haven't thought about any of that. And it's the detailed ones\n",
      "who tend to do much better. - So how do you know when to take a leap if you have a cushy job or happy life? - I mean the best reason is just 'cause you can't tolerate\n",
      "not doing it right? Like this is the kind of\n",
      "thing where if you have to be advised into doing it, you\n",
      "probably shouldn't do it. And so it's probably the opposite, which is you just have such\n",
      "a burning sense of this has to be done, I have to do\n",
      "this, I have no choice. - What if it's gonna\n",
      "lead to a lot of pain? - It's gonna lead to a lot\n",
      "of pain. I think that's. - What if it means losing\n",
      "sort of social relationships and damaging your\n",
      "relationship with loved ones and all that kind of stuff. - Yeah, look, so like, it's gonna put you in a\n",
      "social tunnel for sure, right? So you're gonna, like, you know, there's this game you can play on Twitter, which is you can do any whiff\n",
      "of the idea that there's basically any such thing\n",
      "as work life balance and that people should actually work hard and everybody gets mad. But like, the truth is like all the\n",
      "successful founders are working 80 hour weeks and they're\n",
      "working, you know, they form very, very strong social bonds with\n",
      "the people they work with. They tend to lose a lot of\n",
      "friends on the outside or put those friendships on ice. Like that's just the nature of the thing, you know, for most people\n",
      "that's worth the trade off. You know, the advantage, you know, maybe younger founders have\n",
      "is maybe they have less, you know, maybe they're\n",
      "not, you know, for example, if they're not married yet\n",
      "or don't have kids yet, that's an easier thing to bite off. - Can you be an older founder? - Yeah. You definitely can. Yeah. Yeah. Many of the most\n",
      "successful founders are second, third, fourth time founders. They're in their thirties,\n",
      "forties, fifties. The good news with being an\n",
      "older founder is, you know, more and you, you know, a\n",
      "lot more about what to do, which is very helpful.\n",
      "The problem is, okay, now you've got like a spouse\n",
      "and a family and kids and like, you've gotta go to the\n",
      "baseball game and like, you can't go to the base,\n",
      "you know, and so it's. - [Lex] Life is full of difficult choices. - Yes. - Marc Andreessen, you've written a blog post\n",
      "on what you've been up to. You wrote this in October, 2022, \"Mostly I try to learn a lot. For example, the political events of 2014\n",
      "to 2016 make clear to me that I didn't understand\n",
      "politics at all referencing maybe some of this book here. So I deliberately withdrew\n",
      "from political engagement and fundraising and instead\n",
      "read my way back into history and as far to the political left and political right as I could.\" So just high level question, what's your approach to learning? - Yeah, so it's basically, I would say, I'm an AutoID direct,\n",
      "so it's sort of goes, it's going down the rabbit holes. So it's a combination. I kind of allude to it\n",
      "in that, in that quote, it's a combination of breadth and depth. And so I tend to, yeah, I tend to, I go broad by the nature\n",
      "of what I do, I go broad, but then I tend to go deep\n",
      "in a rabbit hole for a while, read everything I can\n",
      "and then come out of it. And I might not revisit\n",
      "that rabbit hole for, you know, another decade. - And in that blog post that I recommend people go check out, you actually list a bunch\n",
      "of different books that you recommend on different\n",
      "topics on the American left, on the American right. It's just a lot of really good stuff. The best explanation for\n",
      "the current structure of our society and politics. You give to recommendations, four books on the Spanish Civil War, six books on deep history\n",
      "of the American right comprehensive biographies. These of Adolf Hitler, one of which I read can\n",
      "recommend six books on the deep history of the American\n",
      "left. So the American right, American left looking at the\n",
      "history to give you the context biography of later Lennon, two of them on the French\n",
      "Revolution. I actually, I have never read a\n",
      "biography on Lennon maybe that would be useful. Everything's been so Marc's focused. - The Sebastian biography\n",
      "of Lennon is extraordinary. - [Lex] Victor Sebestyen. Okay. - Blow your mind. Yeah. - [Lex] So it's still useful to read. - It's incredible. Yeah, it's incredible. I actually think it's the single best book on the Soviet Union. - So that the perspective of Lennon, it might be the best way to\n",
      "look at the Soviet Union versus Stalin versus Marx\n",
      "versus, very interesting. So two books on fascism and\n",
      "anti-fascism by the same author, Paul Gottfried, brilliant book on the\n",
      "nature of mass movements and collective psychology, the definitive work on\n",
      "intellectual life under totalitarianism, the Captive Mind, the definitive worked\n",
      "on the practical life under totalitarianism. There's a bunch. There's a bunch. And the single best book, first of all, the list here is just incredible. But you say the single best\n",
      "book I have found on who we are and how we got here is the Ancient City by Numa Dennis Fustel De Coulanges. I like it. What did you learn about who\n",
      "we are as a human civilization from that book? - Yeah, so this is a fascinating book. This one's free, it's a free, by the way, it's a book in the 1860s. You can download it or\n",
      "you can buy printouts up prints of it. But it was this guy who was\n",
      "a professor at the savant in the 1860s and he was\n",
      "apparently a savant on antiquity on Greek and Roman antiquity and the reason I say that is\n",
      "because his sources are 100% original Greek and Roman sources. So he wrote a basically\n",
      "history of western civilization from, on the order of 4,000 years ago to basically the present\n",
      "times entirely working on fresh original Greek and Roman sources. And what he was specifically\n",
      "trying to do was he was trying to reconstruct from the stories of the Greeks and the Romans, he was trying to reconstruct\n",
      "what life in the west was like before the Greeks and the Romans, which was in the civilization known as the Indo Europeans. And the short answer is,\n",
      "and this is sort of 4,000, you know, 2000 BC to, you know, sort of 500 BC kind of\n",
      "that 1500 year stretch for civilization developed. And his conclusion was basically cults. They were basically cults\n",
      "and civilization was, or organized into cults. And the intensity of the cults was like a million fold beyond anything that we would recognize today. Like it was a level of\n",
      "all encompassing belief and an action around religion that was at a level of extremeness that we wouldn't even recognize it and so specifically he\n",
      "tells the story of basically there were three levels of cults. There was the family cult, the tribal cult, and then the city cult as society scaled up. And then each cult was a\n",
      "joint cult of family gods, which were ancestor gods. And then nature gods and then\n",
      "your bonding into a family, a tribe or a city was\n",
      "based on your adherence to that religion. People who were not of your\n",
      "family, tribe, city, worship, different gods, which gave you not just the\n",
      "right with or responsibility to kill them on site. - [Lex] So they were\n",
      "serious about their cults. - Hardcore, by the way,\n",
      "shocking development. I did not realize this zero\n",
      "concept of individual rights. Like even even up through the Greeks, and even in the Romans, they didn't have, have the concept of individual rights. Like the idea that as an\n",
      "individual you have like some rights just like, nope. Right? And you look back\n",
      "and you're just like, wow, that's just like cr\n",
      "like fascist in a degree that we wouldn't recognize today. But it's like, well, they were living under\n",
      "extreme pressure for survival. And you, and you know, the theory goes, you could not have people\n",
      "running around making claims, individual rights when\n",
      "you're just trying to get like your tribe through the winter, right? Like you need like hardcore\n",
      "command and control. And actually what if through\n",
      "modern political lens, those cults were basically\n",
      "both fascist and communist. They were fascist in\n",
      "terms of social control, and then they were communist\n",
      "in terms of economics. - But you think that's\n",
      "fundamentally that like pull towards cults is within us. - Well, so my conclusion from this book, so the way we naturally\n",
      "think about the world we live in today is like, we basically have such an\n",
      "improved version of everything that came before us, right? Like, we have basically, we've figured out all these\n",
      "things around morality and ethics and democracy\n",
      "and all these things. And like, they were basically\n",
      "stupid and retrograde and we're like smart and sophisticated. And we've improved all this after reading that book, I now believe in many ways\n",
      "the opposite, which is no, actually we are still running\n",
      "in that original model. We're just running in an\n",
      "incredibly diluted version of it. So we're still running,\n",
      "basically in cults. It's just our cults are at like\n",
      "a thousandth or a millionth, the level of intensity, right? And so our, so just as to\n",
      "take religions, you know, the modern experience of\n",
      "a Christian in our time, even somebody who considers\n",
      "him a devout Christian, is just a shadow of the level\n",
      "of intensity of somebody who belonged to a religion\n",
      "back in that period. And then by the way, we have cons. It goes back to our AI discussion. We then sort of endlessly\n",
      "create new cults. Like we're trying to fill the void, right? And the void is a void of bonding. - [Lex] Okay. - Living in their era. Like\n",
      "everybody living today, transporting that era\n",
      "would view it as just like, completely intolerable in terms of like the loss of freedom and the level of basically of fascist control. However, every single person in that era, and he really stresses this. They knew exactly where they stood. They knew exactly where they belonged. They knew exactly what their purpose was. They knew exactly what they\n",
      "needed to do every day. They knew exactly why they were doing it. They had total certainty about\n",
      "their place in the universe. - So the question of meaning, the question of purpose\n",
      "was very distinctly, clearly defined for them. - Absolutely overwhelmingly\n",
      "undisputably undeniably. - As we turn the volume\n",
      "down on the cultism-- - [Marc] Yes. - We start to, the search for meaning starts\n",
      "getting harder and harder. - Yes. 'cause we don't have that. We are ungrounded. We are uncentered and\n",
      "we all feel it. Right? And that's why we reach for, you know, it's why we still reach for religion. It's why we reach for, you know, we people start\n",
      "to take on, you know, let's say, you know, a faith in science maybe beyond\n",
      "where they should put it. You know and by the way,\n",
      "like, sports teams are like a, you know, they're like a tiny\n",
      "little version of a cult. And you know, apple keynotes are a tiny\n",
      "little version of a cult. Right. And, you know, political, you know. And there's cult, you know, there's full-blown cults on both sides of the political spectrum\n",
      "right now. Right. You know, operating in plain stuff. - But still not full blown\n",
      "compared as to what it was. - Compared to what it used to. I mean, we would today consider\n",
      "full blown, but like, yes, they're at like, I don't know, a hundred thousandth or\n",
      "something of the intensity of what people had back then. So, we live in a world today\n",
      "that in many ways is more advanced and moral and so forth. And it's certainly a lot nicer,\n",
      "much nicer world to live in. But we live in a world\n",
      "that's like very washed out. It's like everything has\n",
      "become very colorless and gray as compared to how people\n",
      "used to experience things. Which is I think why we're\n",
      "so prone to reach for drama. 'Cause there's something in us that's deeply evolved\n",
      "where we want that back. - And I wonder where it's all\n",
      "headed as we turn the volume down more and more. What advice would you\n",
      "give to young folks today in high school and college? How to be successful in their career? How to be successful in their life? - Yeah. So the tools that\n",
      "are available today, I mean, are just like, I\n",
      "sometimes, you know, bore, I sometimes bore, you know, kids by describing like what\n",
      "it was like to go look up a book, you know, to try to like discover\n",
      "a fact in, you know, in the old days, the 1970s, 1980s, to go to the library and the card catalog and the whole thing. You go through all that work\n",
      "and then the book is checked out and you have to wait two weeks and like to be in a world, not only where you can get\n",
      "the answer to any question, but also the world now, you know, the AI world where you've\n",
      "got like the assistant that will help you do\n",
      "anything, help you teach, learn anything, like your ability both to learn and also to produce\n",
      "is just like, I don't know, a million fold beyond what it used to be. I have a blog post I've\n",
      "been wanting to write, which I call where are the\n",
      "hyper-productive people? Like-- - [Lex] That's a good question, right? - Like with these tools, like there should be authors\n",
      "that are writing like hundreds or thousands of like, outstanding books. - Well, with the authors there's\n",
      "a consumption question too, but yeah. Well, maybe not, maybe not. You're right. But, so the tools are much more powerful. Getting much more powerful. - Artists, musicians. Right. Why aren't musicians producing\n",
      "a thousand times the number of songs, right? Like what, like the tools are spectacular. - So, what's the explanation? And by way of advice, like, is motivation starting to\n",
      "be turned down a little bit? Or what? - I think it might be distraction. - [Lex] Distraction. - It's so easy to just sit and consume that I think people get distracted from production.\n",
      "But if you wanted to, you know, as a young person, if you\n",
      "wanted to really stand out, you could get on a, like a hyper productivity curve very early on. There's a great, you know, this story, there's a great story in\n",
      "Roman history of plenty of the elder who was\n",
      "this legendary statesman, died in the Vesuvius eruption\n",
      "trying to rescue his friends. But he was famous both for being basically being a polymath,\n",
      "but also being an author. And he wrote apparently\n",
      "like hundreds of books, most of us had been lost. But he like wrote all these\n",
      "encyclopedias and he literally like would be reading and\n",
      "writing all day long no matter what else was going on. And so he would like travel\n",
      "with like four slaves. And two of them were\n",
      "responsible for reading to him, and two of them were responsible\n",
      "for taking dictation. And so like, he'd be going\n",
      "cross country and like, literally he would be writing\n",
      "books like all the time. And apparently they were spectacular. There's only a few that have survived, but apparently they were amazing. - There's a lot of value to being somebody who finds focus in this life. - Yeah. Like and there are\n",
      "examples, like there are, you know, there's this guy,\n",
      "judge, what's his name? Posner, who wrote like 40 books and was also a great federal judge. You know, there's our friend Balaji, I think is like this, he's\n",
      "one of these, you know, where his output is just prodigious. And so it's like, yeah, I mean,\n",
      "with these tools, why not? And I kind of think we're at this interesting\n",
      "kind of freeze frame moment where like this, these tools are now in everybody's hands and everybody's just kind\n",
      "of staring at them trying to figure out what to do. The new tools. - We have discovered fire. - [Marc] Yeah. - And trying to figure\n",
      "out how to use it to cook. - [Marc] Yeah. Right. - You told Tim Ferriss that\n",
      "the perfect day is caffeine for 10 hours and alcohol for four hours. You didn't think I'd be\n",
      "mentioning this, did you? It balances everything\n",
      "out perfectly as you said. So, perfect. So let me ask, what's the secret to balance\n",
      "and maybe to happiness in life? - I don't believe in balance, so I'm the wrong person to ask that. - Can you elaborate why you\n",
      "don't believe in balance? - I mean, I maybe it's just,\n",
      "and I look, I think people, I think people are wired differently. So, I think it's hard to\n",
      "generalize this kind of thing, but I am much happier and more satisfied when I'm fully committed to something. So I'm very much in favor\n",
      "of all in of imbalance. - Imbalance. And that applies to work, to life, to everything. - Yeah. No, no. I happen to have whatever twist\n",
      "of personality traits lead that in non-destructive\n",
      "dimensions in including the fact that I've actually, I now no\n",
      "longer do the ten-four plan. I stopped drinking. I do the caffeine, but not the alcohol. So there's something in my personality where I whatever mal-adaption\n",
      "I have is inclining me towards productive things,\n",
      "not unproductive things. - So you're one of the\n",
      "wealthiest people in the world. What's the relationship\n",
      "between wealth and happiness? Money and happiness. - So I think happiness, I don't think happiness is the thing. - To strive for. - I think satisfaction is the thing. - That just sounds like\n",
      "happiness, but turned down a bit. - No deeper. So happiness is, you know, a walk in the woods at sunset,\n",
      "an ice cream cone, a kiss, the first ice cream cone is great. The thousandth ice\n",
      "cream cone, not so much. At some point the walks\n",
      "in the woods get boring. - What's the distinction between```\n",
      "            \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSummarize the youtube video whose transcript is provided within backticks             ```happiness and satisfaction? - I think satisfaction is a deeper thing, which is like having found a purpose and fulfilling it, being useful. - So just something that\n",
      "permeates all your days, just this general\n",
      "contentment of being useful. - That I'm fully satisfying my faculties, that I'm fully delivering, right? On the gifts that I've been\n",
      "given, that I'm, you know, net making the world better, that I'm contributing to\n",
      "the people around me, right. And that I can look back\n",
      "and say, wow, that was hard, but it was worth it. Think generally, it seems to lead people in\n",
      "a better state than pursuit of pleasure, pursuit of\n",
      "quote unquote happiness. - Does money have\n",
      "anything to do with that? - I think the founders and\n",
      "the founding fathers in the US threw this off kilter when\n",
      "they used the phrase pursuit of happiness. I think they should have said. - [Lex] Pursuit of satisfaction. - They said, pursuit of satisfaction. We might live in a better world today. - Well, they, you know, they could have elaborated on a lot of things right in the box. - [Marc] They could have\n",
      "tweaked the second amendment. - I think they were\n",
      "smarter than they realized. They said, you know we're gonna make it ambiguous and let these humans figure out the rest, these tribal cult-like\n",
      "humans figure out the rest. But money empowers that. - So I think, and I think there, I mean, look, I think Elon is, I don't think I'm even a great example, but I think Elon would be\n",
      "the great example of this, which is like, you know, look,\n",
      "he's a guy who from every, every day of his life, from the day he started\n",
      "making money at all, he just plows into the next thing. And so I think, I think money is definitely\n",
      "an enabler for satisfaction. Way money applied to\n",
      "happiness leads people down very dark paths. Very destructive avenues. Money applied to satisfaction,\n",
      "I think could be, is a real tool. I always, by the way,\n",
      "I was like, you know, Elon is the case study for behavior. But the other thing that I\n",
      "always really made me think is Larry Page was asked one time what his approach to philanthropy was. And he said, oh, I'm just, my philanthropic plan is just give all the money to Elon. (both laugh) - Well, let me actually\n",
      "ask you about Elon. You've interacted with quite a lot of successful engineers\n",
      "and business people. What do you think is special about Elon? We talked about Steve Jobs. What do you think is special\n",
      "about him as a leader? As an innovator? - Yeah. So the core of it is he's back to the future. So he is doing the most\n",
      "leading-edge things in the world, but with a really deeply\n",
      "old-school approach. And so to find comparisons to Elon, you need to go to like\n",
      "Henry Ford and Thomas Watson and Howard Hughes and\n",
      "Andrew Carnegie, right. Leland Stanford, John Rockefeller, right. You need to go to what were called the\n",
      "bourgeois capitalists, like the hardcore business owner operators who basically built, you know, basically built industrialized\n",
      "society, Vanderbilt. And it's a level of hands-on commitment and depth in the business, coupled with an absolute priority\n",
      "towards truth and towards, how to put it, science and technology\n",
      "town to first principles that is just like absolute, is just like unbelievably absolute. He really is ideal that he's\n",
      "only ever talking to engineers. Like he does not tolerate. He has less tolerance than\n",
      "anybody I've ever met. He wants ground truth\n",
      "on every single topic. And he runs his businesses\n",
      "directly day-to-day, devoted to getting to ground\n",
      "truth in every single topic. - So you think it was a good decision for him to buy Twitter? - I have developed a view in life to not second guess Elon Musk, I know this is gonna sound\n",
      "great, crazy and unfounded, but. - Well, I mean, he's got\n",
      "a quite a track record. - I mean, look, the car was\n",
      "a crazy, I mean, the car was, I mean, look. - He's done a lot of\n",
      "things that seem crazy. - Starting a new car company in the United States of America. The last time somebody\n",
      "really tried to do that was the 1950s and it was\n",
      "called Tucker Automotive. And it was such a disaster. They made a movie about\n",
      "what a disaster it was, and then rockets like, who does that? Like, there's obviously no way to start a new rocket company. Like those days are over. And then to do those at the same time. So after he pulled those\n",
      "two off, like, okay, fine. Like, this is one of my areas of like, whatever opinions I had about\n",
      "that, that is just like, okay, clearly are not relevant. Like this is you just, you at some point you just\n",
      "like bet on the person. - And in general, I wish more people would lean\n",
      "on celebrating and supporting versus deriding and destroying. - Oh yeah. I mean, look,\n",
      "he drives resentment. Like it's a resentment. Like he is a magnet for resentment. Like his critics are the\n",
      "most miserable, like, resentful people in the world. Like it's almost a perfect match\n",
      "of like the most idealized, you know, technologist, you know, of the century coupled with like, just his critics are\n",
      "just bitter as can be. And I mean, it's sort of\n",
      "very darkly comic to watch. - Well, he fuels the fire of that by being on Twitter at times. And which is fascinating\n",
      "to watch the drama of human civilization, given our cult roots just fully on fire. - [Marc] He's running a cult. - You could say that. - [Marc] Very successfully. - So now that our cults\n",
      "have gone and we searched for meaning, what do\n",
      "you think is the meaning of this whole thing? What's the meaning of\n",
      "life Marc Andreessen? - I don't know the answer\n",
      "to that. I think the meaning of the closest I get to it is what I said about satisfaction. So it's basically like, okay, we were given what we have, like we should basically do our best. - What's the role of love in that mix? - I mean, like, what's the point of life if you're without love, like, yeah. - So love is a big part\n",
      "of that satisfaction. - Yeah. And look like\n",
      "taking care of people is like a wonderful thing. Like it, you know, mentality, you know, there are pathological forms\n",
      "of taking care of people, but there's also a very\n",
      "fundamental, you know, kind of aspect of taking care of people. Like, for example, I happen to be somebody who\n",
      "believes that capitalism and taking care of people are actually, they're actually the same thing. Somebody once said, capitalism is how you take\n",
      "care of people you don't know. Right, right. And so like, yeah, I think it's like deeply\n",
      "woven into the whole thing, you know, there's a long conversation to be had about that, but yeah. - Yeah. Creating products that are used by millions of people and bring them joy in smaller, big ways. And then capitalism kind of\n",
      "enables that, encourages that. - David Friedman says, there's only three ways to\n",
      "get somebody to do something for somebody else. Love, money and force. And love and money are better. - [Lex] Yeah. Of course. That's a good ordering. I think. - We should bet on those. - Try love first. If that doesn't work, then money. - [Marc] Yes. - And then force. Well, don't even try that one. Marc, you're an incredible person. I've been a huge fan. I'm glad to finally got a chance to talk. I'm a fan of everything\n",
      "you do, everything you do, including on Twitter. It's a huge honor to meet\n",
      "you, to talk with you. Thanks again for doing this. - Awesome. Thank you, Lex. - Thanks for listening\n",
      "to this conversation with Marc Andreessen. To support this podcast, please check out our\n",
      "sponsors in the description. And now let me leave you with some words from Marc Andreessen himself. \"The world is a very malleable place. If you know what you\n",
      "want and you go for it, with maximum energy and drive and passion, the world will often\n",
      "reconfigure itself around you much more quickly and easily\n",
      "than you would think.\" Thank you for listening and\n",
      "hope to see you next time.```\n",
      "            \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mCombine all the youtube video transcripts  provided within backticks             ```Marc Andreessen talks about the future of the internet, search, and AI. He believes that LLMs will eventually replace search engines and that the majority of content on the internet will be human conversations with AIs. He also discusses the trillion dollar question of whether or not synthetic training data is useful for LLMs.\n",
      "\n",
      "Summary: Marc Andreessen and Lex Fridman discuss the role of large language models (LLMs) in society and the future of media. They discuss the importance of human values and how to ensure that LLMs are used for good. They also talk about the potential for LLMs to change the way we understand reality and how they could be used to create a more personalized and engaging experience for users.\n",
      "\n",
      "\n",
      "\n",
      "The video is about the history of the internet. Marc Andreessen, the co-founder of Mosaic, talks about the early days of the internet and how it has evolved over time. He discusses the challenges and opportunities that the internet has created, and how it has changed the way we live and work.\n",
      "\n",
      "The transcript is about Marc Andreessen's experience founding Netscape and the early days of the web. He talks about the engineering challenges of creating a web browser, the design decisions that were made, and the acquisition of Netscape by AOL.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The video is about the risks of artificial intelligence and how to mitigate them. The presenter talks about the importance of open source and the need to avoid censorship. He also discusses the possibility of AI being used for good or for evil.\n",
      "\n",
      "Summary: The AI risk debate is a waste of time. The best way to deal with AI risk is to build defenses against it. China is the biggest risk to global AI dominance because they have a plan for how to use AI to control their population. The US should focus on developing open source AI technology so that it is not controlled by a single entity. The tech industry has shifted from being a tools industry to an applications industry. The biggest winners in the AI space will be companies that build AI applications that directly serve customers. The best advice for a startup founder is to be super smart, super energetic, and super courageous.\n",
      "\n",
      "\n",
      "\n",
      "In this podcast, Marc Andreessen, the co-founder of Netscape and VC firm Andreessen Horowitz, discusses the meaning of life, happiness, satisfaction, and success with Lex Fridman.\n",
      "Andreessen believes that satisfaction is a deeper thing than happiness, and that it comes from fulfilling your purpose and being useful to others. He says that money can be an enabler for satisfaction, but that it is important to use it for good. He also believes that love is a big part of satisfaction, and that capitalism is a way to take care of people you don't know.```\n",
      "            Provide a concise summary between 8 to 10 sentences.\n",
      "            \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def youtube_video_url_is_valid(url: str) -> bool:\n",
    "    pattern = r'^https:\\/\\/www\\.youtube\\.com\\/watch\\?v=([a-zA-Z0-9_-]+)(\\&ab_channel=[\\w\\d]+)?$'\n",
    "    match = re.match(pattern, url)\n",
    "    return match is not None\n",
    "\n",
    "\n",
    "# def find_insights(api_key: str, url: str) -> str:\n",
    "def find_insights(url: str) -> str:\n",
    "    try:\n",
    "        loader = YoutubeLoader.from_youtube_url(url)\n",
    "        transcript = loader.load()\n",
    "    except Exception as e:\n",
    "        return f\"Error while loading YouTube video and transcript: {e}\"\n",
    "    try:\n",
    "        # llm = OpenAI(temperature=0.6, openai_api_key=api_key)\n",
    "        llm = VertexAI(temperature=0.6)\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"Summarize the youtube video whose transcript is provided within backticks \\\n",
    "            ```{text}```\n",
    "            \"\"\", input_variables=[\"text\"]\n",
    "        )\n",
    "        combine_prompt = PromptTemplate(\n",
    "            template=\"\"\"Combine all the youtube video transcripts  provided within backticks \\\n",
    "            ```{text}```\n",
    "            Provide a concise summary between 8 to 10 sentences.\n",
    "            \"\"\", input_variables=[\"text\"]\n",
    "        )\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=20000, chunk_overlap=50)\n",
    "        text = text_splitter.split_documents(transcript)\n",
    "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True,\n",
    "                                     map_prompt=prompt, combine_prompt=combine_prompt)\n",
    "        answer = chain.run(text)\n",
    "    except Exception as e:\n",
    "        return f\"Error while processing and summarizing text: {e}\"\n",
    "\n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "youtube_video_url = \"https://www.youtube.com/watch?v=-hxeDjAxvJ8\"\n",
    "if not youtube_video_url_is_valid(youtube_video_url):\n",
    "    print(\"Please enter a valid youtube video URL.\")\n",
    "\n",
    "answer = find_insights(youtube_video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marc Andreessen, the co-founder of Netscape and VC firm Andreessen Horowitz,\n",
      "discusses the future of the internet, search, and AI. He believes that LLMs will\n",
      "eventually replace search engines and that the majority of content on the\n",
      "internet will be human conversations with AIs. He also discusses the trillion\n",
      "dollar question of whether or not synthetic training data is useful for LLMs.\n",
      "Andreessen also talks about the history of the internet, his experience founding\n",
      "Netscape, and the risks of artificial intelligence. He believes that\n",
      "satisfaction is a deeper thing than happiness, and that it comes from fulfilling\n",
      "your purpose and being useful to others.\n",
      "---------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply word wrapping and print the wrapped text\n",
    "wrapped_text = textwrap.fill(answer, width=width)\n",
    "print(wrapped_text)\n",
    "# print(answer)\n",
    "print(\"---------------------------------\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vertexai-sdk-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
